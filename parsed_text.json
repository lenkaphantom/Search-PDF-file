["", "", "Data Structures and\nAlgorithms in Python\nMichael T. Goodrich\nDepartment of Computer Science\nUniversity of California, Irvine\nRoberto Tamassia\nDepartment of Computer Science\nBrown University\nMichael H. Goldwasser\nDepartment of Mathematics and Computer Science\nSaint Louis University\n", "VP & PUBLISHER Don Fowley\nEXECUTIVE EDITOR Beth Lang Golub\nEDITORIAL PROGRAM ASSISTANT Katherine WillisMARKETING MANAGER Christopher RuelDESIGNER Kenji NgiengSENIOR PRODUCTION MANAGER Janis SooASSOCIATE PRODUCTION MANAGER Joyce Poh\nThis book was set in L aTEX by the authors. Printed and bound by Courier Westford.\nThe cover was printed by Courier Westford.\nThis book is printed on acid free paper. Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for \nmore than 200 years, helping people around the world meet their needs and ful\ufb01  ll their aspirations. Our company is built on a foundation of principles that include responsibility to the communities we serve and where we live and work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental, social, economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon impact, paper speci\ufb01  cations and procurement, ethical conduct within our business and among our vendors, and community and charitable support. For more information, please visit our website: www.wiley.com/go/citizenship. \nCopyright \u00a9 2013 John Wiley & Sons, Inc. All rights reserved. No part of this publication may be \nreproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, Inc. 222 Rosewood Drive, Danvers, MA 01923, website www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030-5774, (201)748-6011, fax (201)748-6008, website http://www.wiley.com/go/permissions.\n Evaluation copies are provided to quali\ufb01  ed academics and professionals for review purposes only, for use \nin their courses during the next academic year.  These copies are licensed and may not be sold or transferred to a third party. Upon completion of the review period, please return the evaluation copy to Wiley.  Return instructions and a free of charge return mailing label are available at www.wiley.com/go/returnlabel.  If you have chosen to adopt this textbook for use in your course, please accept this book as your complimentary desk copy. Outside of the United States, please contact your local sales representative. \nPrinted in the United States of America10 9 8 7 6 5 4 3 2 1", "To Karen, Paul, Anna, and Jack\n\u2013Michael T. Goodrich\nTo Isabel\n\u2013Roberto Tamassia\nTo Susan, Calista, and Maya\n\u2013Michael H. Goldwasser", "", "Preface\nThe design and analysis of ef\ufb01cient data structures has long been recognized as a\nvital subject in computing and is part of the core curriculum of computer scienceand computer engineering undergraduate degrees. Data Structures and Algorithms\nin Python provides an introduction to data structures and algorithms, including their\ndesign, analysis, and implementation. This book is designed for use in a beginning-level data structures course, or in an intermediate-level introduction to algorithmscourse. We discuss its use for such courses in more detail later in this preface.\nTo promote the development of robust and reusable software, we have tried to\ntake a consistent object-oriented viewpoint throughout this text. One of the mainideas of the object-oriented approach is that data should be presented as being en-capsulated with the methods that access and modify them. That is, rather thansimply viewing data as a collection of bytes and addresses, we think of data ob-\njects as instances of an abstract data type (ADT ), which includes a repertoire of\nmethods for performing operations on data objects of this type. We then empha-\nsize that there may be several different implementation strategies for a particularADT, and explore the relative pros and cons of these choices. We provide complete\nPython implementations for almost all data structures and algorithms discussed,\nand we introduce important object-oriented design patterns as means to organize\nthose implementations into reusable components.\nDesired outcomes for readers of our book include that:\n\u2022They have knowledge of the most common abstractions for data collections\n(e.g., stacks, queues, lists, trees, maps).\n\u2022They understand algorithmic strategies for producing ef\ufb01cient realizations ofcommon data structures.\n\u2022They can analyze algorithmic performance, both theoretically and experi-mentally, and recognize common trade-offs between competing strategies.\n\u2022They can wisely use existing data structures and algorithms found in modern\nprogramming language libraries.\n\u2022They have experience working with concrete implementations for most foun-\ndational data structures and algorithms.\n\u2022They can apply data structures and algorithms to solve complex problems.\nIn support of the last goal, we present many example applications of data structuresthroughout the book, including the processing of \ufb01le systems, matching of tags\nin structured formats such as HTML, simple cryptography, text frequency analy-\nsis, automated geometric layout, Huffman coding, DNA sequence alignment, andsearch engine indexing.\nv", "vi Preface\nBook Features\nThis book is based upon the book Data Structures and Algorithms in Java by\nGoodrich and Tamassia, and the related Data Structures and Algorithms in C++\nby Goodrich, Tamassia, and Mount. However, this book is not simply a translation\nof those other books to Python. In adapting the material for this book, we have\nsigni\ufb01cantly redesigned the organization and content of the book as follows:\n\u2022The code base has been entirely redesigned to take advantage of the features\nof Python, such as use of generators for iterating elements of a collection.\n\u2022Many algorithms that were presented as pseudo-code in the Java and C++\nversions are directly presented as complete Python code.\n\u2022In general, ADTs are de\ufb01ned to have consistent interface with Python\u2019s built-\nin data types and those in Python\u2019s collections module.\n\u2022Chapter 5 provides an in-depth exploration of the dynamic array-based un-\nderpinnings of Python\u2019s built-in list,tuple ,a n dstrclasses. New Appendix A\nserves as an additional reference regarding the functionality of the strclass.\n\u2022Over 450 illustrations have been created or revised.\n\u2022New and revised exercises bring the overall total number to 750.\nOnline Resources\nThis book is accompanied by an extensive set of online resources, which can be\nfound at the following Web site:\nwww.wiley.com/college/goodrich\nStudents are encouraged to use this site along with the book, to help with exer-\ncises and increase understanding of the subject. Instructors are likewise welcome\nto use the site to help plan, organize, and present their course materials. Included\non this Web site is a collection of educational aids that augment the topics of this\nbook, for both students and instructors. Because of their added value, some of theseonline resources are password protected.\nFor all readers, and especially for students, we include the following resources:\n\u2022All the Python source code presented in this book.\n\u2022PDF handouts of Powerpoint slides (four-per-page) provided to instructors.\n\u2022A database of hints to allexercises, indexed by problem number.\nFor instructors using this book, we include the following additional teaching aids:\n\u2022Solutions to hundreds of the book\u2019s exercises.\n\u2022Color versions of all \ufb01gures and illustrations from the book.\n\u2022Slides in Powerpoint and PDF (one-per-page) format.\nThe slides are fully editable, so as to allow an instructor using this book full free-dom in customizing his or her presentations. All the online resources are providedat no extra charge to any instructor adopting this book for his or her course.", "Preface vii\nContents and Organization\nThe chapters for this book are organized to provide a pedagogical path that starts\nwith the basics of Python programming and object-oriented design. We then add\nfoundational techniques like algorithm analysis and recursion. In the main portionof the book, we present fundamental data structures and algorithms, concludingwith a discussion of memory management (that is, the architectural underpinningsof data structures). Speci\ufb01cally, the chapters for this book are organized as follows:\n1.Python Primer\n2.Object-Oriented Programming\n3.Algorithm Analysis\n4.Recursion\n5.Array-Based Sequences\n6.Stacks, Queues, and Deques\n7.Linked Lists\n8.Trees\n9.Priority Queues\n10.Maps, Hash Tables, and Skip Lists\n11.Search Trees\n12.Sorting and Selection\n13.Text Processing\n14.Graph Algorithms\n15.Memory Management and B-Trees\nA.Character Strings in Python\nB.Useful Mathematical Facts\nA more detailed table of contents follows this preface, beginning on page xi.\nPrerequisites\nWe assume that the reader is at least vaguely familiar with a high-level program-ming language, such as C, C++, Python, or Java, and that he or she understands themain constructs from such a high-level language, including:\n\u2022Variables and expressions.\n\u2022Decision structures (such as if-statements and switch-statements).\n\u2022Iteration structures (for loops and while loops).\n\u2022Functions (whether stand-alone or object-oriented methods).\nFor readers who are familiar with these concepts, but not with how they are ex-pressed in Python, we provide a primer on the Python language in Chapter 1. Still,this book is primarily a data structures book, not a Python book; hence, it does not\ngive a comprehensive treatment of Python.", "viii Preface\nWe delay treatment of object-oriented programming in Python until Chapter 2.\nThis chapter is useful for those new to Python, and for those who may be familiar\nwith Python, yet not with object-oriented programming.\nIn terms of mathematical background, we assume the reader is somewhat famil-\niar with topics from high-school mathematics. Even so, in Chapter 3, we discussthe seven most-important functions for algorithm analysis. In fact, sections that use\nsomething other than one of these seven functions are considered optional, and are\nindicated with a star (\u22c6). We give a summary of other useful mathematical facts,\nincluding elementary probability, in Appendix B.\nRelation to Computer Science Curriculum\nTo assist instructors in designing a course in the context of the IEEE/ACM 2013\nComputing Curriculum, the following table describes curricular knowledge unitsthat are covered within this book.\nKnowledge Unit\n Relevant Material\nAL/Basic Analysis\n Chapter 3 and Sections 4.2 & 12.2.4\nAL/Algorithmic Strategies\n Sections 12.2.1, 13.2.1, 13.3, & 13.4.2\nAL/Fundamental Data Structures\nand Algorithms\nSections 4.1.3, 5.5.2, 9.4.1, 9.3, 10.2, 11.1, 13.2,Chapter 12 & much of Chapter 14\nAL/Advanced Data Structures\nSections 5.3, 10.4, 11.2 through 11.6, 12.3.1,\n13.5, 14.5.1, & 15.3\nAR/Memory System Organizationand Architecture\n Chapter 15\nDS/Sets, Relations and Functions\n Sections 10.5.1, 10.5.2, & 9.4\nDS/Proof Techniques\n Sections 3.4, 4.2, 5.3.2, 9.3.6, & 12.4.1\nDS/Basics of Counting\n Sections 2.4.2, 6.2.2, 12.2.4, 8.2.2 & Appendix B\nDS/Graphs and Trees\n Much of Chapters 8 and 14\nDS/Discrete Probability\n Sections 1.11.1, 10.2, 10.4.2, & 12.3.1\nPL/Object-Oriented Programming\nMuch of the book, yet especially Chapter 2 and\nSections 7.4, 9.5.1, 10.1.3, & 11.2.1\nPL/Functional Programming\n Section 1.10\nSDF/Algorithms and Design\n Sections 2.1, 3.3, & 12.2.1\nSDF/Fundamental Programming\nConcepts\nChapters 1 & 4\nSDF/Fundamental Data Structures\nChapters 6 & 7, Appendix A, and Sections 1.2.1,\n5.2, 5.4, 9.1, & 10.1\nSDF/Developmental Methods\n Sections 1.7 & 2.2\nSE/Software Design\n Sections 2.1 & 2.1.3\nMapping IEEE/ACM 2013 Computing Curriculum knowledge units to coverage in\nthis book.", "Preface ix\nAbout the Authors\nMichael Goodrich received his Ph.D. in Computer Science from Purdue University\nin 1987. He is currently a Chancellor\u2019s Professor in the Department of ComputerScience at University of California, Irvine. Previously, he was a professor at Johns\nHopkins University. He is a Fulbright Scholar and a Fellow of the American As-\nsociation for the Advancement of Science (AAAS), Association for ComputingMachinery (ACM), and Institute of Electrical and Electronics Engineers (IEEE).He is a recipient of the IEEE Computer Society Technical Achievement Award,\nthe ACM Recognition of Service Award, and the Pond Award for Excellence in\nUndergraduate Teaching.\nRoberto Tamassia received his Ph.D. in Electrical and Computer Engineering\nfrom the University of Illinois at Urbana-Champaign in 1988. He is the Plastech\nProfessor of Computer Science and the Chair of the Department of Computer Sci-\nence at Brown University. He is also the Director of Brown\u2019s Center for Geometric\nComputing. His research interests include information security, cryptography, anal-ysis, design, and implementation of algorithms, graph drawing and computationalgeometry. He is a Fellow of the American Association for the Advancement of\nScience (AAAS), Association for Computing Machinery (ACM) and Institute for\nElectrical and Electronic Engineers (IEEE). He is also a recipient of the TechnicalAchievement Award from the IEEE Computer Society.\nMichael Goldwasser received his Ph.D. in Computer Science from Stanford\nUniversity in 1997. He is currently a Professor in the Department of Mathematics\nand Computer Science at Saint Louis University and the Director of their Com-\nputer Science program. Previously, he was a faculty member in the Departmentof Computer Science at Loyola University Chicago. His research interests focuson the design and implementation of algorithms, having published work involving\napproximation algorithms, online computation, computational biology, and compu-\ntational geometry. He is also active in the computer science education community.\nAdditional Books by These Authors\n\u2022M.T. Goodrich and R. Tamassia, Data Structures and Algorithms in Java , Wiley.\n\u2022M.T. Goodrich, R. Tamassia, and D.M. Mount, Data Structures and Algorithms\nin C++ , Wiley.\n\u2022M.T. Goodrich and R. Tamassia, Algorithm Design: Foundations, Analysis, and\nInternet Examples , Wiley.\n\u2022M.T. Goodrich and R. Tamassia, Introduction to Computer Security , Addison-\nWesley.\n\u2022M.H. Goldwasser and D. Letscher, Object-Oriented Programming in Python ,\nPrentice Hall.", "x Preface\nAcknowledgments\nWe have depended greatly upon the contributions of many individuals as part of\nthe development of this book. We begin by acknowledging the wonderful team atWiley. We are grateful to our editor, Beth Golub, for her enthusiastic support of\nthis project, from beginning to end. The efforts of Elizabeth Mills and Katherine\nWillis were critical in keeping the project moving, from its early stages as an initialproposal, through the extensive peer review process. We greatly appreciate theattention to detail demonstrated by Julie Kennedy, the copyeditor for this book.\nFinally, many thanks are due to Joyce Poh for managing the \ufb01nal months of the\nproduction process.\nWe are truly indebted to the outside reviewers and readers for their copious\ncomments, emails, and constructive criticism, which were extremely useful in writ-\ning this edition. We therefore thank the following reviewers for their comments and\nsuggestions: Claude Anderson (Rose Hulman Institute of Technology), Alistair\nCampbell (Hamilton College), Barry Cohen (New Jersey Institute of Technology),Robert Franks (Central College), Andrew Harrington (Loyola University Chicago),Dave Musicant (Carleton College), and Victor Norman (Calvin College). We wish\nto particularly acknowledge Claude for going above and beyond the call of duty,\nproviding us with an enumeration of 400 detailed corrections or suggestions.\nWe thank David Mount, of University of Maryland, for graciously sharing the\nwisdom gained from his experience with the C++ version of this text. We are grate-ful to Erin Chambers and David Letscher, of Saint Louis University, for their intan-\ngible contributions during many hallway conversations about the teaching of data\nstructures, and to David for comments on early versions of the Python code base forthis book. We thank David Zampino, a student at Loyola University Chicago, forhis feedback while using a draft of this book during an independent study course,\nand to Andrew Harrington for supervising David\u2019s studies.\nWe also wish to reiterate our thanks to the many research collaborators and\nteaching assistants whose feedback shaped the previous Java and C++ versions of\nthis material. The bene\ufb01ts of those contributions carry forward to this book.\nFinally, we would like to warmly thank Susan Goldwasser, Isabel Cruz, Karen\nGoodrich, Giuseppe Di Battista, Franco Preparata, Ioannis Tollis, and our parents\nfor providing advice, encouragement, and support at various stages of the prepa-\nration of this book, and Calista and Maya Goldwasser for offering their adviceregarding the artistic merits of many illustrations. More importantly, we thank allof these people for reminding us that there are things in life beyond writing books.\nMichael T. Goodrich\nRoberto Tamassia\nMichael H. Goldwasser", "Contents\nPreface ................................. v\n1P y t h o n P r i m e r 1\n1.1 Python Overview ......................... 2\n1 . 1 . 1 T h e P y t h o n I n t e r p r e t e r .................. 2\n1 . 1 . 2 P r e v i e wo fa P y t h o n P r o g r a m .............. 3\n1.2 Objects in Python ........................ 4\n1.2.1 Identi\ufb01ers, Objects, and the Assignment Statement . . . 4\n1 . 2 . 2 C r e a t i n g a n d U s i n g O b j e c t s................ 6\n1 . 2 . 3 P y t h o n \u2019 sB u i l t - I n C l a s s e s ................. 7\n1.3 Expressions, Operators, and Precedence ........... 12\n1.3.1 Compound Expressions and Operator Precedence . . . . 17\n1.4 Control Flow ........................... 18\n1 . 4 . 1 C o n d i t i o n a l s........................ 18\n1 . 4 . 2 L o o p s ........................... 20\n1.5 Functions ............................. 23\n1 . 5 . 1 I n f o r m a t i o nP a s s i n g.................... 24\n1 . 5 . 2 P y t h o n \u2019 sB u i l t - I n F u n c t i o n s................ 28\n1.6 Simple Input and Output .................... 30\n1.6.1 Console Input and Output . . . . ............ 30\n1 . 6 . 2 F i l e s ............................ 31\n1.7 Exception Handling ....................... 33\n1 . 7 . 1 R a i s i n g a n E x c e p t i o n ................... 34\n1 . 7 . 2 C a t c h i n g a n E x c e p t i o n .................. 36\n1.8 Iterators and Generators .................... 39\n1.9 Additional Python Conveniences ................ 42\n1 . 9 . 1 C o n d i t i o n a l E x p r e s s i o n s.................. 42\n1 . 9 . 2 C o m p r e h e n s i o nS y n t a x .................. 43\n1 . 9 . 3 P a c k i n ga n d U n p a c k i n g o fS e q u e n c e s .......... 44\n1.10 Scopes and Namespaces .................... 46\n1.11 Modules and the Import Statement .............. 48\n1 . 1 1 . 1E x i s t i n g M o d u l e s ..................... 49\n1.12 Exercises ............................. 51\nxi", "xii Contents\n2 Object-Oriented Programming 56\n2.1 Goals, Principles, and Patterns ................ 57\n2 . 1 . 1 O b j e c t - O r i e n t e dD e s i g n G o a l s .............. 57\n2 . 1 . 2 O b j e c t - O r i e n t e dD e s i g n P r i n c i p l e s ............ 58\n2 . 1 . 3 D e s i g nP a t t e r n s...................... 61\n2.2 Software Development ..................... 62\n2 . 2 . 1 D e s i g n........................... 62\n2 . 2 . 2 P s e u d o - C o d e ....................... 64\n2 . 2 . 3 C o d i n g S t y l e a n d D o c u m e n t a t i o n............. 64\n2 . 2 . 4 T e s t i n g a n d D e b u g g i n g .................. 67\n2.3 Class De\ufb01nitions ......................... 69\n2 . 3 . 1 E x a m p l e :C r e d i t C a r dC l a s s ................ 69\n2.3.2 Operator Overloading and Python\u2019s Special Methods . . 74\n2 . 3 . 3 E x a m p l e :M u l t i d i m e n s i o n a l V e c t o rC l a s s......... 77\n2 . 3 . 4 I t e r a t o r s .......................... 79\n2 . 3 . 5 E x a m p l e :R a n g e C l a s s ................... 80\n2.4 Inheritance ............................ 82\n2 . 4 . 1 E x t e n d i n g t h e C r e d i t C a r d C l a s s.............. 83\n2 . 4 . 2 H i e r a r c h yo f N u m e r i cP r o g r e s s i o n s............ 87\n2 . 4 . 3 A b s t r a c t B a s e C l a s s e s................... 93\n2.5 Namespaces and Object-Orientation ............. 96\n2 . 5 . 1 I n s t a n c e a n d C l a s s N a m e s p a c e s.............. 96\n2 . 5 . 2 N a m e R e s o l u t i o na n d D y n a m i c D i s p a t c h......... 100\n2.6 Shallow and Deep Copying ................... 101\n2.7 Exercises ............................. 103\n3 Algorithm Analysis 109\n3.1 Experimental Studies ...................... 111\n3 . 1 . 1 M o v i n g B e y o n dE x p e r i m e n t a l A n a l y s i s.......... 113\n3.2 The Seven Functions Used in This Book ........... 115\n3 . 2 . 1 C o m p a r i n gG r o w t hR a t e s................. 122\n3.3 Asymptotic Analysis ....................... 123\n3 . 3 . 1 T h e \u201c B i g - O h \u201d N o t a t i o n.................. 123\n3 . 3 . 2 C o m p a r a t i v e A n a l y s i s................... 128\n3 . 3 . 3 E x a m p l e so f A l g o r i t h mA n a l y s i s ............. 130\n3.4 Simple Justi\ufb01cation Techniques ................ 137\n3 . 4 . 1 B y E x a m p l e ........................ 137\n3 . 4 . 2 T h e \u201c C o n t r a \u201d A t t a c k................... 137\n3.4.3 Induction and Loop Invariants . . ............ 138\n3.5 Exercises ............................. 141", "Contents xiii\n4 Recursion 148\n4.1 Illustrative Examples ...................... 150\n4 . 1 . 1 T h e F a c t o r i a l F u n c t i o n .................. 150\n4 . 1 . 2 D r a w i n g a n E n g l i s h R u l e r................. 152\n4 . 1 . 3 B i n a r y S e a r c h ....................... 155\n4 . 1 . 4 F i l e S y s t e m s........................ 157\n4.2 Analyzing Recursive Algorithms ................ 161\n4.3 Recursion Run Amok ...................... 165\n4 . 3 . 1 M a x i m u m R e c u r s i v e D e p t h i n P y t h o n .......... 168\n4.4 Further Examples of Recursion ................. 169\n4 . 4 . 1 L i n e a rR e c u r s i o n...................... 169\n4 . 4 . 2 B i n a r y R e c u r s i o n ..................... 174\n4 . 4 . 3 M u l t i p l e R e c u r s i o n .................... 175\n4.5 Designing Recursive Algorithms ................ 177\n4.6 Eliminating Tail Recursion ................... 178\n4.7 Exercises ............................. 180\n5 Array-Based Sequences 183\n5.1 Python\u2019s Sequence Types .................... 184\n5.2 Low-Level Arrays ......................... 185\n5 . 2 . 1 R e f e r e n t i a l A r r a y s..................... 187\n5 . 2 . 2 C o m p a c tA r r a y s i n P y t h o n ................ 190\n5.3 Dynamic Arrays and Amortization ............... 192\n5 . 3 . 1 I m p l e m e n t i n ga D y n a m i c A r r a y.............. 195\n5 . 3 . 2 A m o r t i z e dA n a l y s i s o f D y n a m i c A r r a y s.......... 197\n5 . 3 . 3 P y t h o n \u2019 sL i s t C l a s s .................... 201\n5.4 E\ufb03ciency of Python\u2019s Sequence Types ............ 202\n5 . 4 . 1 P y t h o n \u2019 sL i s t a n d T u p l e C l a s s e s ............. 202\n5 . 4 . 2 P y t h o n \u2019 sS t r i n g C l a s s................... 208\n5.5 Using Array-Based Sequences ................. 210\n5 . 5 . 1 S t o r i n g H i g hS c o r e sf o ra G a m e ............. 210\n5 . 5 . 2 S o r t i n g a S e q u e n c e .................... 214\n5 . 5 . 3 S i m p l e C r y p t o g r a p h y ................... 216\n5.6 Multidimensional Data Sets .................. 219\n5.7 Exercises ............................. 224\n6 Stacks, Queues, and Deques 228\n6.1 Stacks ............................... 229\n6 . 1 . 1 T h e S t a c k A b s t r a c t D a t a T y p e.............. 230\n6 . 1 . 2 S i m p l e A r r a y - B a s e d S t a c k I m p l e m e n t a t i o n........ 231\n6 . 1 . 3 R e v e r s i n g D a t a U s i n g a S t a c k .............. 235\n6 . 1 . 4 M a t c h i n g P a r e n t h e s e sa n d H T M L T a g s ......... 236", "xiv Contents\n6.2 Queues .............................. 239\n6 . 2 . 1 T h e Q u e u e A b s t r a c t D a t a T y p e ............. 240\n6 . 2 . 2 A r r a y - B a s e d Q u e u e I m p l e m e n t a t i o n ........... 241\n6.3 Double-Ended Queues ...................... 247\n6 . 3 . 1 T h e D e q u e A b s t r a c t D a t a T y p e ............. 247\n6 . 3 . 2 I m p l e m e n t i n ga D e q u e w i t h a C i r c u l a rA r r a y....... 248\n6 . 3 . 3 D e q u e si n t h e P y t h o nC o l l e c t i o n sM o d u l e ........ 249\n6.4 Exercises ............................. 250\n7 Linked Lists 255\n7.1 Singly Linked Lists ........................ 256\n7 . 1 . 1 I m p l e m e n t i n ga S t a c k w i t h a S i n g l y L i n k e dL i s t..... 261\n7 . 1 . 2 I m p l e m e n t i n ga Q u e u e w i t h a S i n g l y L i n k e dL i s t ..... 264\n7.2 Circularly Linked Lists ...................... 266\n7.2.1 Round-Robin Schedulers . . . . . ............ 267\n7.2.2 Implementing a Queue with a Circularly Linked List . . . 268\n7.3 Doubly Linked Lists ....................... 270\n7 . 3 . 1 B a s i c I m p l e m e n t a t i o no f a D o u b l y L i n k e dL i s t...... 273\n7.3.2 Implementing a Deque with a Doubly Linked List . . . . 275\n7.4 The Positional List ADT .................... 277\n7 . 4 . 1 T h e P o s i t i o n a l L i s t A b s t r a c t D a t a T y p e ......... 279\n7 . 4 . 2 D o u b l y L i n k e dL i s t I m p l e m e n t a t i o n............ 281\n7.5 Sorting a Positional List .................... 285\n7.6 Case Study: Maintaining Access Frequencies ........ 286\n7 . 6 . 1 U s i n g a S o r t e dL i s t .................... 286\n7 . 6 . 2 U s i n g a L i s t w i t h t h e M o v e - t o - F r o n tH e u r i s t i c...... 289\n7.7 Link-Based vs. Array-Based Sequences ............ 292\n7.8 Exercises ............................. 294\n8 Trees 299\n8.1 General Trees ........................... 300\n8 . 1 . 1 T r e eD e \ufb01 n i t i o n s a n d P r o p e r t i e s.............. 301\n8 . 1 . 2 T h e T r e e A b s t r a c t D a t a T y p e .............. 305\n8 . 1 . 3 C o m p u t i n g D e p t h a n d H e i g h t ............... 308\n8.2 Binary Trees ........................... 311\n8 . 2 . 1 T h e B i n a r y T r e e A b s t r a c t D a t a T y p e ........... 313\n8 . 2 . 2 P r o p e r t i e so fB i n a r y T r e e s ................ 315\n8.3 Implementing Trees ....................... 317\n8 . 3 . 1 L i n k e dS t r u c t u r e f o rB i n a r y T r e e s ............. 317\n8 . 3 . 2 A r r a y - B a s e d R e p r e s e n t a t i o no fa B i n a r y T r e e ...... 325\n8 . 3 . 3 L i n k e dS t r u c t u r e f o rG e n e r a l T r e e s............ 327\n8.4 Tree Traversal Algorithms ................... 328", "Contents xv\n8.4.1 Preorder and Postorder Traversals of General Trees . . . 328\n8 . 4 . 2 B r e a d t h - F i r s t T r e e T r a v e r s a l ............... 330\n8 . 4 . 3 I n o r d e rT r a v e r s a l o fa B i n a r y T r e e ............ 331\n8 . 4 . 4 I m p l e m e n t i n gT r e e T r a v e r s a l s i n P y t h o n......... 333\n8 . 4 . 5 A p p l i c a t i o n s o f T r e eT r a v e r s a l s.............. 337\n8.4.6 Euler Tours and the Template Method Pattern \u22c6.... 341\n8.5 Case Study: An Expression Tree ................ 348\n8.6 Exercises ............................. 352\n9 Priority Queues 362\n9.1 The Priority Queue Abstract Data Type ........... 363\n9 . 1 . 1 P r i o r i t i e s.......................... 363\n9 . 1 . 2 T h e P r i o r i t yQ u e u e A D T ................. 364\n9.2 Implementing a Priority Queue ................ 365\n9 . 2 . 1 T h e C o m p o s i t i o n D e s i g nP a t t e r n............. 365\n9 . 2 . 2 I m p l e m e n t a t i o nw i t h a n U n s o r t e dL i s t .......... 366\n9 . 2 . 3 I m p l e m e n t a t i o nw i t h a S o r t e dL i s t ............ 368\n9.3 Heaps ............................... 370\n9 . 3 . 1 T h e H e a pD a t a S t r u c t u r e................. 370\n9 . 3 . 2 I m p l e m e n t i n ga P r i o r i t yQ u e u e w i t h a H e a p....... 372\n9.3.3 Array-Based Representation of a Complete Binary Tree . 376\n9 . 3 . 4 P y t h o nH e a pI m p l e m e n t a t i o n............... 376\n9 . 3 . 5 A n a l y s i s o fa H e a p - B a s e d P r i o r i t yQ u e u e......... 379\n9.3.6 Bottom-Up Heap Construction \u22c6............. 380\n9 . 3 . 7 P y t h o n \u2019 sh e a p q M o d u l e.................. 384\n9.4 Sorting with a Priority Queue ................. 385\n9 . 4 . 1 S e l e c t i o n - S o r ta n d I n s e r t i o n - S o r t............. 386\n9 . 4 . 2 H e a p - S o r t ......................... 388\n9.5 Adaptable Priority Queues ................... 390\n9 . 5 . 1 L o c a t o r s.......................... 390\n9 . 5 . 2 I m p l e m e n t i n ga n A d a p t a b l e P r i o r i t yQ u e u e ....... 391\n9.6 Exercises ............................. 395\n10 Maps, Hash Tables, and Skip Lists 401\n10.1 Maps and Dictionaries ..................... 402\n1 0 . 1 . 1T h e M a p A D T ...................... 403\n1 0 . 1 . 2A p p l i c a t i o n :C o u n t i n g W o r dF r e q u e n c i e s......... 405\n1 0 . 1 . 3P y t h o n \u2019 sM u t a b l e M a p p i n g A b s t r a c t B a s e C l a s s ..... 406\n1 0 . 1 . 4O u rM a p B a s e C l a s s.................... 407\n1 0 . 1 . 5S i m p l e U n s o r t e dM a p I m p l e m e n t a t i o n .......... 408\n10.2 Hash Tables ........................... 410\n1 0 . 2 . 1H a s hF u n c t i o n s ...................... 411", "xvi Contents\n10.2.2 Collision-Handling Schemes . . . . ............ 417\n1 0 . 2 . 3L o a dF a c t o r s ,R e h a s h i n g , a n d E \ufb03 c i e n c y ......... 420\n1 0 . 2 . 4P y t h o nH a s h T a b l e I m p l e m e n t a t i o n ........... 422\n10.3 Sorted Maps ........................... 427\n1 0 . 3 . 1S o r t e dS e a r c hT a b l e s ................... 428\n1 0 . 3 . 2T w oA p p l i c a t i o n s o f S o r t e dM a p s ............ 434\n10.4 Skip Lists ............................. 437\n1 0 . 4 . 1S e a r c ha n d U p d a t e O p e r a t i o n s i n a S k i p L i s t ...... 439\n10.4.2 Probabilistic Analysis of Skip Lists \u22c6........... 443\n10.5 Sets, Multisets, and Multimaps ................ 446\n1 0 . 5 . 1T h e S e t A D T ....................... 446\n1 0 . 5 . 2P y t h o n \u2019 sM u t a b l e S e t A b s t r a c t B a s e C l a s s ........ 448\n1 0 . 5 . 3I m p l e m e n t i n gS e t s , M u l t i s e t s , a n d M u l t i m a p s ...... 450\n10.6 Exercises ............................. 452\n11 Search Trees 459\n11.1 Binary Search Trees ....................... 460\n1 1 . 1 . 1N a v i g a t i n g a B i n a r y S e a r c hT r e e ............. 461\n1 1 . 1 . 2S e a r c h e s.......................... 463\n1 1 . 1 . 3I n s e r t i o n sa n d D e l e t i o n s.................. 465\n1 1 . 1 . 4P y t h o nI m p l e m e n t a t i o n.................. 468\n1 1 . 1 . 5P e r f o r m a n c eo fa B i n a r y S e a r c hT r e e........... 473\n11.2 Balanced Search Trees ..................... 475\n1 1 . 2 . 1P y t h o nF r a m e w o r kf o rB a l a n c i n g S e a r c hT r e e s ...... 478\n11.3 AVL Trees ............................. 481\n1 1 . 3 . 1U p d a t e O p e r a t i o n s .................... 483\n1 1 . 3 . 2P y t h o nI m p l e m e n t a t i o n.................. 488\n11.4 Splay Trees ............................ 490\n1 1 . 4 . 1S p l a y i n g.......................... 490\n1 1 . 4 . 2W h e nt o S p l a y ....................... 494\n1 1 . 4 . 3P y t h o nI m p l e m e n t a t i o n.................. 496\n11.4.4 Amortized Analysis of Splaying \u22c6............ 497\n11.5 (2,4) Trees ............................ 502\n1 1 . 5 . 1M u l t i w a y S e a r c hT r e e s .................. 502\n1 1 . 5 . 2( 2 , 4 ) - T r e eO p e r a t i o n s................... 505\n11.6 Red-Black Trees ......................... 512\n1 1 . 6 . 1R e d - B l a c k T r e e O p e r a t i o n s................ 514\n1 1 . 6 . 2P y t h o nI m p l e m e n t a t i o n.................. 525\n11.7 Exercises ............................. 528", "Contents xvii\n12 Sorting and Selection 536\n12.1 Why Study Sorting Algorithms? ................ 537\n12.2 Merge-Sort ............................ 538\n12.2.1 Divide-and-Conquer . . . . . . . . ............ 538\n1 2 . 2 . 2A r r a y - B a s e d I m p l e m e n t a t i o no fM e r g e - S o r t ....... 543\n12.2.3 The Running Time of Merge-Sort ............ 544\n12.2.4 Merge-Sort and Recurrence Equations \u22c6......... 546\n1 2 . 2 . 5A l t e r n a t i v e I m p l e m e n t a t i o n so f M e r g e - S o r t ....... 547\n12.3 Quick-Sort ............................ 550\n1 2 . 3 . 1R a n d o m i z e d Q u i c k - S o r t.................. 557\n1 2 . 3 . 2A d d i t i o n a l O p t i m i z a t i o n s f o rQ u i c k - S o r t ......... 559\n12.4 Studying Sorting through an Algorithmic Lens ....... 562\n1 2 . 4 . 1L o w e rB o u n d f o rS o r t i n g ................. 562\n12.4.2 Linear-Time Sorting: Bucket-Sort and Radix-Sort . . . . 564\n12.5 Comparing Sorting Algorithms ................. 567\n12.6 Python\u2019s Built-In Sorting Functions .............. 569\n1 2 . 6 . 1S o r t i n g A c c o r d i n gt o a K e y F u n c t i o n........... 569\n12.7 Selection ............................. 571\n12.7.1 Prune-and-Search . . . . . . . . . ............ 571\n1 2 . 7 . 2R a n d o m i z e d Q u i c k - S e l e c t................. 572\n1 2 . 7 . 3A n a l y z i n g R a n d o m i z e d Q u i c k - S e l e c t ........... 573\n12.8 Exercises ............................. 574\n13 Text Processing 581\n13.1 Abundance of Digitized Text .................. 582\n1 3 . 1 . 1N o t a t i o n sf o rS t r i n g s a n d t h e P y t h o ns t r C l a s s ...... 583\n13.2 Pattern-Matching Algorithms ................. 584\n1 3 . 2 . 1B r u t e F o r c e ........................ 584\n1 3 . 2 . 2T h e B o y e r - M o o r eA l g o r i t h m ............... 586\n1 3 . 2 . 3T h e K n u t h - M o r r i s - P r a t t A l g o r i t h m............ 590\n13.3 Dynamic Programming ..................... 594\n1 3 . 3 . 1M a t r i x C h a i n - P r o d u c t................... 594\n1 3 . 3 . 2D N A a n d T e x tS e q u e n c e A l i g n m e n t ........... 597\n13.4 Text Compression and the Greedy Method ......... 601\n1 3 . 4 . 1T h e H u \ufb00 m a n C o d i n g A l g o r i t h m ............. 602\n1 3 . 4 . 2T h e G r e e d yM e t h o d.................... 603\n13.5 Tries ................................ 604\n1 3 . 5 . 1S t a n d a r d T r i e s....................... 604\n1 3 . 5 . 2C o m p r e s s e dT r i e s ..................... 608\n1 3 . 5 . 3S u \ufb03 x T r i e s ........................ 610\n1 3 . 5 . 4S e a r c hE n g i n e I n d e x i n g.................. 612", "xviii Contents\n13.6 Exercises ............................. 613\n14 Graph Algorithms 619\n14.1 Graphs ............................... 620\n1 4 . 1 . 1T h e G r a p h A D T ...................... 626\n14.2 Data Structures for Graphs ................... 627\n1 4 . 2 . 1E d g eL i s t S t r u c t u r e.................... 628\n1 4 . 2 . 2A d j a c e n c yL i s t S t r u c t u r e ................. 630\n1 4 . 2 . 3A d j a c e n c yM a p S t r u c t u r e................. 632\n1 4 . 2 . 4A d j a c e n c yM a t r i x S t r u c t u r e................ 633\n1 4 . 2 . 5P y t h o nI m p l e m e n t a t i o n.................. 634\n14.3 Graph Traversals ......................... 638\n1 4 . 3 . 1D e p t h - F i r s t S e a r c h .................... 639\n1 4 . 3 . 2D F S I m p l e m e n t a t i o na n d E x t e n s i o n s........... 644\n1 4 . 3 . 3B r e a d t h - F i r s t S e a r c h ................... 648\n14.4 Transitive Closure ........................ 651\n14.5 Directed Acyclic Graphs .................... 655\n1 4 . 5 . 1T o p o l o g i c a lO r d e r i n g ................... 655\n14.6 Shortest Paths .......................... 659\n1 4 . 6 . 1W e i g h t e dG r a p h s ..................... 659\n1 4 . 6 . 2D i j k s t r a \u2019 s A l g o r i t h m.................... 661\n14.7 Minimum Spanning Trees .................... 670\n14.7.1 Prim-Jarn \u00b4 \u0131kA l g o r i t h m .................. 672\n1 4 . 7 . 2K r u s k a l \u2019 sA l g o r i t h m.................... 676\n14.7.3 Disjoint Partitions and Union-Find Structures . . . . . . 681\n14.8 Exercises ............................. 686\n15 Memory Management and B-Trees 697\n15.1 Memory Management ...................... 698\n1 5 . 1 . 1M e m o r yA l l o c a t i o n .................... 699\n1 5 . 1 . 2G a r b a g e C o l l e c t i o n .................... 700\n15.1.3 Additional Memory Used by the Python Interpreter . . . 703\n15.2 Memory Hierarchies and Caching ............... 705\n1 5 . 2 . 1M e m o r yS y s t e m s ..................... 705\n1 5 . 2 . 2C a c h i n g S t r a t e g i e s .................... 706\n15.3 External Searching and B-Trees ................ 711\n15.3.1 ( a,b)T r e e s ......................... 712\n1 5 . 3 . 2B - T r e e s .......................... 714\n15.4 External-Memory Sorting .................... 715\n1 5 . 4 . 1M u l t i w a y M e r g i n g..................... 716\n15.5 Exercises ............................. 717", "Contents xix\nA Character Strings in Python 721\nB Useful Mathematical Facts 725\nBibliography 732\nIndex 737", "", "Chapter\n1Python Primer\nContents\n1 . 1 P y t h o nO v e r v i e w........................ 2\n1 . 1 . 1 T h e P y t h o n I n t e r p r e t e r ................... 2\n1 . 1 . 2 P r e v i e wo f aP y t h o n P r o g r a m ............... 3\n1 . 2 O b j e c t si nP y t h o n....................... 4\n1.2.1 Identi\ufb01ers, Objects, and the Assignment Statement . . . . 4\n1 . 2 . 2 C r e a t i n g a n d U s i n g O b j e c t s................. 6\n1 . 2 . 3 P y t h o n \u2019 s B u i l t - I n C l a s s e s .................. 7\n1 . 3 E x p r e s s i o n s ,O p e r a t o r s ,a n dP r e c e d e n c e........... 1 2\n1.3.1 Compound Expressions and Operator Precedence . . . . . 17\n1 . 4 C o n t r o lF l o w.......................... 1 8\n1 . 4 . 1 C o n d i t i o n a l s......................... 1 8\n1 . 4 . 2 L o o p s ............................ 2 0\n1 . 5 F u n c t i o n s............................ 2 3\n1 . 5 . 1 I n f o r m a t i o n P a s s i n g..................... 2 41 . 5 . 2 P y t h o n \u2019 s B u i l t - I n F u n c t i o n s................. 2 8\n1.6 Simple Input and Output . . . . ............... 3 0\n1.6.1 Console Input and Output . . . . . . . . . . . . . . . . . 301 . 6 . 2 F i l e s ............................. 3 1\n1 . 7 E x c e p t i o nH a n d l i n g ...................... 3 3\n1 . 7 . 1 R a i s i n g a n E x c e p t i o n .................... 3 41 . 7 . 2 C a t c h i n g a nE x c e p t i o n ................... 3 6\n1 . 8 I t e r a t o r sa n dG e n e r a t o r s ................... 3 9\n1 . 9 A d d i t i o n a lP y t h o nC o n v e n i e n c e s............... 4 2\n1 . 9 . 1 C o n d i t i o n a l E x p r e s s i o n s................... 4 2\n1 . 9 . 2 C o m p r e h e n s i o n S y n t a x ................... 4 3\n1.9.3 Packing and Unpacking of Sequences . . . . . . . . . . . 44\n1 . 1 0S c o p e sa n dN a m e s p a c e s ................... 4 61 . 1 1M o d u l e sa n dt h eI m p o r tS t a t e m e n t............. 4 8\n1 . 1 1 . 1E x i s t i n g M o d u l e s ...................... 4 9\n1 . 1 2E x e r c i s e s ............................ 5 1\n", "2 Chapter 1. Python Primer\n1.1 Python Overview\nBuilding data structures and algorithms requires that we communicate detailed in-\nstructions to a computer. An excellent way to perform such communications isusing a high-level computer language, such as Python. The Python programming\nlanguage was originally developed by Guido van Rossum in the early 1990s, and\nhas since become a prominently used language in industry and education. The sec-ond major version of the language, Python 2, was released in 2000, and the thirdmajor version, Python 3, released in 2008. We note that there are signi\ufb01cant in-compatibilities between Python 2 and Python 3. This book is based on Python 3\n(more speci\ufb01cally, Python 3.1 or later). The latest version of the language is freely\navailable at www.python.org, along with documentation and tutorials.\nIn this chapter, we provide an overview of the Python programming language,\nand we continue this discussion in the next chapter, focusing on object-orientedprinciples. We assume that readers of this book have prior programming experi-\nence, although not necessarily using Python. This book does not provide a com-\nplete description of the Python language (there are numerous language referencesfor that purpose), but it does introduce all aspects of the language that are used incode fragments later in this book.\n1.1.1 The Python Interpreter\nPython is formally an interpreted language. Commands are executed through a\npiece of software known as the Python interpreter . The interpreter receives a com-\nmand, evaluates that command, and reports the result of the command. While theinterpreter can be used interactively (especially when debugging), a programmertypically de\ufb01nes a series of commands in advance and saves those commands in aplain text \ufb01le known as source code or ascript . For Python, source code is conven-\ntionally stored in a \ufb01le named with the .pysuf\ufb01x (e.g., demo.py).\nOn most operating systems, the Python interpreter can be started by typing\npython from the command line. By default, the interpreter starts in interactive\nmode with a clean workspace. Commands from a prede\ufb01ned script saved in a\ufb01le (e.g., demo.py) are executed by invoking the interpreter with the \ufb01lename as\nan argument (e.g., python demo.py), or using an additional -i\ufb02 a gi no r d e rt o\nexecute a script and then enter interactive mode (e.g., python -i demo.py).\nMany integrated development environments (IDEs) provide richer software\ndevelopment platforms for Python, including one named IDLE that is includedwith the standard Python distribution. IDLE provides an embedded text-editor withsupport for displaying and editing Python code, and a basic debugger, allowing\nstep-by-step execution of a program while examining key variable values.", "1.1. Python Overview 3\n1.1.2 Preview of a Python Program\nAs a simple introduction, Code Fragment 1.1 presents a Python program that com-\nputes the grade-point average (GPA) for a student based on letter grades that areentered by a user. Many of the techniques demonstrated in this example will bediscussed in the remainder of this chapter. At this point, we draw attention to a fewhigh-level issues, for readers who are new to Python as a programming language.\nPython\u2019s syntax relies heavily on the use of whitespace. Individual statements\nare typically concluded with a newline character, although a command can extendto another line, either with a concluding backslash character ( \\), or if an opening\ndelimiter has not yet been closed, such as the {character in de\ufb01ning value\nmap.\nWhitespace is also key in delimiting the bodies of control structures in Python.\nSpeci\ufb01cally, a block of code is indented to designate it as the body of a controlstructure, and nested control structures use increasing amounts of indentation. InCode Fragment 1.1, the body of the while loop consists of the subsequent 8 lines,\nincluding a nested conditional structure.\nComments are annotations provided for human readers, yet ignored by the\nPython interpreter. The primary syntax for comments in Python is based on useof the #character, which designates the remainder of the line as a comment.\nprint(\nWelcome to the GPA calculator.\n )\nprint(\n Please enter all your letter grades, one per line.\n )\nprint(\n Enter a blank line to designate the end.\n )\n# map from letter grade to point value\npoints = {\nA+\n:4.0,\n A\n:4.0,\n A-\n:3.67,\n B+\n:3.33,\n B\n:3.0,\n B-\n:2.67,\nC+\n:2.33,\n C\n:2.0,\n C\n:1.67,\n D+\n:1.33,\n D\n:1.0,\n F\n:0.0}\nnum\ncourses = 0\ntotal\npoints = 0\ndone = False\nwhile not done:\ngrade = input( ) # read line from user\nifgrade ==\n : # empty line was entered\ndone = True\nelifgradenot in points: # unrecognized grade entered\nprint( \"Unknown grade\n {0}\n being ignored\" .format(grade))\nelse:\nnum\ncourses += 1\ntotal\npoints += points[grade]\nifnum\ncourses >0: # avoid division by zero\nprint(\n Your GPA is {0:.3}\n .format(total\n points / num\n courses))\nCode Fragment 1.1: A Python program that computes a grade-point average (GPA).", "4 Chapter 1. Python Primer\n1.2 Objects in Python\nPython is an object-oriented language and classes form the basis for all data types.\nIn this section, we describe key aspects of Python\u2019s object model, and we intro-\nduce Python\u2019s built-in classes, such as the intclass for integers, the \ufb02oat class\nfor \ufb02oating-point values, and the strclass for character strings. A more thorough\npresentation of object-orientation is the focus of Chapter 2.\n1.2.1 Identi\ufb01ers, Objects, and the Assignment Statement\nThe most important of all Python commands is an assignment statement ,s u c ha s\ntemperature = 98.6\nThis command establishes temperature as an identi\ufb01er (also known as a name ),\nand then associates it with the object expressed on the right-hand side of the equal\nsign, in this case a \ufb02oating-point object with value 98.6. We portray the outcome\nof this assignment in Figure 1.1.\n\ufb02oat\n98.6temperature\nFigure 1.1: The identi\ufb01er temperature references an instance of the \ufb02oat class\nhaving value 98.6.\nIdenti\ufb01ers\nIdenti\ufb01ers in Python are case-sensitive ,s otemperature andTemperature are dis-\ntinct names. Identi\ufb01ers can be composed of almost any combination of letters,\nnumerals, and underscore characters (or more general Unicode characters). Theprimary restrictions are that an identi\ufb01er cannot begin with a numeral (thus 9lives\nis an illegal name), and that there are 33 specially reserved words that cannot beused as identi\ufb01ers, as shown in Table 1.1.\nReserved Words\nFalse as continue else from in not return yield\nNone assert def except global is or try\nTrue break del \ufb01nally if lambda pass while\nand class elif for import nonlocal raise with\nTable 1.1: A listing of the reserved words in Python. These names cannot be used\nas identi\ufb01ers.", "1.2. Objects in Python 5\nFor readers familiar with other programming languages, the semantics of a\nPython identi\ufb01er is most similar to a reference variable in Java or a pointer variable\nin C++. Each identi\ufb01er is implicitly associated with the memory address of the\nobject to which it refers. A Python identi\ufb01er may be assigned to a special objectnamed None , serving a similar purpose to a null reference in Java or C++.\nUnlike Java and C++, Python is a dynamically typed language, as there is no\nadvance declaration associating an identi\ufb01er with a particular data type. An iden-\nti\ufb01er can be associated with any type of object, and it can later be reassigned to\nanother object of the same (or different) type. Although an identi\ufb01er has no de-clared type, the object to which it refers has a de\ufb01nite type. In our \ufb01rst example,the characters 98.6 are recognized as a \ufb02oating-point literal, and thus the identi\ufb01er\ntemperature is associated with an instance of the \ufb02oat class having that value.\nA programmer can establish an alias by assigning a second identi\ufb01er to an\nexisting object. Continuing with our earlier example, Figure 1.2 portrays the resultof a subsequent assignment, original = temperature .\n\ufb02oat\n98.6original temperature\nFigure 1.2: Identi\ufb01ers temperature andoriginal are aliases for the same object.\nOnce an alias has been established, either name can be used to access the under-\nlying object. If that object supports behaviors that affect its state, changes enactedthrough one alias will be apparent when using the other alias (because they refer tothe same object). However, if one of the names is reassigned to a new value using\na subsequent assignment statement, that does not affect the aliased object, rather itbreaks the alias. Continuing with our concrete example, we consider the command:\ntemperature = temperature + 5.0\nThe execution of this command begins with the evaluation of the expression on theright-hand side of the =operator. That expression, temperature + 5.0 ,i se v a l -\nuated based on the existing binding of the name temperature , and so the result\nhas value 103.6, that is, 98.6 + 5.0. That result is stored as a new \ufb02oating-point\ninstance, and only then is the name on the left-hand side of the assignment state-\nment,temperature , (re)assigned to the result. The subsequent con\ufb01guration is dia-\ngrammed in Figure 1.3. Of particular note, this last command had no effect on thevalue of the existing \ufb02oat instance that identi\ufb01er original continues to reference.\n98.6\ufb02oat\n103.6temperature original\ufb02oat\nFigure 1.3: Thetemperature identi\ufb01er has been assigned to a new value, while\noriginal continues to refer to the previously existing value.", "6 Chapter 1. Python Primer\n1.2.2 Creating and Using Objects\nInstantiation\nThe process of creating a new instance of a class is known as instantiation .I n\ngeneral, the syntax for instantiating an object is to invoke the constructor of a class.\nFor example, if there were a class named Widget , we could create an instance of\nthat class using a syntax such as w=W i d g e t ( ) , assuming that the constructor does\nnot require any parameters. If the constructor does require parameters, we might\nuse a syntax such as Widget(a, b, c) to construct a new instance.\nMany of Python\u2019s built-in classes (discussed in Section 1.2.3) support what is\nknown as a literal form for designating new instances. For example, the command\ntemperature = 98.6 results in the creation of a new instance of the \ufb02oat class; the\nterm98.6 in that expression is a literal form. We discuss further cases of Python\nliterals in the coming section.\nFrom a programmer\u2019s perspective, yet another way to indirectly create a new\ninstance of a class is to call a function that creates and returns such an instance. For\nexample, Python has a built-in function named sorted (see Section 1.5.2) that takes\na sequence of comparable elements as a parameter and returns a new instance ofthelistclass containing those elements in sorted order.\nCalling Methods\nPython supports traditional functions (see Section 1.5) that are invoked with a syn-tax such as sorted(data) , in which case data is a parameter sent to the function.\nPython\u2019s classes may also de\ufb01ne one or more methods (also known as member\nfunctions ), which are invoked on a speci\ufb01c instance of a class using the dot (\u201c.\u201d)\noperator. For example, Python\u2019s listclass has a method named sort that can be\ninvoked with a syntax such as data.sort(). This particular method rearranges the\ncontents of the list so that they are sorted.\nThe expression to the left of the dot identi\ufb01es the object upon which the method\nis invoked. Often, this will be an identi\ufb01er (e.g., data), but we can use the dot op-\nerator to invoke a method upon the immediate result of some other operation. For\nexample, if response identi\ufb01es a string instance (we will discuss strings later in this\nsection), the syntax response.lower().startswith(\ny\n)\ufb01rst evaluates the method\ncall,response.lower(), which itself returns a new string instance, and then the\nstartswith(\n y\n)method is called on that intermediate string.\nWhen using a method of a class, it is important to understand its behavior.\nSome methods return information about the state of an object, but do not change\nthat state. These are known as accessors . Other methods, such as the sortmethod\nof the listclass, do change the state of an object. These methods are known as\nmutators orupdate methods .", "1.2. Objects in Python 7\n1.2.3 Python\u2019s Built-In Classes\nTable 1.2 provides a summary of commonly used, built-in classes in Python; we\ntake particular note of which classes are mutable and which are immutable. A classisimmutable if each object of that class has a \ufb01xed value upon instantiation that\ncannot subsequently be changed. For example, the \ufb02oat class is immutable. Once\nan instance has been created, its value cannot be changed (although an identi\ufb01er\nreferencing that object can be reassigned to a different value).\nClass\n Description\n Immutable?\nbool\n Boolean value\n /check\nint\n integer (arbitrary magnitude)\n /check\n\ufb02oat\n \ufb02oating-point number\n /check\nlist\n mutable sequence of objects\ntuple\n immutable sequence of objects\n /check\nstr\n character string\n /check\nset\n unordered set of distinct objects\nfrozenset\n immutable form of setclass\n /check\ndict\n associative mapping (aka dictionary)\nTable 1.2: Commonly used built-in classes for Python\nIn this section, we provide an introduction to these classes, discussing their\npurpose and presenting several means for creating instances of the classes. Literalforms (such as 98.6) exist for most of the built-in classes, and all of the classes\nsupport a traditional constructor form that creates instances that are based upon\none or more existing values. Operators supported by these classes are described inSection 1.3. More detailed information about these classes can be found in laterchapters as follows: lists and tuples (Chapter 5); strings (Chapters 5 and 13, and\nAppendix A); sets and dictionaries (Chapter 10).\nThe bool Class\nThebool class is used to manipulate logical (Boolean) values, and the only two\ninstances of that class are expressed as the literals True andFalse . The default\nconstructor, bool() , returns False , but there is no reason to use that syntax rather\nthan the more direct literal form. Python allows the creation of a Boolean value\nfrom a nonboolean type using the syntax bool(foo) for value foo. The interpretation\ndepends upon the type of the parameter. Numbers evaluate to False if zero, and\nTrue if nonzero. Sequences and other container types, such as strings and lists,\nevaluate to False if empty and True if nonempty. An important application of this\ninterpretation is the use of a nonboolean value as a condition in a control structure.", "8 Chapter 1. Python Primer\nThe int Class\nTheintand\ufb02oat classes are the primary numeric types in Python. The intclass is\ndesigned to represent integer values with arbitrary magnitude. Unlike Java and\nC++, which support different integral types with different precisions (e.g., int,\nshort ,long), Python automatically chooses the internal representation for an in-\nteger based upon the magnitude of its value. Typical literals for integers include 0,\n137,a n d\u221223. In some contexts, it is convenient to express an integral value using\nbinary, octal, or hexadecimal. That can be done by using a pre\ufb01x of the number 0\nand then a character to describe the base. Example of such literals are respectively\n0b1011 ,0o52 ,a n d0x7f.\nThe integer constructor, int() , returns value 0 by default. But this constructor\ncan be used to construct an integer value based upon an existing value of anothertype. For example, if frepresents a \ufb02oating-point value, the syntax int(f) produces\nthetruncated value of f. For example, both int(3.14) andint(3.99) produce the\nvalue3, while int(\u22123.9) produces the value \u22123. The constructor can also be used\nto parse a string that is presumed to represent an integral value (such as one en-tered by a user). If srepresents a string, then int(s) produces the integral value\nthat string represents. For example, the expression int(\n137\n)produces the inte-\nger value 137. If an invalid string is given as a parameter, as in int(\n hello\n ),a\nValueError is raised (see Section 1.7 for discussion of Python\u2019s exceptions). By de-\nfault, the string must use base 10. If conversion from a different base is desired, thatbase can be indicated as a second, optional, parameter. For example, the expression\nint(\n7f\n, 16) evaluates to the integer 127.\nThe \ufb02oat Class\nThe\ufb02oat class is the sole \ufb02oating-point type in Python, using a \ufb01xed-precision\nrepresentation. Its precision is more akin to a double in Java or C++, rather than\nthose languages\u2019 \ufb02oat type. We have already discussed a typical literal form, 98.6.\nWe note that the \ufb02oating-point equivalent of an integral number can be expressed\ndirectly as 2.0. Technically, the trailing zero is optional, so some programmers\nmight use the expression 2.to designate this \ufb02oating-point literal. One other form\nof literal for \ufb02oating-point values uses scienti\ufb01c notation. For example, the literal6.022e23 represents the mathematical value 6 .022\u00d710\n23.\nThe constructor form of \ufb02oat() returns 0.0. When given a parameter, the con-\nstructor attempts to return the equivalent \ufb02oating-point value. For example, the call\ufb02oat(2) returns the \ufb02oating-point value 2.0. If the parameter to the constructor is\na string, as with \ufb02oat(\n3.14\n ), it attempts to parse that string as a \ufb02oating-point\nvalue, raising a ValueError as an exception.", "1.2. Objects in Python 9\nSequence Types: The list, tuple, and str Classes\nThelist,tuple ,a n dstrclasses are sequence types in Python, representing a col-\nlection of values in which the order is signi\ufb01cant. The listclass is the most general,\nrepresenting a sequence of arbitrary objects (akin to an \u201carray\u201d in other languages).\nThetuple class is an immutable version of the listclass, bene\ufb01ting from a stream-\nlined internal representation. The strclass is specially designed for representing\nan immutable sequence of text characters. We note that Python does not have aseparate class for characters; they are just strings with length one.\nThe list Class\nAlistinstance stores a sequence of objects. A list is a referential structure, as it\ntechnically stores a sequence of references to its elements (see Figure 1.4). El-\nements of a list may be arbitrary objects (including the None object). Lists are\narray-based sequences and are zero-indexed , thus a list of length nhas elements\nindexed from 0 to n\u22121 inclusive. Lists are perhaps the most used container type in\nPython and they will be extremely central to our study of data structures and algo-rithms. They have many valuable behaviors, including the ability to dynamically\nexpand and contract their capacities as needed. In this chapter, we will discuss only\nthe most basic properties of lists. We revisit the inner working of all of Python\u2019ssequence types as the focus of Chapter 5.\nPython uses the characters []as delimiters for a listliteral, with []itself being\nan empty list. As another example, [\nred\n,\ngreen\n ,\nblue\n ]is a list containing\nthree string instances. The contents of a list literal need not be expressed as literals;if identi\ufb01ers aandbhave been established, then syntax [a, b] is legitimate.\nThelist() constructor produces an empty list by default. However, the construc-\ntor will accept any parameter that is of an iterable type. We will discuss iteration\nfurther in Section 1.8, but examples of iterable types include all of the standard con-tainer types (e.g., strings, list, tuples, sets, dictionaries). For example, the syntaxlist(\nhello\n )produces a list of individual characters, [\nh\n,\ne\n,\nl\n,\nl\n,\no\n].\nBecause an existing list is itself iterable, the syntax backup = list(data) can be\nused to construct a new list instance referencing the same contents as the original.\n3 4 5 6 7 012 1 0 9 8primes :13 19 23 29 31 7 5 3 21 1 17\nFigure 1.4: Python\u2019s internal representation of a list of integers, instantiated as\nprime = [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31] . The implicit indices of the ele-\nments are shown below each entry.", "10 Chapter 1. Python Primer\nT h et u p l eC l a s s\nThetuple class provides an immutable version of a sequence, and therefore its\ninstances have an internal representation that may be more streamlined than that of\na list. While Python uses the []characters to delimit a list, parentheses delimit a\ntuple, with ()being an empty tuple. There is one important subtlety. To express\na tuple of length one as a literal, a comma must be placed after the element, but\nwithin the parentheses. For example, (17,) is a one-element tuple. The reason for\nthis requirement is that, without the trailing comma, the expression (17) is viewed\nas a simple parenthesized numeric expression.\nThe str Class\nPython\u2019s strclass is speci\ufb01cally designed to ef\ufb01ciently represent an immutable\nsequence of characters, based upon the Unicode international character set. Strings\nhave a more compact internal representation than the referential lists and tuples, as\nportrayed in Figure 1.5.\n0ASM P L E\n345 12\nFigure 1.5: A Python string, which is an indexed sequence of characters.\nString literals can be enclosed in single quotes, as in\n hello\n , or double\nquotes, as in \"hello\" . This choice is convenient, especially when using an-\nother of the quotation characters as an actual character in the sequence, as in\"Don\nt worry\" . Alternatively, the quote delimiter can be designated using a\nbackslash as a so-called escape character ,a si n\n Don\\\n t worry\n . Because the\nbackslash has this purpose, the backslash must itself be escaped to occur as a natu-\nral character of the string literal, as in\n C:\\\\Python\\\\\n , for a string that would be\ndisplayed as C:\\Python\\ . Other commonly escaped characters are \\nfor newline\nand\\tfor tab. Unicode characters can be included, such as\n 20\\u20AC\n for the\nstring 20\n.\nPython also supports using the delimiter\n or\"\"\"to begin and end a string\nliteral. The advantage of such triple-quoted strings is that newline characters can\nbe embedded naturally (rather than escaped as \\n). This can greatly improve the\nreadability of long, multiline strings in source code. For example, at the beginningof Code Fragment 1.1, rather than use separate print statements for each line of\nintroductory output, we can use a single print statement, as follows:\nprint(\u201d\u201d\u201dWelcome to the GPA calculator.\nPlease enter all your letter grades, one per line.Enter a blank line to designate the end.\u201d\u201d\u201d)", "1.2. Objects in Python 11\nThe set and frozenset Classes\nPython\u2019s setclass represents the mathematical notion of a set, namely a collection\nof elements, without duplicates, and without an inherent order to those elements.\nThe major advantage of using a set, as opposed to a list,i st h a ti th a sah i g h l y\noptimized method for checking whether a speci\ufb01c element is contained in the set.This is based on a data structure known as a hash table (which will be the primary\ntopic of Chapter 10). However, there are two important restrictions due to the\nalgorithmic underpinnings. The \ufb01rst is that the set does not maintain the elements\nin any particular order. The second is that only instances of immutable types can be\nadded to a Python set. Therefore, objects such as integers, \ufb02oating-point numbers,\nand character strings are eligible to be elements of a set. It is possible to maintain a\nset of tuples, but not a set of lists or a set of sets, as lists and sets are mutable. The\nfrozenset class is an immutable form of the settype, so it is legal to have a set of\nfrozensets.\nPython uses curly braces {and}as delimiters for a set, for example, as {17}\nor{\nred\n,\ngreen\n ,\nblue\n }. The exception to this rule is that {}does not\nrepresent an empty set; for historical reasons, it represents an empty dictionary(see next paragraph). Instead, the constructor syntax set() produces an empty set.\nIf an iterable parameter is sent to the constructor, then the set of distinct elements\nis produced. For example, set(\nhello\n )produces {\nh\n,\ne\n,\nl\n,\no\n}.\nThe dict Class\nPython\u2019s dict class represents a dictionary ,o rmapping , from a set of distinct keys\nto associated values . For example, a dictionary might map from unique student ID\nnumbers, to larger student records (such as the student\u2019s name, address, and course\ngrades). Python implements a dictusing an almost identical approach to that of a\nset, but with storage of the associated values.\nA dictionary literal also uses curly braces, and because dictionaries were intro-\nduced in Python prior to sets, the literal form {}produces an empty dictionary.\nA nonempty dictionary is expressed using a comma-separated series of key:valuepairs. For example, the dictionary {\nga\n:\nIrish\n ,\nde\n:\nGerman\n }maps\nga\nto\nIrish\n and\n de\nto\nGerman\n .\nThe constructor for the dictclass accepts an existing mapping as a parameter,\nin which case it creates a new dictionary with identical associations as the existingone. Alternatively, the constructor accepts a sequence of key-value pairs as a pa-\nrameter, as in dict(pairs) withpairs = [(\nga\n,\nIrish\n ), (\nde\n,\nGerman\n )].", "12 Chapter 1. Python Primer\n1.3 Expressions, Operators, and Precedence\nIn the previous section, we demonstrated how names can be used to identify ex-\nisting objects, and how literals and constructors can be used to create instances of\nbuilt-in classes. Existing values can be combined into larger syntactic expressions\nusing a variety of special symbols and keywords known as operators . The seman-\ntics of an operator depends upon the type of its operands. For example, when a\nandbare numbers, the syntax a+b indicates addition, while if aandbare strings,\nthe operator indicates concatenation. In this section, we describe Python\u2019s opera-tors in various contexts of the built-in types.\nWe continue, in Section 1.3.1, by discussing compound expressions ,s u c ha s\na+b\nc, which rely on the evaluation of two or more operations. The order\nin which the operations of a compound expression are evaluated can affect theoverall value of the expression. For this reason, Python de\ufb01nes a speci\ufb01c order of\nprecedence for evaluating operators, and it allows a programmer to override this\norder by using explicit parentheses to group subexpressions.\nLogical Operators\nPython supports the following keyword operators for Boolean values:\nnot unary negation\nand conditional and\nor conditional or\nTheand andoroperators short-circuit , in that they do not evaluate the second\noperand if the result can be determined based on the value of the \ufb01rst operand.\nThis feature is useful when constructing Boolean expressions in which we \ufb01rst test\nthat a certain condition holds (such as a reference not being None ), and then test a\ncondition that could have otherwise generated an error condition had the prior testnot succeeded.\nEquality Operators\nPython supports the following operators to test two notions of equality:\nis same identity\nis not different identity\n== equivalent\n!= not equivalent\nThe expression aisbevaluates to True , precisely when identi\ufb01ers aandbare\naliases for the same object. The expression a= =b tests a more general notion of\nequivalence. If identi\ufb01ers aandbrefer to the same object, then a= =b should also\nevaluate to True .Y e ta= =b also evaluates to True when the identi\ufb01ers refer to", "1.3. Expressions, Operators, and Precedence 13\ndifferent objects that happen to have values that are deemed equivalent. The precise\nnotion of equivalence depends on the data type. For example, two strings are con-\nsidered equivalent if they match character for character. Two sets are equivalent ifthey have the same contents, irrespective of order. In most programming situations,the equivalence tests ==and!=are the appropriate operators; use of isandis not\nshould be reserved for situations in which it is necessary to detect true aliasing.\nComparison Operators\nData types may de\ufb01ne a natural order via the following operators:\n< less than\n<=less than or equal to\n> greater than\n>=greater than or equal to\nThese operators have expected behavior for numeric types, and are de\ufb01ned lexi-cographically, and case-sensitively, for strings. An exception is raised if operandshave incomparable types, as with 5<\nhello\n .\nArithmetic Operators\nPython supports the following arithmetic operators:\n+ addition\n\u2212 subtraction\nmultiplication\n/true division\n// integer division\n% the modulo operator\nThe use of addition, subtraction, and multiplication is straightforward, noting that ifboth operands have type int, then the result is an intas well; if one or both operands\nhave type \ufb02oat , the result will be a \ufb02oat .\nPython takes more care in its treatment of division. We \ufb01rst consider the case\nin which both operands have type int, for example, the quantity 27 divided by\n4. In mathematical notation, 27 \u00f74=6\n3\n4=6.75. In Python, the /operator\ndesignates true division, returning the \ufb02oating-point result of the computation.\nThus, 2 7/4 results in the \ufb02oat value6.75. Python supports the pair of opera-\ntors//and%to perform the integral calculations, with expression 27 // 4 evalu-\nating to intvalue 6 (the mathematical \ufb02oor of the quotient), and expression 27 % 4\nevaluating to intvalue 3, the remainder of the integer division. We note that lan-\nguages such as C, C++, and Java do not support the //operator; instead, the /op-\nerator returns the truncated quotient when both operands have integral type, and the\nresult of true division when at least one operand has a \ufb02oating-point type.", "14 Chapter 1. Python Primer\nPython carefully extends the semantics of //and%to cases where one or both\noperands are negative. For the sake of notation, let us assume that variables n\nandmrepresent respectively the dividend anddivisor of a quotientn\nm,a n dt h a t\nq=n/ /m andr=n%m . Python guarantees that q\nm+r will equal n.W e\nalready saw an example of this identity with positive operands, as 6 \u22174+3=27.\nWhen the divisor mis positive, Python further guarantees that 0 \u2264r<m.A s\na consequence, we \ufb01nd that \u221227 // 4 evaluates to \u22127and\u221227 % 4 evaluates\nto1,a s (\u22127)\u22174+1=\u221227. When the divisor is negative, Python guarantees that\nm<r\u22640. As an example, 27 // \u22124is\u22127and27 % \u22124is\u22121, satisfying the\nidentity 27 =(\u22127)\u2217(\u22124)+(\u22121).\nThe conventions for the //and%operators are even extended to \ufb02oating-\npoint operands, with the expression q=n/ /m being the integral \ufb02oor of the\nquotient, and r=n%m being the \u201cremainder\u201d to ensure that q\nm+r equals\nn. For example, 8.2 // 3.14 evaluates to 2.0and8.2 % 3.14 evaluates to 1.92,a s\n2.0\u22173.14+1.92=8.2.\nBitwise Operators\nPython provides the following bitwise operators for integers:\n\u223c bitwise complement (pre\ufb01x unary operator)\n& bitwise and\n| bitwise or\n\u02c6 bitwise exclusive-or\n<< shift bits left, \ufb01lling in with zeros\n>> shift bits right, \ufb01lling in with sign bit\nSequence Operators\nEach of Python\u2019s built-in sequence types ( str,tuple ,a n dlist) support the following\noperator syntaxes:\ns[j] element at index j\ns[start:stop] slice including indices [start,stop)\ns[start:stop:step] slice including indices start ,start + step ,\nstart + 2\n step,...,u pt ob u tn o te q u a lling or stop\ns+t concatenation of sequences\nk\ns shorthand for s+s+s+. . . (k times)\nvalins containment check\nvalnot in s non-containment check\nPython relies on zero-indexing of sequences, thus a sequence of length nhas ele-\nments indexed from 0 to n\u22121 inclusive. Python also supports the use of negative\nindices , which denote a distance from the end of the sequence; index \u22121 denotes\nthe last element, index \u22122 the second to last, and so on. Python uses a slicing", "1.3. Expressions, Operators, and Precedence 15\nnotation to describe subsequences of a sequence. Slices are described as half-open\nintervals, with a start index that is included, and a stop index that is excluded. For\nexample, the syntax data[3:8] denotes a subsequence including the \ufb01ve indices:\n3,4,5,6,7. An optional \u201cstep\u201d value, possibly negative, can be indicated as a third\nparameter of the slice. If a start index or stop index is omitted in the slicing nota-tion, it is presumed to designate the respective extreme of the original sequence.\nBecause lists are mutable, the syntax s[j] = val can be used to replace an ele-\nment at a given index. Lists also support a syntax, dels[j], that removes the desig-\nnated element from the list. Slice notation can also be used to replace or delete asublist.\nThe notation valinscan be used for any of the sequences to see if there is an\nelement equivalent to valin the sequence. For strings, this syntax can be used to\ncheck for a single character or for a larger substring, as with\namp\nin\nexample\n .\nAll sequences de\ufb01ne comparison operations based on lexicographic order ,p e r -\nforming an element by element comparison until the \ufb01rst difference is found. For\nexample, [ 5 ,6 ,9 ] <[5, 7] because of the entries at index 1. Therefore, the follow-\ning operations are supported by sequence types:\ns= =t equivalent (element by element)\ns! =t not equivalent\ns<tlexicographically less than\ns<=t lexicographically less than or equal to\ns>tlexicographically greater than\ns>=t lexicographically greater than or equal to\nOperators for Sets and Dictionaries\nSets and frozensets support the following operators:\nkeyins containment check\nkeynot in snon-containment check\ns1 == s2 s1 is equivalent to s2\ns1 != s2 s1 is not equivalent to s2\ns1<=s 2 s 1 is subset of s2\ns1<s2 s1 is proper subset of s2\ns1>=s 2 s 1 is superset of s2\ns1>s2 s1 is proper superset of s2\ns1|s2 the union of s1ands2\ns1 & s2 the intersection of s1ands2\ns1\u2212s2 the set of elements in s1but not s2\ns1 \u02c6 s2 the set of elements in precisely one of s1ors2\nNote well that sets do not guarantee a particular order of their elements, so the\ncomparison operators, such as <, are not lexicographic; rather, they are based on\nthe mathematical notion of a subset. As a result, the comparison operators de\ufb01ne", "16 Chapter 1. Python Primer\na partial order, but not a total order, as disjoint sets are neither \u201cless than,\u201d \u201cequal\nto,\u201d or \u201cgreater than\u201d each other. Sets also support many fundamental behaviors\nthrough named methods (e.g., add,remove ); we will explore their functionality\nmore fully in Chapter 10.\nDictionaries, like sets, do not maintain a well-de\ufb01ned order on their elements.\nFurthermore, the concept of a subset is not typically meaningful for dictionaries, so\nthedictclass does not support operators such as <. Dictionaries support the notion\nof equivalence, with d1 == d2 if the two dictionaries contain the same set of key-\nvalue pairs. The most widely used behavior of dictionaries is accessing a value\nassociated with a particular key kwith the indexing syntax, d[k]. The supported\noperators are as follows:\nd[key] value associated with given key\nd[key] = value set (or reset) the value associated with given key\ndeld[key] remove key and its associated value from dictionary\nkeyind containment check\nkeynot in d non-containment check\nd1 == d2 d1 is equivalent to d2\nd1 != d2 d1 is not equivalent to d2\nDictionaries also support many useful behaviors through named methods, whichwe explore more fully in Chapter 10.\nExtended Assignment Operators\nPython supports an extended assignment operator for most binary operators, forexample, allowing a syntax such as count += 5 . By default, this is a shorthand for\nthe more verbose count = count + 5 . For an immutable type, such as a number or\na string, one should not presume that this syntax changes the value of the existingobject, but instead that it will reassign the identi\ufb01er to a newly constructed value.(See discussion of Figure 1.3.) However, it is possible for a type to rede\ufb01ne suchsemantics to mutate the object, as the listclass does for the +=operator.\nalpha = [1, 2, 3]beta = alpha # an alias for alpha\nbeta += [4, 5] # extends the original list with two more elements\nbeta = beta + [6, 7] # reassigns beta to a new list [1, 2, 3, 4, 5, 6, 7]\nprint(alpha) #w i l lb e[ 1 ,2 ,3 ,4 ,5 ]\nThis example demonstrates the subtle difference between the list semantics for the\nsyntax beta += foo versus beta = beta + foo .", "1.3. Expressions, Operators, and Precedence 17\n1.3.1 Compound Expressions and Operator Precedence\nProgramming languages must have clear rules for the order in which compound\nexpressions, such as 5+2\n 3, are evaluated. The formal order of precedence\nfor operators in Python is given in Table 1.3. Operators in a category with higher\nprecedence will be evaluated before those with lower precedence, unless the expres-sion is otherwise parenthesized. Therefore, we see that Python gives precedence to\nmultiplication over addition, and therefore evaluates the expression 5+2\n3as\n5+( 2\n 3), with value 11, but the parenthesized expression (5 + 2)\n 3evalu-\nates to value 21. Operators within a category are typically evaluated from left to\nright, thus 5\u22122+3 has value 6. Exceptions to this rule include that unary oper-\nators and exponentiation are evaluated from right to left.\nPython allows a chained assignment ,s u c ha s x=y=0 , to assign multiple\nidenti\ufb01ers to the rightmost value. Python also allows the chaining of comparison\noperators. For example, the expression 1<=x+y <=1 0 is evaluated as the\ncompound (1<=x+y ) and(x + y <= 10) , but without computing the inter-\nmediate value x+y twice.\nOperator Precedence\nType\n Symbols\n1\nmember access\n expr.member\n2\nfunction/method calls\n expr(...)\ncontainer subscripts/slices\n expr[...]\n3\nexponentiation\n4\nunary operators\n +expr ,\u2212expr,\u02dcexpr\n5\nmultiplication, division\n ,/,//,%\n6\naddition, subtraction\n +,\u2212\n7\nbitwise shifting\n <<,>>\n8\nbitwise-and\n &\n9\nbitwise-xor\n \u02c6\n10\n bitwise-or\n |\n11\ncomparisons\n is,is not ,==,!=,<,<=,>,>=\ncontainment\n in,not in\n12\n logical-not\n notexpr\n13\n logical-and\n and\n14\n logical-or\n or\n15\n conditional\n val1ifcondelseval2\n16\n assignments\n =,+=,\u2212=,\n=, etc.\nTable 1.3: Operator precedence in Python, with categories ordered from highest\nprecedence to lowest precedence. When stated, we use expr to denote a literal,\nidenti\ufb01er, or result of a previously evaluated expression. All operators withoutexplicit mention of expr are binary operators, with syntax expr1 operator expr2 .", "18 Chapter 1. Python Primer\n1.4 Control Flow\nIn this section, we review Python\u2019s most fundamental control structures: condi-\ntional statements and loops. Common to all control structures is the syntax usedin Python for de\ufb01ning blocks of code. The colon character is used to delimit the\nbeginning of a block of code that acts as a body for a control structure. If the body\ncan be stated as a single executable statement, it can technically placed on the sameline, to the right of the colon. However, a body is more typically typeset as anindented block starting on the line following the colon. Python relies on the inden-\ntation level to designate the extent of that block of code, or any nested blocks of\ncode within. The same principles will be applied when designating the body of a\nfunction (see Section 1.5), and the body of a class (see Section 2.3).\n1.4.1 Conditionals\nConditional constructs (also known as ifstatements) provide a way to execute a\nchosen block of code based on the run-time evaluation of one or more Booleanexpressions. In Python, the most general form of a conditional is written as follows:\nif\ufb01rst\ncondition:\n\ufb01rst\n body\nelif second\n condition:\nsecond\n body\nelif third\n condition:\nthird\n body\nelse:\nfourth\n body\nEach condition is a Boolean expression, and each body contains one or more com-mands that are to be executed conditionally. If the \ufb01rst condition succeeds, the \ufb01rst\nbody will be executed; no other conditions or bodies are evaluated in that case.\nIf the \ufb01rst condition fails, then the process continues in similar manner with theevaluation of the second condition. The execution of this overall construct willcause precisely one of the bodies to be executed. There may be any number ofelifclauses (including zero), and the \ufb01nal else clause is optional. As described on\npage 7, nonboolean types may be evaluated as Booleans with intuitive meanings.For example, if response is a string that was entered by a user, and we want to\ncondition a behavior on this being a nonempty string, we may write\nifresponse:\nas a shorthand for the equivalent,\nifresponse !=\n:", "1.4. Control Flow 19\nAs a simple example, a robot controller might have the following logic:\nifdoor\nis\nclosed:\nopen\ndoor()\nadvance()\nNotice that the \ufb01nal command, advance() , is not indented and therefore not part of\nthe conditional body. It will be executed unconditionally (although after opening a\nclosed door).\nWe may nest one control structure within another, relying on indentation to\nmake clear the extent of the various bodies. Revisiting our robot example, here is amore complex control that accounts for unlocking a closed door.\nifdoor\nis\nclosed:\nifdoor\nis\nlocked:\nunlock\n door()\nopen\ndoor()\nadvance()\nThe logic expressed by this example can be diagrammed as a traditional \ufb02owchart ,\nas portrayed in Figure 1.6.\nopen\ndoor()Falsedoor\nis\nclosed\nadvance()door\nis\nlocked\nunlock\n door()True FalseTrue\nFigure 1.6: A \ufb02owchart describing the logic of nested conditional statements.", "20 Chapter 1. Python Primer\n1.4.2 Loops\nPython offers two distinct looping constructs. A while loop allows general repeti-\ntion based upon the repeated testing of a Boolean condition. A forloop provides\nconvenient iteration of values from a de\ufb01ned series (such as characters of a string,\nelements of a list, or numbers within a given range). We discuss both forms in thissection.\nWhile Loops\nThe syntax for a while loop in Python is as follows:\nwhile condition:\nbody\nAs with an ifstatement, condition can be an arbitrary Boolean expression, and\nbody can be an arbitrary block of code (including nested control structures). The\nexecution of a while loop begins with a test of the Boolean condition. If that condi-tion evaluates to True , the body of the loop is performed. After each execution of\nthe body, the loop condition is retested, and if it evaluates to True , another iteration\nof the body is performed. When the conditional test evaluates to False (assuming\nit ever does), the loop is exited and the \ufb02ow of control continues just beyond thebody of the loop.\nAs an example, here is a loop that advances an index through a sequence of\ncharacters until \ufb01nding an entry with value\nX\nor reaching the end of the sequence.\nj=0while j<len(data) anddata[j] !=\nX\n:\nj+ =1\nThelenfunction, which we will introduce in Section 1.5.2, returns the length of a\nsequence such as a list or string. The correctness of this loop relies on the short-circuiting behavior of the and operator, as described on page 12. We intention-\nally test j<len(data) to ensure that jis a valid index, prior to accessing element\ndata[j] . Had we written that compound condition with the opposite order, the eval-\nuation of data[j] would eventually raise an IndexError when\nX\nis not found. (See\nSection 1.7 for discussion of exceptions.)\nAs written, when this loop terminates, variable j\u2019s value will be the index of\nthe leftmost occurrence of\n X\n, if found, or otherwise the length of the sequence\n(which is recognizable as an invalid index to indicate failure of the search). It isworth noting that this code behaves correctly, even in the special case when the listis empty, as the condition j<len(data) will initially fail and the body of the loop\nwill never be executed.", "1.4. Control Flow 21\nFor Loops\nPython\u2019s for-loop syntax is a more convenient alternative to a while loop when\niterating through a series of elements. The for-loop syntax can be used on any\ntype of iterable structure, such as a list,tuple str ,set,dict,o r\ufb01le(we will discuss\niterators more formally in Section 1.8). Its general syntax appears as follows.\nfor element initerable :\nbody #b o d ym a yr e f e rt o\n element\n as an identi\ufb01er\nFor readers familiar with Java, the semantics of Python\u2019s for loop is similar to the\u201cfor each\u201d loop style introduced in Java 1.5.\nAs an instructive example of such a loop, we consider the task of computing\nthe sum of a list of numbers. (Admittedly, Python has a built-in function, sum,f o r\nthis purpose.) We perform the calculation with a for loop as follows, assuming thatdata identi\ufb01es the list:\ntotal = 0\nforvalindata:\ntotal += val # note use of the loop variable, val\nThe loop body executes once for each element of the data sequence, with the iden-\nti\ufb01er, val, from the for-loop syntax assigned at the beginning of each pass to a\nrespective element. It is worth noting that valis treated as a standard identi\ufb01er. If\nthe element of the original data happens to be mutable, the validenti\ufb01er can be\nused to invoke its methods. But a reassignment of identi\ufb01er valto a new value has\nno affect on the original data, nor on the next iteration of the loop.\nAs a second classic example, we consider the task of \ufb01nding the maximum\nvalue in a list of elements (again, admitting that Python\u2019s built-in max function\nalready provides this support). If we can assume that the list, data, has at least one\nelement, we could implement this task as follows:\nbiggest = data[0] # as we assume nonempty list\nforvalindata:\nifval>biggest:\nbiggest = val\nAlthough we could accomplish both of the above tasks with a while loop, the\nfor-loop syntax had an advantage of simplicity, as there is no need to manage anexplicit index into the list nor to author a Boolean loop condition. Furthermore, wecan use a for loop in cases for which a while loop does not apply, such as wheniterating through a collection, such as a set, that does not support any direct form\nof indexing.", "22 Chapter 1. Python Primer\nIndex-Based For Loops\nThe simplicity of a standard for loop over the elements of a list is wonderful; how-\never, one limitation of that form is that we do not know where an element resides\nwithin the sequence. In some applications, we need knowledge of the index of anelement within the sequence. For example, suppose that we want to know where\nthe maximum element in a list resides.\nRather than directly looping over the elements of the list in that case, we prefer\nto loop over all possible indices of the list. For this purpose, Python providesa built-in class named range that generates integer sequences. (We will discuss\ngenerators in Section 1.8.) In simplest form, the syntax range(n) generates the\nseries of nvalues from 0 to n\u22121. Conveniently, these are precisely the series of\nvalid indices into a sequence of length n. Therefore, a standard Python idiom for\nlooping through the series of indices of a data sequence uses a syntax,\nforjinrange(len(data)):\nIn this case, identi\ufb01er jis not an element of the data\u2014it is an integer. But the\nexpression data[j] can be used to retrieve the respective element. For example, we\ncan \ufb01nd the index of the maximum element of a list as follows:\nbig\nindex = 0\nforjinrange(len(data)):\nifdata[j] >data[big\n index]:\nbig\nindex = j\nBreak and Continue Statements\nPython supports a break statement that immediately terminate a while or for loop\nwhen executed within its body. More formally, if applied within nested controlstructures, it causes the termination of the most immediately enclosing loop. Asa typical example, here is code that determines whether a target value occurs in adata set:\nfound = False\nforitemindata:\nifitem == target:\nfound = True\nbreak\nPython also supports a continue statement that causes the current iteration of a\nloop body to stop, but with subsequent passes of the loop proceeding as expected.\nWe recommend that the break andcontinue statements be used sparingly. Yet,\nthere are situations in which these commands can be effectively used to avoid in-\ntroducing overly complex logical conditions.", "1.5. Functions 23\n1.5 Functions\nIn this section, we explore the creation of and use of functions in Python. As we\ndid in Section 1.2.2, we draw a distinction between functions andmethods .W e\nuse the general term function to describe a traditional, stateless function that is in-\nvoked without the context of a particular class or an instance of that class, such as\nsorted(data) . We use the more speci\ufb01c term method to describe a member function\nthat is invoked upon a speci\ufb01c object using an object-oriented message passing syn-\ntax, such as data.sort() . In this section, we only consider pure functions; methods\nwill be explored with more general object-oriented principles in Chapter 2.\nWe begin with an example to demonstrate the syntax for de\ufb01ning functions in\nPython. The following function counts the number of occurrences of a given targetvalue within any form of iterable data set.\ndefcount(data, target):\nn=0\nforitemindata:\nifitem == target: # found a match\nn+ =1\nreturn n\nThe \ufb01rst line, beginning with the keyword def, serves as the function\u2019s signature .\nThis establishes a new identi\ufb01er as the name of the function ( count , in this exam-\nple), and it establishes the number of parameters that it expects, as well as namesidentifying those parameters ( data andtarget , in this example). Unlike Java and\nC++, Python is a dynamically typed language, and therefore a Python signaturedoes not designate the types of those parameters, nor the type (if any) of a return\nvalue. Those expectations should be stated in the function\u2019s documentation (see\nSection 2.2.3) and can be enforced within the body of the function, but misuse of afunction will only be detected at run-time.\nThe remainder of the function de\ufb01nition is known as the body of the func-\ntion. As is the case with control structures in Python, the body of a function is\ntypically expressed as an indented block of code. Each time a function is called,\nPython creates a dedicated activation record that stores information relevant to the\ncurrent call. This activation record includes what is known as a namespace (see\nSection 1.10) to manage all identi\ufb01ers that have local scope within the current call.\nThe namespace includes the function\u2019s parameters and any other identi\ufb01ers that arede\ufb01ned locally within the body of the function. An identi\ufb01er in the local scopeof the function caller has no relation to any identi\ufb01er with the same name in thecaller\u2019s scope (although identi\ufb01ers in different scopes may be aliases to the sameobject). In our \ufb01rst example, the identi\ufb01er nhas scope that is local to the function\ncall, as does the identi\ufb01er item, which is established as the loop variable.", "24 Chapter 1. Python Primer\nReturn Statement\nAreturn statement is used within the body of a function to indicate that the func-\ntion should immediately cease execution, and that an expressed value should be\nreturned to the caller. If a return statement is executed without an explicit argu-\nment, the None value is automatically returned. Likewise, None will be returned if\nthe \ufb02ow of control ever reaches the end of a function body without having executeda return statement. Often, a return statement will be the \ufb01nal command within the\nbody of the function, as was the case in our earlier example of a count function.\nHowever, there can be multiple return statements in the same function, with con-\nditional logic controlling which such command is executed, if any. As a furtherexample, consider the following function that tests if a value exists in a sequence.\ndefcontains(data, target):\nforitemintarget:\nifitem == target: # found a match\nreturn True\nreturn False\nIf the conditional within the loop body is ever satis\ufb01ed, the return True statement is\nexecuted and the function immediately ends, with True designating that the target\nvalue was found. Conversely, if the for loop reaches its conclusion without ever\n\ufb01nding the match, the \ufb01nal return False statement will be executed.\n1.5.1 Information Passing\nTo be a successful programmer, one must have clear understanding of the mech-\nanism in which a programming language passes information to and from a func-\ntion. In the context of a function signature, the identi\ufb01ers used to describe the\nexpected parameters are known as formal parameters , and the objects sent by the\ncaller when invoking the function are the actual parameters . Parameter passing\nin Python follows the semantics of the standard assignment statement .W h e n a\nfunction is invoked, each identi\ufb01er that serves as a formal parameter is assigned, inthe function\u2019s local scope, to the respective actual parameter that is provided by thecaller of the function.\nFor example, consider the following call to our count function from page 23:\nprizes = count(grades,\nA\n)\nJust before the function body is executed, the actual parameters, grades and\n A\n,\nare implicitly assigned to the formal parameters, data andtarget , as follows:\ndata = gradestarget =\nA\n", "1.5. Functions 25\nThese assignment statements establish identi\ufb01er data as an alias for grades and\ntarget as a name for the string literal\n A\n.( S e eF i g u r e1 . 7 . )\n...str\nA\ndata target grades\nlist\nFigure 1.7: A portrayal of parameter passing in Python, for the function call\ncount(grades,\n A\n). Identi\ufb01ers data andtarget are formal parameters de\ufb01ned\nwithin the local scope of the count function.\nThe communication of a return value from the function back to the caller is\nsimilarly implemented as an assignment. Therefore, with our sample invocation of\nprizes = count(grades,\n A\n), the identi\ufb01er prizes in the caller\u2019s scope is assigned\nto the object that is identi\ufb01ed as nin the return statement within our function body.\nAn advantage to Python\u2019s mechanism for passing information to and from a\nfunction is that objects are not copied. This ensures that the invocation of a function\nis ef\ufb01cient, even in a case where a parameter or return value is a complex object.\nMutable Parameters\nPython\u2019s parameter passing model has additional implications when a parameter is\na mutable object. Because the formal parameter is an alias for the actual parameter,\nthe body of the function may interact with the object in ways that change its state.Considering again our sample invocation of the count function, if the body of the\nfunction executes the command data.append(\nF\n), the new entry is added to the\nend of the list identi\ufb01ed as data within the function, which is one and the same as\nthe list known to the caller as grades . As an aside, we note that reassigning a new\nvalue to a formal parameter with a function body, such as by setting data = [ ] ,\ndoes not alter the actual parameter; such a reassignment simply breaks the alias.\nOur hypothetical example of a count method that appends a new element to a\nlist lacks common sense. There is no reason to expect such a behavior, and it would\nbe quite a poor design to have such an unexpected effect on the parameter. There\nare, however, many legitimate cases in which a function may be designed (andclearly documented) to modify the state of a parameter. As a concrete example,we present the following implementation of a method named scale that\u2019s primary\npurpose is to multiply all entries of a numeric data set by a given factor.\ndefscale(data, factor):\nforjinrange(len(data)):\ndata[j]\n=f a c t o r", "26 Chapter 1. Python Primer\nDefault Parameter Values\nPython provides means for functions to support more than one possible calling\nsignature. Such a function is said to be polymorphic (which is Greek for \u201cmany\nforms\u201d). Most notably, functions can declare one or more default values for pa-rameters, thereby allowing the caller to invoke a function with varying numbers ofactual parameters. As an arti\ufb01cial example, if a function is declared with signature\ndeffoo(a, b=15, c=27):\nthere are three parameters, the last two of which offer default values. A caller is\nwelcome to send three actual parameters, as in foo(4, 12, 8) , in which case the de-\nfault values are not used. If, on the other hand, the caller only sends one parameter,\nfoo(4) , the function will execute with parameters values a=4, b=15, c=27 .I f a\ncaller sends two parameters, they are assumed to be the \ufb01rst two, with the third be-ing the default. Thus, foo(8, 20) executes with a=8, b=20, c=27 .H o w e v e r ,i ti s\nillegal to de\ufb01ne a function with a signature such as bar(a, b=15, c) withbhaving\na default value, yet not the subsequent c; if a default parameter value is present for\none parameter, it must be present for all further parameters.\nAs a more motivating example for the use of a default parameter, we revisit\nthe task of computing a student\u2019s GPA (see Code Fragment 1.1). Rather than as-sume direct input and output with the console, we prefer to design a function thatcomputes and returns a GPA. Our original implementation uses a \ufb01xed mapping\nfrom each letter grade (such as a B\u2212) to a corresponding point value (such as\n2.67). While that point system is somewhat common, it may not agree with the\nsystem used by all schools. (For example, some may assign an\nA+\ngrade a value\nhigher than 4.0.) Therefore, we design a compute\n gpafunction, given in Code\nFragment 1.2, which allows the caller to specify a custom mapping from grades to\nvalues, while offering the standard point system as a default.\ndefcompute\n gpa(grades, points= {\nA+\n:4.0,\n A\n:4.0,\n A-\n:3.67,\n B+\n:3.33,\nB\n:3.0,\n B-\n:2.67,\n C+\n:2.33,\n C\n:2.0,\nC\n:1.67,\n D+\n:1.33,\n D\n:1.0,\n F\n:0.0}):\nnum\ncourses = 0\ntotal\npoints = 0\nforgingrades:\nifginpoints: # a recognizable grade\nnum\ncourses += 1\ntotal\npoints += points[g]\nreturn total\npoints / num\n courses\nCode Fragment 1.2: A function that computes a student\u2019s GPA with a point value\nsystem that can be customized as an optional parameter.", "1.5. Functions 27\nAs an additional example of an interesting polymorphic function, we consider\nPython\u2019s support for range . (Technically, this is a constructor for the range class,\nbut for the sake of this discussion, we can treat it as a pure function.) Three calling\nsyntaxes are supported. The one-parameter form, range(n) , generates a sequence of\nintegers from 0 up to but not including n. A two-parameter form, range(start,stop)\ngenerates integers from start up to, but not including, stop. A three-parameter\nform,range(start, stop, step) , generates a similar range as range(start, stop) ,b u t\nwith increments of size step rather than 1.\nThis combination of forms seems to violate the rules for default parameters.\nIn particular, when a single parameter is sent, as in range(n) , it serves as the stop\nvalue (which is the second parameter); the value of start is effectively 0 in that\ncase. However, this effect can be achieved with some sleight of hand, as follows:\ndefrange(start, stop= None,s t e p = 1 ) :\nifstopis None :\nstop = startstart = 0\n...\nFrom a technical perspective, when range(n) is invoked, the actual parameter nwill\nbe assigned to formal parameter start . Within the body, if only one parameter is\nreceived, the start and stop values are reassigned to provide the desired semantics.\nKeyword Parameters\nThe traditional mechanism for matching the actual parameters sent by a caller, tothe formal parameters declared by the function signature is based on the conceptofpositional arguments . For example, with signature foo(a=10, b=20, c=30) ,\nparameters sent by the caller are matched, in the given order, to the formal param-eters. An invocation of foo(5) indicates that a=5, while bandcare assigned their\ndefault values.\nPython supports an alternate mechanism for sending a parameter to a function\nknown as a keyword argument . A keyword argument is speci\ufb01ed by explicitly\nassigning an actual parameter to a formal parameter by name. For example, withthe above de\ufb01nition of function foo, a call foo(c=5) will invoke the function with\nparameters a=10, b=20, c=5 .\nA function\u2019s author can require that certain parameters be sent only through the\nkeyword-argument syntax. We never place such a restriction in our own functionde\ufb01nitions, but we will see several important uses of keyword-only parameters inPython\u2019s standard libraries. As an example, the built-in max function accepts a\nkeyword parameter, coincidentally named key, that can be used to vary the notion\nof \u201cmaximum\u201d that is used.", "28 Chapter 1. Python Primer\nBy default, max operates based upon the natural order of elements according\nto the <operator for that type. But the maximum can be computed by comparing\nsome other aspect of the elements. This is done by providing an auxiliary function\nthat converts a natural element to some other value for the sake of comparison.\nFor example, if we are interested in \ufb01nding a numeric value with magnitude that is\nmaximal (i.e., considering \u221235to be larger than +20), we can use the calling syn-\ntaxmax(a, b, key=abs) . In this case, the built-in absfunction is itself sent as the\nvalue associated with the keyword parameter key. (Functions are \ufb01rst-class objects\nin Python; see Section 1.10.) When max is called in this way, it will compare abs(a)\ntoabs(b) , rather than atob. The motivation for the keyword syntax as an alternate\nto positional arguments is important in the case of max. This function is polymor-\nphic in the number of arguments, allowing a call such as max(a,b,c,d) ; therefore,\nit is not possible to designate a key function as a traditional positional element.Sorting functions in Python also support a similar keyparameter for indicating a\nnonstandard order. (We explore this further in Section 9.4 and in Section 12.6.1,\nwhen discussing sorting algorithms).\n1.5.2 Python\u2019s Built-In Functions\nTable 1.4 provides an overview of common functions that are automatically avail-\nable in Python, including the previously discussed abs,max,a n drange .W h e n\nchoosing names for the parameters, we use identi\ufb01ers x,y,zfor arbitrary numeric\ntypes, kfor an integer, and a,b,a n d cfor arbitrary comparable types. We use\nthe identi\ufb01er, iterable , to represent an instance of any iterable type (e.g., str,list,\ntuple ,set,dict); we will discuss iterators and iterable data types in Section 1.8.\nA sequence represents a more narrow category of indexable classes, including str,\nlist,a n dtuple , but neither setnordict. Most of the entries in Table 1.4 can be\ncategorized according to their functionality as follows:\nInput/Output: print ,input ,a n dopen will be more fully explained in Section 1.6.\nCharacter Encoding: ordandchrrelate characters and their integer code points.\nFor example, ord(\n A\n)is 65 and chr(65) is\nA\n.\nMathematics: abs,divmod ,pow,round ,a n dsum provide common mathematical\nfunctionality; an additional math module will be introduced in Section 1.11.\nOrdering: max andminapply to any data type that supports a notion of compar-\nison, or to any collection of such values. Likewise, sorted can be used to produce\nan ordered list of elements drawn from any existing collection.Collections/Iterations: range generates a new sequence of numbers; lenreports\nthe length of any existing collection; functions reversed ,all,any,a n dmap oper-\nate on arbitrary iterations as well; iterandnext provide a general framework for\niteration through elements of a collection, and are discussed in Section 1.8.", "1.5. Functions 29\nCommon Built-In Functions\nCalling Syntax\n Description\nabs(x)\n Return the absolute value of a number.\nall(iterable)\n Return True ifbool(e) isTrue for each element e.\nany(iterable)\n Return True ifbool(e) isTrue for at least one element e.\nchr(integer)\n Return a one-character string with the given Unicode code point.\ndivmod(x, y)\n Return (x // y, x % y) as tuple, if xandyare integers.\nhash(obj)\n Return an integer hash value for the object (see Chapter 10).\nid(obj)\n Return the unique integer serving as an \u201cidentity\u201d for the object.\ninput(prompt)\n Return a string from standard input; the prompt is optional.\nisinstance(obj, cls)\n Determine if objis an instance of the class (or a subclass).\niter(iterable)\n Return a new iterator object for t he parameter (see Section 1.8).\nlen(iterable)\n Return the number of elements in the given iteration.\nmap(f, iter1, iter2, ...)\nReturn an iterator yielding the result of function calls f(e1, e2, ...)\nfor respective elements e1\u2208iter1,e2\u2208iter2,...\nmax(iterable)\n Return the largest element of the given iteration.\nmax(a, b, c, ...)\n Return the largest of the arguments.\nmin(iterable)\n Return the smallest element of the given iteration.\nmin(a, b, c, ...)\n Return the smallest of the arguments.\nnext(iterator)\n Return the next element reported by the iterator (see Section 1.8).\nopen(\ufb01lename, mode)\n Open a \ufb01le with the given name and access mode.\nord(char)\n Return the Unicode code point of the given character.\npow(x, y)\nReturn the value xy(as an integer if xandyare integers);\nequivalent to x\ny.\npow(x, y, z)\n Return the value (xymod z)as an integer.\nprint(obj1, obj2, ...)\n Print the arguments, with separating spaces and trailing newline.\nrange(stop)\n Construct an iteration of values 0 ,1,...,stop\u22121.\nrange(start, stop)\n Construct an iteration of values start,start +1,...,stop\u22121.\nrange(start, stop, step)\n Construct an iteration of values start,start +step,start +2\nstep,...\nreversed(sequence)\n Return an iteration of the sequence in reverse.\nround(x)\n Return the nearest intvalue (a tie is broken toward the even value).\nround(x, k)\n Return the value rounded to the nearest 10\u2212k(return-type matches x).\nsorted(iterable)\n Return a list containing elements of the iterable in sorted order.\nsum(iterable)\n Return the sum of the elements in the iterable (must be numeric).\ntype(obj)\n Return the class to which the instance objbelongs.\nTable 1.4: Commonly used built-in function in Python.", "30 Chapter 1. Python Primer\n1.6 Simple Input and Output\nIn this section, we address the basics of input and output in Python, describing stan-\ndard input and output through the user console, and Python\u2019s support for reading\nand writing text \ufb01les.\n1.6.1 Console Input and Output\nThe print Function\nThe built-in function, print , is used to generate standard output to the console.\nIn its simplest form, it prints an arbitrary sequence of arguments, separated byspaces, and followed by a trailing newline character. For example, the command\nprint(\nmaroon\n ,5 )outputs the string\n maroon 5\\n\n . Note that arguments need\nnot be string instances. A nonstring argument xwill be displayed as str(x) . Without\nany arguments, the command print() outputs a single newline character.\nTheprint function can be customized through the use of the following keyword\nparameters (see Section 1.5 for a discussion of keyword parameters):\n\u2022By default, the print function inserts a separating space into the output be-\ntween each pair of arguments. The separator can be customized by providinga desired separating string as a keyword parameter, sep. For example, colon-\nseparated output can be produced as print(a, b, c, sep=\n:\n). The separating\nstring need not be a single character; it can be a longer string, and it can bethe empty string, sep=\n, causing successive arguments to be directly con-\ncatenated.\n\u2022By default, a trailing newline is output after the \ufb01nal argument. An alterna-tive trailing string can be designated using a keyword parameter, end. Des-\nignating the empty string end=\nsuppresses all trailing characters.\n\u2022By default, the print function sends its output to the standard console. How-\never, output can be directed to a \ufb01le by indicating an output \ufb01le stream (seeSection 1.6.2) using \ufb01leas a keyword parameter.\nThe input Function\nThe primary means for acquiring information from the user console is a built-infunction named input . This function displays a prompt, if given as an optional pa-\nrameter, and then waits until the user enters some sequence of characters followedby the return key. The formal return value of the function is the string of charactersthat were entered strictly before the return key (i.e., no newline character exists in\nthe returned string).", "1.6. Simple Input and Output 31\nWhen reading a numeric value from the user, a programmer must use the input\nfunction to get the string of characters, and then use the intor\ufb02oat syntax to\nconstruct the numeric value that character string represents. That is, if a call to\nresponse = input() reports that the user entered the characters,\n 2013\n , the syntax\nint(response) could be used to produce the integer value 2013 . It is quite common\nto combine these operations with a syntax such as\nyear = int(input(\n In what year were you born?\n ))\nif we assume that the user will enter an appropriate response. (In Section 1.7 wediscuss error handling in such a situation.)\nBecause input returns a string as its result, use of that function can be combined\nwith the existing functionality of the string class, as described in Appendix A. For\nexample, if the user enters multiple pieces of information on the same line, it is\ncommon to call the split method on the result, as in\nreply = input(\nEnter x and y, separated by spaces:\n )\npieces = reply.split( ) # returns a list of strings, as separated by spaces\nx=\ufb02oat(pieces[0])\ny=\ufb02oat(pieces[1])\nA Sample Program\nHere is a simple, but complete, program that demonstrates the use of the input\nandprint functions. The tools for formatting the \ufb01nal output is discussed in Ap-\npendix A.\nage = int(input(\n Enter your age in years:\n ))\nmax\nheart\n rate = 206.9 \u2212(0.67\n age) # as per Med Sci Sports Exerc.\ntarget = 0.65\n max\nheart\n rate\nprint(\n Your target fat-burning heart rate is\n ,t a r g e t )\n1.6.2 Files\nFiles are typically accessed in Python beginning with a call to a built-in function,named open , that returns a proxy for interactions with the underlying \ufb01le. For\nexample, the command, fp = open(\nsample.txt\n ), attempts to open a \ufb01le named\nsample.txt, returning a proxy that allows read-only access to the text \ufb01le.\nTheopen function accepts an optional second parameter that determines the\naccess mode. The default mode is\n r\nfor reading. Other common modes are\n w\nfor writing to the \ufb01le (causing any existing \ufb01le with that name to be overwritten),or\na\nfor appending to the end of an existing \ufb01le. Although we focus on use of\ntext \ufb01les, it is possible to work with binary \ufb01les, using access modes such as\n rb\nor\nwb\n.", "32 Chapter 1. Python Primer\nWhen processing a \ufb01le, the proxy maintains a current position within the \ufb01le as\nan offset from the beginning, measured in number of bytes. When opening a \ufb01le\nwith mode\n r\nor\nw\n, the position is initially 0; if opened in append mode,\n a\n,\nthe position is initially at the end of the \ufb01le. The syntax fp.close() closes the \ufb01le\nassociated with proxy fp, ensuring that any written contents are saved. A summary\nof methods for reading and writing a \ufb01le is given in Table 1.5\nCalling Syntax\n Description\nfp.read()\n Return the (remaining) contents of a readable \ufb01le as a string.\nfp.read(k)\n Return the next kbytes of a readable \ufb01le as a string.\nfp.readline()\n Return (remainder of) the current line of a readable \ufb01le as a string.\nfp.readlines()\n Return all (remaining) lines of a readable \ufb01le as a list of strings.\nforlineinfp:\n Iterate all (remaining) lines of a readable \ufb01le.\nfp.seek(k)\n Change the current position to be at the kthbyte of the \ufb01le.\nfp.tell()\n Return the current position, measured as byte-offset from the start.\nfp.write(string)\n Write given string at current position of the writable \ufb01le.\nfp.writelines(seq)\nWrite each of the strings of the given sequence at the current\nposition of the writable \ufb01le. This command does notinsert\nany newlines, beyond those that are embedded in the strings.\nprint(..., \ufb01le=fp)\n Redirect output of print function to the \ufb01le.\nTable 1.5: Behaviors for interacting with a text \ufb01le via a \ufb01le proxy (named fp).\nReading from a File\nThe most basic command for reading via a proxy is the read method. When invoked\non \ufb01le proxy fp,a sfp.read(k) , the command returns a string representing the next k\nbytes of the \ufb01le, starting at the current position. Without a parameter, the syntaxfp.read() returns the remaining contents of the \ufb01le in entirety. For convenience,\n\ufb01les can be read a line at a time, using the readline method to read one line, or\nthereadlines method to return a list of all remaining lines. Files also support the\nfor-loop syntax, with iteration being line by line (e.g., forlineinfp:).\nWriting to a File\nWhen a \ufb01le proxy is writable, for example, if created with access mode\n w\nor\na\n, text can be written using methods write orwritelines . For example, if we de-\n\ufb01nefp = open(\n results.txt\n ,\nw\n), the syntax fp.write(\n Hello World.\\n\n )\nwrites a single line to the \ufb01le with the given string. Note well that write does not\nexplicitly add a trailing newline, so desired newline characters must be embeddeddirectly in the string parameter. Recall that the output of the print method can be\nredirected to a \ufb01le using a keyword parameter, as described in Section 1.6.", "1.7. Exception Handling 33\n1.7 Exception Handling\nExceptions are unexpected events that occur during the execution of a program.\nAn exception might result from a logical error or an unanticipated situation. In\nPython, exceptions (also known as errors ) are objects that are raised (orthrown )b y\ncode that encounters an unexpected circumstance. The Python interpreter can alsoraise an exception should it encounter an unexpected condition, like running out ofmemory. A raised error may be caught by a surrounding context that \u201chandles\u201d the\nexception in an appropriate fashion. If uncaught, an exception causes the interpreterto stop executing the program and to report an appropriate message to the console.In this section, we examine the most common error types in Python, the mechanismfor catching and handling errors that have been raised, and the syntax for raisingerrors from within user-de\ufb01ned blocks of code.\nCommon Exception Types\nPython includes a rich hierarchy of exception classes that designate various cate-gories of errors; Table 1.6 shows many of those classes. The Exception class serves\nas a base class for most other error types. An instance of the various subclassesencodes details about a problem that has occurred. Several of these errors may beraised in exceptional cases by behaviors introduced in this chapter. For example,\nuse of an unde\ufb01ned identi\ufb01er in an expression causes a NameError , and errant use\nof the dot notation, as in foo.bar(), will generate an AttributeError if object foo\ndoes not support a member named bar.\nClass\n Description\nException\n A base class for most error types\nAttributeError\n Raised by syntax obj.foo ,i fobjhas no member named foo\nEOFError\n Raised if \u201cend of \ufb01le\u201d reached for console or \ufb01le input\nIOError\n Raised upon failure of I/O operation (e.g., opening \ufb01le)\nIndexError\n Raised if index to sequence is out of bounds\nKeyError\n Raised if nonexistent key requested for set or dictionary\nKeyboardInterrupt\n Raised if user types ctrl-C while program is executing\nNameError\n Raised if nonexistent identi\ufb01er used\nStopIteration\n Raised by next(iterator) if no element; see Section 1.8\nTypeError\n Raised when wrong type of parameter is sent to a function\nValueError\n Raised when parameter has invalid value (e.g., sqrt(\u22125))\nZeroDivisionError\n Raised when any division operator used with 0 as divisor\nTable 1.6: Common exception classes in Python", "34 Chapter 1. Python Primer\nSending the wrong number, type, or value of parameters to a function is another\ncommon cause for an exception. For example, a call to abs(\n hello\n )will raise a\nTypeError because the parameter is not numeric, and a call to abs(3, 5) will raise\naTypeError because one parameter is expected. A ValueError is typically raised\nwhen the correct number and type of parameters are sent, but a value is illegitimate\nfor the context of the function. For example, the intconstructor accepts a string,\nas with int(\n 137\n),b u ta ValueError is raised if that string does not represent an\ninteger, as with int(\n 3.14\n )orint(\n hello\n ).\nPython\u2019s sequence types (e.g., list,tuple ,a n dstr) raise an IndexError when\nsyntax such as data[k] is used with an integer kthat is not a valid index for the given\nsequence (as described in Section 1.2.3). Sets and dictionaries raise a KeyError\nwhen an attempt is made to access a nonexistent element.\n1.7.1 Raising an Exception\nAn exception is thrown by executing the raise statement, with an appropriate in-\nstance of an exception class as an argument that designates the problem. For exam-ple, if a function for computing a square root is sent a negative value as a parameter,it can raise an exception with the command:\nraiseValueError(\nx cannot be negative\n )\nThis syntax raises a newly created instance of the ValueError class, with the error\nmessage serving as a parameter to the constructor. If this exception is not caughtwithin the body of the function, the execution of the function immediately ceasesand the exception is propagated to the calling context (and possibly beyond).\nWhen checking the validity of parameters sent to a function, it is customary\nto \ufb01rst verify that a parameter is of an appropriate type, and then to verify that ithas an appropriate value. For example, the sqrt function in Python\u2019s math library\nperforms error-checking that might be implemented as follows:\ndefsqrt(x):\nif not isinstance(x, ( int,\ufb02oat)):\nraiseTypeError(\nx must be numeric\n )\nelifx<0:\nraiseValueError(\n x cannot be negative\n )\n# do the real work here...\nChecking the type of an object can be performed at run-time using the built-infunction, isinstance . In simplest form, isinstance(obj, cls) returns True if object,\nobj, is an instance of class, cls, or any subclass of that type. In the above example, a\nmore general form is used with a tuple of allowable types indicated with the secondparameter. After con\ufb01rming that the parameter is numeric, the function enforces\nan expectation that the number be nonnegative, raising a ValueError otherwise.", "1.7. Exception Handling 35\nHow much error-checking to perform within a function is a matter of debate.\nChecking the type and value of each parameter demands additional execution time\nand, if taken to an extreme, seems counter to the nature of Python. Consider thebuilt-in sum function, which computes a sum of a collection of numbers. An im-\nplementation with rigorous error-checking might be written as follows:\ndefsum(values):\nif not isinstance(values, collections.Iterable):\nraiseTypeError(\nparameter must be an iterable type\n )\ntotal = 0forvinvalues:\nif not isinstance(v, ( int,\ufb02oat)):\nraiseTypeError(\nelements must be numeric\n )\ntotal = total+ v\nreturn total\nThe abstract base class, collections.Iterable , includes all of Python\u2019s iterable con-\ntainers types that guarantee support for the for-loop syntax (e.g., list,tuple ,set);\nwe discuss iterables in Section 1.8, and the use of modules, such as collections,i n\nSection 1.11. Within the body of the for loop, each element is veri\ufb01ed as numericbefore being added to the total. A far more direct and clear implementation of this\nfunction can be written as follows:\ndefsum(values):\ntotal = 0\nforvinvalues:\ntotal = total + v\nreturn total\nInterestingly, this simple implementation performs exactly like Python\u2019s built-inversion of the function. Even without the explicit checks, appropriate exceptions\nare raised naturally by the code. In particular, if values is not an iterable type, the\nattempt to use the for-loop syntax raises a TypeError reporting that the object is not\niterable. In the case when a user sends an iterable type that includes a nonnumer-\nical element, such as sum([3.14,\noops\n ]),aTypeError is naturally raised by the\nevaluation of expression total + v . The error message\nunsupported operand type(s) for +: \u2019float\u2019 and \u2019str\u2019\nshould be suf\ufb01ciently informative to the caller. Perhaps slightly less obvious is theerror that results from sum([\nalpha\n ,\nbeta\n ]). It will technically report a failed\nattempt to add an intandstr, due to the initial evaluation of total +\n alpha\n ,\nwhentotal has been initialized to 0.\nIn the remainder of this book, we tend to favor the simpler implementations\nin the interest of clean presentation, performing minimal error-checking in most\nsituations.", "36 Chapter 1. Python Primer\n1.7.2 Catching an Exception\nThere are several philosophies regarding how to cope with possible exceptional\ncases when writing code. For example, if a division x/yis to be computed, there\nis clear risk that a ZeroDivisionError will be raised when variable yhas value 0.I n\nan ideal situation, the logic of the program may dictate that yhas a nonzero value,\nthereby removing the concern for error. However, for more complex code, or in\na case where the value of ydepends on some external input to the program, there\nremains some possibility of an error.\nOne philosophy for managing exceptional cases is to \u201clook before you leap.\u201d\nThe goal is to entirely avoid the possibility of an exception being raised through\nthe use of a proactive conditional test. Revisiting our division example, we mightavoid the offending situation by writing:\nify! =0 :\nratio = x / y\nelse:\n... do something else ...\nA second philosophy, often embraced by Python programmers, is that \u201cit is\neasier to ask for forgiveness than it is to get permission.\u201d This quote is attributed\nto Grace Hopper, an early pioneer in computer science. The sentiment is that weneed not spend extra execution time safeguarding against every possible excep-tional case, as long as there is a mechanism for coping with a problem after itarises. In Python, this philosophy is implemented using a try-except control struc-\nture. Revising our \ufb01rst example, the division operation can be guarded as follows:\ntry:\nratio = x / y\nexcept ZeroDivisionError:\n... do something else ...\nIn this structure, the \u201ctry\u201d block is the primary code to be executed. Although itis a single command in this example, it can more generally be a larger block ofindented code. Following the try-block are one or more \u201cexcept\u201d cases, each with\nan identi\ufb01ed error type and an indented block of code that should be executed if the\ndesignated error is raised within the try-block.\nThe relative advantage of using a try-except structure is that the non-exceptional\ncase runs ef\ufb01ciently, without extraneous checks for the exceptional condition. How-ever, handling the exceptional case requires slightly more time when using a try-except structure than with a standard conditional statement. For this reason, thetry-except clause is best used when there is reason to believe that the exceptionalcase is relatively unlikely, or when it is prohibitively expensive to proactively eval-\nuate a condition to avoid the exception.", "1.7. Exception Handling 37\nException handling is particularly useful when working with user input, or\nwhen reading from or writing to \ufb01les, because such interactions are inherently less\npredictable. In Section 1.6.2, we suggest the syntax, fp = open(\n sample.txt\n ),\nfor opening a \ufb01le with read access. That command may raise an IOError for a vari-\nety of reasons, such as a non-existent \ufb01le, or lack of suf\ufb01cient privilege for openinga \ufb01le. It is signi\ufb01cantly easier to attempt the command and catch the resulting error\nthan it is to accurately predict whether the command will succeed.\nWe continue by demonstrating a few other forms of the try-except syntax. Ex-\nceptions are objects that can be examined when caught. To do so, an identi\ufb01er must\nbe established with a syntax as follows:\ntry:\nfp = open(\nsample.txt\n )\nexcept IOError ase:\nprint(\n Unable to open the file:\n ,e )\nIn this case, the name, e, denotes the instance of the exception that was thrown, and\nprinting it causes a detailed error message to be displayed (e.g., \u201c\ufb01le not found\u201d).\nA try-statement may handle more than one type of exception. For example,\nconsider the following command from Section 1.6.1:\nage = int(input(\n Enter your age in years:\n ))\nThis command could fail for a variety of reasons. The call to input will raise an\nEOFError if the console input fails. If the call to input completes successfully, the\nintconstructor raises a ValueError if the user has not entered characters represent-\ning a valid integer. If we want to handle two or more types of errors in the sameway, we can use a single except-statement, as in the following example:\nage = \u22121 # an initially invalid choice\nwhile age<=0 :\ntry:\nage = int(input(\nEnter your age in years:\n ))\nifage<=0 :\nprint(\n Your age must be positive\n )\nexcept (ValueError, EOFError):\nprint(\n Invalid response\n )\nWe use the tuple, (ValueError, EOFError) , to designate the types of errors that we\nwish to catch with the except-clause. In this implementation, we catch either error,print a response, and continue with another pass of the enclosing while loop. Wenote that when an error is raised within the try-block, the remainder of that bodyis immediately skipped. In this example, if the exception arises within the call toinput , or the subsequent call to the intconstructor, the assignment to agenever\noccurs, nor the message about needing a positive value. Because the value of age", "38 Chapter 1. Python Primer\nwill be unchanged, the while loop will continue. If we preferred to have the while\nloop continue without printing the\n Invalid response\n message, we could have\nwritten the exception-clause as\nexcept (ValueError, EOFError):\npass\nThe keyword, pass , is a statement that does nothing, yet it can serve syntactically\nas a body of a control structure. In this way, we quietly catch the exception, therebyallowing the surrounding while loop to continue.\nIn order to provide different responses to different types of errors, we may use\ntwo or more except-clauses as part of a try-structure. In our previous example, an\nEOFError suggests a more insurmountable error than simply an errant value being\nentered. In that case, we might wish to provide a more speci\ufb01c error message, orperhaps to allow the exception to interrupt the loop and be propagated to a higher\ncontext. We could implement such behavior as follows:\nage = \u22121 # an initially invalid choice\nwhile age<=0 :\ntry:\nage = int(input(\nEnter your age in years:\n ))\nifage<=0 :\nprint(\n Your age must be positive\n )\nexcept ValueError:\nprint(\n That is an invalid age specification\n )\nexcept EOFError:\nprint(\n There was an unexpected error reading input.\n )\nraise #l e t\ns re-raise this exception\nIn this implementation, we have separate except-clauses for the ValueError and\nEOFError cases. The body of the clause for handling an EOFError relies on another\ntechnique in Python. It uses the raise statement without any subsequent argument,\nto re-raise the same exception that is currently being handled. This allows us to\nprovide our own response to the exception, and then to interrupt the while loop andpropagate the exception upward.\nIn closing, we note two additional features of try-except structures in Python.\nIt is permissible to have a \ufb01nal except-clause without any identi\ufb01ed error types,\nusing syntax except :, to catch any other exceptions that occurred. However, this\ntechnique should be used sparingly, as it is dif\ufb01cult to suggest how to handle an\nerror of an unknown type. A try-statement can have a \ufb01nally clause, with a body of\ncode that will always be executed in the standard or exceptional cases, even whenan uncaught or re-raised exception occurs. That block is typically used for critical\ncleanup work, such as closing an open \ufb01le.", "1.8. Iterators and Generators 39\n1.8 Iterators and Generators\nIn Section 1.4.2, we introduced the for-loop syntax beginning as:\nfor element initerable :\nand we noted that there are many types of objects in Python that qualify as being\niterable. Basic container types, such as list,tuple ,a n dset, qualify as iterable types.\nFurthermore, a string can produce an iteration of its characters, a dictionary can\nproduce an iteration of its keys, and a \ufb01le can produce an iteration of its lines. User-\nde\ufb01ned types may also support iteration. In Python, the mechanism for iteration isbased upon the following conventions:\n\u2022Aniterator is an object that manages an iteration through a series of values. If\nvariable, i, identi\ufb01es an iterator object, then each call to the built-in function,\nnext(i) , produces a subsequent element from the underlying series, with a\nStopIteration exception raised to indicate that there are no further elements.\n\u2022Aniterable is an object, obj, that produces an iterator via the syntax iter(obj) .\nBy these de\ufb01nitions, an instance of a listis an iterable, but not itself an iterator.\nWithdata = [1, 2, 4, 8] , it is not legal to call next(data) . However, an iterator\nobject can be produced with syntax, i = iter(data) , and then each subsequent call\ntonext(i) will return an element of that list. The for-loop syntax in Python simply\nautomates this process, creating an iterator for the give iterable, and then repeatedly\ncalling for the next element until catching the StopIteration exception.\nMore generally, it is possible to create multiple iterators based upon the same\niterable object, with each iterator maintaining its own state of progress. However,\niterators typically maintain their state with indirect reference back to the original\ncollection of elements. For example, calling iter(data) on a list instance produces\nan instance of the list\niterator class. That iterator does not store its own copy of the\nlist of elements. Instead, it maintains a current index into the original list, represent-\ning the next element to be reported. Therefore, if the contents of the original list\nare modi\ufb01ed after the iterator is constructed, but before the iteration is complete,\nthe iterator will be reporting the updated contents of the list.\nPython also supports functions and classes that produce an implicit iterable se-\nries of values, that is, without constructing a data structure to store all of its values\nat once. For example, the call range(1000000) does notreturn a list of numbers; it\nreturns a range object that is iterable. This object generates the million values one\nat a time, and only as needed. Such a lazy evaluation technique has great advan-\ntage. In the case of range, it allows a loop of the form, forjinrange(1000000): ,\nto execute without setting aside memory for storing one million values. Also, ifsuch a loop were to be interrupted in some fashion, no time will have been spent\ncomputing unused values of the range.", "40 Chapter 1. Python Primer\nWe see lazy evaluation used in many of Python\u2019s libraries. For example, the\ndictionary class supports methods keys() ,values() ,a n ditems() , which respec-\ntively produce a \u201cview\u201d of all keys, values, or (key,value) pairs within a dictionary.\nNone of these methods produces an explicit list of results. Instead, the views thatare produced are iterable objects based upon the actual contents of the dictionary.An explicit list of values from such an iteration can be immediately constructed by\ncalling the listclass constructor with the iteration as a parameter. For example, the\nsyntax list(range(1000)) produces a list instance with values from 0 to 999, while\nthe syntax list(d.values()) produces a list that has elements based upon the current\nvalues of dictionary d. We can similarly construct a tuple orsetinstance based\nupon a given iterable.\nGenerators\nIn Section 2.3.4, we will explain how to de\ufb01ne a class whose instances serve as\niterators. However, the most convenient technique for creating iterators in Python\nis through the use of generators . A generator is implemented with a syntax that\nis very similar to a function, but instead of returning values, a yield statement is\nexecuted to indicate each element of the series. As an example, consider the goal\nof determining all factors of a positive integer. For example, the number 100 hasfactors 1, 2, 4, 5, 10, 20, 25, 50, 100. A traditional function might produce and\nreturn a list containing all factors, implemented as:\ndeffactors(n): # traditional function that computes factors\nresults = [ ] # store factors in a new list\nforkinrange(1,n+1):\nifn%k= =0 : # divides evenly, thus k is a factor\nresults.append(k) # add k to the list of factors\nreturn results # return the entire list\nIn contrast, an implementation of a generator for computing those factors could be\nimplemented as follows:\ndeffactors(n): # generator that computes factors\nforkinrange(1,n+1):\nifn%k= =0 : # divides evenly, thus k is a factor\nyieldk # yield this factor as next result\nNotice use of the keyword yield rather than return to indicate a result. This indi-\ncates to Python that we are de\ufb01ning a generator, rather than a traditional function. It\nis illegal to combine yield andreturn statements in the same implementation, other\nthan a zero-argument return statement to cause a generator to end its execution. If\na programmer writes a loop such as forfactorinfactors(100): , an instance of our\ngenerator is created. For each iteration of the loop, Python executes our procedure", "1.8. Iterators and Generators 41\nuntil a yield statement indicates the next value. At that point, the procedure is tem-\nporarily interrupted, only to be resumed when another value is requested. When\nthe \ufb02ow of control naturally reaches the end of our procedure (or a zero-argumentreturn statement), a StopIteration exception is automatically raised. Although this\nparticular example uses a single yield statement in the source code, a generator can\nrely on multiple yield statements in different constructs, with the generated series\ndetermined by the natural \ufb02ow of control. For example, we can greatly improvethe ef\ufb01ciency of our generator for computing factors of a number, n, by only test-\ning values up to the square root of that number, while reporting the factor n//k\nthat is associated with each k(unless n//kequals k). We might implement such a\ngenerator as follows:\ndeffactors(n): # generator that computes factors\nk=1while k\nk<n: # while k <sqrt(n)\nifn%k= =0 :\nyieldk\nyieldn/ /k\nk+ =1\nifk\nk= =n : # special case if n is perfect square\nyieldk\nWe should note that this generator differs from our \ufb01rst version in that the factorsare not generated in strictly increasing order. For example, factors(100) generates\nthe series 1 ,100,2,50,4,25,5,20,10.\nIn closing, we wish to emphasize the bene\ufb01ts of lazy evaluation when using a\ngenerator rather than a traditional function. The results are only computed if re-quested, and the entire series need not reside in memory at one time. In fact, a\ngenerator can effectively produce an in\ufb01nite series of values. As an example, the\nFibonacci numbers form a classic mathematical sequence, starting with value 0,then value 1, and then each subsequent value being the sum of the two precedingvalues. Hence, the Fibonacci series begins as: 0 ,1,1,2,3,5,8,13,.... The follow-\ning generator produces this in\ufb01nite series.\ndef\ufb01bonacci():\na=0b=1\nwhile True : # keep going...\nyielda # report value, a, during this pass\nf u t u r e=a+b\na=b # this will be next value reported\nb = future # and subsequently this", "42 Chapter 1. Python Primer\n1.9 Additional Python Conveniences\nIn this section, we introduce several features of Python that are particularly conve-\nnient for writing clean, concise code. Each of these syntaxes provide functionality\nthat could otherwise be accomplished using functionality that we have introduced\nearlier in this chapter. However, at times, the new syntax is a more clear and directexpression of the logic.\n1.9.1 Conditional Expressions\nPython supports a conditional expression syntax that can replace a simple control\nstructure. The general syntax is an expression of the form:\nexpr1ifcondition else expr2\nThis compound expression evaluates to expr1 if the condition is true, and otherwise\nevaluates to expr2 . For those familiar with Java or C++, this is equivalent to the\nsyntax, condition ?expr1 :expr2 , in those languages.\nAs an example, consider the goal of sending the absolute value of a variable, n,\nto a function (and without relying on the built-in absfunction, for the sake of ex-\nample). Using a traditional control structure, we might accomplish this as follows:\nifn>=0 :\nparam = n\nelse:\nparam = \u2212n\nresult = foo(param) # call the function\nWith the conditional expression syntax, we can directly assign a value to variable,param , as follows:\nparam = n ifn>=0else\u2212n # pick the appropriate value\nresult = foo(param) # call the function\nIn fact, there is no need to assign the compound expression to a variable. A condi-\ntional expression can itself serve as a parameter to the function, written as follows:\nresult = foo(n ifn>=0else\u2212n)\nSometimes, the mere shortening of source code is advantageous because it\navoids the distraction of a more cumbersome control structure. However, we rec-\nommend that a conditional expression be used only when it improves the readabilityof the source code, and when the \ufb01rst of the two options is the more \u201cnatural\u201d case,given its prominence in the syntax. (We prefer to view the alternative value as more\nexceptional.)", "1.9. Additional Python Conveniences 43\n1.9.2 Comprehension Syntax\nA very common programming task is to produce one series of values based upon\nthe processing of another series. Often, this task can be accomplished quite simplyin Python using what is known as a comprehension syntax . We begin by demon-\nstrating list comprehension , as this was the \ufb01rst form to be supported by Python.\nIts general form is as follows:\n[expression for valueiniterable ifcondition ]\nWe note that both expression and condition may depend on value , and that the\nif-clause is optional. The evaluation of the comprehension is logically equivalentto the following traditional control structure for computing a resulting list:\nresult = [ ]for valueiniterable :\nifcondition:\nresult.append( expression )\nAs a concrete example, a list of the squares of the numbers from 1 to n,t h a ti s\n[1,4,9,16,25,..., n\n2], can be created by traditional means as follows:\nsquares = [ ]\nforkinrange(1, n+1):\nsquares.append(k\n k)\nWith list comprehension, this logic is expressed as follows:\nsquares = [k\n kforkinrange(1, n+1)]\nAs a second example, Section 1.8 introduced the goal of producing a list of factors\nfor an integer n. That task is accomplished with the following list comprehension:\nfactors = [k forkinrange(1,n+1) ifn%k= =0 ]\nPython supports similar comprehension syntaxes that respectively produce a\nset, generator, or dictionary. We compare those syntaxes using our example forproducing the squares of numbers.\n[k\nkforkinrange(1, n+1) ] list comprehension\n{k\nkforkinrange(1, n+1) } set comprehension\n(k\nkforkinrange(1, n+1) ) generator comprehension\n{k:k\nkforkinrange(1, n+1) } dictionary comprehension\nThe generator syntax is particularly attractive when results do not need to be storedin memory. For example, to compute the sum of the \ufb01rst nsquares, the genera-\ntor syntax, total = sum(k\nkforkinrange(1, n+1)) , is preferred to the use of an\nexplicitly instantiated list comprehension as the parameter.", "44 Chapter 1. Python Primer\n1.9.3 Packing and Unpacking of Sequences\nPython provides two additional conveniences involving the treatment of tuples and\nother sequence types. The \ufb01rst is rather cosmetic. If a series of comma-separatedexpressions are given in a larger context, they will be treated as a single tuple, evenif no enclosing parentheses are provided. For example, the assignment\nd a t a=2 ,4 ,6 ,8\nresults in identi\ufb01er, data, being assigned to the tuple ( 2 ,4 ,6 ,8 ) .T h i s b e h a v i o r\nis called automatic packing of a tuple. One common use of packing in Python is\nwhen returning multiple values from a function. If the body of a function executesthe command,\nreturn x, y\nit will be formally returning a single object that is the tuple (x, y) .\nAs a dual to the packing behavior, Python can automatically unpack a se-\nquence, allowing one to assign a series of individual identi\ufb01ers to the elementsof sequence. As an example, we can write\na, b, c, d = range(7, 11)\nwhich has the effect of assigning a=7,b=8,c=9,a n dd=10 , as those are the four\nvalues in the sequence returned by the call to range . For this syntax, the right-hand\nside expression can be any iterable type, as long as the number of variables on the\nleft-hand side is the same as the number of elements in the iteration.\nThis technique can be used to unpack tuples returned by a function. For exam-\nple, the built-in function, divmod(a, b) , returns the pair of values (a // b, a % b)\nassociated with an integer division. Although the caller can consider the return\nvalue to be a single tuple, it is possible to write\nquotient, remainder = divmod(a, b)\nto separately identify the two entries of the returned tuple. This syntax can also beused in the context of a for loop, when iterating over a sequence of iterables, as in\nforx, yin[ (7, 2), (5, 8), (6, 4) ]:\nIn this example, there will be three iterations of the loop. During the \ufb01rst pass, x=7\nandy=2, and so on. This style of loop is quite commonly used to iterate through\nkey-value pairs that are returned by the items() method of the dictclass, as in:\nfork, vinmapping.items():", "1.9. Additional Python Conveniences 45\nSimultaneous Assignments\nThe combination of automatic packing and unpacking forms a technique known\nassimultaneous assignment , whereby we explicitly assign a series of values to a\nseries of identi\ufb01ers, using a syntax:\nx, y, z = 6, 2, 5\nIn effect, the right-hand side of this assignment is automatically packed into a tuple,and then automatically unpacked with its elements assigned to the three identi\ufb01erson the left-hand side.\nWhen using a simultaneous assignment, all of the expressions are evaluated\non the right-hand side before any of the assignments are made to the left-handvariables. This is signi\ufb01cant, as it provides a convenient means for swapping thevalues associated with two variables:\nj, k = k, j\nWith this command, jwill be assigned to the oldvalue of k,a n dkwill be assigned\nto the oldvalue of j. Without simultaneous assignment, a swap typically requires\nmore delicate use of a temporary variable, such as\ntemp = jj=kk=t e m p\nWith the simultaneous assignment, the unnamed tuple representing the packed val-ues on the right-hand side implicitly serves as the temporary variable when per-\nforming such a swap.\nThe use of simultaneous assignments can greatly simplify the presentation of\ncode. As an example, we reconsider the generator on page 41 that produces the\nFibonacci series. The original code requires separate initialization of variables a\nandbto begin the series. Within each pass of the loop, the goal was to reassign a\nandb, respectively, to the values of banda+b. At the time, we accomplished this\nwith brief use of a third variable. With simultaneous assignments, that generatorcan be implemented more directly as follows:\ndef\ufb01bonacci():\na, b = 0, 1while True :\nyielda\na, b = b, a+b", "46 Chapter 1. Python Primer\n1.10 Scopes and Namespaces\nWhen computing a sum with the syntax x+y in Python, the names xandymust\nhave been previously associated with objects that serve as values; a NameError\nwill be raised if no such de\ufb01nitions are found. The process of determining the\nvalue associated with an identi\ufb01er is known as name resolution .\nWhenever an identi\ufb01er is assigned to a value, that de\ufb01nition is made with a\nspeci\ufb01c scope . Top-level assignments are typically made in what is known as global\nscope. Assignments made within the body of a function typically have scope that is\nlocal to that function call. Therefore, an assignment, x=5 , within a function has\nno effect on the identi\ufb01er, x, in the broader scope.\nEach distinct scope in Python is represented using an abstraction known as a\nnamespace . A namespace manages all identi\ufb01ers that are currently de\ufb01ned in a\ngiven scope. Figure 1.8 portrays two namespaces, one being that of a caller to our\ncount function from Section 1.5, and the other being the local namespace during\nthe execution of that function.\nA-\nstr\nA\nstr\nCS\n\ufb02oat\n3.56int\n2\nitemdatagrades\nmajorgpatargetn\nlist\nstr\nB+\nstr\nA-\nstr\nFigure 1.8: A portrayal of the two namespaces associated with a user\u2019s call\ncount(grades,\n A\n), as de\ufb01ned in Section 1.5. The left namespace is the caller\u2019s\nand the right namespace represents the local scope of the function.\nPython implements a namespace with its own dictionary that maps each iden-\ntifying string (e.g.,\n n\n) to its associated value. Python provides several ways to\nexamine a given namespace. The function, dir, reports the names of the identi\ufb01ers\nin a given namespace (i.e., the keys of the dictionary), while the function, vars,\nreturns the full dictionary. By default, calls to dir() andvars() report on the most\nlocally enclosing namespace in which they are executed.", "1.10. Scopes and Namespaces 47\nWhen an identi\ufb01er is indicated in a command, Python searches a series of\nnamespaces in the process of name resolution. First, the most locally enclosing\nscope is searched for a given name. If not found there, the next outer scope issearched, and so on. We will continue our examination of namespaces, in Sec-tion 2.5, when discussing Python\u2019s treatment of object-orientation. We will seethat each object has its own namespace to store its attributes, and that classes each\nhave a namespace as well.\nFirst-Class Objects\nIn the terminology of programming languages, \ufb01rst-class objects are instances of\na type that can be assigned to an identi\ufb01er, passed as a parameter, or returned by\na function. All of the data types we introduced in Section 1.2.3, such as intand\nlist, are clearly \ufb01rst-class types in Python. In Python, functions and classes are also\ntreated as \ufb01rst-class objects. For example, we could write the following:\nscream = print # assign name \u2019scream\u2019 to the function denoted as \u2019print\u2019\nscream(\n Hello\n )# call that function\nIn this case, we have not created a new function, we have simply de\ufb01ned scream\nas an alias for the existing print function. While there is little motivation for pre-\ncisely this example, it demonstrates the mechanism that is used by Python to al-\nlow one function to be passed as a parameter to another. On page 28, we noted\nthat the built-in function, max, accepts an optional keyword parameter to specify\na non-default order when computing a maximum. For example, a caller can use\nthe syntax, max(a, b, key=abs) , to determine which value has the larger absolute\nvalue. Within the body of that function, the formal parameter, key, is an identi\ufb01er\nthat will be assigned to the actual parameter, abs.\nIn terms of namespaces, an assignment such as scream = print , introduces the\nidenti\ufb01er, scream , into the current namespace, with its value being the object that\nrepresents the built-in function, print . The same mechanism is applied when a user-\nde\ufb01ned function is declared. For example, our count function from Section 1.5\nbeings with the following syntax:\ndefcount(data, target):\n...\nSuch a declaration introduces the identi\ufb01er, count , into the current namespace,\nwith the value being a function instance representing its implementation. In similarfashion, the name of a newly de\ufb01ned class is associated with a representation of\nthat class as its value. (Class de\ufb01nitions will be introduced in the next chapter.)", "48 Chapter 1. Python Primer\n1.11 Modules and the Import Statement\nWe have already introduced many functions (e.g., max) and classes (e.g., list)\nthat are de\ufb01ned within Python\u2019s built-in namespace. Depending on the version of\nPython, there are approximately 130\u2013150 de\ufb01nitions that were deemed signi\ufb01cantenough to be included in that built-in namespace.\nBeyond the built-in de\ufb01nitions, the standard Python distribution includes per-\nhaps tens of thousands of other values, functions, and classes that are organized inadditional libraries, known as modules , that can be imported from within a pro-\ngram. As an example, we consider the math module. While the built-in namespace\nincludes a few mathematical functions (e.g., abs,min,max,round ), many more\nare relegated to the math module (e.g., sin,cos,sqrt). That module also de\ufb01nes\napproximate values for the mathematical constants, piande.\nPython\u2019s import statement loads de\ufb01nitions from a module into the current\nnamespace. One form of an import statement uses a syntax such as the following:\nfrommathimport pi, sqrt\nThis command adds both piandsqrt, as de\ufb01ned in the math module, into the cur-\nrent namespace, allowing direct use of the identi\ufb01er, pi, or a call of the function,\nsqrt(2) . If there are many de\ufb01nitions from the same module to be imported, an\nasterisk may be used as a wild card, as in, frommathimport\n, but this form\nshould be used sparingly. The danger is that some of the names de\ufb01ned in the mod-ule may con\ufb02ict with names already in the current namespace (or being importedfrom another module), and the import causes the new de\ufb01nitions to replace existingones.\nAnother approach that can be used to access many de\ufb01nitions from the same\nmodule is to import the module itself, using a syntax such as:\nimport math\nFormally, this adds the identi\ufb01er, math , to the current namespace, with the module\nas its value. (Modules are also \ufb01rst-class objects in Python.) Once imported, in-dividual de\ufb01nitions from the module can be accessed using a fully-quali\ufb01ed name,\nsuch as math.pi ormath.sqrt(2) .\nCreating a New Module\nTo create a new module, one simply has to put the relevant de\ufb01nitions in a \ufb01le\nnamed with a .py suf\ufb01x. Those de\ufb01nitions can be imported from any other .py\n\ufb01le within the same project directory. For example, if we were to put the de\ufb01nitionof our count function (see Section 1.5) into a \ufb01le named utility.py, we could\nimport that function using the syntax, fromutilityimport count .", "1.11. Modules and the Import Statement 49\nIt is worth noting that top-level commands with the module source code are\nexecuted when the module is \ufb01rst imported, almost as if the module were its own\nscript. There is a special construct for embedding commands within the modulethat will be executed if the module is directly invoked as a script, but not whenthe module is imported from another script. Such commands should be placed in abody of a conditional statement of the following form,\nif\nname\n ==\n __main__\n :\nUsing our hypothetical utility.py module as an example, such commands will\nbe executed if the interpreter is started with a command python utility.py,b u t\nnot when the utility module is imported into another context. This approach is often\nused to embed what are known as unit tests within the module; we will discuss unit\ntesting further in Section 2.2.4.\n1.11.1 Existing Modules\nTable 1.7 provides a summary of a few available modules that are relevant to a\nstudy of data structures. We have already discussed the math module brie\ufb02y. In the\nremainder of this section, we highlight another module that is particularly important\nfor some of the data structures and algorithms that we will study later in this book.\nExisting Modules\nModule Name\n Description\narray\n Provides compact array storage for primitive types.\ncollections\nDe\ufb01nes additional data structures and abstract base classes\ninvolving collections of objects.\ncopy\n De\ufb01nes general functions for m aking copies of objects.\nheapq\n Provides heap-based priority queue functions (see Section 9.3.7).\nmath\n De\ufb01nes common mathematical constants and functions.\nos\n Provides support for interactions with the operating system.\nrandom\n Provides random number generation.\nre\n Provides support for processing regular expressions.\nsys\n Provides additional level of in teraction with the Python interpreter.\ntime\n Provides support for measuring time, or delaying a program.\nTable 1.7: Some existing Python modules relevant to data structures and algorithms.\nPseudo-Random Number Generation\nPython\u2019s random module provides the ability to generate pseudo-random numbers,\nthat is, numbers that are statistically random (but not necessarily truly random).\nApseudo-random number generator uses a deterministic formula to generate the", "50 Chapter 1. Python Primer\nnext number in a sequence based upon one or more past numbers that it has gen-\nerated. Indeed, a simple yet popular pseudo-random number generator chooses its\nnext number based solely on the most recently chosen number and some additionalparameters using the following formula.\nnext =(a*current +b)%n;\nwhere a,b,a n d nare appropriately chosen integers. Python uses a more advanced\ntechnique known as a Mersenne twister . It turns out that the sequences generated\nby these techniques can be proven to be statistically uniform, which is usually\ngood enough for most applications requiring random numbers, such as games. For\napplications, such as computer security settings, where one needs unpredictablerandom sequences, this kind of formula should not be used. Instead, one shouldideally sample from a source that is actually random, such as radio static coming\nfrom outer space.\nSince the next number in a pseudo-random generator is determined by the pre-\nvious number(s), such a generator always needs a place to start, which is called its\nseed. The sequence of numbers generated for a given seed will always be the same.\nOne common trick to get a different sequence each time a program is run is to use\na seed that will be different for each run. For example, we could use some timed\ninput from a user or the current system time in milliseconds.\nPython\u2019s random module provides support for pseudo-random number gener-\nation by de\ufb01ning a Random class; instances of that class serve as generators with\nindependent state. This allows different aspects of a program to rely on their ownpseudo-random number generator, so that calls to one generator do not affect thesequence of numbers produced by another. For convenience, all of the methodssupported by the Random class are also supported as stand-alone functions of the\nrandom module (essentially using a single generator instance for all top-level calls).\nSyntax\n Description\nseed(hashable)\nInitializes the pseudo-random number generator\nbased upon the hash value of the parameter\nrandom()\nReturns a pseudo-random \ufb02oating-point\nvalue in the interval [0.0,1.0).\nrandint(a,b)\nReturns a pseudo-random integer\nin the closed interval [a,b].\nrandrange(start, stop, step)\nReturns a pseudo-random integer in the standard\nPython range indicated by the parameters.\nchoice(seq)\nReturns an element of the given sequence\nchosen pseudo-randomly.\nshu\ufb04e(seq)\nReorders the elements of the given\nsequence pseudo-randomly.\nTable 1.8: Methods supported by instances of the Random class, and as top-level\nfunctions of the random module.", "1.12. Exercises 51\n1.12 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-1.1 Write a short Python function, is\nmultiple(n, m) , that takes two integer\nvalues and returns True ifnis a multiple of m,t h a ti s , n=mifor some\ninteger i,a n dFalse otherwise.\nR-1.2 Write a short Python function, is\neven(k) , that takes an integer value and\nreturns True ifkis even, and False otherwise. However, your function\ncannot use the multiplication, modulo, or division operators.\nR-1.3 Write a short Python function, minmax(data) , that takes a sequence of\none or more numbers, and returns the smallest and largest numbers, in the\nform of a tuple of length two. Do not use the built-in functions min or\nmax in implementing your solution.\nR-1.4 Write a short Python function that takes a positive integer nand returns\nthe sum of the squares of all the positive integers smaller than n.\nR-1.5 Give a single command that computes the sum from Exercise R-1.4, rely-ing on Python\u2019s comprehension syntax and the built-in sum function.\nR-1.6 Write a short Python function that takes a positive integer nand returns\nthe sum of the squares of all the odd positive integers smaller than n.\nR-1.7 Give a single command that computes the sum from Exercise R-1.6, rely-ing on Python\u2019s comprehension syntax and the built-in sum function.\nR-1.8 Python allows negative integers to be used as indices into a sequence,such as a string. If string shas length n, and expression s[k]is used for in-\ndex\u2212n\u2264k<0, what is the equivalent index j\u22650 such that s[j]references\nthe same element?\nR-1.9 What parameters should be sent to the range constructor, to produce a\nrange with values 50, 60, 70, 80?\nR-1.10 What parameters should be sent to the range constructor, to produce a\nrange with values 8, 6, 4, 2, 0, \u22122,\u22124,\u22126,\u22128?\nR-1.11 Demonstrate how to use Python\u2019s list comprehension syntax to producethe list [1, 2, 4, 8, 16, 32, 64, 128, 256] .\nR-1.12 Python\u2019s random module includes a function choice(data) that returns a\nrandom element from a non-empty sequence. The random module in-\ncludes a more basic function randrange , with parameterization similar to\nthe built-in range function,\n that return a random choice from the given\nrange. Using only the randrange function, implement your own version\nof the choice function.", "52 Chapter 1. Python Primer\nCreativity\nC-1.13 Write a pseudo-code description of a function that reverses a list of n\nintegers, so that the numbers are listed in the opposite order than they\nwere before, and compare this method to an equivalent Python function\nfor doing the same thing.\nC-1.14 Write a short Python function that takes a sequence of integer values and\ndetermines if there is a distinct pair of numbers in the sequence whose\nproduct is odd.\nC-1.15 Write a Python function that takes a sequence of numbers and determinesif all the numbers are different from each other (that is, they are distinct).\nC-1.16 In our implementation of the scale function (page 25), the body of the loop\nexecutes the command data[j]\n=f a c t o r . We have discussed that numeric\ntypes are immutable, and that use of the\n =operator in this context causes\nthe creation of a new instance (not the mutation of an existing instance).How is it still possible, then, that our implementation of scale changes the\nactual parameter sent by the caller?\nC-1.17 Had we implemented the scale function (page 25) as follows, does it work\nproperly?\ndefscale(data, factor):\nforvalindata:\nval\n=f a c t o r\nExplain why or why not.\nC-1.18 Demonstrate how to use Python\u2019s list comprehension syntax to producethe list [0, 2, 6, 12, 20, 30, 42, 56, 72, 90] .\nC-1.19 Demonstrate how to use Python\u2019s list comprehension syntax to produce\nthe list [\na\n,\nb\n,\nc\n, ...,\n z\n], but without having to type all 26 such\ncharacters literally.\nC-1.20 Python\u2019s random module includes a function shu\ufb04e(data) that accepts a\nlist of elements and randomly reorders the elements so that each possi-\nble order occurs with equal probability. The random module includes a\nmore basic function randint(a, b) that returns a uniformly random integer\nfrom atob(including both endpoints). Using only the randint function,\nimplement your own version of the shu\ufb04e function.\nC-1.21 Write a Python program that repeatedly reads lines from standard inputuntil an EOFError is raised, and then outputs those lines in reverse order\n(a user can indicate end of input by typing ctrl-D).", "1.12. Exercises 53\nC-1.22 Write a short Python program that takes two arrays aandbof length n\nstoring intvalues, and returns the dot product of aandb. That is, it returns\nan array cof length nsuch that c[i]=a[i]\u00b7b[i],f o r i=0,..., n\u22121.\nC-1.23 Give an example of a Python code fragment that attempts to write an ele-\nment to a list based on an index that may be out of bounds. If that indexis out of bounds, the program should catch the exception that results, andprint the following error message:\u201cDon\u2019t try buffer overflow attacks in Python!\u201d\nC-1.24 Write a short Python function that counts the number of vowels in a givencharacter string.\nC-1.25 Write a short Python function that takes a string s, representing a sentence,\nand returns a copy of the string with all punctuation removed. For exam-ple, if given the string \"Let\ns try, Mike.\" , this function would return\n\"Lets try Mike\" .\nC-1.26 Write a short program that takes as input three integers, a,b,a n d c, from\nthe console and determines if they can be used in a correct arithmeticformula (in the given order), like \u201c a+b=c,\u201d \u201ca=b\u2212c,\u201d or \u201c a\u2217b=c.\u201d\nC-1.27 In Section 1.8, we provided three different implementations of a generatorthat computes factors of a given integer. The third of those implementa-tions, from page 41, was the most ef\ufb01cient, but we noted that it did not\nyield the factors in increasing order. Modify the generator so that it reports\nfactors in increasing order, while maintaining its general performance ad-vantages.\nC-1.28 Thep-norm of a vector v=(v\n1,v2,..., vn)inn-dimensional space is de-\n\ufb01ned as\n/bardblv/bardbl=p/radicalBig\nvp\n1+vp\n2+\u00b7\u00b7\u00b7+vp\nn.\nFor the special case of p=2, this results in the traditional Euclidean\nnorm , which represents the length of the vector. For example, the Eu-\nclidean norm of a two-dimensional vector with coordinates (4,3)has a\nEuclidean norm of\u221a\n42+32=\u221a\n16+9=\u221a\n25=5. Give an implemen-\ntation of a function named norm such that norm(v, p) returns the p-norm\nvalue of vandnorm(v) returns the Euclidean norm of v. You may assume\nthatvis a list of numbers.", "54 Chapter 1. Python Primer\nProjects\nP-1.29 Write a Python program that outputs all possible strings formed by using\nthe characters\n c\n,\na\n,\nt\n,\nd\n,\no\n,a n d\n g\nexactly once.\nP-1.30 Write a Python program that can take a positive integer greater than 2 asinput and write out the number of times one must repeatedly divide this\nnumber by 2 before getting a value less than 2.\nP-1.31 Write a Python program that can \u201cmake change.\u201d Your program shouldtake two numbers as input, one that is a monetary amount charged and the\nother that is a monetary amount given. It should then return the numberof each kind of bill and coin to give back as change for the differencebetween the amount given and the amount charged. The values assigned\nto the bills and coins can be based on the monetary system of any current\nor former government. Try to design your program so that it returns asfew bills and coins as possible.\nP-1.32 Write a Python program that can simulate a simple calculator, using theconsole as the exclusive input and output device. That is, each input to thecalculator, be it a number, like 12.34 or1034, or an operator, like +or=,\ncan be done on a separate line. After each such input, you should output\nto the Python console what would be displayed on your calculator.\nP-1.33 Write a Python program that simulates a handheld calculator. Your pro-gram should process input from the Python console representing buttons\nthat are \u201cpushed,\u201d and then output the contents of the screen after each op-eration is performed. Minimally, your calculator should be able to processthe basic arithmetic operations and a reset/clear operation.\nP-1.34 A common punishment for school children is to write out a sentence mul-tiple times. Write a Python stand-alone program that will write out the\nfollowing sentence one hundred times: \u201cI will never spam my friends\nagain.\u201d Your program should number each of the sentences and it shouldmake eight different random-looking typos.\nP-1.35 Thebirthday paradox says that the probability that two people in a room\nwill have the same birthday is more than half, provided n, the number of\npeople in the room, is more than 23. This property is not really a paradox,but many people \ufb01nd it surprising. Design a Python program that can test\nthis paradox by a series of experiments on randomly generated birthdays,\nwhich test this paradox for n=5,10,15,20,..., 100.\nP-1.36 Write a Python program that inputs a list of words, separated by white-space, and outputs how many times each word appears in the list. Youneed not worry about ef\ufb01ciency at this point, however, as this topic issomething that will be addressed later in this book.", "Chapter Notes 55\nChapter Notes\nThe of\ufb01cial Python Web site ( http://www.python.org ) has a wealth of information, in-\ncluding a tutorial and full documentation of the built-in functions, classes, and standard\nmodules. The Python interpreter is itself a useful reference, as the interactive command\nhelp(foo) provides documentation for any function, class, or module that fooidenti\ufb01es.\nBooks providing an introduction to programming in Python include titles authored by\nCampbell et al. [22], Cedar [25], Dawson [32], Goldwasser and Letscher [43], Lutz [72],\nPerkovic [82], and Zelle [105]. More comp lete reference books on Python include titles by\nBeazley [12], and Summer\ufb01eld [91].", "Chapter\n2Object-Oriented Programming\nContents\n2 . 1 G o a l s ,P r i n c i p l e s ,a n dP a t t e r n s................ 5 7\n2 . 1 . 1 O b j e c t - O r i e n t e d D e s i g n G o a l s ............... 5 7\n2 . 1 . 2 O b j e c t - O r i e n t e d D e s i g n P r i n c i p l e s ............. 5 8\n2 . 1 . 3 D e s i g n P a t t e r n s....................... 6 1\n2 . 2 S o f t w a r eD e v e l o p m e n t .................... 6 2\n2 . 2 . 1 D e s i g n............................ 6 22 . 2 . 2 P s e u d o - C o d e ........................ 6 4\n2 . 2 . 3 C o d i n g S t y l e a n d D o c u m e n t a t i o n .............. 6 42 . 2 . 4 T e s t i n g a n d D e b u g g i n g................... 6 7\n2 . 3 C l a s sD e \ufb01 n i t i o n s........................ 6 9\n2 . 3 . 1 E x a m p l e :C r e d i t C a r dC l a s s ................. 6 9\n2.3.2 Operator Overloading and Python\u2019s Special Methods . . . 74\n2 . 3 . 3 E x a m p l e :M u l t i d i m e n s i o n a l V e c t o r C l a s s.......... 7 7\n2 . 3 . 4 I t e r a t o r s........................... 7 9\n2 . 3 . 5 E x a m p l e :R a n g e C l a s s .................... 8 0\n2 . 4 I n h e r i t a n c e........................... 8 2\n2 . 4 . 1 E x t e n d i n g t h e C r e d i t C a r d C l a s s............... 8 3\n2 . 4 . 2 H i e r a r c h y o f N u m e r i c P r o g r e s s i o n s............. 8 7\n2 . 4 . 3 A b s t r a c t B a s e C l a s s e s.................... 9 3\n2 . 5 N a m e s p a c e sa n dO b j e c t - O r i e n t a t i o n............. 9 6\n2 . 5 . 1 I n s t a n c e a n d C l a s s N a m e s p a c e s ............... 9 62.5.2 Name Resolution and Dynamic Dispatch . . . . . . . . . . 100\n2 . 6 S h a l l o wa n dD e e pC o p y i n g .................. 1 0 1\n2 . 7 E x e r c i s e s ............................ 1 0 3\n", "2.1. Goals, Principles, and Patterns 57\n2.1 Goals, Principles, and Patterns\nAs the name implies, the main \u201cactors\u201d in the object-oriented paradigm are called\nobjects . Each object is an instance of a class . Each class presents to the outside\nworld a concise and consistent view of the objects that are instances of this class,without going into too much unnecessary detail or giving others access to the inner\nworkings of the objects. The class de\ufb01nition typically speci\ufb01es instance variables ,\nalso known as data members , that the object contains, as well as the methods ,a l s o\nknown as member functions , that the object can execute. This view of computing\nis intended to ful\ufb01ll several goals and incorporate several design principles, whichwe discuss in this chapter.\n2.1.1 Object-Oriented Design Goals\nSoftware implementations should achieve robustness ,adaptability ,a n d reusabil-\nity. (See Figure 2.1.)\nRobustness Adaptability Reusability\nFigure 2.1: Goals of object-oriented design.\nRobustness\nEvery good programmer wants to develop software that is correct, which means that\na program produces the right output for all the anticipated inputs in the program\u2019s\napplication. In addition, we want software to be robust , that is, capable of handling\nunexpected inputs that are not explicitly de\ufb01ned for its application. For example,\nif a program is expecting a positive integer (perhaps representing the price of anitem) and instead is given a negative integer, then the program should be able to\nrecover gracefully from this error. More importantly, in life-critical applications ,\nwhere a software error can lead to injury or loss of life, software that is not robust\ncould be deadly. This point was driven home in the late 1980s in accidents involv-ing Therac-25, a radiation-therapy machine, which severely overdosed six patientsbetween 1985 and 1987, some of whom died from complications resulting from\ntheir radiation overdose. All six accidents were traced to software errors.", "58 Chapter 2. Object-Oriented Programming\nAdaptability\nModern software applications, such as Web browsers and Internet search engines,\ntypically involve large programs that are used for many years. Software, there-\nfore, needs to be able to evolve over time in response to changing conditions in its\nenvironment. Thus, another important goal of quality software is that it achievesadaptability (also called evolvability ). Related to this concept is portability ,w h i c h\nis the ability of software to run with minimal change on different hardware andoperating system platforms. An advantage of writing software in Python is the\nportability provided by the language itself.\nReusability\nGoing hand in hand with adaptability is the desire that software be reusable, that\nis, the same code should be usable as a component of different systems in variousapplications. Developing quality software can be an expensive enterprise, and itscost can be offset somewhat if the software is designed in a way that makes it easilyreusable in future applications. Such reuse should be done with care, however, for\none of the major sources of software errors in the Therac-25 came from inappropri-\nate reuse of Therac-20 software (which was not object-oriented and not designedfor the hardware platform used with the Therac-25).\n2.1.2 Object-Oriented Design Principles\nChief among the principles of the object-oriented approach, which are intended tofacilitate the goals outlined above, are the following (see Figure 2.2):\n\u2022Modularity\n\u2022Abstraction\n\u2022Encapsulation\nModularity Abstraction Encapsulation\nFigure 2.2: Principles of object-oriented design.", "2.1. Goals, Principles, and Patterns 59\nModularity\nModern software systems typically consist of several different components that\nmust interact correctly in order for the entire system to work properly. Keepingthese interactions straight requires that these different components be well orga-\nnized. Modularity refers to an organizing principle in which different components\nof a software system are divided into separate functional units.\nAs a real-world analogy, a house or apartment can be viewed as consisting of\nseveral interacting units: electrical, heating and cooling, plumbing, and structural.Rather than viewing these systems as one giant jumble of wires, vents, pipes, andboards, the organized architect designing a house or apartment will view them asseparate modules that interact in well-de\ufb01ned ways. In so doing, he or she is usingmodularity to bring a clarity of thought that provides a natural way of organizing\nfunctions into distinct manageable units.\nIn like manner, using modularity in a software system can also provide a pow-\nerful organizing framework that brings clarity to an implementation. In Python,we have already seen that a module is a collection of closely related functions and\nclasses that are de\ufb01ned together in a single \ufb01le of source code. Python\u2019s standard\nlibraries include, for example, the math module, which provides de\ufb01nitions for key\nmathematical constants and functions, and the osmodule, which provides support\nfor interacting with the operating system.\nThe use of modularity helps support the goals listed in Section 2.1.1. Robust-\nness is greatly increased because it is easier to test and debug separate componentsbefore they are integrated into a larger software system. Furthermore, bugs that per-\nsist in a complete system might be traced to a particular component, which can be\n\ufb01xed in relative isolation. The structure imposed by modularity also helps enablesoftware reusability. If software modules are written in a general way, the modulescan be reused when related need arises in other contexts. This is particularly rel-\nevant in a study of data structures, which can typically be designed with suf\ufb01cient\nabstraction and generality to be reused in many applications.\nAbstraction\nThe notion of abstraction is to distill a complicated system down to its most funda-\nmental parts. Typically, describing the parts of a system involves naming them and\nexplaining their functionality. Applying the abstraction paradigm to the design of\ndata structures gives rise to abstract data types (ADTs). An ADT is a mathematical\nmodel of a data structure that speci\ufb01es the type of data stored, the operations sup-ported on them, and the types of parameters of the operations. An ADT speci\ufb01eswhat each operation does, but not how it does it. We will typically refer to the\ncollective set of behaviors supported by an ADT as its public interface .", "60 Chapter 2. Object-Oriented Programming\nAs a programming language, Python provides a great deal of latitude in regard\nto the speci\ufb01cation of an interface. Python has a tradition of treating abstractions\nimplicitly using a mechanism known as duck typing . As an interpreted and dy-\nnamically typed language, there is no \u201ccompile time\u201d checking of data types inPython, and no formal requirement for declarations of abstract base classes. In-stead programmers assume that an object supports a set of known behaviors, with\nthe interpreter raising a run-time error if those assumptions fail. The description\nof this as \u201cduck typing\u201d comes from an adage attributed to poet James WhitcombRiley, stating that \u201cwhen I see a bird that walks like a duck and swims like a duckand quacks like a duck, I call that bird a duck.\u201d\nMore formally, Python supports abstract data types using a mechanism known\nas an abstract base class (ABC). An abstract base class cannot be instantiated\n(i.e., you cannot directly create an instance of that class), but it de\ufb01nes one or morecommon methods that all implementations of the abstraction must have. An ABCis realized by one or more concrete classes that inherit from the abstract base class\nwhile providing implementations for those method declared by the ABC. Python\u2019s\nabcmodule provides formal support for ABCs, although we omit such declarations\nfor simplicity. We will make use of several existing abstract base classes coming\nfrom Python\u2019s collections module, which includes de\ufb01nitions for several common\ndata structure ADTs, and concrete implementations of some of those abstractions.\nEncapsulation\nAnother important principle of object-oriented design is encapsulation . Different\ncomponents of a software system should not reveal the internal details of theirrespective implementations. One of the main advantages of encapsulation is that it\ngives one programmer freedom to implement the details of a component, without\nconcern that other programmers will be writing code that intricately depends onthose internal decisions. The only constraint on the programmer of a componentis to maintain the public interface for the component, as other programmers willbe writing code that depends on that interface. Encapsulation yields robustness\nand adaptability, for it allows the implementation details of parts of a program to\nchange without adversely affecting other parts, thereby making it easier to \ufb01x bugsor add new functionality with relatively local changes to a component.\nThroughout this book, we will adhere to the principle of encapsulation, making\nclear which aspects of a data structure are assumed to be public and which are\nassumed to be internal details. With that said, Python provides only loose support\nfor encapsulation. By convention, names of members of a class (both data membersand member functions) that start with a single underscore character (e.g.,\nsecret )\nare assumed to be nonpublic and should not be relied upon. Those conventionsare reinforced by the intentional omission of those members from automatically\ngenerated documentation.", "2.1. Goals, Principles, and Patterns 61\n2.1.3 Design Patterns\nObject-oriented design facilitates reusable, robust, and adaptable software. De-\nsigning good code takes more than simply understanding object-oriented method-\nologies, however. It requires the effective use of object-oriented design techniques.\nComputing researchers and practitioners have developed a variety of organiza-\ntional concepts and methodologies for designing quality object-oriented softwarethat is concise, correct, and reusable. Of special relevance to this book is the con-cept of a design pattern , which describes a solution to a \u201ctypical\u201d software design\nproblem. A pattern provides a general template for a solution that can be applied in\nmany different situations. It describes the main elements of a solution in an abstract\nway that can be specialized for a speci\ufb01c problem at hand. It consists of a name,which identi\ufb01es the pattern; a context, which describes the scenarios for which thispattern can be applied; a template, which describes how the pattern is applied; and\na result, which describes and analyzes what the pattern produces.\nWe present several design patterns in this book, and we show how they can be\nconsistently applied to implementations of data structures and algorithms. These\ndesign patterns fall into two groups\u2014patterns for solving algorithm design prob-lems and patterns for solving software engineering problems. The algorithm designpatterns we discuss include the following:\n\u2022Recursion (Chapter 4)\n\u2022Amortization (Sections 5.3 and 11.4)\n\u2022Divide-and-conquer (Section 12.2.1)\n\u2022Prune-and-search, also known as decrease-and-conquer (Section 12.7.1)\n\u2022Brute force (Section 13.2.1)\n\u2022Dynamic programming (Section 13.3).\n\u2022The greedy method (Sections 13.4.2, 14.6.2, and 14.7)\nLikewise, the software engineering design patterns we discuss include:\n\u2022Iterator (Sections 1.8 and 2.3.4)\n\u2022Adapter (Section 6.1.2)\n\u2022Position (Sections 7.4 and 8.1.2)\n\u2022Composition (Sections 7.6.1, 9.2.1, and 10.1.4)\n\u2022Template method (Sections 2.4.3, 8.4.6, 10.1.3, 10.5.2, and 11.2.1)\n\u2022Locator (Section 9.5.1)\n\u2022Factory method (Section 11.2.1)\nRather than explain each of these concepts here, however, we introduce them\nthroughout the text as noted above. For each pattern, be it for algorithm engineeringor software engineering, we explain its general use and we illustrate it with at least\none concrete example.", "62 Chapter 2. Object-Oriented Programming\n2.2 Software Development\nTraditional software development involves several phases. Three major steps are:\n1. Design\n2. Implementation\n3. Testing and Debugging\nIn this section, we brie\ufb02y discuss the role of these phases, and we introduce sev-\neral good practices for programming in Python, including coding style, naming\nconventions, formal documentation, and unit testing.\n2.2.1 Design\nFor object-oriented programming, the design step is perhaps the most important\nphase in the process of developing software. For it is in the design step that wedecide how to divide the workings of our program into classes, we decide how\nthese classes will interact, what data each will store, and what actions each will\nperform. Indeed, one of the main challenges that beginning programmers face isdeciding what classes to de\ufb01ne to do the work of their program. While generalprescriptions are hard to come by, there are some rules of thumb that we can applywhen determining how to design our classes:\n\u2022Responsibilities : Divide the work into different actors , each with a different\nresponsibility. Try to describe responsibilities using action verbs. Theseactors will form the classes for the program.\n\u2022Independence : De\ufb01ne the work for each class to be as independent from\nother classes as possible. Subdivide responsibilities between classes so thateach class has autonomy over some aspect of the program. Give data (as in-stance variables) to the class that has jurisdiction over the actions that requireaccess to this data.\n\u2022Behaviors : De\ufb01ne the behaviors for each class carefully and precisely, so\nthat the consequences of each action performed by a class will be well un-derstood by other classes that interact with it. These behaviors will de\ufb01nethe methods that this class performs, and the set of behaviors for a class aretheinterface to the class, as these form the means for other pieces of code to\ninteract with objects from the class.\nDe\ufb01ning the classes, together with their instance variables and methods, are keyto the design of an object-oriented program. A good programmer will naturallydevelop greater skill in performing these tasks over time, as experience teacheshim or her to notice patterns in the requirements of a program that match patterns\nthat he or she has seen before.", "2.2. Software Development 63\nA common tool for developing an initial high-level design for a project is the\nuse of CRC cards . Class-Responsibility-Collaborator (CRC) cards are simple in-\ndex cards that subdivide the work required of a program. The main idea behind this\ntool is to have each card represent a component, which will ultimately become aclass in the program. We write the name of each component on the top of an indexcard. On the left-hand side of the card, we begin writing the responsibilities for\nthis component. On the right-hand side, we list the collaborators for this compo-\nnent, that is, the other components that this component will have to interact with toperform its duties.\nThe design process iterates through an action/actor cycle, where we \ufb01rst iden-\ntify an action (that is, a responsibility), and we then determine an actor (that is, a\ncomponent) that is best suited to perform that action. The design is complete when\nwe have assigned all actions to actors. In using index cards for this process (ratherthan larger pieces of paper), we are relying on the fact that each component shouldhave a small set of responsibilities and collaborators. Enforcing this rule helps keep\nthe individual classes manageable.\nAs the design takes form, a standard approach to explain and document the\ndesign is the use of UML (Uni\ufb01ed Modeling Language) diagrams to express the\norganization of a program. UML diagrams are a standard visual notation to expressobject-oriented software designs. Several computer-aided tools are available to\nbuild UML diagrams. One type of UML \ufb01gure is known as a class diagram .A n\nexample of such a diagram is given in Figure 2.3, for a class that represents a\nconsumer credit card. The diagram has three portions, with the \ufb01rst designatingthe name of the class, the second designating the recommended instance variables,and the third designating the recommended methods of the class. In Section 2.2.3,\nwe discuss our naming conventions, and in Section 2.3.1, we provide a complete\nimplementation of a Python CreditCard class based on this design.\nClass:\nFields:\nBehaviors:\nmake\n payment(amount)\ncustomer\naccount\nget\ncustomer()\nget\nbank()\nbank\nget\naccount()\nbalance\nlimit\nget\nbalance()\nget\nlimit()CreditCard\ncharge(price)\nFigure 2.3: Class diagram for a proposed CreditCard class.", "64 Chapter 2. Object-Oriented Programming\n2.2.2 Pseudo-Code\nAs an intermediate step before the implementation of a design, programmers are\noften asked to describe algorithms in a way that is intended for human eyes only.Such descriptions are called pseudo-code . Pseudo-code is not a computer program,\nbut is more structured than usual prose. It is a mixture of natural language and\nhigh-level programming constructs that describe the main ideas behind a generic\nimplementation of a data structure or algorithm. Because pseudo-code is designedfor a human reader, not a computer, we can communicate high-level ideas, withoutbeing burdened with low-level implementation details. At the same time, we shouldnot gloss over important steps. Like many forms of human communication, \ufb01nding\nthe right balance is an important skill that is re\ufb01ned through practice.\nIn this book, we rely on a pseudo-code style that we hope will be evident to\nPython programmers, yet with a mix of mathematical notations and English prose.\nFor example, we might use the phrase \u201cindicate an error\u201d rather than a formal raise\nstatement. Following conventions of Python, we rely on indentation to indicatethe extent of control structures and on an indexing notation in which entries of a\nsequence Awith length nare indexed from A[0]toA[n\u22121]. However, we choose\nto enclose comments within curly braces {like these }in our pseudo-code, rather\nthan using Python\u2019s #character.\n2.2.3 Coding Style and Documentation\nPrograms should be made easy to read and understand. Good programmers shouldtherefore be mindful of their coding style, and develop a style that communicates\nthe important aspects of a program\u2019s design for both humans and computers. Con-ventions for coding style tend to vary between different programming communities.The of\ufb01cial Style Guide for Python Code is available online at\nhttp://www.python.org/dev/peps/pep-0008/\nThe main principles that we adopt are as follows:\n\u2022Python code blocks are typically indented by 4 spaces. However, to avoidhaving our code fragments overrun the book\u2019s margins, we use 2 spaces foreach level of indentation. It is strongly recommended that tabs be avoided, astabs are displayed with differing widths across systems, and tabs and spacesare not viewed as identical by the Python interpreter. Many Python-aware\neditors will automatically replace tabs with an appropriate number of spaces.", "2.2. Software Development 65\n\u2022Use meaningful names for identi\ufb01ers. Try to choose names that can be read\naloud, and choose names that re\ufb02ect the action, responsibility, or data each\nidenti\ufb01er is naming.\n\u25e6Classes (other than Python\u2019s built-in classes) should have a name thatserves as a singular noun, and should be capitalized (e.g., Date rather\nthandate orDates ). When multiple words are concatenated to form a\nclass name, they should follow the so-called \u201cCamelCase\u201d conventionin which the \ufb01rst letter of each word is capitalized (e.g., CreditCard ).\n\u25e6Functions, including member functions of a class, should be lowercase.If multiple words are combined, they should be separated by under-\nscores (e.g., make\npayment ). The name of a function should typically\nbe a verb that describes its affect. However, if the only purpose of the\nfunction is to return a value, the function name may be a noun thatdescribes the value (e.g., sqrtrather than calculate\nsqrt).\n\u25e6Names that identify an individual object (e.g., a parameter, instancevariable, or local variable) should be a lowercase noun (e.g., price ).\nOccasionally, we stray from this rule when using a single uppercaseletter to designate the name of a data structures (such as tree T).\n\u25e6Identi\ufb01ers that represent a value considered to be a constant are tradi-tionally identi\ufb01ed using all capital letters and with underscores to sep-arate words (e.g., MAX\nSIZE ).\nRecall from our discussion of encapsulation that identi\ufb01ers in any context\nthat begin with a single leading underscore (e.g.,\n secret ) are intended to\nsuggest that they are only for \u201cinternal\u201d use to a class or module, and not partof a public interface.\n\u2022Use comments that add meaning to a program and explain ambiguous orconfusing constructs. In-line comments are good for quick explanations;they are indicated in Python following the #character, as in\nifn%2= =1 : #ni so d d\nMultiline block comments are good for explaining more complex code sec-tions. In Python, these are technically multiline string literals, typically de-\nlimited with triple quotes ( \u201d\u201d\u201d), which have no effect when executed. In the\nnext section, we discuss the use of block comments for documentation.", "66 Chapter 2. Object-Oriented Programming\nDocumentation\nPython provides integrated support for embedding formal documentation directly\nin source code using a mechanism known as a docstring . Formally, any string literal\nthat appears as the \ufb01rststatement within the body of a module, class, or function\n(including a member function of a class) will be considered to be a docstring. By\nconvention, those string literals should be delimited within triple quotes ( \u201d\u201d\u201d). As\nan example, our version of the scale function from page 25 could be documented\nas follows:\ndefscale(data, factor):\n\u201d\u201d\u201dMultiply all entries of numeric data list by the given factor.\u201d\u201d\u201d\nforjinrange(len(data)):\ndata[j]\n =f a c t o r\nIt is common to use the triple-quoted string delimiter for a docstring, even whenthe string \ufb01ts on a single line, as in the above example. More detailed docstringsshould begin with a single line that summarizes the purpose, followed by a blankline, and then further details. For example, we might more clearly document thescale function as follows:\ndefscale(data, factor):\n\u201d\u201d\u201dMultiply all entries of numeric data list by the given factor.\ndata an instance of any mutable sequence type (such as a list)\ncontaining numeric elements\nfactor a number that serves as the multiplicative factor for scaling\n\u201d\u201d\u201d\nforjinrange(len(data)):\ndata[j]\n=f a c t o r\nA docstring is stored as a \ufb01eld of the module, function, or class in which it\nis declared. It serves as documentation and can be retrieved in a variety of ways.\nFor example, the command help(x) , within the Python interpreter, produces the\ndocumentation associated with the identi\ufb01ed object x. An external tool named\npydoc is distributed with Python and can be used to generate formal documentation\nas text or as a Web page. Guidelines for authoring useful docstrings are available\nat:\nhttp://www.python.org/dev/peps/pep-0257/\nIn this book, we will try to present docstrings when space allows. Omitted\ndocstrings can be found in the online version of our source code.", "2.2. Software Development 67\n2.2.4 Testing and Debugging\nTesting is the process of experimentally checking the correctness of a program,\nwhile debugging is the process of tracking the execution of a program and discov-ering the errors in it. Testing and debugging are often the most time-consuming\nactivity in the development of a program.\nTesting\nA careful testing plan is an essential part of writing a program. While verifying the\ncorrectness of a program over all possible inputs is usually infeasible, we shouldaim at executing the program on a representative subset of inputs. At the very\nminimum, we should make sure that every method of a class is tested at least once\n(method coverage). Even better, each code statement in the program should beexecuted at least once (statement coverage).\nPrograms often tend to fail on special cases of the input. Such cases need to be\ncarefully identi\ufb01ed and tested. For example, when testing a method that sorts (thatis, puts in order) a sequence of integers, we should consider the following inputs:\n\u2022The sequence has zero length (no elements).\n\u2022The sequence has one element.\n\u2022All the elements of the sequence are the same.\n\u2022The sequence is already sorted.\n\u2022The sequence is reverse sorted.\nIn addition to special inputs to the program, we should also consider special\nconditions for the structures used by the program. For example, if we use a Pythonlist to store data, we should make sure that boundary cases, such as inserting orremoving at the beginning or end of the list, are properly handled.\nWhile it is essential to use handcrafted test suites, it is also advantageous to\nrun the program on a large collection of randomly generated inputs. The random\nmodule in Python provides several means for generating random numbers, or forrandomizing the order of collections.\nThe dependencies among the classes and functions of a program induce a hi-\nerarchy. Namely, a component Ais above a component Bin the hierarchy if A\ndepends upon B, such as when function Acalls function B, or function Arelies on\na parameter that is an instance of class B. There are two main testing strategies,\ntop-down andbottom-up , which differ in the order in which components are tested.\nTop-down testing proceeds from the top to the bottom of the program hierarchy.\nIt is typically used in conjunction with stubbing , a boot-strapping technique that\nreplaces a lower-level component with a stub, a replacement for the component\nthat simulates the functionality of the original. For example, if function Acalls\nfunction Bto get the \ufb01rst line of a \ufb01le, when testing Awe can replace Bwith a stub\nthat returns a \ufb01xed string.", "68 Chapter 2. Object-Oriented Programming\nBottom-up testing proceeds from lower-level components to higher-level com-\nponents. For example, bottom-level functions, which do not invoke other functions,\nare tested \ufb01rst, followed by functions that call only bottom-level functions, and soon. Similarly a class that does not depend upon any other classes can be testedbefore another class that depends on the former. This form of testing is usuallydescribed as unit testing , as the functionality of a speci\ufb01c component is tested in\nisolation of the larger software project. If used properly, this strategy better isolatesthe cause of errors to the component being tested, as lower-level components uponwhich it relies should have already been thoroughly tested.\nPython provides several forms of support for automated testing. When func-\ntions or classes are de\ufb01ned in a module, testing for that module can be embeddedin the same \ufb01le. The mechanism for doing so was described in Section 1.11. Codethat is shielded in a conditional construct of the form\nif\nname\n ==\n __main__\n :\n# perform tests...\nwill be executed when Python is invoked directly on that module, but not when themodule is imported for use in a larger software project. It is common to put testsin such a construct to test the functionality of the functions and classes speci\ufb01callyde\ufb01ned in that module.\nMore robust support for automation of unit testing is provided by Python\u2019s\nunittest module. This framework allows the grouping of individual test cases into\nlarger test suites, and provides support for executing those suites, and reporting oranalyzing the results of those tests. As software is maintained, the act of regression\ntesting is used, whereby all previous tests are re-executed to ensure that changes to\nthe software do not introduce new bugs in previously tested components.\nDebugging\nThe simplest debugging technique consists of using print statements to track the\nvalues of variables during the execution of the program. A problem with this ap-proach is that eventually the print statements need to be removed or commentedout, so they are not executed when the software is \ufb01nally released.\nA better approach is to run the program within a debugger , which is a special-\nized environment for controlling and monitoring the execution of a program. The\nbasic functionality provided by a debugger is the insertion of breakpoints within\nthe code. When the program is executed within the debugger, it stops at each\nbreakpoint. While the program is stopped, the current value of variables can beinspected.\nThe standard Python distribution includes a module named pdb, which provides\ndebugging support directly within the interpreter. Most IDEs for Python, such asIDLE, provide debugging environments with graphical user interfaces.", "2.3. Class De\ufb01nitions 69\n2.3 Class De\ufb01nitions\nA class serves as the primary means for abstraction in object-oriented program-\nming. In Python, every piece of data is represented as an instance of some class.\nA class provides a set of behaviors in the form of member functions (also known\nasmethods ), with implementations that are common to all instances of that class.\nA class also serves as a blueprint for its instances, effectively determining the way\nthat state information for each instance is represented in the form of attributes (also\nknown as \ufb01elds ,instance variables ,o rdata members ).\n2.3.1 Example: CreditCard Class\nAs a \ufb01rst example, we provide an implementation of a CreditCard class based on\nthe design we introduced in Figure 2.3 of Section 2.2.1. The instances de\ufb01ned by\ntheCreditCard class provide a simple model for traditional credit cards. They have\nidentifying information about the customer, bank, account number, credit limit, and\ncurrent balance. The class restricts charges that would cause a card\u2019s balance to goover its spending limit, but it does not charge interest or late payments (we revisitsuch themes in Section 2.4.1).\nOur code begins in Code Fragment 2.1 and continues in Code Fragment 2.2.\nThe construct begins with the keyword, class , followed by the name of the class, a\ncolon, and then an indented block of code that serves as the body of the class. Thebody includes de\ufb01nitions for all methods of the class. These methods are de\ufb01ned as\nfunctions, using techniques introduced in Section 1.5, yet with a special parameter,\nnamed self, that serves to identify the particular instance upon which a member is\ninvoked.\nThe self Identi\ufb01er\nIn Python, the selfidenti\ufb01er plays a key role. In the context of the CreditCard\nclass, there can presumably be many different CreditCard instances, and each must\nmaintain its own balance, its own credit limit, and so on. Therefore, each instancestores its own instance variables to re\ufb02ect its current state.\nSyntactically, selfidenti\ufb01es the instance upon which a method is invoked. For\nexample, assume that a user of our class has a variable, my\ncard, that identi\ufb01es\nan instance of the CreditCard class. When the user calls my\ncard.get\n balance() ,\nidenti\ufb01er self, within the de\ufb01nition of the get\nbalance method, refers to the card\nknown as my\ncard by the caller. The expression, self.\nbalance refers to an instance\nvariable, named\n balance , stored as part of that particular credit card\u2019s state.", "70 Chapter 2. Object-Oriented Programming\n1classCreditCard:\n2\u201d\u201d\u201dA consumer credit card.\u201d\u201d\u201d\n3\n4def\n init\n(self, customer, bank, acnt, limit):\n5 \u201d\u201d\u201dCreate a new credit card instance.\n67 The initial balance is zero.\n89 customer the name of the customer (e.g.,\nJohn Bowman\n )\n10 bank the name of the bank (e.g.,\n California Savings\n )\n11 acnt the acount identi\ufb01er (e.g.,\n 5391 0375 9387 5309\n )\n12 limit credit limit (measured in dollars)\n13 \u201d\u201d\u201d\n14 self.\ncustomer = customer\n15 self.\nbank = bank\n16 self.\naccount = acnt\n17 self.\nlimit = limit\n18 self.\nbalance = 0\n19\n20defget\ncustomer( self):\n21 \u201d\u201d\u201dReturn name of the customer.\u201d\u201d\u201d\n22 return self .\ncustomer\n23\n24defget\nbank(self):\n25 \u201d\u201d\u201dReturn the bank\n s name.\u201d\u201d\u201d\n26 return self .\nbank\n2728defget\naccount( self):\n29 \u201d\u201d\u201dReturn the card identifying number (typically stored as a string).\u201d\u201d\u201d\n30 return self .\naccount\n3132defget\nlimit(self):\n33 \u201d\u201d\u201dReturn current credit limit.\u201d\u201d\u201d\n34 return self .\nlimit\n3536defget\nbalance( self):\n37 \u201d\u201d\u201dReturn current balance.\u201d\u201d\u201d\n38 return self .\nbalance\nCode Fragment 2.1: The beginning of the CreditCard class de\ufb01nition (continued in\nCode Fragment 2.2).", "2.3. Class De\ufb01nitions 71\n39defcharge( self,p r i c e ) :\n40 \u201d\u201d\u201dCharge given price to the card, assuming su\ufb03cient credit limit.\n41\n42 Return True if charge was processed; False if charge was denied.\n43 \u201d\u201d\u201d\n44 ifprice + self.\nbalance >self.\nlimit: # if charge would exceed limit,\n45 return False # cannot accept charge\n46 else:\n47 self.\nbalance += price\n48 return True\n4950defmake\npayment( self, amount):\n51 \u201d\u201d\u201dProcess customer payment that reduces balance.\u201d\u201d\u201d\n52 self.\nbalance \u2212= amount\nCode Fragment 2.2: The conclusion of the CreditCard class de\ufb01nition (continued\nfrom Code Fragment 2.1). These methods are indented within the class de\ufb01nition.\nWe draw attention to the difference between the method signature as declared\nwithin the class versus that used by a caller. For example, from a user\u2019s perspec-tive we have seen that the get\nbalance method takes zero parameters, yet within\nthe class de\ufb01nition, selfis an explicit parameter. Likewise, the charge method is\ndeclared within the class having two parameters ( selfandprice ), even though this\nmethod is called with one parameter, for example, as my\ncard.charge(200) .T h e\ninterpretter automatically binds the instance upon which the method is invoked totheselfparameter.\nThe Constructor\nA user can create an instance of the CreditCard class using a syntax as:\ncc = CreditCard(\n John Doe,\n 1st Bank\n ,\n5391 0375 9387 5309\n , 1000)\nInternally, this results in a call to the specially named\n init\n method that serves\nas the constructor of the class. Its primary responsibility is to establish the state of\na newly created credit card object with appropriate instance variables. In the caseof the CreditCard class, each object maintains \ufb01ve instance variables, which we\nname:\ncustomer ,\nbank ,\naccount ,\nlimit ,a n d\n balance . The initial values for the\n\ufb01rst four of those \ufb01ve are provided as explicit parameters that are sent by the userwhen instantiating the credit card, and assigned within the body of the construc-tor. For example, the command, self.\ncustomer = customer , assigns the instance\nvariable self.\ncustomer to the parameter customer ; note that because customer is\nunquali\ufb01ed on the right-hand side, it refers to the parameter in the local namespace.", "72 Chapter 2. Object-Oriented Programming\nEncapsulation\nBy the conventions described in Section 2.2.3, a single leading underscore in the\nname of a data member, such as\n balance , implies that it is intended as nonpublic .\nUsers of a class should not directly access such members.\nAs a general rule, we will treat all data members as nonpublic. This allows\nus to better enforce a consistent state for all instances. We can provide accessors,\nsuch as get\nbalance , to provide a user of our class read-only access to a trait. If\nwe wish to allow the user to change the state, we can provide appropriate update\nmethods. In the context of data structures, encapsulating the internal representationallows us greater \ufb02exibility to redesign the way a class works, perhaps to improve\nthe ef\ufb01ciency of the structure.\nAdditional Methods\nThe most interesting behaviors in our class are charge andmake\n payment .T h e\ncharge function typically adds the given price to the credit card balance, to re\ufb02ect\na purchase of said price by the customer. However, before accepting the charge,\nour implementation veri\ufb01es that the new purchase would not cause the balance toexceed the credit limit. The make\npayment charge re\ufb02ects the customer sending\npayment to the bank for the given amount, thereby reducing the balance on thecard. We note that in the command, self.\nbalance \u2212= amount , the expression\nself.\nbalance is quali\ufb01ed with the selfidenti\ufb01er because it represents an instance\nvariable of the card, while the unquali\ufb01ed amount represents the local parameter.\nError Checking\nOur implementation of the CreditCard class is not particularly robust. First, we\nnote that we did not explicitly check the types of the parameters to charge and\nmake\n payment , nor any of the parameters to the constructor. If a user were to make\na call such as visa.charge(\n candy\n ), our code would presumably crash when at-\ntempting to add that parameter to the current balance. If this class were to be widelyused in a library, we might use more rigorous techniques to raise a TypeError when\nfacing such misuse (see Section 1.7).\nBeyond the obvious type errors, our implementation may be susceptible to log-\nical errors. For example, if a user were allowed to charge a negative price, suchasvisa.charge( \u2212300) , that would serve to lower the customer\u2019s balance. This pro-\nvides a loophole for lowering a balance without making a payment. Of course,this might be considered valid usage if modeling the credit received when a cus-tomer returns merchandise to a store. We will explore some such issues with the\nCreditCard class in the end-of-chapter exercises.", "2.3. Class De\ufb01nitions 73\nTesting the Class\nIn Code Fragment 2.3, we demonstrate some basic usage of the CreditCard class,\ninserting three cards into a list named wallet . We use loops to make some charges\nand payments, and use various accessors to print results to the console.\nThese tests are enclosed within a conditional, if\nname\n ==\n __main__\n :,\nso that they can be embedded in the source code with the class de\ufb01nition. Using\nthe terminology of Section 2.2.4, these tests provide method coverage , as each of\nthe methods is called at least once, but it does not provide statement coverage ,a s\nthere is never a case in which a charge is rejected due to the credit limit. Thisis not a particular advanced from of testing as the output of the given tests mustbe manually audited in order to determine whether the class behaved as expected.Python has tools for more formal testing (see discussion of the unittest module\nin Section 2.2.4), so that resulting values can be automatically compared to thepredicted outcomes, with output generated only when an error is detected.\n53if\nname\n ==\n __main__\n :\n54 wallet = [ ]\n55 wallet.append(CreditCard(\n John Bowman\n ,\nCalifornia Savings\n ,\n56\n 5391 0375 9387 5309\n , 2500) )\n57 wallet.append(CreditCard(\n John Bowman\n ,\nCalifornia Federal\n ,\n58\n 3485 0399 3395 1954\n , 3500) )\n59 wallet.append(CreditCard(\n John Bowman\n ,\nCalifornia Finance\n ,\n60\n 5391 0375 9387 5309\n , 5000) )\n6162forvalinrange(1, 17):\n63 wallet[0].charge(val)\n64 wallet[1].charge(2\nval)\n65 wallet[2].charge(3\n val)\n66\n67forcinrange(3):\n68 print(\n Customer =\n , wallet[c].get\n customer())\n69 print(\n Bank =\n , wallet[c].get\n bank())\n70 print(\n Account =\n , wallet[c].get\n account())\n71 print(\n Limit =\n , wallet[c].get\n limit())\n72 print(\n Balance =\n , wallet[c].get\n balance())\n73 while wallet[c].get\n balance( ) >100:\n74 wallet[c].make\n payment(100)\n75 print(\n New balance =\n , wallet[c].get\n balance())\n76 print()\nCode Fragment 2.3: Testing the CreditCard class.", "74 Chapter 2. Object-Oriented Programming\n2.3.2 Operator Overloading and Python\u2019s Special Methods\nPython\u2019s built-in classes provide natural semantics for many operators. For ex-\nample, the syntax a+b invokes addition for numeric types, yet concatenation for\nsequence types. When de\ufb01ning a new class, we must consider whether a syntaxlikea+b should be de\ufb01ned when aorbis an instance of that class.\nBy default, the +operator is unde\ufb01ned for a new class. However, the author\nof a class may provide a de\ufb01nition using a technique known as operator overload-\ning. This is done by implementing a specially named method. In particular, the\n+operator is overloaded by implementing a method named\nadd\n ,w h i c ht a k e s\nthe right-hand operand as a parameter and which returns the result of the expres-sion. That is, the syntax, a+b , is converted to a method call on object aof the\nform, a.\nadd\n (b). Similar specially named methods exist for other operators.\nTable 2.1 provides a comprehensive list of such methods.\nWhen a binary operator is applied to two instances of different types, as in\n3\n love me\n , Python gives deference to the class of the leftoperand. In this\nexample, it would effectively check if the intclass provides a suf\ufb01cient de\ufb01nition\nfor how to multiply an instance by a string, via the\n mul\n method. However,\nif that class does not implement such a behavior, Python checks the class de\ufb01ni-tion for the right-hand operand, in the form of a special method named\nrmul\n(i.e., \u201cright multiply\u201d). This provides a way for a new user-de\ufb01ned class to supportmixed operations that involve an instance of an existing class (given that the exist-ing class would presumably not have de\ufb01ned a behavior involving this new class).\nThe distinction between\nmul\n and\n rmul\n also allows a class to de\ufb01ne dif-\nferent semantics in cases, such as matrix multiplication, in which an operation is\nnoncommutative (that is, A\nxmay differ from x\nA).\nNon-Operator Overloads\nIn addition to traditional operator overloading, Python relies on specially namedmethods to control the behavior of various other functionality, when applied touser-de\ufb01ned classes. For example, the syntax, str(foo) , is formally a call to the\nconstructor for the string class. Of course, if the parameter is an instance of a user-de\ufb01ned class, the original authors of the string class could not have known how\nthat instance should be portrayed. So the string constructor calls a specially named\nmethod, foo.\nstr\n(), that must return an appropriate string representation.\nSimilar special methods are used to determine how to construct an int,\ufb02oat ,o r\nbool based on a parameter from a user-de\ufb01ned class. The conversion to a Boolean\nvalue is particularly important, because the syntax, iffoo:, can be used even when\nfoois not formally a Boolean value (see Section 1.4.1). For a user-de\ufb01ned class,\nthat condition is evaluated by the special method foo.\n bool\n ().", "2.3. Class De\ufb01nitions 75\nCommon Syntax\n Special Method Form\na+b\n a.\nadd\n (b); alternatively b.\nradd\n (a)\na\u2212b\n a.\nsub\n(b); alternatively b.\nrsub\n (a)\na\nb\n a.\nmul\n (b); alternatively b.\nrmul\n (a)\na/b\n a.\ntruediv\n (b); alternatively b.\nrtruediv\n (a)\na/ /b\n a.\n\ufb02oordiv\n (b); alternatively b.\nr\ufb02oordiv\n (a)\na%b\n a.\nmod\n (b); alternatively b.\nrmod\n (a)\na\nb\n a.\npow\n (b); alternatively b.\nrpow\n (a)\na<<b\n a.\nlshift\n (b); alternatively b.\nrlshift\n (a)\na>>b\n a.\nrshift\n (b); alternatively b.\nrrshift\n (a)\na&b\n a.\nand\n (b); alternatively b.\nrand\n (a)\na\u02c6b\n a.\nxor\n(b); alternatively b.\nrxor\n (a)\na|b\n a.\nor\n(b); alternatively b.\nror\n(a)\na+ =b\n a.\niadd\n (b)\na\u2212=b\n a.\nisub\n (b)\na\n=b\n a.\nimul\n (b)\n...\n ...\n+a\n a.\npos\n()\n\u2212a\n a.\nneg\n ()\n\u02dca\n a.\ninvert\n ()\nabs(a)\n a.\nabs\n()\na<b\n a.\nlt\n(b)\na<=b\n a.\nle\n(b)\na>b\n a.\ngt\n(b)\na>=b\n a.\nge\n(b)\na= =b\n a.\neq\n(b)\na! =b\n a.\nne\n(b)\nvi na\n a.\ncontains\n (v)\na[k]\n a.\ngetitem\n (k)\na[k] = v\n a.\nsetitem\n (k,v)\ndel a[k]\n a.\ndelitem\n (k)\na(arg1, arg2, ...)\n a.\ncall\n(arg1, arg2, ...)\nlen(a)\n a.\nlen\n()\nhash(a)\n a.\nhash\n ()\niter(a)\n a.\niter\n()\nnext(a)\n a.\nnext\n ()\nbool(a)\n a.\nbool\n ()\n\ufb02oat(a)\n a.\n\ufb02oat\n ()\nint(a)\n a.\nint\n()\nrepr(a)\n a.\nrepr\n ()\nreversed(a)\n a.\nreversed\n ()\nstr(a)\n a.\nstr\n()\nTable 2.1: Overloaded operations, implemented with Python\u2019s special methods.", "76 Chapter 2. Object-Oriented Programming\nSeveral other top-level functions rely on calling specially named methods. For\nexample, the standard way to determine the size of a container type is by calling\nthe top-level lenfunction. Note well that the calling syntax, len(foo) , is not the\ntraditional method-calling syntax with the dot operator. However, in the case of auser-de\ufb01ned class, the top-level lenfunction relies on a call to a specially named\nlen\n method of that class. That is, the call len(foo) is evaluated through a\nmethod call, foo.\n len\n(). When developing data structures, we will routinely\nde\ufb01ne the\n len\n method to return a measure of the size of the structure.\nImplied Methods\nAs a general rule, if a particular special method is not implemented in a user-de\ufb01ned\nclass, the standard syntax that relies upon that method will raise an exception. For\nexample, evaluating the expression, a+b , for instances of a user-de\ufb01ned class\nwithout\n add\n or\nradd\n will raise an error.\nHowever, there are some operators that have default de\ufb01nitions provided by\nPython, in the absence of special methods, and there are some operators whosede\ufb01nitions are derived from others. For example, the\nbool\n method, which\nsupports the syntax iffoo:, has default semantics so that every object other than\nNone is evaluated as True . However, for container types, the\n len\n method is\ntypically de\ufb01ned to return the size of the container. If such a method exists, thenthe evaluation of bool(foo) is interpreted by default to be True for instances with\nnonzero length, and False for instances with zero length, allowing a syntax such as\nifwaitlist: to be used to test whether there are one or more entries in the waitlist.\nIn Section 2.3.4, we will discuss Python\u2019s mechanism for providing iterators\nfor collections via the special method,\niter\n . With that said, if a container class\nprovides implementations for both\n len\n and\n getitem\n , a default iteration is\nprovided automatically (using means we describe in Section 2.3.4). Furthermore,once an iterator is de\ufb01ned, default functionality of\ncontains\n is provided.\nIn Section 1.3 we drew attention to the distinction between expression aisb\nand expression a= =b , with the former evaluating whether identi\ufb01ers aandbare\naliases for the same object, and the latter testing a notion of whether the two iden-ti\ufb01ers reference equivalent values. The notion of \u201cequivalence\u201d depends upon the\ncontext of the class, and semantics is de\ufb01ned with the\neq\n method. However, if\nno implementation is given for\n eq\n , the syntax a= =b is legal with semantics\nofaisb, that is, an instance is equivalent to itself and no others.\nWe should caution that some natural implications are notautomatically pro-\nvided by Python. For example, the\n eq\n method supports syntax a= =b ,b u t\nproviding that method does not affect the evaluation of syntax a! =b .( T h e\n ne\nmethod should be provided, typically returning not(a == b) as a result.) Simi-\nlarly, providing a\n lt\n method supports syntax a<b, and indirectly b>a,b u t\nproviding both\n lt\n and\n eq\n does notimply semantics for a<=b.", "2.3. Class De\ufb01nitions 77\n2.3.3 Example: Multidimensional Vector Class\nTo demonstrate the use of operator overloading via special methods, we provide\nan implementation of a Vector class, representing the coordinates of a vector in a\nmultidimensional space. For example, in a three-dimensional space, we might wishto represent a vector with coordinates /angbracketleft5,\u22122,3/angbracketright. Although it might be tempting to\ndirectly use a Python listto represent those coordinates, a list does not provide an\nappropriate abstraction for a geometric vector. In particular, if using lists, the ex-pression [5,\u22122, 3] + [1, 4, 2] results in the list [5,\u22122, 3, 1, 4, 2] . When working\nwith vectors, if u=/angbracketleft5,\u22122,3/angbracketrightandv=/angbracketleft1,4,2/angbracketright, one would expect the expression,\nu+v , to return a three-dimensional vector with coordinates /angbracketleft6,2,5/angbracketright.\nWe therefore de\ufb01ne a Vector class, in Code Fragment 2.4, that provides a better\nabstraction for the notion of a geometric vector. Internally, our vector relies uponan instance of a list, named\ncoords , as its storage mechanism. By keeping the\ninternal list encapsulated, we can enforce the desired public interface for instancesof our class. A demonstration of supported behaviors includes the following:\nv = Vector(5) # construct \ufb01ve-dimensional <0, 0, 0, 0, 0 >\nv[1] = 23 #<0, 23, 0, 0, 0 >(based on use of\nsetitem\n )\nv[\u22121] = 45 #<0, 23, 0, 0, 45 >(also via\n setitem\n )\nprint(v[4]) # print 45 (via\n getitem\n )\nu=v+v #<0, 46, 0, 0, 90 >(via\n add\n)\nprint(u) # print <0, 46, 0, 0, 90 >\ntotal = 0\nforentryinv: # implicit iteration via\n len\nand\n getitem\ntotal += entry\nWe implement many of the behaviors by trivially invoking a similar behavior\non the underlying list of coordinates. However, our implementation of\n add\nis customized. Assuming the two operands are vectors with the same length, thismethod creates a new vector and sets the coordinates of the new vector to be equalto the respective sum of the operands\u2019 elements.\nIt is interesting to note that the class de\ufb01nition, as given in Code Fragment 2.4,\nautomatically supports the syntax u = v + [5, 3, 10, \u22122, 1] , resulting in a new\nvector that is the element-by-element \u201csum\u201d of the \ufb01rst vector and the list in-\nstance. This is a result of Python\u2019s polymorphism . Literally, \u201cpolymorphism\u201d\nmeans \u201cmany forms.\u201d Although it is tempting to think of the other parameter of\nour\nadd\n method as another Vector instance, we never declared it as such.\nWithin the body, the only behaviors we rely on for parameter other is that it sup-\nportslen(other) and access to other[j] . Therefore, our code executes when the\nright-hand operand is a list of numbers (with matching length).", "78 Chapter 2. Object-Oriented Programming\n1classVector:\n2\u201d\u201d\u201dRepresent a vector in a multidimensional space.\u201d\u201d\u201d\n3\n4def\n init\n(self,d ) :\n5 \u201d\u201d\u201dCreate d-dimensional vector of zeros.\u201d\u201d\u201d\n6 self.\ncoords = [0]\n d\n78def\nlen\n(self):\n9 \u201d\u201d\u201dReturn the dimension of the vector.\u201d\u201d\u201d\n10 return len(self.\ncoords)\n1112def\ngetitem\n (self,j ) :\n13 \u201d\u201d\u201dReturn jth coordinate of vector.\u201d\u201d\u201d\n14 return self .\ncoords[j]\n15\n16def\n setitem\n (self,j ,v a l ) :\n17 \u201d\u201d\u201dSet jth coordinate of vector to given value.\u201d\u201d\u201d\n18 self.\ncoords[j] = val\n19\n20def\n add\n (self,o t h e r ) :\n21 \u201d\u201d\u201dReturn sum of two vectors.\u201d\u201d\u201d\n22 iflen(self) != len(other): # relies on\n len\nmethod\n23 raiseValueError(\n dimensions must agree\n )\n24 result = Vector(len( self)) # start with vector of zeros\n25 forjinrange(len( self)):\n26 result[j] = self[j] + other[j]\n27 return result\n2829def\neq\n(self,o t h e r ) :\n30 \u201d\u201d\u201dReturn True if vector has same coordinates as other.\u201d\u201d\u201d\n31 return self .\ncoords == other.\n coords\n3233def\nne\n(self,o t h e r ) :\n34 \u201d\u201d\u201dReturn True if vector di\ufb00ers from other.\u201d\u201d\u201d\n35 return not self == other #r e l yo ne x i s t i n g\n eq\nde\ufb01nition\n3637def\nstr\n(self):\n38 \u201d\u201d\u201dProduce string representation of vector.\u201d\u201d\u201d\n39 return\n <\n+str(self.\ncoords)[1: \u22121] +\n >\n# adapt list representation\nCode Fragment 2.4: De\ufb01nition of a simple Vector class.", "2.3. Class De\ufb01nitions 79\n2.3.4 Iterators\nIteration is an important concept in the design of data structures. We introduced\nPython\u2019s mechanism for iteration in Section 1.8. In short, an iterator for a collec-\ntion provides one key behavior: It supports a special method named\n next\n that\nreturns the next element of the collection, if any, or raises a StopIteration exception\nto indicate that there are no further elements.\nFortunately, it is rare to have to directly implement an iterator class. Our pre-\nferred approach is the use of the generator syntax (also described in Section 1.8),\nwhich automatically produces an iterator of yielded values.\nPython also helps by providing an automatic iterator implementation for any\nclass that de\ufb01nes both\n len\n and\n getitem\n . To provide an instructive exam-\nple of a low-level iterator, Code Fragment 2.5 demonstrates just such an iteratorclass that works on any collection that supports both\nlen\n and\n getitem\n .\nThis class can be instantiated as SequenceIterator(data) . It operates by keeping an\ninternal reference to the data sequence, as well as a current index into the sequence.Each time\nnext\n is called, the index is incremented, until reaching the end of\nthe sequence.\n1classSequenceIterator:\n2\u201d\u201d\u201dAn iterator for any of Python\n s sequence types.\u201d\u201d\u201d\n34def\ninit\n(self, sequence):\n5 \u201d\u201d\u201dCreate an iterator for the given sequence.\u201d\u201d\u201d\n6 self.\nseq = sequence # keep a reference to the underlying data\n7 self.\nk=\u22121 # will increment to 0 on \ufb01rst call to next\n89def\nnext\n (self):\n10 \u201d\u201d\u201dReturn the next element, or else raise StopIteration error.\u201d\u201d\u201d\n11 self.\nk+ =1 # advance to next index\n12 if self.\nk<len(self.\nseq):\n13 return (self.\nseq[self.\nk]) # return the data element\n14 else:\n15 raiseStopIteration( ) # there are no more elements\n1617def\niter\n(self):\n18 \u201d\u201d\u201dBy convention, an iterator must return itself as an iterator.\u201d\u201d\u201d\n19 return self\nCode Fragment 2.5: An iterator class for any sequence type.", "80 Chapter 2. Object-Oriented Programming\n2.3.5 Example: Range Class\nAs the \ufb01nal example for this section, we develop our own implementation of a\nclass that mimics Python\u2019s built-in range class. Before introducing our class, we\ndiscuss the history of the built-in version. Prior to Python 3 being released, range\nwas implemented as a function, and it returned a listinstance with elements in\nthe speci\ufb01ed range. For example, range(2, 10, 2) returned the list [ 2 ,4 ,6 ,8 ] .\nHowever, a typical use of the function was to support a for-loop syntax, such as\nforkinrange(10000000) . Unfortunately, this caused the instantiation and initial-\nization of a list with the range of numbers. That was an unnecessarily expensivestep, in terms of both time and memory usage.\nThe mechanism used to support ranges in Python 3 is entirely different (to be\nfair, the \u201cnew\u201d behavior existed in Python 2 under the name xrange ). It uses a\nstrategy known as lazy evaluation . Rather than creating a new list instance, range\nis a class that can effectively represent the desired range of elements without everstoring them explicitly in memory. To better explore the built-in range class, we\nrecommend that you create an instance as r = range(8, 140, 5) . The result is a\nrelatively lightweight object, an instance of the range class, that has only a few\nbehaviors. The syntax len(r) will report the number of elements that are in the\ngiven range (27, in our example). A range also supports the\ngetitem\n method,\nso that syntax r[15] reports the sixteenth element in the range (as r[0]is the \ufb01rst\nelement). Because the class supports both\n len\n and\n getitem\n , it inherits\nautomatic support for iteration (see Section 2.3.4), which is why it is possible toexecute a for loop over a range.\nAt this point, we are ready to demonstrate our own version of such a class. Code\nFragment 2.6 provides a class we name Range (so as to clearly differentiate it from\nbuilt-in range) . The biggest challenge in the implementation is properly computing\nthe number of elements that belong in the range, given the parameters sent by the\ncaller when constructing a range. By computing that value in the constructor, and\nstoring it as self.\nlength , it becomes trivial to return it from the\n len\n method. To\nproperly implement a call to\n getitem\n (k), we simply take the starting value of\nthe range plus ktimes the step size (i.e., for k=0, we return the start value). There\nare a few subtleties worth examining in the code:\n\u2022To properly support optional parameters, we rely on the technique describedon page 27, when discussing a functional version of range.\n\u2022We compute the number of elements in the range as\nmax(0, (stop \u2212start + step \u22121) // step)\nIt is worth testing this formula for both positive and negative step sizes.\n\u2022The\ngetitem\n method properly supports negative indices by converting\nan index \u2212ktolen(self) \u2212kbefore computing the result.", "2.3. Class De\ufb01nitions 81\n1classRange:\n2\u201d\u201d\u201dA class that mimic\n s the built-in range class.\u201d\u201d\u201d\n3\n4def\n init\n(self, start, stop= None,s t e p = 1 ) :\n5 \u201d\u201d\u201dInitialize a Range instance.\n6\n7 Semantics is similar to built-in range class.\n8 \u201d\u201d\u201d\n9 ifstep == 0:\n10 raiseValueError(\n step cannot be 0\n )\n1112 ifstopis None : # special case of range(n)\n13 start, stop = 0, start # should be treated as if range(0,n)\n1415 # calculate the e\ufb00ective length once\n16 self.\nlength = max(0, (stop \u2212start + step \u22121) // step)\n17\n18 # need knowledge of start and step (but not stop) to support\n getitem\n19 self.\nstart = start\n20 self.\nstep = step\n21\n22def\n len\n(self):\n23 \u201d\u201d\u201dReturn number of entries in the range.\u201d\u201d\u201d\n24 return self .\nlength\n2526def\ngetitem\n (self,k ) :\n27 \u201d\u201d\u201dReturn entry at index k (using standard interpretation if negative).\u201d\u201d\u201d\n28 ifk<0:\n29 k+ =l e n ( self) # attempt to convert negative index\n3031 if not 0<=k<self.\nlength:\n32 raiseIndexError(\n index out of range\n )\n3334 return self .\nstart + k\n self.\nstep\nCode Fragment 2.6: Our own implementation of a Range class.", "82 Chapter 2. Object-Oriented Programming\n2.4 Inheritance\nA natural way to organize various structural components of a software package\nis in a hierarchical fashion, with similar abstract de\ufb01nitions grouped together in\na level-by-level manner that goes from speci\ufb01c to more general as one traversesup the hierarchy. An example of such a hierarchy is shown in Figure 2.4. Using\nmathematical notations, the set of houses is a subset of the set of buildings, but a\nsuperset of the set of ranches. The correspondence between levels is often referred\nto as an \u201cis a\u201d relationship , as a house is a building, and a ranch is a house.\nBuilding\nLow-rise\nApartmentHigh-rise\nApartmentTwo-story\nHouseRanch SkyscraperCommercial\nBuildingHouse Apartment\nFigure 2.4: An example of an \u201cis a\u201d hierarchy involving architectural buildings.\nA hierarchical design is useful in software development, as common function-\nality can be grouped at the most general level, thereby promoting reuse of code,\nwhile differentiated behaviors can be viewed as extensions of the general case, Inobject-oriented programming, the mechanism for a modular and hierarchical orga-nization is a technique known as inheritance . This allows a new class to be de\ufb01ned\nbased upon an existing class as the starting point. In object-oriented terminology,the existing class is typically described as the base class ,parent class ,o rsuper-\nclass , while the newly de\ufb01ned class is known as the subclass orchild class .\nThere are two ways in which a subclass can differentiate itself from its su-\nperclass. A subclass may specialize an existing behavior by providing a new im-\nplementation that overrides an existing method. A subclass may also extend its\nsuperclass by providing brand new methods.", "2.4. Inheritance 83\nPython\u2019s Exception Hierarchy\nAnother example of a rich inheritance hierarchy is the organization of various ex-\nception types in Python. We introduced many of those classes in Section 1.7, but\ndid not discuss their relationship with each other. Figure 2.5 illustrates a (small)portion of that hierarchy. The BaseException class is the root of the entire hierar-\nchy, while the more speci\ufb01c Exception class includes most of the error types that\nwe have discussed. Programmers are welcome to de\ufb01ne their own special exceptionclasses to denote errors that may occur in the context of their application. Thoseuser-de\ufb01ned exception types should be declared as subclasses of Exception .\nValueErrorException KeyboardInterrupt SystemExitBaseException\nIndexError KeyError ZeroDivisionErrorLookupError ArithmeticError\nFigure 2.5: A portion of Python\u2019s hierarchy of exception types.\n2.4.1 Extending the CreditCard Class\nTo demonstrate the mechanisms for inheritance in Python, we revisit the CreditCard\nclass of Section 2.3, implementing a subclass that, for lack of a better name, wename PredatoryCreditCard . The new class will differ from the original in two\nways: (1) if an attempted charge is rejected because it would have exceeded thecredit limit, a $5 fee will be charged, and (2) there will be a mechanism for assess-\ning a monthly interest charge on the outstanding balance, based upon an Annual\nPercentage Rate (APR) speci\ufb01ed as a constructor parameter.\nIn accomplishing this goal, we demonstrate the techniques of specialization\nand extension. To charge a fee for an invalid charge attempt, we override the\nexisting charge method, thereby specializing it to provide the new functionality\n(although the new version takes advantage of a call to the overridden version). Toprovide support for charging interest, we extend the class with a new method named\nprocess\nmonth .", "84 Chapter 2. Object-Oriented Programming\nClass:\nFields:Behaviors:Class:\nFields:\nBehaviors:\nprocess\n month()\napr\ncustomer\naccount\nget\ncustomer()\nget\nbank()\nbank\nget\naccount()\nbalance\nlimit\nget\nbalance()\nget\nlimit()\ncharge(price)\nmake\n payment(amount)\nPredatoryCreditCardCreditCard\ncharge(price)\nFigure 2.6: Diagram of an inheritance relationship.\nFigure 2.6 provides an overview of our use of inheritance in designing the new\nPredatoryCreditCard class, and Code Fragment 2.7 gives a complete Python im-\nplementation of that class.\nTo indicate that the new class inherits from the existing CreditCard class, our\nde\ufb01nition begins with the syntax, classPredatoryCreditCard(CreditCard) .T h e\nbody of the new class provides three member functions:\n init\n ,charge ,a n d\nprocess\n month .T h e\n init\n constructor serves a very similar role to the original\nCreditCard constructor, except that for our new class, there is an extra parameter\nto specify the annual percentage rate. The body of our new constructor relies upon\nmaking a call to the inherited constructor to perform most of the initialization (infact, everything other than the recording of the percentage rate). The mechanismfor calling the inherited constructor relies on the syntax, super(). Speci\ufb01cally, at\nline 15 the command\nsuper().\ninit\n(customer, bank, acnt, limit)\ncalls the\n init\n method that was inherited from the CreditCard superclass. Note\nwell that this method only accepts four parameters. We record the APR value in a\nnew \ufb01eld named\n apr.\nIn similar fashion, our PredatoryCreditCard class provides a new implemen-\ntation of the charge method that overrides the inherited method. Yet, our imple-\nmentation of the new method relies on a call to the inherited method, with syntaxsuper().charge(price) at line 24. The return value of that call designates whether", "2.4. Inheritance 85\n1classPredatoryCreditCard(CreditCard):\n2\u201d\u201d\u201dAn extension to CreditCard that compounds interest and fees.\u201d\u201d\u201d\n3\n4def\n init\n(self, customer, bank, acnt, limit, apr):\n5 \u201d\u201d\u201dCreate a new predatory credit card instance.\n67 The initial balance is zero.\n89 customer the name of the customer (e.g.,\nJohn Bowman\n )\n10 bank the name of the bank (e.g.,\n California Savings\n )\n11 acnt the acount identi\ufb01er (e.g.,\n 5391 0375 9387 5309\n )\n12 limit credit limit (measured in dollars)\n13 apr annual percentage rate (e.g., 0.0825 for 8.25% APR)\n14 \u201d\u201d\u201d\n15 super().\ninit\n(customer, bank, acnt, limit) # call super constructor\n16 self.\napr = apr\n1718defcharge( self,p r i c e ) :\n19 \u201d\u201d\u201dCharge given price to the card, assuming su\ufb03cient credit limit.\n2021 Return True if charge was processed.\n22 Return False and assess\n5 fee if charge is denied.\n23 \u201d\u201d\u201d\n24 success = super().charge(price) # call inherited method\n25 if not success:\n26 self.\nbalance += 5 # assess penalty\n27 return success # caller expects return value\n28\n29defprocess\n month( self):\n30 \u201d\u201d\u201dAssess monthly interest on outstanding balance.\u201d\u201d\u201d\n31 if self.\nbalance >0:\n32 # if positive balance, convert APR to monthly multiplicative factor\n33 monthly\n factor = pow(1 + self.\napr, 1/12)\n34 self.\nbalance\n =m o n t h l y\n factor\nCode Fragment 2.7: A subclass of CreditCard that assesses interest and fees.", "86 Chapter 2. Object-Oriented Programming\nthe charge was successful. We examine that return value to decide whether to as-\nsess a fee, and in turn we return that value to the caller of method, so that the new\nversion of charge has a similar outward interface as the original.\nTheprocess\n month method is a new behavior, so there is no inherited version\nupon which to rely. In our model, this method should be invoked by the bank,once each month, to add new interest charges to the customer\u2019s balance. The mostchallenging aspect in implementing this method is making sure we have working\nknowledge of how an annual percentage rate translates to a monthly rate. We do\nnot simply divide the annual rate by twelve to get a monthly rate (that would be toopredatory, as it would result in a higher APR than advertised). The correct com-putation is to take the twelfth-root of 1+s e l f .\napr, and use that as a multiplica-\ntive factor. For example, if the APR is 0.0825 (representing 8.25%), we compute\n12\u221a\n1.0825\u22481.006628, and therefore charge 0 .6628% interest per month. In this\nway, each $100 of debt will amass $8.25 of compounded interest in a year.\nProtected Members\nOurPredatoryCreditCard subclass directly accesses the data member self.\nbalance ,\nwhich was established by the parent CreditCard class. The underscored name, by\nconvention, suggests that this is a nonpublic member, so we might ask if it is okay\nthat we access it in this fashion. While general users of the class should not bedoing so, our subclass has a somewhat privileged relationship with the superclass.\nSeveral object-oriented languages (e.g., Java, C++) draw a distinction for nonpub-\nlic members, allowing declarations of protected orprivate access modes. Members\nthat are declared as protected are accessible to subclasses, but not to the generalpublic, while members that are declared as private are not accessible to either. In\nthis respect, we are using\nbalance as if it were protected (but not private).\nPython does not support formal access control, but names beginning with a sin-\ngle underscore are conventionally akin to protected, while names beginning with a\ndouble underscore (other than special methods) are akin to private. In choosing touse protected data, we have created a dependency in that our PredatoryCreditCard\nclass might be compromised if the author of the CreditCard class were to change\nthe internal design. Note that we could have relied upon the public get\nbalance()\nmethod to retrieve the current balance within the process\n month method. But the\ncurrent design of the CreditCard class does not afford an effective way for a sub-\nclass to change the balance, other than by direct manipulation of the data member.\nIt may be tempting to use charge to add fees or interest to the balance. However,\nthat method does not allow the balance to go above the customer\u2019s credit limit,\neven though a bank would presumably let interest compound beyond the creditlimit, if warranted. If we were to redesign the original CreditCard class, we might\nadd a nonpublic method,\nset\nbalance , that could be used by subclasses to affect a\nchange without directly accessing the data member\n balance .", "2.4. Inheritance 87\n2.4.2 Hierarchy of Numeric Progressions\nAs a second example of the use of inheritance, we develop a hierarchy of classes for\niterating numeric progressions. A numeric progression is a sequence of numbers,where each number depends on one or more of the previous numbers. For example,anarithmetic progression determines the next number by adding a \ufb01xed constant\nto the previous value, and a geometric progression determines the next number\nby multiplying the previous value by a \ufb01xed constant. In general, a progressionrequires a \ufb01rst value, and a way of identifying a new value based on one or moreprevious values.\nTo maximize reusability of code, we develop a hierarchy of classes stemming\nfrom a general base class that we name Progression (see Figure 2.7). Technically,\ntheProgression class produces the progression of whole numbers: 0, 1, 2, . . ..\nHowever, this class is designed to serve as the base class for other progression types,providing as much common functionality as possible, and thereby minimizing the\nburden on the subclasses.\nFibonacciProgressionProgression\nArithmeticProgression GeometricProgression\nFigure 2.7: Our hierarchy of progression classes.\nOur implementation of the basic Progression class is provided in Code Frag-\nment 2.8. The constructor for this class accepts a starting value for the progression\n(0 by default), and initializes a data member, self.\ncurrent , to that value.\nTheProgression class implements the conventions of a Python iterator (see\nSection 2.3.4), namely the special\n next\n and\n iter\n methods. If a user of\nthe class creates a progression as seq = Progression() , each call to next(seq) will\nreturn a subsequent element of the progression sequence. It would also be possi-ble to use a for-loop syntax, forvalueinseq:, although we note that our default\nprogression is de\ufb01ned as an in\ufb01nite sequence.\nTo better separate the mechanics of the iterator convention from the core logic\nof advancing the progression, our framework relies on a nonpublic method named\nadvance to update the value of the self.\ncurrent \ufb01eld. In the default implementa-\ntion,\n advance adds one to the current value, but our intent is that subclasses will\noverride\n advance to provide a different rule for computing the next entry.\nFor convenience, the Progression class also provides a utility method, named\nprint\nprogression , that displays the next nvalues of the progression.", "88 Chapter 2. Object-Oriented Programming\n1classProgression:\n2\u201d\u201d\u201dIterator producing a generic progression.\n3\n4Default iterator produces the whole numbers 0, 1, 2, ...\n5\u201d\u201d\u201d\n67def\ninit\n(self,s t a r t = 0 ) :\n8 \u201d\u201d\u201dInitialize current to the \ufb01rst value of the progression.\u201d\u201d\u201d\n9 self.\ncurrent = start\n1011def\nadvance( self):\n12 \u201d\u201d\u201dUpdate self.\n current to a new value.\n13\n14 This should be overridden by a subclass to customize progression.\n15\n16 By convention, if current is set to None, this designates the\n17 end of a \ufb01nite progression.\n18 \u201d\u201d\u201d\n19 self.\ncurrent += 1\n2021def\nnext\n (self):\n22 \u201d\u201d\u201dReturn the next element, or else raise StopIteration error.\u201d\u201d\u201d\n23 if self.\ncurrent is None : # our convention to end a progression\n24 raiseStopIteration()\n25 else:\n26 answer = self.\ncurrent # record current value to return\n27 self.\nadvance( ) # advance to prepare for next time\n28 return answer # return the answer\n2930def\niter\n(self):\n31 \u201d\u201d\u201dBy convention, an iterator must return itself as an iterator.\u201d\u201d\u201d\n32 return self\n3334defprint\nprogression( self,n ) :\n35 \u201d\u201d\u201dPrint next n values of the progression.\u201d\u201d\u201d\n36 print(\n .join(str(next(self))forjinrange(n)))\nCode Fragment 2.8: A general numeric progression class.", "2.4. Inheritance 89\nAn Arithmetic Progression Class\nOur \ufb01rst example of a specialized progression is an arithmetic progression. While\nthe default progression increases its value by one in each step, an arithmetic pro-gression adds a \ufb01xed constant to one term of the progression to produce the next.For example, using an increment of 4 for an arithmetic progression that starts at 0\nresults in the sequence 0 ,4,8,12,....\nCode Fragment 2.9 presents our implementation of an ArithmeticProgression\nclass, which relies on Progression as its base class. The constructor for this new\nclass accepts both an increment value and a starting value as parameters, although\ndefault values for each are provided. By our convention, ArithmeticProgression(4)\nproduces the sequence 0 ,4,8,12,...,a n dArithmeticProgression(4, 1) produces\nthe sequence 1 ,5,9,13,....\nThe body of the ArithmeticProgression constructor calls the super constructor\nto initialize the\ncurrent data member to the desired start value. Then it directly\nestablishes the new\n increment data member for the arithmetic progression. The\nonly remaining detail in our implementation is to override the\n advance method so\nas to add the increment to the current value.\n1classArithmeticProgression(Progression): # inherit from Progression\n2\u201d\u201d\u201dIterator producing an arithmetic progression.\u201d\u201d\u201d\n34def\ninit\n(self, increment=1, start=0):\n5 \u201d\u201d\u201dCreate a new arithmetic progression.\n67 increment the \ufb01xed constant to add to each term (default 1)\n8 start the \ufb01rst term of the progression (default 0)\n9 \u201d\u201d\u201d\n10 super().\ninit\n(start) # initialize base class\n11 self.\nincrement = increment\n1213def\nadvance( self): # override inherited version\n14 \u201d\u201d\u201dUpdate current value by adding the \ufb01xed increment.\u201d\u201d\u201d\n15 self.\ncurrent += self.\nincrement\nCode Fragment 2.9: A class that produces an arithmetic progression.", "90 Chapter 2. Object-Oriented Programming\nA Geometric Progression Class\nOur second example of a specialized progression is a geometric progression, in\nwhich each value is produced by multiplying the preceding value by a \ufb01xed con-stant, known as the base of the geometric progression. The starting point of a ge-\nometric progression is traditionally 1, rather than 0, because multiplying 0 by any\nfactor results in 0. As an example, a geometric progression with base 2 proceeds as\n1,2,4,8,16,....\nCode Fragment 2.10 presents our implementation of a GeometricProgression\nclass. The constructor uses 2 as a default base and 1 as a default starting value, but\neither of those can be varied using optional parameters.\n1classGeometricProgression(Progression): # inherit from Progression\n2\u201d\u201d\u201dIterator producing a geometric progression.\u201d\u201d\u201d\n3\n4def\ninit\n(self, base=2, start=1):\n5 \u201d\u201d\u201dCreate a new geometric progression.\n67 base the \ufb01xed constant multiplied to each term (default 2)\n8 start the \ufb01rst term of the progression (default 1)\n9 \u201d\u201d\u201d\n10 super().\ninit\n(start)\n11 self.\nbase = base\n1213def\nadvance( self): # override inherited version\n14 \u201d\u201d\u201dUpdate current value by multiplying it by the base value.\u201d\u201d\u201d\n15 self.\ncurrent\n =self.\nbase\nCode Fragment 2.10: A class that produces a geometric progression.\nA Fibonacci Progression Class\nAs our \ufb01nal example, we demonstrate how to use our progression framework to\nproduce a Fibonacci progression . We originally discussed the Fibonacci series\non page 41 in the context of generators. Each value of a Fibonacci series is the\nsum of the two most recent values. To begin the series, the \ufb01rst two values areconventionally 0 and 1, leading to the Fibonacci series 0 ,1,1,2,3,5,8,....M o r e\ngenerally, such a series can be generated from any two starting values. For example,\nif we start with values 4 and 6, the series proceeds as 4 ,6,10,16,26,42,....", "2.4. Inheritance 91\n1classFibonacciProgression(Progression):\n2\u201d\u201d\u201dIterator producing a generalized Fibonacci progression.\u201d\u201d\u201d\n3\n4def\n init\n(self, \ufb01rst=0, second=1):\n5 \u201d\u201d\u201dCreate a new \ufb01bonacci progression.\n67 \ufb01rst the \ufb01rst term of the progression (default 0)\n8 second the second term of the progression (default 1)\n9 \u201d\u201d\u201d\n10 super().\ninit\n(\ufb01rst) # start progression at \ufb01rst\n11 self.\nprev = second \u2212\ufb01rst # \ufb01ctitious value preceding the \ufb01rst\n1213def\nadvance( self):\n14 \u201d\u201d\u201dUpdate current value by taking sum of previous two.\u201d\u201d\u201d\n15 self.\nprev,self.\ncurrent = self.\ncurrent, self.\nprev + self.\ncurrent\nCode Fragment 2.11: A class that produces a Fibonacci progression.\nWe use our progression framework to de\ufb01ne a new FibonacciProgression class,\nas shown in Code Fragment 2.11. This class is markedly different from those for the\narithmetic and geometric progressions because we cannot determine the next value\nof a Fibonacci series solely from the current one. We must maintain knowledge ofthe two most recent values. The base Progression class already provides storage\nof the most recent value as the\ncurrent data member. Our FibonacciProgression\nclass introduces a new member, named\n prev, to store the value that proceeded the\ncurrent one.\nWith both previous values stored, the implementation of\n advance is relatively\nstraightforward. (We use a simultaneous assignment similar to that on page 45.)However, the question arises as to how to initialize the previous value in the con-structor. The desired \ufb01rst and second values are provided as parameters to the\nconstructor. The \ufb01rst should be stored as\ncurrent so that it becomes the \ufb01rst\none that is reported. Looking ahead, once the \ufb01rst value is reported, we will do\nan assignment to set the new current value (which will be the second value re-ported), equal to the \ufb01rst value plus the \u201cprevious.\u201d By initializing the previous\nvalue to (second \u2212\ufb01rst) , the initial advancement will set the new current value to\n\ufb01rst + (second \u2212\ufb01rst) = second , as desired.\nTesting Our Progressions\nTo complete our presentation, Code Fragment 2.12 provides a unit test for all ofour progression classes, and Code Fragment 2.13 shows the output of that test.", "92 Chapter 2. Object-Oriented Programming\nif\nname\n ==\n __main__\n :\nprint(\n Default progression:\n )\nProgression().print\n progression(10)\nprint(\n Arithmetic progression with increment 5:\n )\nArithmeticProgression(5).print\n progression(10)\nprint(\n Arithmetic progression with increment 5 and start 2:\n )\nArithmeticProgression(5, 2).print\n progression(10)\nprint(\n Geometric progression with default base:\n )\nGeometricProgression().print\n progression(10)\nprint(\n Geometric progression with base 3:\n )\nGeometricProgression(3).print\n progression(10)\nprint(\n Fibonacci progression with default start values:\n )\nFibonacciProgression().print\n progression(10)\nprint(\n Fibonacci progression with start values 4 and 6:\n )\nFibonacciProgression(4, 6).print\n progression(10)\nCode Fragment 2.12: Unit tests for our progression classes.\nDefault progression:\n0123456789Arithmetic progression with increment 5:051 01 52 02 53 03 54 04 5\nArithmetic progression with increment 5 and start 2:\n271 21 72 22 73 23 74 24 7Geometric progression with default base:12481 63 26 41 2 82 5 65 1 2Geometric progression with base 3:\n1 3 9 27 81 243 729 2187 6561 19683\nFibonacci progression with default start values:01123581 32 13 4Fibonacci progression with start values 4 and 6:\n461 01 62 64 26 81 1 01 7 82 8 8\nCode Fragment 2.13: Output of the unit tests from Code Fragment 2.12.", "2.4. Inheritance 93\n2.4.3 Abstract Base Classes\nWhen de\ufb01ning a group of classes as part of an inheritance hierarchy, one technique\nfor avoiding repetition of code is to design a base class with common function-ality that can be inherited by other classes that need it. As an example, the hi-erarchy from Section 2.4.2 includes a Progression class, which serves as a base\nclass for three distinct subclasses: ArithmeticProgression ,GeometricProgression ,\nandFibonacciProgression . Although it is possible to create an instance of the\nProgression base class, there is little value in doing so because its behavior is sim-\nply a special case of an ArithmeticProgression with increment 1. The real purpose\nof the Progression class was to centralize the implementations of behaviors that\nother progressions needed, thereby streamlining the code that is relegated to thosesubclasses.\nIn classic object-oriented terminology, we say a class is an abstract base class\nif its only purpose is to serve as a base class through inheritance. More formally,\nan abstract base class is one that cannot be directly instantiated, while a concrete\nclass is one that can be instantiated. By this de\ufb01nition, our Progression class is\ntechnically concrete, although we essentially designed it as an abstract base class.\nIn statically typed languages such as Java and C++, an abstract base class serves\nas a formal type that may guarantee one or more abstract methods . This provides\nsupport for polymorphism, as a variable may have an abstract base class as its de-\nclared type, even though it refers to an instance of a concrete subclass. Becausethere are no declared types in Python, this kind of polymorphism can be accom-\nplished without the need for a unifying abstract base class. For this reason, there\nis not as strong a tradition of de\ufb01ning abstract base classes in Python, althoughPython\u2019s abcmodule provides support for de\ufb01ning a formal abstract base class.\nOur reason for focusing on abstract base classes in our study of data structures\nis that Python\u2019s collections module provides several abstract base classes that assist\nwhen de\ufb01ning custom data structures that share a common interface with some ofPython\u2019s built-in data structures. These rely on an object-oriented software designpattern known as the template method pattern . The template method pattern is\nwhen an abstract base class provides concrete behaviors that rely upon calls toother abstract behaviors. In that way, as soon as a subclass provides de\ufb01nitions forthe missing abstract behaviors, the inherited concrete behaviors are well de\ufb01ned.\nAs a tangible example, the collections.Sequence abstract base class de\ufb01nes be-\nhaviors common to Python\u2019s list,str,a n d tuple classes, as sequences that sup-\nport element access via an integer index. More so, the collections.Sequence class\nprovides concrete implementations of methods, count ,index ,a n d\ncontains\nthat can be inherited by any class that provides concrete implementations of both\nlen\n and\n getitem\n . For the purpose of illustration, we provide a sample\nimplementation of such a Sequence abstract base class in Code Fragment 2.14.", "94 Chapter 2. Object-Oriented Programming\n1fromabcimport ABCMeta, abstractmethod # need these de\ufb01nitions\n2\n3classSequence(metaclass=ABCMeta):\n4\u201d\u201d\u201dOur own version of collections.Sequence abstract base class.\u201d\u201d\u201d\n5\n6@abstractmethod\n7def\n len\n(self):\n8 \u201d\u201d\u201dReturn the length of the sequence.\u201d\u201d\u201d\n9\n10 @abstractmethod\n11def\n getitem\n (self,j ) :\n12 \u201d\u201d\u201dReturn the element at index j of the sequence.\u201d\u201d\u201d\n13\n14def\n contains\n (self,v a l ) :\n15 \u201d\u201d\u201dReturn True if val found in the sequence; False otherwise.\u201d\u201d\u201d\n16 forjinrange(len( self)):\n17 if self[j] == val: # found match\n18 return True\n19 return False\n2021defindex(self,v a l ) :\n22 \u201d\u201d\u201dReturn leftmost index at which val is found (or raise ValueError).\u201d\u201d\u201d\n23 forjinrange(len( self)):\n24 if self[j] == val: # leftmost match\n25 return j\n26 raiseValueError(\nvalue not in sequence\n )# never found a match\n2728defcount(self,v a l ) :\n29 \u201d\u201d\u201dReturn the number of elements equal to given value.\u201d\u201d\u201d\n30 k=0\n31 forjinrange(len( self)):\n32 if self[j] == val: # found a match\n33 k+ =1\n34 return k\nCode Fragment 2.14: An abstract base class akin to collections.Sequence .\nThis implementation relies on two advanced Python techniques. The \ufb01rst is that\nwe declare the ABCMeta class of the abcmodule as a metaclass of our Sequence\nclass. A metaclass is different from a superclass, in that it provides a template forthe class de\ufb01nition itself. Speci\ufb01cally, the ABCMeta declaration assures that the\nconstructor for the class raises an error.", "2.4. Inheritance 95\nThe second advanced technique is the use of the @abstractmethod decorator\nimmediately before the\n len\n and\n getitem\n methods are declared. That de-\nclares these two particular methods to be abstract, meaning that we do not provide\nan implementation within our Sequence base class, but that we expect any concrete\nsubclasses to support those two methods. Python enforces this expectation, by dis-allowing instantiation for any subclass that does not override the abstract methods\nwith concrete implementations.\nThe rest of the Sequence class de\ufb01nition provides tangible implementations for\nother behaviors, under the assumption that the abstract\nlen\n and\n getitem\nmethods will exist in a concrete subclass. If you carefully examine the source code,\nthe implementations of methods\n contains\n ,index ,a n dcount do not rely on any\nassumption about the selfinstances, other than that syntax len(self) andself[j] are\nsupported (by special methods\n len\n and\n getitem\n , respectively). Support\nfor iteration is automatic as well, as described in Section 2.3.4.\nIn the remainder of this book, we omit the formality of using the abcmodule.\nIf we need an \u201cabstract\u201d base class, we simply document the expectation that sub-\nclasses provide assumed functionality, without technical declaration of the methods\nas abstract. But we will make use of the wonderful abstract base classes that arede\ufb01ned within the collections module (such as Sequence ). To use such a class, we\nneed only rely on standard inheritance techniques.\nFor example, our Range class, from Code Fragment 2.6 of Section 2.3.5, is an\nexample of a class that supports the\nlen\n and\n getitem\n methods. But that\nclass does not support methods count orindex . Had we originally declared it with\nSequence as a superclass, then it would also inherit the count andindex methods.\nThe syntax for such a declaration would begin as:\nclassRange(collections.Sequence):\nFinally, we emphasize that if a subclass provides its own implementation of\nan inherited behaviors from a base class, the new de\ufb01nition overrides the inherited\none. This technique can be used when we have the ability to provide a more ef\ufb01-\ncient implementation for a behavior than is achieved by the generic approach. As\nan example, the general implementation of\n contains\n for a sequence is based\non a loop used to search for the desired value. For our Range class, there is an\nopportunity for a more ef\ufb01cient determination of containment. For example, it\nis evident that the expression, 100000 inRange(0, 2000000, 100) , should evalu-\nate toTrue , even without examining the individual elements of the range, because\nthe range starts with zero, has an increment of 100, and goes until 2 million; it\nmust include 100000, as that is a multiple of 100 that is between the start andstop values. Exercise C-2.27 explores the goal of providing an implementation of\nRange.\ncontains\n that avoids the use of a (time-consuming) loop.", "96 Chapter 2. Object-Oriented Programming\n2.5 Namespaces and Object-Orientation\nAnamespace is an abstraction that manages all of the identi\ufb01ers that are de\ufb01ned in\na particular scope, mapping each name to its associated value. In Python, functions,\nclasses, and modules are all \ufb01rst-class objects, and so the \u201cvalue\u201d associated with\nan identi\ufb01er in a namespace may in fact be a function, class, or module.\nIn Section 1.10 we explored Python\u2019s use of namespaces to manage identi\ufb01ers\nthat are de\ufb01ned with global scope, versus those de\ufb01ned within the local scope of\na function call. In this section, we discuss the important role of namespaces inPython\u2019s management of object-orientation.\n2.5.1 Instance and Class Namespaces\nWe begin by exploring what is known as the instance namespace , which man-\nages attributes speci\ufb01c to an individual object. For example, each instance of our\nCreditCard class maintains a distinct balance, a distinct account number, a distinct\ncredit limit, and so on (even though some instances may coincidentally have equiv-\nalent balances, or equivalent credit limits). Each credit card will have a dedicatedinstance namespace to manage such values.\nThere is a separate class namespace for each class that has been de\ufb01ned. This\nnamespace is used to manage members that are to be shared by all instances of\na class, or used without reference to any particular instance. For example, themake\npayment method of the CreditCard class from Section 2.3 is not stored\nindependently by each instance of that class. That member function is stored\nwithin the namespace of the CreditCard class. Based on our de\ufb01nition from Code\nFragments 2.1 and 2.2, the CreditCard class namespace includes the functions:\ninit\n ,get\ncustomer ,get\nbank ,get\naccount ,get\nbalance ,get\nlimit,charge ,\nandmake\n payment .O u rPredatoryCreditCard class has its own namespace, con-\ntaining the three methods we de\ufb01ned for that subclass:\n init\n ,charge ,a n d\nprocess\n month .\nFigure 2.8 provides a portrayal of three such namespaces: a class namespace\ncontaining methods of the CreditCard class, another class namespace with meth-\nods of the PredatoryCreditCard class, and \ufb01nally a single instance namespace for\na sample instance of the PredatoryCreditCard class. We note that there are two\ndifferent de\ufb01nitions of a function named charge , one in the CreditCard class, and\nthen the overriding method in the PredatoryCreditCard class. In similar fashion,\nthere are two distinct\n init\n implementations. However, process\n month is a\nname that is only de\ufb01ned within the scope of the PredatoryCreditCard class. The\ninstance namespace includes all data members for the instance (including the\n apr\nmember that is established by the PredatoryCreditCard constructor).", "2.5. Namespaces and Object-Orientation 97\nget\nbank\nget\naccount\nmake\n payment\nget\nbalance\nget\nlimit\ncharge\ninit\n function\nfunction\nfunctionfunctionfunctionfunctionfunctionget\ncustomerfunction\ncharge\ninit\nfunction\nfunctionprocess\n monthfunction\nbank\naccount\nbalance\nlimit\napr1234.56\n2500\nJohn Bowman\nCalifornia Savings\n5391 0375 9387 5309\ncustomer\n0.0825\n(a) (b) (c)\nFigure 2.8: Conceptual view of three namespaces: (a) the class namespace for\nCreditCard ; (b) the class namespace for PredatoryCreditCard ; (c) the instance\nnamespace for a PredatoryCreditCard object.\nHow Entries Are Established in a Namespace\nIt is important to understand why a member such as\n balance resides in a credit\ncard\u2019s instance namespace, while a member such as make\n payment resides in the\nclass namespace. The balance is established within the\n init\n method when a\nnew credit card instance is constructed. The original assignment uses the syntax,\nself.\nbalance = 0 ,w h e r e selfis an identi\ufb01er for the newly constructed instance.\nThe use of selfas a quali\ufb01er for self.\nbalance in such an assignment causes the\nbalance identi\ufb01er to be added directly to the instance namespace.\nWhen inheritance is used, there is still a single instance namespace per object.\nFor example, when an instance of the PredatoryCreditCard class is constructed,\nthe\naprattribute as well as attributes such as\n balance and\nlimit all reside in that\ninstance\u2019s namespace, because all are assigned using a quali\ufb01ed syntax, such as\nself.\napr.\nAclass namespace includes all declarations that are made directly within the\nbody of the class de\ufb01nition. For example, our CreditCard class de\ufb01nition included\nthe following structure:\nclassCreditCard:\ndefmake\n payment( self, amount):\n...\nBecause the make\n payment function is declared within the scope of the CreditCard\nclass, that function becomes associated with the name make\n payment within the\nCreditCard class namespace. Although member functions are the most typical\ntypes of entries that are declared in a class namespace, we next discuss how othertypes of data values, or even other classes can be declared within a class namespace.", "98 Chapter 2. Object-Oriented Programming\nClass Data Members\nA class-level data member is often used when there is some value, such as a con-\nstant, that is to be shared by all instances of a class. In such a case, it would\nbe unnecessarily wasteful to have each instance store that value in its instance\nnamespace. As an example, we revisit the PredatoryCreditCard introduced in Sec-\ntion 2.4.1. That class assesses a $5 fee if an attempted charge is denied becauseof the credit limit. Our choice of $5 for the fee was somewhat arbitrary, and ourcoding style would be better if we used a named variable rather than embedding\nthe literal value in our code. Often, the amount of such a fee is determined by the\nbank\u2019s policy and does not vary for each customer. In that case, we could de\ufb01neand use a class data member as follows:\nclassPredatoryCreditCard(CreditCard):\nOVERLIMIT\nFEE = 5 #t h i si sac l a s s - l e v e lm e m b e r\ndefcharge( self,p r i c e ) :\nsuccess = super().charge(price)\nif not success:\nself.\nbalance += PredatoryCreditCard.OVERLIMIT\n FEE\nreturn success\nThe data member, OVERLIMIT\n FEE, is entered into the PredatoryCreditCard\nclass namespace because that assignment takes place within the immediate scope\nof the class de\ufb01nition, and without any qualifying identi\ufb01er.\nNested Classes\nIt is also possible to nest one class de\ufb01nition within the scope of another class.\nThis is a useful construct, which we will exploit several times in this book in theimplementation of data structures. This can be done by using a syntax such as\nclassA: #t h eo u t e rc l a s s\nclassB: # the nested class\n...\nIn this case, class Bis the nested class. The identi\ufb01er Bis entered into the name-\nspace of class Aassociated with the newly de\ufb01ned class. We note that this technique\nis unrelated to the concept of inheritance, as class Bdoes not inherit from class A.\nNesting one class in the scope of another makes clear that the nested class\nexists for support of the outer class. Furthermore, it can help reduce potential name\ncon\ufb02icts, because it allows for a similarly named class to exist in another context.For example, we will later introduce a data structure known as a linked list and will\nde\ufb01ne a nested node class to store the individual components of the list. We will\nalso introduce a data structure known as a treethat depends upon its own nested", "2.5. Namespaces and Object-Orientation 99\nnode class. These two structures rely on different node de\ufb01nitions, and by nesting\nthose within the respective container classes, we avoid ambiguity.\nAnother advantage of one class being nested as a member of another is that it\nallows for a more advanced form of inheritance in which a subclass of the outer\nclass overrides the de\ufb01nition of its nested class. We will make use of that techniquein Section 11.2.1 when specializing the nodes of a tree structure.\nDictionaries and the\n slots\n Declaration\nBy default, Python represents each namespace with an instance of the built-in dict\nclass (see Section 1.2.3) that maps identifying names in that scope to the associatedobjects. While a dictionary structure supports relatively ef\ufb01cient name lookups,it requires additional memory usage beyond the raw data that it stores (we willexplore the data structure used to implement dictionaries in Chapter 10).\nPython provides a more direct mechanism for representing instance namespaces\nthat avoids the use of an auxiliary dictionary. To use the streamlined representation\nfor all instances of a class, that class de\ufb01nition must provide a class-level member\nnamed\nslots\n that is assigned to a \ufb01xed sequence of strings that serve as names\nfor instance variables. For example, with our CreditCard class, we would declare\nthe following:\nclassCreditCard:\nslots\n =\n_customer\n ,\n_bank\n ,\n_account\n ,\n_balance\n ,\n_limit\nIn this example, the right-hand side of the assignment is technically a tuple (see\ndiscussion of automatic packing of tuples in Section 1.9.3).\nWhen inheritance is used, if the base class declares\n slots\n , a subclass must\nalso declare\n slots\n to avoid creation of instance dictionaries. The declaration\nin the subclass should only include names of supplemental methods that are newly\nintroduced. For example, our PredatoryCreditCard declaration would include the\nfollowing declaration:\nclassPredatoryCreditCard(CreditCard):\nslots\n =\n_apr\n # in addition to the inherited members\nWe could choose to use the\n slots\n declaration to streamline every class in\nthis book. However, we do not do so because such rigor would be atypical for\nPython programs. With that said, there are a few classes in this book for which\nwe expect to have a large number of instances, each representing a lightweightconstruct. For example, when discussing nested classes, we suggest linked listsand trees as data structures that are often comprised of a large number of individualnodes. To promote greater ef\ufb01ciency in memory usage, we will use an explicit\nslots\n declaration in any nested classes for which we expect many instances.", "100 Chapter 2. Object-Oriented Programming\n2.5.2 Name Resolution and Dynamic Dispatch\nIn the previous section, we discussed various namespaces, and the mechanism for\nestablishing entries in those namespaces. In this section, we examine the processthat is used when retrieving a name in Python\u2019s object-oriented framework. When\nthe dot operator syntax is used to access an existing member, such as obj.foo ,t h e\nPython interpreter begins a name resolution process, described as follows:\n1. The instance namespace is searched; if the desired name is found, its associ-\nated value is used.\n2. Otherwise the class namespace, for the class to which the instance belongs,\nis searched; if the name is found, its associated value is used.\n3. If the name was not found in the immediate class namespace, the search con-\ntinues upward through the inheritance hierarchy, checking the class name-space for each ancestor (commonly by checking the superclass class, then its\nsuperclass class, and so on). The \ufb01rst time the name is found, its associate\nvalue is used.\n4. If the name has still not been found, an AttributeError is raised.\nAs a tangible example, let us assume that mycard identi\ufb01es an instance of the\nPredatoryCreditCard class. Consider the following possible usage patterns.\n\u2022mycard.\nbalance (or equivalently, self.\nbalance from within a method body):\nthe\nbalance method is found within the instance namespace formycard .\n\u2022mycard.process\n month() : the search begins in the instance namespace, but\nthe name process\n month is not found in that namespace. As a result, the\nPredatoryCreditCard class namespace is searched; in this case, the name is\nfound and that method is called.\n\u2022mycard.make\n payment(200) : the search for the name, make\n payment , fails\nin the instance namespace and in the PredatoryCreditCard namespace. The\nname is resolved in the namespace for superclass CreditCard and thus the\ninherited method is called.\n\u2022mycard.charge(50) : the search for name charge fails in the instance name-\nspace. The next namespace checked is for the PredatoryCreditCard class,\nbecause that is the true type of the instance. There is a de\ufb01nition for a charge\nfunction in that class, and so that is the one that is called.\nIn the last case shown, notice that the existence of a charge function in the\nPredatoryCreditCard class has the effect of overriding the version of that function\nthat exists in the CreditCard namespace. In traditional object-oriented terminol-\nogy, Python uses what is known as dynamic dispatch (ordynamic binding )t o\ndetermine, at run-time, which implementation of a function to call based upon thetype of the object upon which it is invoked. This is in contrast to some languagesthat use static dispatching , making a compile-time decision as to which version of\na function to call, based upon the declared type of a variable.", "2.6. Shallow and Deep Copying 101\n2.6 Shallow and Deep Copying\nIn Chapter 1, we emphasized that an assignment statement foo = bar makes the\nname fooanalias for the object identi\ufb01ed as bar. In this section, we consider\nthe task of making a copy of an object, rather than an alias. This is necessary in\napplications when we want to subsequently modify either the original or the copy\nin an independent manner.\nConsider a scenario in which we manage various lists of colors, with each color\nrepresented by an instance of a presumed color class. We let identi\ufb01er warmtones\ndenote an existing list of such colors (e.g., oranges, browns). In this application,we wish to create a new list named palette , which is a copy of the warmtones list.\nHowever, we want to subsequently be able to add additional colors to palette ,o r\nto modify or remove some of the existing colors, without affecting the contents ofwarmtones . If we were to execute the command\npalette = warmtones\nthis creates an alias, as shown in Figure 2.9. No new list is created; instead, the\nnew identi\ufb01er palette references the original list.\nred\ngreen\nbluecolor\n52163169listwarmtones\nred\ngreen\nbluecolor\n43124249palette\nFigure 2.9: Two aliases for the same list of colors.\nUnfortunately, this does not meet our desired criteria, because if we subsequently\nadd or remove colors from \u201cpalette,\u201d we modify the list identi\ufb01ed as warmtones .\nWe can instead create a new instance of the listclass by using the syntax:\npalette = list(warmtones)\nIn this case, we explicitly call the listconstructor, sending the \ufb01rst list as a param-\neter. This causes a new list to be created, as shown in Figure 2.10; however, it iswhat is known as a shallow copy . The new list is initialized so that its contents are\nprecisely the same as the original sequence. However, Python\u2019s lists are referential\n(see page 9 of Section 1.2.3), and so the new list represents a sequence of references\nto the same elements as in the \ufb01rst.", "102 Chapter 2. Object-Oriented Programming\nred\ngreen\nbluecolor\n52163169list listwarmtones palette\nred\ngreen\nbluecolor\n43124249\nFigure 2.10: A shallow copy of a list of colors.\nThis is a better situation than our \ufb01rst attempt, as we can legitimately add\nor remove elements from palette without affecting warmtones .H o w e v e r , i f w e\nedit a color instance from the palette list, we effectively change the contents of\nwarmtones . Although palette andwarmtones are distinct lists, there remains indi-\nrect aliasing, for example, with palette[0] andwarmtones[0] as aliases for the same\ncolor instance.\nWe prefer that palette be what is known as a deep copy ofwarmtones .I n a\ndeep copy, the new copy references its own copies of those objects referenced by\nthe original version. (See Figure 2.11.)\nbluecolor\n52163169list\nred\ngreen\nbluecolor\n43124249\nred\ngreen\nbluecolor\n52163169list\nred\ngreen\nbluecolor\n43124249warmtones palette\nred\ngreen\nFigure 2.11: A deep copy of a list of colors.\nPython\u2019s copy Module\nTo create a deep copy, we could populate our list by explicitly making copies of\nthe original color instances, but this requires that we know how to make copies of\ncolors (rather than aliasing). Python provides a very convenient module, named\ncopy , that can produce both shallow copies and deep copies of arbitrary objects.\nThis module supports two functions: the copy function creates a shallow copy\nof its argument, and the deepcopy function creates a deep copy of its argument.\nAfter importing the module, we may create a deep copy for our example, as shownin Figure 2.11, using the command:\npalette = copy.deepcopy(warmtones)", "2.7. Exercises 103\n2.7 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-2.1 Give three examples of life-critical software applications.\nR-2.2 Give an example of a software application in which adaptability can mean\nthe difference between a prolonged lifetime of sales and bankruptcy.\nR-2.3 Describe a component from a text-editor GUI and the methods that it en-capsulates.\nR-2.4 Write a Python class, Flower , that has three instance variables of type str,\nint,a n d\ufb02oat , that respectively represent the name of the \ufb02ower, its num-\nber of petals, and its price. Your class must include a constructor methodthat initializes each variable to an appropriate value, and your class shouldinclude methods for setting the value of each type, and retrieving the value\nof each type.\nR-2.5 Use the techniques of Section 1.7 to revise the charge andmake\npayment\nmethods of the CreditCard class to ensure that the caller sends a number\nas a parameter.\nR-2.6 If the parameter to the make\n payment method of the CreditCard class\nwere a negative number, that would have the effect of raising the balance\non the account. Revise the implementation so that it raises a ValueError if\na negative value is sent.\nR-2.7 TheCreditCard class of Section 2.3 initializes the balance of a new ac-\ncount to zero. Modify that class so that a new account can be given anonzero balance using an optional \ufb01fth parameter to the constructor. The\nfour-parameter constructor syntax should continue to produce an accountwith zero balance.\nR-2.8 Modify the declaration of the \ufb01rst for loop in the CreditCard tests, from\nCode Fragment 2.3, so that it will eventually cause exactly one of the three\ncredit cards to go over its credit limit. Which credit card is it?\nR-2.9 Implement the\nsub\n method for the Vector class of Section 2.3.3, so\nthat the expression u\u2212vreturns a new vector instance representing the\ndifference between two vectors.\nR-2.10 Implement the\n neg\n method for the Vector class of Section 2.3.3, so\nthat the expression \u2212vreturns a new vector instance whose coordinates\nare all the negated values of the respective coordinates of v.", "104 Chapter 2. Object-Oriented Programming\nR-2.11 In Section 2.3.3, we note that our Vector class supports a syntax such as\nv=u+[ 5 ,3 ,1 0 , \u22122, 1] , in which the sum of a vector and list returns\na new vector. However, the syntax v=[ 5 ,3 ,1 0 , \u22122, 1] + u is illegal.\nExplain how the Vector class de\ufb01nition can be revised so that this syntax\ngenerates a new vector.\nR-2.12 Implement the\n mul\n method for the Vector class of Section 2.3.3, so\nthat the expression v\n3returns a new vector with coordinates that are 3\ntimes the respective coordinates of v.\nR-2.13 Exercise R-2.12 asks for an implementation of\n mul\n ,f o rt h e Vector\nclass of Section 2.3.3, to provide support for the syntax v\n3. Implement\nthe\n rmul\n method, to provide additional support for syntax 3\nv.\nR-2.14 Implement the\n mul\n method for the Vector class of Section 2.3.3, so\nthat the expression u\nvreturns a scalar that represents the dot product of\nthe vectors, that is, \u2211d\ni=1ui\u00b7vi.\nR-2.15 TheVector class of Section 2.3.3 provides a constructor that takes an in-\ntegerd, and produces a d-dimensional vector with all coordinates equal to\n0. Another convenient form for creating a new vector would be to send the\nconstructor a parameter that is some iterable type representing a sequenceof numbers, and to create a vector with dimension equal to the length of\nthat sequence and coordinates equal to the sequence values. For example,\nVector([4, 7, 5]) would produce a three-dimensional vector with coordi-\nnates<4, 7, 5 >. Modify the constructor so that either of these forms is\nacceptable; that is, if a single integer is sent, it produces a vector of that\ndimension with all zeros, but if a sequence of numbers is provided, it pro-\nduces a vector with coordinates based on that sequence.\nR-2.16 OurRange class, from Section 2.3.5, relies on the formula\nmax(0, (stop \u2212start + step \u22121) // step)\nto compute the number of elements in the range. It is not immediately ev-ident why this formula provides the correct calculation, even if assuming\na positive step size. Justify this formula, in your own words.\nR-2.17 Draw a class inheritance diagram for the following set of classes:\n\u2022Class Goat extends object and adds an instance variable\ntailand\nmethods milk() andjump() .\n\u2022Class Pigextends object and adds an instance variable\n nose and\nmethods eat(food) andwallow() .\n\u2022ClassHorse extends object and adds instance variables\n height and\ncolor , and methods run() andjump() .\n\u2022ClassRacer extends Horse and adds a method race() .\n\u2022ClassEquestrian extends Horse , adding an instance variable\n weight\nand methods trot() andis\ntrained() .", "2.7. Exercises 105\nR-2.18 Give a short fragment of Python code that uses the progression classes\nfrom Section 2.4.2 to \ufb01nd the 8thvalue of a Fibonacci progression that\nstarts with 2 and 2 as its \ufb01rst two values.\nR-2.19 When using the ArithmeticProgression class of Section 2.4.2 with an in-\ncrement of 128 and a start of 0, how many calls to next can we make\nbefore we reach an integer of 263or larger?\nR-2.20 What are some potential ef\ufb01ciency disadvantages of having very deep in-heritance trees, that is, a large set of classes, A,B,C, and so on, such that\nBextends A,Cextends B,Dextends C,e t c . ?\nR-2.21 What are some potential ef\ufb01ciency disadvantages of having very shallow\ninheritance trees, that is, a large set of classes, A,B,C, and so on, such\nthat all of these classes extend a single class, Z?\nR-2.22 Thecollections.Sequence abstract base class does not provide support for\ncomparing two sequences to each other. Modify our Sequence class from\nCode Fragment 2.14 to include a de\ufb01nition for the\neq\n method, so\nthat expression seq1 == seq2 will return True precisely when the two\nsequences are element by element equivalent.\nR-2.23 In similar spirit to the previous problem, augment the Sequence class with\nmethod\n lt\n, to support lexicographic comparison seq1<seq2.\nCreativity\nC-2.24 Suppose you are on the design team for a new e-book reader. What are the\nprimary classes and methods that the Python software for your reader will\nneed? You should include an inheritance diagram for this code, but youdo not need to write any actual code. Your software architecture shouldat least include ways for customers to buy new books, view their list ofpurchased books, and read their purchased books.\nC-2.25 Exercise R-2.12 uses the\nmul\n method to support multiplying a Vector\nby a number, while Exercise R-2.14 uses the\n mul\n method to support\ncomputing a dot product of two vectors. Give a single implementation of\nVector.\n mul\n that uses run-time type checking to support both syntaxes\nu\nvandu\nk,w h e r e uandvdesignate vector instances and krepresents\na number.\nC-2.26 TheSequenceIterator class of Section 2.3.4 provides what is known as a\nforward iterator. Implement a class named ReversedSequenceIterator that\nserves as a reverse iterator for any Python sequence type. The \ufb01rst call to\nnext should return the last element of the sequence, the second call to next\nshould return the second-to-last element, and so forth.", "106 Chapter 2. Object-Oriented Programming\nC-2.27 In Section 2.3.5, we note that our version of the Range class has im-\nplicit support for iteration, due to its explicit support of both\n len\nand\n getitem\n . The class also receives implicit support of the Boolean\ntest, \u201ckinr\u201d for Range r. This test is evaluated based on a forward itera-\ntion through the range, as evidenced by the relative quickness of the test\n2inRange(10000000) versus 9999999 inRange(10000000) . Provide a\nmore ef\ufb01cient implementation of the\n contains\n method to determine\nwhether a particular value lies within a given range. The running time ofyour method should be independent of the length of the range.\nC-2.28 ThePredatoryCreditCard class of Section 2.4.1 provides a process\nmonth\nmethod that models the completion of a monthly cycle. Modify the class\nso that once a customer has made ten calls to charge in the current month,\neach additional call to that function results in an additional $1 surcharge.\nC-2.29 Modify the PredatoryCreditCard class from Section 2.4.1 so that a cus-\ntomer is assigned a minimum monthly payment, as a percentage of the\nbalance, and so that a late fee is assessed if the customer does not subse-quently pay that minimum amount before the next monthly cycle.\nC-2.30 At the close of Section 2.4.1, we suggest a model in which the CreditCard\nclass supports a nonpublic method,\nset\nbalance(b) , that could be used\nby subclasses to affect a change to the balance, without directly accessingthe\nbalance data member. Implement such a model, revising both the\nCreditCard andPredatoryCreditCard classes accordingly.\nC-2.31 Write a Python class that extends the Progression class so that each value\nin the progression is the absolute value of the difference between the pre-vious two values. You should include a constructor that accepts a pair ofnumbers as the \ufb01rst two values, using 2 and 200 as the defaults.\nC-2.32 Write a Python class that extends the Progression class so that each value\nin the progression is the square root of the previous value. (Note that\nyou can no longer represent each value with an integer.) Your construc-\ntor should accept an optional parameter specifying the start value, using65,536 as a default.\nProjects\nP-2.33 Write a Python program that inputs a polynomial in standard algebraicnotation and outputs the \ufb01rst derivative of that polynomial.\nP-2.34 Write a Python program that inputs a document and then outputs a bar-chart plot of the frequencies of each alphabet character that appears in\nthat document.", "2.7. Exercises 107\nP-2.35 Write a set of Python classes that can simulate an Internet application in\nwhich one party, Alice, is periodically creating a set of packets that she\nwants to send to Bob. An Internet process is continually checking if Alicehas any packets to send, and if so, it delivers them to Bob\u2019s computer, andBob is periodically checking if his computer has a packet from Alice, and,if so, he reads and deletes it.\nP-2.36 Write a Python program to simulate an ecosystem containing two typesof creatures, bears and\ufb01sh. The ecosystem consists of a river, which is\nmodeled as a relatively large list. Each element of the list should be aBear object, a Fish object, or None . In each time step, based on a random\nprocess, each animal either attempts to move into an adjacent list locationor stay where it is. If two animals of the same type are about to collide inthe same cell, then they stay where they are, but they create a new instanceof that type of animal, which is placed in a random empty (i.e., previously\nNone ) location in the list. If a bear and a \ufb01sh collide, however, then the\n\ufb01sh dies (i.e., it disappears).\nP-2.37 Write a simulator, as in the previous project, but add a Boolean gender\n\ufb01eld and a \ufb02oating-point strength \ufb01eld to each animal, using an Animal\nclass as a base class. If two animals of the same type try to collide, then\nthey only create a new instance of that type of animal if they are of differ-ent genders. Otherwise, if two animals of the same type and gender try tocollide, then only the one of larger strength survives.\nP-2.38 Write a Python program that simulates a system that supports the func-tions of an e-book reader. You should include methods for users of yoursystem to \u201cbuy\u201d new books, view their list of purchased books, and readtheir purchased books. Your system should use actual books, which have\nexpired copyrights and are available on the Internet, to populate your set\nof available books for users of your system to \u201cpurchase\u201d and read.\nP-2.39 Develop an inheritance hierarchy based upon a Polygon class that has\nabstract methods area() andperimeter() . Implement classes Triangle ,\nQuadrilateral ,Pentagon ,Hexagon ,a n d Octagon that extend this base\nclass, with the obvious meanings for the area() andperimeter() methods.\nAlso implement classes, IsoscelesTriangle ,EquilateralTriangle ,Rectan-\ngle,a n dSquare , that have the appropriate inheritance relationships. Fi-\nnally, write a simple program that allows users to create polygons of the\nvarious types and input their geometric dimensions, and the program then\noutputs their area and perimeter. For extra effort, allow users to inputpolygons by specifying their vertex coordinates and be able to test if twosuch polygons are similar.", "108 Chapter 2. Object-Oriented Programming\nChapter Notes\nFor a broad overview of developments in comput er science and engineering, we refer the\nreader to The Computer Science and Engineering Handbook [96]. For more information\nabout the Therac-25 incident, please see the paper by Leveson and Turner [69].\nThe reader interested in studying object -oriented programming further, is referred to\nthe books by Booch [17], Budd [20], and Liskov and Guttag [71]. Liskov and Guttag\nalso provide a nice discussion of abstract data types, as does the survey paper by Cardelli\nand Wegner [23] and the book chapter by Demurjian [33] in the The Computer Science\nand Engineering Handbook [96]. Design patterns are described in the book by Gamma et\nal.[41].\nBooks with speci\ufb01c focus on object-oriented programming in Python include those\nby Goldwasser and Letscher [43] at the introductory level, and by Phillips [83] at a moreadvanced level,", "Chapter\n3Algorithm Analysis\nContents\n3 . 1 E x p e r i m e n t a lS t u d i e s ..................... 1 1 1\n3.1.1 Moving Beyond Experimental Analysis . . . . . . . . . . . 113\n3 . 2 T h eS e v e nF u n c t i o n sU s e di nT h i sB o o k .......... 1 1 5\n3 . 2 . 1 C o m p a r i n g G r o w t h R a t e s..................1 2 2\n3 . 3 A s y m p t o t i cA n a l y s i s...................... 1 2 3\n3 . 3 . 1 T h e \u201c B i g - O h \u201d N o t a t i o n...................1 2 3\n3 . 3 . 2 C o m p a r a t i v e A n a l y s i s....................1 2 8\n3 . 3 . 3 E x a m p l e s o f A l g o r i t h m A n a l y s i s ..............1 3 0\n3 . 4 S i m p l eJ u s t i \ufb01 c a t i o nT e c h n i q u e s ............... 1 3 7\n3 . 4 . 1 B y E x a m p l e .........................1 3 73 . 4 . 2 T h e \u201c C o n t r a \u201d A t t a c k....................1 3 7\n3.4.3 Induction and Loop Invariants . . . . . . . . . . . . . . . 138\n3 . 5 E x e r c i s e s ............................ 1 4 1\n", "110 Chapter 3. Algorithm Analysis\nIn a classic story, the famous mathematician Archimedes was asked to deter-\nmine if a golden crown commissioned by the king was indeed pure gold, and not\npart silver, as an informant had claimed. Archimedes discovered a way to performthis analysis while stepping into a bath. He noted that water spilled out of the bathin proportion to the amount of him that went in. Realizing the implications of thisfact, he immediately got out of the bath and ran naked through the city shouting,\n\u201cEureka, eureka!\u201d for he had discovered an analysis tool (displacement), which,\nwhen combined with a simple scale, could determine if the king\u2019s new crown wasgood or not. That is, Archimedes could dip the crown and an equal-weight amountof gold into a bowl of water to see if they both displaced the same amount. This\ndiscovery was unfortunate for the goldsmith, however, for when Archimedes did\nhis analysis, the crown displaced more water than an equal-weight lump of puregold, indicating that the crown was not, in fact, pure gold.\nIn this book, we are interested in the design of \u201cgood\u201d data structures and algo-\nrithms. Simply put, a data structure is a systematic way of organizing and access-\ning data, and an algorithm is a step-by-step procedure for performing some task in\na \ufb01nite amount of time. These concepts are central to computing, but to be able toclassify some data structures and algorithms as \u201cgood,\u201d we must have precise waysof analyzing them.\nThe primary analysis tool we will use in this book involves characterizing the\nrunning times of algorithms and data structure operations, with space usage also\nbeing of interest. Running time is a natural measure of \u201cgoodness,\u201d since time is a\nprecious resource\u2014computer solutions should run as fast as possible. In general,the running time of an algorithm or data structure operation increases with the inputsize, although it may also vary for different inputs of the same size. Also, the run-\nning time is affected by the hardware environment (e.g., the processor, clock rate,\nmemory, disk) and software environment (e.g., the operating system, programminglanguage) in which the algorithm is implemented and executed. All other factorsbeing equal, the running time of the same algorithm on the same input data will besmaller if the computer has, say, a much faster processor or if the implementation\nis done in a program compiled into native machine code instead of an interpreted\nimplementation. We begin this chapter by discussing tools for performing exper-imental studies, yet also limitations to the use of experiments as a primary meansfor evaluating algorithm ef\ufb01ciency.\nFocusing on running time as a primary measure of goodness requires that we be\nable to use a few mathematical tools. In spite of the possible variations that comefrom different environmental factors, we would like to focus on the relationshipbetween the running time of an algorithm and the size of its input. We are interestedin characterizing an algorithm\u2019s running time as a function of the input size. Butwhat is the proper way of measuring it? In this chapter, we \u201croll up our sleeves\u201d\nand develop a mathematical way of analyzing algorithms.", "3.1. Experimental Studies 111\n3.1 Experimental Studies\nIf an algorithm has been implemented, we can study its running time by executing\nit on various test inputs and recording the time spent during each execution. Asimple approach for doing this in Python is by using the time function of the time\nmodule. This function reports the number of seconds, or fractions thereof, that haveelapsed since a benchmark time known as the epoch. The choice of the epoch isnot signi\ufb01cant to our goal, as we can determine the elapsed time by recording the\ntime just before the algorithm and the time just after the algorithm, and computing\ntheir difference, as follows:\nfromtimeimport time\nstart\ntime = time( ) # record the starting time\nrun algorithmend\ntime = time( ) # record the ending time\nelapsed = end\n time\u2212start\ntime # compute the elapsed time\nWe will demonstrate use of this approach, in Chapter 5, to gather experimental dataon the ef\ufb01ciency of Python\u2019s listclass. An elapsed time measured in this fashion\nis a decent re\ufb02ection of the algorithm ef\ufb01ciency, but it is by no means perfect. The\ntime function measures relative to what is known as the \u201cwall clock.\u201d Because\nmany processes share use of a computer\u2019s central processing unit (orCPU ), the\nelapsed time will depend on what other processes are running on the computer\nwhen the test is performed. A fairer metric is the number of CPU cycles that are\nused by the algorithm. This can be determined using the clock function of the time\nmodule, but even this measure might not be consistent if repeating the identicalalgorithm on the identical input, and its granularity will depend upon the computer\nsystem. Python includes a more advanced module, named timeit ,t oh e l pa u t o m a t e\nsuch evaluations with repetition to account for such variance among trials.\nBecause we are interested in the general dependence of running time on the size\nand structure of the input, we should perform independent experiments on many\ndifferent test inputs of various sizes. We can then visualize the results by plottingthe performance of each run of the algorithm as a point with x-coordinate equal to\nthe input size, n,a n d y-coordinate equal to the running time, t. Figure 3.1 displays\nsuch hypothetical data. This visualization may provide some intuition regardingthe relationship between problem size and execution time for the algorithm. Thismay lead to a statistical analysis that seeks to \ufb01t the best function of the input sizeto the experimental data. To be meaningful, this analysis requires that we choosegood sample inputs and test enough of them to be able to make sound statistical\nclaims about the algorithm\u2019s running time.", "112 Chapter 3. Algorithm AnalysisRunning Time (ms)\n100300400500\n100000\n0 5000 15000200\nInput Size\nFigure 3.1:Results of an experimental study on the running time of an algorithm.\nA dot with coordinates (n,t)indicates that on an input of size n, the running time\nof the algorithm was measured as tmilliseconds (ms).\nChallenges of Experimental Analysis\nWhile experimental studies of running times are valuable, especially when \ufb01ne-\ntuning production-quality code, there are three major limitations to their use for\nalgorithm analysis:\n\u2022Experimental running times of two algorithms are dif\ufb01cult to directly com-\npare unless the experiments are performed in the same hardware and softwareenvironments.\n\u2022Experiments can be done only on a limited set of test inputs; hence, theyleave out the running times of inputs not included in the experiment (and\nthese inputs may be important).\n\u2022An algorithm must be fully implemented in order to execute it to study its\nrunning time experimentally.\nThis last requirement is the most serious drawback to the use of experimental stud-ies. At early stages of design, when considering a choice of data structures oralgorithms, it would be foolish to spend a signi\ufb01cant amount of time implementing\nan approach that could easily be deemed inferior by a higher-level analysis.", "3.1. Experimental Studies 113\n3.1.1 Moving Beyond Experimental Analysis\nOur goal is to develop an approach to analyzing the ef\ufb01ciency of algorithms that:\n1. Allows us to evaluate the relative ef\ufb01ciency of any two algorithms in a way\nthat is independent of the hardware and software environment.\n2. Is performed by studying a high-level description of the algorithm without\nneed for implementation.\n3. Takes into account all possible inputs.\nCounting Primitive Operations\nTo analyze the running time of an algorithm without performing experiments, we\nperform an analysis directly on a high-level description of the algorithm (either in\nthe form of an actual code fragment, or language-independent pseudo-code). Wede\ufb01ne a set of primitive operations such as the following:\n\u2022Assigning an identi\ufb01er to an object\n\u2022Determining the object associated with an identi\ufb01er\n\u2022Performing an arithmetic operation (for example, adding two numbers)\n\u2022Comparing two numbers\n\u2022Accessing a single element of a Python listby index\n\u2022Calling a function (excluding operations executed within the function)\n\u2022Returning from a function.\nFormally, a primitive operation corresponds to a low-level instruction with an exe-cution time that is constant. Ideally, this might be the type of basic operation that isexecuted by the hardware, although many of our primitive operations may be trans-\nlated to a small number of instructions. Instead of trying to determine the speci\ufb01c\nexecution time of each primitive operation, we will simply count how many prim-itive operations are executed, and use this number tas a measure of the running\ntime of the algorithm.\nThis operation count will correlate to an actual running time in a speci\ufb01c com-\nputer, for each primitive operation corresponds to a constant number of instructions,and there are only a \ufb01xed number of primitive operations. The implicit assumptionin this approach is that the running times of different primitive operations will befairly similar. Thus, the number, t, of primitive operations an algorithm performs\nwill be proportional to the actual running time of that algorithm.\nMeasuring Operations as a Function of Input Size\nTo capture the order of growth of an algorithm\u2019s running time, we will associate,with each algorithm, a function f(n)that characterizes the number of primitive\noperations that are performed as a function of the input size n. Section 3.2 will in-\ntroduce the seven most common functions that arise, and Section 3.3 will introduce\na mathematical framework for comparing functions to each other.", "114 Chapter 3. Algorithm Analysis\nFocusing on the Worst-Case Input\nAn algorithm may run faster on some inputs than it does on others of the same size.\nThus, we may wish to express the running time of an algorithm as the function of\nthe input size obtained by taking the average over all possible inputs of the same\nsize. Unfortunately, such an average-case analysis is typically quite challenging.\nIt requires us to de\ufb01ne a probability distribution on the set of inputs, which is oftena dif\ufb01cult task. Figure 3.2 schematically shows how, depending on the input distri-bution, the running time of an algorithm can be anywhere between the worst-case\ntime and the best-case time. For example, what if inputs are really only of types\n\u201cA\u201d or \u201cD\u201d?\nAn average-case analysis usually requires that we calculate expected running\ntimes based on a given input distribution, which usually involves sophisticated\nprobability theory. Therefore, for the remainder of this book, unless we specify\notherwise, we will characterize running times in terms of the worst case ,a saf u n c -\ntion of the input size, n, of the algorithm.\nWorst-case analysis is much easier than average-case analysis, as it requires\nonly the ability to identify the worst-case input, which is often simple. Also, this\napproach typically leads to better algorithms. Making the standard of success for an\nalgorithm to perform well in the worst case necessarily requires that it will do wellonevery input. That is, designing for the worst case leads to stronger algorithmic\n\u201cmuscles,\u201d much like a track star who always practices by running up an incline.\nbest-case time\nBCDEFGaverage-case time?\nA/bracerightbigg\nInput Instance1m s2m s3m s4m s5m sRunning Time (ms)worst-case time\nFigure 3.2: The difference between best-case and worst-case time. Each bar repre-\nsents the running time of some algorithm on a different possible input.", "3.2. The Seven Functions Used in This Book 115\n3.2 The Seven Functions Used in This Book\nIn this section, we brie\ufb02y discuss the seven most important functions used in the\nanalysis of algorithms. We will use only these seven simple functions for almostall the analysis we do in this book. In fact, a section that uses a function other\nthan one of these seven will be marked with a star ( \u22c6) to indicate that it is optional.\nIn addition to these seven fundamental functions, Appendix B contains a list of\nother useful mathematical facts that apply in the analysis of data structures andalgorithms.\nThe Constant Function\nThe simplest function we can think of is the constant function . This is the function,\nf(n)=c,\nfor some \ufb01xed constant c,s u c ha s c=5,c=27, or c=210. T h a ti s ,f o ra n y\nargument n, the constant function f(n)assigns the value c. In other words, it does\nnot matter what the value of nis;f(n)will always be equal to the constant value c.\nBecause we are most interested in integer functions, the most fundamental con-\nstant function is g(n)=1, and this is the typical constant function we use in this\nbook. Note that any other constant function, f(n)=c, can be written as a constant\nctimes g(n).T h a ti s , f(n)=cg(n)in this case.\nAs simple as it is, the constant function is useful in algorithm analysis, because\nit characterizes the number of steps needed to do a basic operation on a computer,like adding two numbers, assigning a value to some variable, or comparing twonumbers.\nThe Logarithm Function\nOne of the interesting and sometimes even surprising aspects of the analysis ofdata structures and algorithms is the ubiquitous presence of the logarithm function ,\nf(n)=log\nbn, for some constant b>1. This function is de\ufb01ned as follows:\nx=logbnif and only if bx=n.\nBy de\ufb01nition, logb1=0. The value bis known as the base of the logarithm.\nThe most common base for the logarithm function in computer science is 2,\nas computers store integers in binary, and because a common operation in manyalgorithms is to repeatedly divide an input in half. In fact, this base is so common\nthat we will typically omit it from the notation when it is 2. That is, for us,\nlogn=log\n2n.", "116 Chapter 3. Algorithm Analysis\nWe note that most handheld calculators have a button marked LOG , but this is\ntypically for calculating the logarithm base-10, not base-two.\nComputing the logarithm function exactly for any integer ninvolves the use\nof calculus, but we can use an approximation that is good enough for our pur-\nposes without calculus. In particular, we can easily compute the smallest integergreater than or equal to log\nbn(its so-called ceiling ,\u2308logbn\u2309). For positive integer,\nn, this value is equal to the number of times we can divide nbybbefore we get\na number less than or equal to 1. For example, the evaluation of \u2308log327\u2309is 3,\nbecause ((27/3)/3)/3=1. Likewise, \u2308log464\u2309is 3, because ((64/4)/4)/4=1,\nand\u2308log212\u2309is 4, because (((12/2)/2)/2)/2=0.75\u22641.\nThe following proposition describes several important identities that involve\nlogarithms for any base greater than 1.\nProposition 3.1 (Logarithm Rules): Given real numbers a>0,b>1,c>0\nandd>1,w eh a v e :\n1. logb(ac)=logba+logbc\n2. logb(a/c)=logba\u2212logbc\n3. logb(ac)=clogba\n4. logba=logda/logdb\n5.blogda=alogdb\nBy convention, the unparenthesized notation log ncdenotes the value log (nc).\nWe use a notational shorthand, logcn, to denote the quantity, (logn)c,i nw h i c ht h e\nresult of the logarithm is raised to a power.\nThe above identities can be derived from converse rules for exponentiation that\nwe will present on page 121. We illustrate these identities with a few examples.\nExample 3.2: We demonstrate below some interesting applications of the loga-\nrithm rules from Proposition 3.1 (using the usual convention that the base of a\nlogarithm is 2 if it is omitted).\n\u2022log (2n)=log2 +logn=1+logn,b yr u l e1\n\u2022log (n/2)=logn\u2212log2 =logn\u22121,b yr u l e2\n\u2022logn3=3log n,b yr u l e3\n\u2022log 2n=nlog 2 =n\u00b71=n,b yr u l e3\n\u2022log4n=(logn)/log 4 =(logn)/2,b yr u l e4\n\u20222logn=nlog 2=n1=n,b yr u l e5 .\nAs a practical matter, we note that rule 4 gives us a way to compute the base-two\nlogarithm on a calculator that has a base-10 logarithm button, LOG ,f o r\nlog2n=LOG n/LOG 2.", "3.2. The Seven Functions Used in This Book 117\nThe Linear Function\nAnother simple yet important function is the linear function ,\nf(n)=n.\nThat is, given an input value n, the linear function fassigns the value nitself.\nThis function arises in algorithm analysis any time we have to do a single basic\noperation for each of nelements. For example, comparing a number xto each\nelement of a sequence of size nwill require ncomparisons. The linear function\nalso represents the best running time we can hope to achieve for any algorithm that\nprocesses each of nobjects that are not already in the computer\u2019s memory, because\nreading in the nobjects already requires noperations.\nThe N-Log- NFunction\nThe next function we discuss in this section is the n-log-n function ,\nf(n)=nlogn,\nthat is, the function that assigns to an input nthe value of ntimes the logarithm\nbase-two of n. This function grows a little more rapidly than the linear function and\na lot less rapidly than the quadratic function; therefore, we would greatly prefer analgorithm with a running time that is proportional to nlogn, than one with quadratic\nrunning time. We will see several important algorithms that exhibit a running time\nproportional to the n-log- nfunction. For example, the fastest possible algorithms\nfor sorting narbitrary values require time proportional to nlogn.\nThe Quadratic Function\nAnother function that appears often in algorithm analysis is the quadratic function ,\nf(n)=n2.\nThat is, given an input value n, the function fassigns the product of nwith itself\n(in other words, \u201c nsquared\u201d).\nThe main reason why the quadratic function appears in the analysis of algo-\nrithms is that there are many algorithms that have nested loops, where the inner\nloop performs a linear number of operations and the outer loop is performed alinear number of times. Thus, in such cases, the algorithm performs n\u00b7n=n\n2\noperations.", "118 Chapter 3. Algorithm Analysis\nNested Loops and the Quadratic Function\nThe quadratic function can also arise in the context of nested loops where the \ufb01rst\niteration of a loop uses one operation, the second uses two operations, the third usesthree operations, and so on. That is, the number of operations is\n1+2+3+\u00b7\u00b7\u00b7+(n\u22122)+(n\u22121)+n.\nIn other words, this is the total number of operations that will be performed by thenested loop if the number of operations performed inside the loop increases by onewith each iteration of the outer loop. This quantity also has an interesting history.\nIn 1787, a German schoolteacher decided to keep his 9- and 10-year-old pupils\noccupied by adding up the integers from 1 to 100. But almost immediately one\nof the children claimed to have the answer! The teacher was suspicious, for the\nstudent had only the answer on his slate. But the answer, 5050, was correct and thestudent, Carl Gauss, grew up to be one of the greatest mathematicians of his time.We presume that young Gauss used the following identity.\nProposition 3.3:\nFor any integer n\u22651,w eh a v e :\n1+2+3+\u00b7\u00b7\u00b7+(n\u22122)+(n\u22121)+n=n(n+1)\n2.\nWe give two \u201cvisual\u201d justi\ufb01cations of Proposition 3.3 in Figure 3.3.\n12 n012n\n33 ...\n1 n/2012n\n3\n2n+1...\n(a) (b)\nFigure 3.3: Visual justi\ufb01cations of Proposition 3.3. Both illustrations visualize the\nidentity in terms of the total area covered by nunit-width rectangles with heights\n1,2,..., n. In (a), the rectangles are shown to cover a big triangle of area n2/2 (base\nnand height n)p l u s nsmall triangles of area 1 /2 each (base 1 and height 1). In\n(b), which applies only when nis even, the rectangles are shown to cover a big\nrectangle of base n/2 and height n+1.", "3.2. The Seven Functions Used in This Book 119\nThe lesson to be learned from Proposition 3.3 is that if we perform an algorithm\nwith nested loops such that the operations in the inner loop increase by one each\ntime, then the total number of operations is quadratic in the number of times, n,\nwe perform the outer loop. To be fair, the number of operations is n2/2+n/2,\nand so this is just over half the number of operations than an algorithm that uses n\noperations each time the inner loop is performed. But the order of growth is still\nquadratic in n.\nThe Cubic Function and Other Polynomials\nContinuing our discussion of functions that are powers of the input, we consider\nthecubic function ,\nf(n)=n3,\nwhich assigns to an input value nthe product of nwith itself three times. This func-\ntion appears less frequently in the context of algorithm analysis than the constant,linear, and quadratic functions previously mentioned, but it does appear from timeto time.\nPolynomials\nMost of the functions we have listed so far can each be viewed as being part of a\nlarger class of functions, the polynomials .Apolynomial function has the form,\nf(n)=a0+a1n+a2n2+a3n3+\u00b7\u00b7\u00b7+adnd,\nwhere a0,a1,..., adare constants, called the coef\ufb01cients of the polynomial, and\nad/negationslash=0. Integer d, which indicates the highest power in the polynomial, is called\nthedegree of the polynomial.\nFor example, the following functions are all polynomials:\n\u2022f(n)=2+5n+n2\n\u2022f(n)=1+n3\n\u2022f(n)=1\n\u2022f(n)=n\n\u2022f(n)=n2\nTherefore, we could argue that this book presents just four important functions used\nin algorithm analysis, but we will stick to saying that there are seven, since the con-stant, linear, and quadratic functions are too important to be lumped in with otherpolynomials. Running times that are polynomials with small degree are generally\nbetter than polynomial running times with larger degree.", "120 Chapter 3. Algorithm Analysis\nSummations\nA notation that appears again and again in the analysis of data structures and algo-\nrithms is the summation , which is de\ufb01ned as follows:\nb\n\u2211\ni=af(i)=f(a)+f(a+1)+f(a+2)+\u00b7\u00b7\u00b7+f(b),\nwhere aandbare integers and a\u2264b. Summations arise in data structure and algo-\nrithm analysis because the running times of loops naturally give rise to summations.\nUsing a summation, we can rewrite the formula of Proposition 3.3 as\nn\n\u2211\ni=1i=n(n+1)\n2.\nLikewise, we can write a polynomial f(n)of degree dwith coef\ufb01cients a0,..., adas\nf(n)=d\n\u2211\ni=0aini.\nThus, the summation notation gives us a shorthand way of expressing sums of in-\ncreasing terms that have a regular structure.\nThe Exponential Function\nAnother function used in the analysis of algorithms is the exponential function ,\nf(n)=bn,\nwhere bis a positive constant, called the base, and the argument nis the exponent .\nThat is, function f(n)assigns to the input argument nthe value obtained by mul-\ntiplying the base bby itself ntimes. As was the case with the logarithm function,\nthe most common base for the exponential function in algorithm analysis is b=2.\nFor example, an integer word containing nbits can represent all the nonnegative\nintegers less than 2n. If we have a loop that starts by performing one operation\nand then doubles the number of operations performed with each iteration, then the\nnumber of operations performed in the nthiteration is 2n.\nWe sometimes have other exponents besides n, however; hence, it is useful\nfor us to know a few handy rules for working with exponents. In particular, thefollowing exponent rules are quite helpful.", "3.2. The Seven Functions Used in This Book 121\nProposition 3.4 (Exponent Rules): Given positive integers a,b,a n d c,w eh a v e\n1.(ba)c=bac\n2.babc=ba+c\n3.ba/bc=ba\u2212c\nFor example, we have the following:\n\u2022256 =162=(24)2=24\u00b72=28=256 (Exponent Rule 1)\n\u2022243 =35=32+3=3233=9\u00b727=243 (Exponent Rule 2)\n\u202216=1024/64=210/26=210\u22126=24=16 (Exponent Rule 3)\nWe can extend the exponential function to exponents that are fractions or real\nnumbers and to negative exponents, as follows. Given a positive integer k,w ed e -\n\ufb01neb1/kto be kthroot of b, that is, the number rsuch that rk=b. For example,\n251/2=5, since 52=25. Likewise, 271/3=3 and 161/4=2. This approach al-\nlows us to de\ufb01ne any power whose exponent can be expressed as a fraction, for\nba/c=(ba)1/c, by Exponent Rule 1. For example, 93/2=(93)1/2=7291/2=27.\nThus, ba/cis really just the cthroot of the integral exponent ba.\nWe can further extend the exponential function to de\ufb01ne bxfor any real number\nx, by computing a series of numbers of the form ba/cfor fractions a/cthat get pro-\ngressively closer and closer to x. Any real number xcan be approximated arbitrarily\nclosely by a fraction a/c; hence, we can use the fraction a/cas the exponent of b\nto get arbitrarily close to bx. For example, the number 2\u03c0is well de\ufb01ned. Finally,\ngiven a negative exponent d,w ed e \ufb01 n e bd=1/b\u2212d, which corresponds to applying\nExponent Rule 3 with a=0a n d c=\u2212d. For example, 2\u22123=1/23=1/8.\nGeometric Sums\nSuppose we have a loop for which each iteration takes a multiplicative factor longerthan the previous one. This loop can be analyzed using the following proposition.\nProposition 3.5:\nFor any integer n\u22650and any real number asuch that a>0and\na/negationslash=1, consider the summation\nn\n\u2211\ni=0ai=1+a+a2+\u00b7\u00b7\u00b7+an\n(remembering that a0=1ifa>0). This summation is equal to\nan+1\u22121\na\u22121.\nSummations as shown in Proposition 3.5 are called geometric summations, be-\ncause each term is geometrically larger than the previous one if a>1. For example,\neveryone working in computing should know that\n1+2+4+8+\u00b7\u00b7\u00b7+2n\u22121=2n\u22121,\nfor this is the largest integer that can be represented in binary notation using nbits.", "122 Chapter 3. Algorithm Analysis\n3.2.1 Comparing Growth Rates\nTo sum up, Table 3.1 shows, in order, each of the seven common functions used in\nalgorithm analysis.\nconstant\n logarithm\n linear\n n-log- n\nquadratic\n cubic\n exponential\n1\n logn\n n\n nlogn\n n2\nn3\nan\nTable 3.1: Classes of functions. Here we assume that a>1 is a constant.\nIdeally, we would like data structure operations to run in times proportional\nto the constant or logarithm function, and we would like our algorithms to run inlinear or n-log- ntime. Algorithms with quadratic or cubic running times are less\npractical, and algorithms with exponential running times are infeasible for all but\nthe smallest sized inputs. Plots of the seven functions are shown in Figure 3.4.f(n)\n107106\nn105104103102LinearExponential\nConstantLogarithmicN-Log- NQuadraticCubic\n10151014101310121011101010910810110010410810121016102010281032103610401044\n1001024\nFigure 3.4: Growth rates for the seven fundamental functions used in algorithm\nanalysis. We use base a=2 for the exponential function. The functions are plotted\non a log-log chart, to compare the growth rates primarily as slopes. Even so, the\nexponential function grows too fast to display all its values on the chart.\nThe Ceiling and Floor Functions\nOne additional comment concerning the functions above is in order. When dis-cussing logarithms, we noted that the value is generally not an integer, yet therunning time of an algorithm is usually expressed by means of an integer quantity,such as the number of operations performed. Thus, the analysis of an algorithm\nmay sometimes involve the use of the \ufb02oor function andceiling function ,w h i c h\nare de\ufb01ned respectively as follows:\n\u2022\u230ax\u230b=the largest integer less than or equal to x.\n\u2022\u2308x\u2309=the smallest integer greater than or equal to x.", "3.3. Asymptotic Analysis 123\n3.3 Asymptotic Analysis\nIn algorithm analysis, we focus on the growth rate of the running time as a function\nof the input size n, taking a \u201cbig-picture\u201d approach. For example, it is often enough\njust to know that the running time of an algorithm grows proportionally to n.\nWe analyze algorithms using a mathematical notation for functions that disre-\ngards constant factors. Namely, we characterize the running times of algorithmsby using functions that map the size of the input, n, to values that correspond to\nthe main factor that determines the growth rate in terms of n. This approach re-\n\ufb02ects that each basic step in a pseudo-code description or a high-level languageimplementation may correspond to a small number of primitive operations. Thus,we can perform an analysis of an algorithm by estimating the number of primitiveoperations executed up to a constant factor, rather than getting bogged down inlanguage-speci\ufb01c or hardware-speci\ufb01c analysis of the exact number of operations\nthat execute on the computer.\nAs a tangible example, we revisit the goal of \ufb01nding the largest element of a\nPython list; we \ufb01rst used this example when introducing for loops on page 21 of\nSection 1.4.2. Code Fragment 3.1 presents a function named \ufb01nd\nmax for this task.\n1def\ufb01nd\nmax(data):\n2\u201d\u201d\u201dReturn the maximum element from a nonempty Python list.\u201d\u201d\u201d\n3biggest = data[0] # The initial value to beat\n4forvalindata: # For each value:\n5 ifval>biggest # if it is greater than the best so far,\n6 biggest = val # we have found a new best (so far)\n7return biggest # When loop ends, biggest is the max\nCode Fragment 3.1: A function that returns the maximum value of a Python list.\nThis is a classic example of an algorithm with a running time that grows pro-\nportional to n, as the loop executes once for each data element, with some \ufb01xed\nnumber of primitive operations executing for each pass. In the remainder of thissection, we provide a framework to formalize this claim.\n3.3.1 The \u201cBig-Oh\u201d Notation\nLetf(n)andg(n)be functions mapping positive integers to positive real numbers.\nWe say that f(n)isO(g(n))if there is a real constant c>0 and an integer constant\nn0\u22651 such that\nf(n)\u2264cg(n),for n\u2265n0.\nThis de\ufb01nition is often referred to as the \u201cbig-Oh\u201d notation, for it is sometimes pro-\nnounced as \u201c f(n)isbig-Oh ofg(n).\u201d Figure 3.5 illustrates the general de\ufb01nition.", "124 Chapter 3. Algorithm Analysis\nInput SizeRunning Timecg(n)\nf(n)\nn0\nFigure 3.5: Illustrating the \u201cbig-Oh\u201d notation. The function f(n)isO(g(n)),s i n c e\nf(n)\u2264c\u00b7g(n)when n\u2265n0.\nExample 3.6: The function 8n+5isO(n).\nJusti\ufb01cation: By the big-Oh de\ufb01nition, we need to \ufb01nd a real constant c>0a n d\nan integer constant n0\u22651 such that 8 n+5\u2264cnfor every integer n\u2265n0. It is easy\nto see that a possible choice is c=9a n d n0=5. Indeed, this is one of in\ufb01nitely\nmany choices available because there is a trade-off between candn0. For example,\nwe could rely on constants c=13 and n0=1.\nThe big-Oh notation allows us to say that a function f(n)is \u201cless than or equal\nto\u201d another function g(n)up to a constant factor and in the asymptotic sense as n\ngrows toward in\ufb01nity. This ability comes from the fact that the de\ufb01nition uses \u201c \u2264\u201d\nto compare f(n)to ag(n)times a constant, c, for the asymptotic cases when n\u2265n0.\nHowever, it is considered poor taste to say \u201c f(n)\u2264O(g(n)),\u201d since the big-Oh\nalready denotes the \u201cless-than-or-equal-to\u201d concept. Likewise, although common,\nit is not fully correct to say \u201c f(n)=O(g(n)),\u201d with the usual understanding of the\n\u201c=\u201d relation, because there is no way to make sense of the symmetric statement,\n\u201cO(g(n)) = f(n).\u201d It is best to say,\n\u201cf(n)isO(g(n)).\u201d\nAlternatively, we can say \u201c f(n)isorder of g(n).\u201d For the more mathematically\ninclined, it is also correct to say, \u201c f(n)\u2208O(g(n)),\u201d for the big-Oh notation, techni-\ncally speaking, denotes a whole collection of functions. In this book, we will stickto presenting big-Oh statements as \u201c f(n)isO(g(n)).\u201d Even with this interpretation,\nthere is considerable freedom in how we can use arithmetic operations with the big-\nOh notation, and with this freedom comes a certain amount of responsibility.", "3.3. Asymptotic Analysis 125\nCharacterizing Running Times Using the Big-Oh Notation\nThe big-Oh notation is used widely to characterize running times and space bounds\nin terms of some parameter n, which varies from problem to problem, but is always\nde\ufb01ned as a chosen measure of the \u201csize\u201d of the problem. For example, if weare interested in \ufb01nding the largest element in a sequence, as with the \ufb01nd\nmax\nalgorithm, we should let ndenote the number of elements in that collection. Using\nthe big-Oh notation, we can write the following mathematically precise statementon the running time of algorithm \ufb01nd\nmax (Code Fragment 3.1) for anycomputer.\nProposition 3.7: The algorithm, \ufb01nd\n max, for computing the maximum element\nof a list of nnumbers, runs in O(n)time.\nJusti\ufb01cation: The initialization before the loop begins requires only a constant\nnumber of primitive operations. Each iteration of the loop also requires only a con-stant number of primitive operations, and the loop executes ntimes. Therefore,\nwe account for the number of primitive operations being c\n/prime+c/prime/prime\u00b7nfor appropriate\nconstants c/primeandc/prime/primethat re\ufb02ect, respectively, the work performed during initializa-\ntion and the loop body. Because each primitive operation runs in constant time, wehave that the running time of algorithm \ufb01nd\nmax on an input of size nis at most a\nconstant times n; that is, we conclude that the running time of algorithm \ufb01nd\nmax\nisO(n).\nSome Properties of the Big-Oh Notation\nThe big-Oh notation allows us to ignore constant factors and lower-order terms andfocus on the main components of a function that affect its growth.\nExample 3.8: 5n\n4+3n3+2n2+4n+1isO(n4).\nJusti\ufb01cation: Note that 5n4+3n3+2n2+4n+1\u2264(5+3+2+4+1)n4=cn4,\nforc=15,w h e n n\u2265n0=1.\nIn fact, we can characterize the growth rate of any polynomial function.\nProposition 3.9: Iff(n)is a polynomial of degree d,t h a ti s ,\nf(n)=a0+a1n+\u00b7\u00b7\u00b7+adnd,\nandad>0,t h e n f(n)isO(nd).\nJusti\ufb01cation: Note that, for n\u22651, we have 1 \u2264n\u2264n2\u2264\u00b7\u00b7\u00b7\u2264 nd; hence,\na0+a1n+a2n2+\u00b7\u00b7\u00b7+adnd\u2264(|a0|+|a1|+|a2|+\u00b7\u00b7\u00b7+|ad|)nd.\nWe show that f(n)isO(nd)by de\ufb01ning c=|a0|+|a1|+\u00b7\u00b7\u00b7+|ad|andn0=1.\n", "126 Chapter 3. Algorithm Analysis\nThus, the highest-degree term in a polynomial is the term that determines the\nasymptotic growth rate of that polynomial. We consider some additional properties\nof the big-Oh notation in the exercises. Let us consider some further examples here,focusing on combinations of the seven fundamental functions used in algorithmdesign. We rely on the mathematical fact that log n\u2264nforn\u22651.\nExample 3.10: 5n\n2+3nlogn+2n+5isO(n2).\nJusti\ufb01cation: 5n2+3nlogn+2n+5\u2264(5+3+2+5)n2=cn2,f o rc=15, when\nn\u2265n0=1.\nExample 3.11: 20n3+10nlogn+5isO(n3).\nJusti\ufb01cation: 20n3+10nlogn+5\u226435n3,f o r n\u22651.\nExample 3.12: 3log n+2isO(logn).\nJusti\ufb01cation: 3log n+2\u22645log n,f o r n\u22652. Note that log nis zero for n=1.\nThat is why we use n\u2265n0=2 in this case.\nExample 3.13: 2n+2isO(2n).\nJusti\ufb01cation: 2n+2=2n\u00b722=4\u00b72n; hence, we can take c=4a n d n0=1i nt h i s\ncase.\nExample 3.14: 2n+100log nisO(n).\nJusti\ufb01cation: 2n+100log n\u2264102n,f o r n\u2265n0=1; hence, we can take c=102\nin this case.\nCharacterizing Functions in Simplest Terms\nIn general, we should use the big-Oh notation to characterize a function as closelyas possible. While it is true that the function f(n)=4n\n3+3n2isO(n5)or even\nO(n4), it is more accurate to say that f(n)isO(n3). Consider, by way of analogy,\na scenario where a hungry traveler driving along a long country road happens upona local farmer walking home from a market. If the traveler asks the farmer how\nmuch longer he must drive before he can \ufb01nd some food, it may be truthful for the\nfarmer to say, \u201ccertainly no longer than 12 hours,\u201d but it is much more accurate(and helpful) for him to say, \u201cyou can \ufb01nd a market just a few minutes drive up thisroad.\u201d Thus, even with the big-Oh notation, we should strive as much as possibleto tell the whole truth.\nIt is also considered poor taste to include constant factors and lower-order terms\nin the big-Oh notation. For example, it is not fashionable to say that the function2n\n2isO(4n2+6nlogn), although this is completely correct. We should strive\ninstead to describe the function in the big-Oh in simplest terms .", "3.3. Asymptotic Analysis 127\nThe seven functions listed in Section 3.2 are the most common functions used\nin conjunction with the big-Oh notation to characterize the running times and space\nusage of algorithms. Indeed, we typically use the names of these functions to referto the running times of the algorithms they characterize. So, for example, we wouldsay that an algorithm that runs in worst-case time 4 n\n2+nlognis aquadratic-time\nalgorithm, since it runs in O(n2)time. Likewise, an algorithm running in time at\nmost 5 n+20log n+4 would be called a linear-time algorithm.\nBig-Omega\nJust as the big-Oh notation provides an asymptotic way of saying that a function is\u201cless than or equal to\u201d another function, the following notations provide an asymp-\ntotic way of saying that a function grows at a rate that is \u201cgreater than or equal to\u201d\nthat of another.\nLetf(n)andg(n)be functions mapping positive integers to positive real num-\nbers. We say that f(n)is\u03a9(g(n)), pronounced \u201c f(n)is big-Omega of g(n),\u201d ifg(n)\nisO(f(n)), that is, there is a real constant c>0 and an integer constant n\n0\u22651s u c h\nthat\nf(n)\u2265cg(n),for n\u2265n0.\nThis de\ufb01nition allows us to say asymptotically that one function is greater than orequal to another, up to a constant factor.\nExample 3.15: 3nlogn\u22122n\nis\u03a9(nlogn).\nJusti\ufb01cation: 3nlogn\u22122n=nlogn+2n(logn\u22121)\u2265nlognforn\u22652; hence,\nwe can take c=1a n d n0=2 in this case.\nBig-Theta\nIn addition, there is a notation that allows us to say that two functions grow at the\nsame rate, up to constant factors. We say that f(n)is\u0398(g(n)), pronounced \u201c f(n)\nis big-Theta of g(n),\u201d if f(n)isO(g(n))andf(n)is\u03a9(g(n)), that is, there are real\nconstants c/prime>0a n d c/prime/prime>0, and an integer constant n0\u22651 such that\nc/primeg(n)\u2264f(n)\u2264c/prime/primeg(n),for n\u2265n0.\nExample 3.16: 3nlogn+4n+5log nis\u0398(nlogn).\nJusti\ufb01cation: 3nlogn\u22643nlogn+4n+5log n\u2264(3+4+5)nlognforn\u22652.\n", "128 Chapter 3. Algorithm Analysis\n3.3.2 Comparative Analysis\nSuppose two algorithms solving the same problem are available: an algorithm A,\nwhich has a running time of O(n), and an algorithm B, which has a running time\nofO(n2). Which algorithm is better? We know that nisO(n2), which implies that\nalgorithm Aisasymptotically better than algorithm B, although for a small value\nofn,Bmay have a lower running time than A.\nWe can use the big-Oh notation to order classes of functions by asymptotic\ngrowth rate. Our seven functions are ordered by increasing growth rate in the fol-\nlowing sequence, that is, if a function f(n)precedes a function g(n)in the sequence,\nthen f(n)isO(g(n)):\n1,logn,n,nlogn,n2,n3,2n.\nWe illustrate the growth rates of the seven functions in Table 3.2. (See also\nFigure 3.4 from Section 3.2.1.)\nn\n lognnn lognn2n32n\n8\n 3 8 24 64 512 256\n16\n 4 16 64 256 4 ,096 65 ,536\n32\n 5 32 160 1 ,024 32 ,768 4 ,294,967,296\n64\n 6 64 384 4 ,096 262 ,144 1 .84\u00d71019\n128\n 7 128 896 16 ,384 2 ,097,152 3 .40\u00d71038\n256\n 8 256 2 ,048 65 ,536 16 ,777,216 1 .15\u00d71077\n512\n 9 512 4 ,608 262 ,144 134 ,217,728 1 .34\u00d710154\nTable 3.2: Selected values of fundamental functions in algorithm analysis.\nWe further illustrate the importance of the asymptotic viewpoint in Table 3.3.\nThis table explores the maximum size allowed for an input instance that is pro-cessed by an algorithm in 1 second, 1 minute, and 1 hour. It shows the importanceof good algorithm design, because an asymptotically slow algorithm is beaten inthe long run by an asymptotically faster algorithm, even if the constant factor for\nthe asymptotically faster algorithm is worse.\nRunning\n Maximum Problem Size ( n)\nTime ( \u03bcs)\n1 second 1 minute 1 hour\n400n\n 2,500 150,000 9,000,000\n2n2\n707 5,477 42,426\n2n\n19 25 31\nTable 3.3: Maximum size of a problem that can be solved in 1 second, 1 minute,\nand 1 hour, for various running times measured in microseconds.", "3.3. Asymptotic Analysis 129\nThe importance of good algorithm design goes beyond just what can be solved\neffectively on a given computer, however. As shown in Table 3.4, even if we\nachieve a dramatic speedup in hardware, we still cannot overcome the handicapof an asymptotically slow algorithm. This table shows the new maximum problemsize achievable for any \ufb01xed amount of time, assuming algorithms with the givenrunning times are now run on a computer 256 times faster than the previous one.\nRunning Time\n New Maximum Problem Size\n400n\n 256m\n2n2\n16m\n2n\nm+8\nTable 3.4: Increase in the maximum size of a problem that can be solved in a \ufb01xed\namount of time, by using a computer that is 256 times faster than the previous one.Each entry is a function of m, the previous maximum problem size.\nSome Words of Caution\nA few words of caution about asymptotic notation are in order at this point. First,note that the use of the big-Oh and related notations can be somewhat misleadingshould the constant factors they \u201chide\u201d be very large. For example, while it is true\nthat the function 10\n100nisO(n), if this is the running time of an algorithm being\ncompared to one whose running time is 10 nlogn, we should prefer the O(nlogn)-\ntime algorithm, even though the linear-time algorithm is asymptotically faster. This\npreference is because the constant factor, 10100, which is called \u201cone googol,\u201d is\nbelieved by many astronomers to be an upper bound on the number of atoms in\nthe observable universe. So we are unlikely to ever have a real-world problem that\nhas this number as its input size. Thus, even when using the big-Oh notation, weshould at least be somewhat mindful of the constant factors and lower-order termswe are \u201chiding.\u201d\nThe observation above raises the issue of what constitutes a \u201cfast\u201d algorithm.\nGenerally speaking, any algorithm running in O(nlogn)time (with a reasonable\nconstant factor) should be considered ef\ufb01cient. Even an O(n\n2)-time function may\nbe fast enough in some contexts, that is, when nis small. But an algorithm running\ninO(2n)time should almost never be considered ef\ufb01cient.\nExponential Running Times\nThere is a famous story about the inventor of the game of chess. He asked only thathis king pay him 1 grain of rice for the \ufb01rst square on the board, 2 grains for thesecond, 4 grains for the third, 8 for the fourth, and so on. It is an interesting test ofprogramming skills to write a program to compute exactly the number of grains of\nrice the king would have to pay.", "130 Chapter 3. Algorithm Analysis\nIf we must draw a line between ef\ufb01cient and inef\ufb01cient algorithms, therefore,\nit is natural to make this distinction be that between those algorithms running in\npolynomial time and those running in exponential time. That is, make the distinc-tion between algorithms with a running time that is O(n\nc), for some constant c>1,\nand those with a running time that is O(bn), for some constant b>1. Like so many\nnotions we have discussed in this section, this too should be taken with a \u201cgrain of\nsalt,\u201d for an algorithm running in O(n100)time should probably not be considered\n\u201cef\ufb01cient.\u201d Even so, the distinction between polynomial-time and exponential-time\nalgorithms is considered a robust measure of tractability.\n3.3.3 Examples of Algorithm Analysis\nNow that we have the big-Oh notation for doing algorithm analysis, let us give someexamples by characterizing the running time of some simple algorithms using thisnotation. Moreover, in keeping with our earlier promise, we illustrate below how\neach of the seven functions given earlier in this chapter can be used to characterize\nthe running time of an example algorithm.\nRather than use pseudo-code in this section, we give complete Python imple-\nmentations for our examples. We use Python\u2019s listclass as the natural representa-\ntion for an \u201carray\u201d of values. In Chapter 5, we will fully explore the underpinningsof Python\u2019s listclass, and the ef\ufb01ciency of the various behaviors that it supports. In\nthis section, we rely on just a few of its behaviors, discussing their ef\ufb01ciencies asintroduced.\nConstant-Time Operations\nGiven an instance, named data, of the Python listclass, a call to the function,\nlen(data) , is evaluated in constant time. This is a very simple algorithm because\nthelistclass maintains, for each list, an instance variable that records the current\nlength of the list. This allows it to immediately report that length, rather than taketime to iteratively count each of the elements in the list. Using asymptotic notation,we say that this function runs in O(1)time; that is, the running time of this function\nis independent of the length, n, of the list.\nAnother central behavior of Python\u2019s listclass is that it allows access to an arbi-\ntrary element of the list using syntax, data[j] , for integer index j. Because Python\u2019s\nlists are implemented as array-based sequences , references to a list\u2019s elements are\nstored in a consecutive block of memory. The j\nthelement of the list can be found,\nnot by iterating through the list one element at a time, but by validating the index,and using it as an offset into the underlying array. In turn, computer hardware sup-ports constant-time access to an element based on its memory address. Therefore,\nwe say that the expression data[j] is evaluated in O(1)time for a Python list.", "3.3. Asymptotic Analysis 131\nRevisiting the Problem of Finding the Maximum of a Sequence\nFor our next example, we revisit the \ufb01nd\nmax algorithm, given in Code Frag-\nment 3.1 on page 123, for \ufb01nding the largest value in a sequence. Proposition 3.7\non page 125 claimed an O(n)run-time for the \ufb01nd\nmax algorithm. Consistent with\nour earlier analysis of syntax data[0] , the initialization uses O(1)time. The loop\nexecutes ntimes, and within each iteration, it performs one comparison and possi-\nbly one assignment statement (as well as maintenance of the loop variable). Finally,\nwe note that the mechanism for enacting a return statement in Python uses O(1)\ntime. Combining these steps, we have that the \ufb01nd\nmax function runs in O(n)time.\nFurther Analysis of the Maximum-Finding Algorithm\nA more interesting question about \ufb01nd\nmax is how many times we might update\nthe current \u201cbiggest\u201d value. In the worst case, if the data is given to us in increasingorder, the biggest value is reassigned n\u22121 times. But what if the input is given\nto us in random order, with all orders equally likely; what would be the expectednumber of times we update the biggest value in this case? To answer this question,note that we update the current biggest in an iteration of the loop only if the currentelement is bigger than all the elements that precede it. If the sequence is given tous in random order, the probability that the j\nthelement is the largest of the \ufb01rst j\nelements is 1 /j(assuming uniqueness). Hence, the expected number of times we\nupdate the biggest (including initialization) is Hn=\u2211n\nj=11/j, which is known as\nthenthHarmonic number . It turns out (see Proposition B.16) that HnisO(logn).\nTherefore, the expected number of times the biggest value is updated by \ufb01nd\nmax\non a randomly ordered sequence is O(logn).\nPre\ufb01x Averages\nThe next problem we consider is computing what are known as pre\ufb01x averages\nof a sequence of numbers. Namely, given a sequence Sconsisting of nnum-\nbers, we want to compute a sequence Asuch that A[j]is the average of elements\nS[0],..., S[j],f o r j=0,..., n\u22121, that is,\nA[j]=\u2211j\ni=0S[i]\nj+1.\nComputing pre\ufb01x averages has many applications in economics and statistics. For\nexample, given the year-by-year returns of a mutual fund, ordered from recent to\npast, an investor will typically want to see the fund\u2019s average annual returns for themost recent year, the most recent three years, the most recent \ufb01ve years, and so on.Likewise, given a stream of daily Web usage logs, a Web site manager may wishto track average usage trends over various time periods. We analyze three different\nimplementations that solve this problem but with rather different running times.", "132 Chapter 3. Algorithm Analysis\nA Quadratic-Time Algorithm\nOur \ufb01rst algorithm for computing pre\ufb01x averages, named pre\ufb01x\n average1 ,i ss h o w n\nin Code Fragment 3.2. It computes every element of Aseparately, using an inner\nloop to compute the partial sum.\n1defpre\ufb01x\n average1(S):\n2\u201d\u201d\u201dReturn list such that, for all j, A[j] equals average of S[0], ..., S[j].\u201d\u201d\u201d\n3n=l e n ( S )\n4A=[ 0 ]\n n # create new list of n zeros\n5forjinrange(n):\n6 total = 0 # begin computing S[0] + ... + S[j]\n7 foriinrange(j + 1):\n8 total += S[i]\n9 A[j] = total / (j+1) # record the average\n10return A\nCode Fragment 3.2: Algorithm pre\ufb01x\n average1 .\nIn order to analyze the pre\ufb01x\n average1 algorithm, we consider the various steps\nthat are executed.\n\u2022The statement, n=l e n ( S ) , executes in constant time, as described at the\nbeginning of Section 3.3.3.\n\u2022The statement, A=[ 0 ]\n n, causes the creation and initialization of a Python\nlist with length n, and with all entries equal to zero. This uses a constant\nnumber of primitive operations per element, and thus runs in O(n)time.\n\u2022There are two nested forloops, which are controlled, respectively, by coun-\nters jandi. The body of the outer loop, controlled by counter j,i se x -\necuted ntimes, for j=0,..., n\u22121. Therefore, statements total = 0 and\nA[j] = total / (j+1) are executed ntimes each. This implies that these two\nstatements, plus the management of counter jin the range, contribute a num-\nber of primitive operations proportional to n,t h a ti s , O(n)time.\n\u2022The body of the inner loop, which is controlled by counter i, is executed j+1\ntimes, depending on the current value of the outer loop counter j. Thus, state-\nmenttotal += S[i] , in the inner loop, is executed 1 +2+3+\u00b7\u00b7\u00b7+ntimes.\nBy recalling Proposition 3.3, we know that 1 +2+3+\u00b7\u00b7\u00b7+n=n(n+1)/2,\nwhich implies that the statement in the inner loop contributes O(n2)time.\nA similar argument can be done for the primitive operations associated with\nmaintaining counter i, which also take O(n2)time.\nThe running time of implementation pre\ufb01x\n average1 is given by the sum of three\nterms. The \ufb01rst and the second terms are O(n), and the third term is O(n2).B y a\nsimple application of Proposition 3.9, the running time of pre\ufb01x\n average1 isO(n2).", "3.3. Asymptotic Analysis 133\nOur second implementation for computing pre\ufb01x averages, pre\ufb01x\n average2 ,i s\npresented in Code Fragment 3.3.\n1defpre\ufb01x\n average2(S):\n2\u201d\u201d\u201dReturn list such that, for all j, A[j] equals average of S[0], ..., S[j].\u201d\u201d\u201d\n3n=l e n ( S )\n4A=[ 0 ]\n n # create new list of n zeros\n5forjinrange(n):\n6 A[j] = sum(S[0:j+1]) / (j+1) # record the average\n7return A\nCode Fragment 3.3: Algorithm pre\ufb01x\n average2 .\nThis approach is essentially the same high-level algorithm as in pre\ufb01x\n average1 ,\nbut we have replaced the inner loop by using the single expression sum(S[0:j+1])\nto compute the partial sum, S[0]+\u00b7\u00b7\u00b7+S[j]. While the use of that function greatly\nsimpli\ufb01es the presentation of the algorithm, it is worth asking how it affects the\nef\ufb01ciency. Asymptotically, this implementation is no better. Even though the ex-pression, sum(S[0:j+1]) , seems like a single command, it is a function call and\nan evaluation of that function takes O(j+1)time in this context. Technically, the\ncomputation of the slice, S[0:j+1] , also uses O(j+1)time, as it constructs a new\nlist instance for storage. So the running time of pre\ufb01x\naverage2 is still dominated\nby a series of steps that take time proportional to 1 +2+3+\u00b7\u00b7\u00b7+n, and thus O(n2).\nA Linear-Time Algorithm\nOur \ufb01nal algorithm, pre\ufb01x\n averages3 , is given in Code Fragment 3.4. Just as with\nour \ufb01rst two algorithms, we are interested in computing, for each j,t h e pre\ufb01x sum\nS[0]+S[1]+\u00b7\u00b7\u00b7+S[j], denoted as total in our code, so that we can then compute\nthe pre\ufb01x average A[j] =total / (j + 1) . However, there is a key difference that\nresults in much greater ef\ufb01ciency.\n1defpre\ufb01x\n average3(S):\n2\u201d\u201d\u201dReturn list such that, for all j, A[j] equals average of S[0], ..., S[j].\u201d\u201d\u201d\n3n=l e n ( S )\n4A=[ 0 ]\n n # create new list of n zeros\n5total = 0 # compute pre\ufb01x sum as S[0] + S[1] + ...\n6forjinrange(n):\n7 total += S[j] # update pre\ufb01x sum to include S[j]\n8 A[j] = total / (j+1) # compute average based on current sum\n9return A\nCode Fragment 3.4: Algorithm pre\ufb01x\n average3 .", "134 Chapter 3. Algorithm Analysis\nIn our \ufb01rst two algorithms, the pre\ufb01x sum is computed anew for each value of j.\nThat contributed O(j)time for each j, leading to the quadratic behavior. In algo-\nrithmpre\ufb01x\n average3 , we maintain the current pre\ufb01x sum dynamically, effectively\ncomputing S[0]+S[1]+\u00b7\u00b7\u00b7+S[j]astotal + S[j] , where value total is equal to the\nsum S[0]+S[1]+\u00b7\u00b7\u00b7+S[j\u22121]computed by the previous pass of the loop over j.\nThe analysis of the running time of algorithm pre\ufb01x\n average3 follows:\n\u2022Initializing variables nandtotal uses O(1)time.\n\u2022Initializing the list Auses O(n)time.\n\u2022T h e r ei sas i n g l e forloop, which is controlled by counter j. The maintenance\nof that counter by the range iterator contributes a total of O(n)time.\n\u2022The body of the loop is executed ntimes, for j=0,..., n\u22121. Thus, state-\nments total += S[j] andA[j] = total / (j+1) are executed ntimes each.\nSince each of these statements uses O(1)time per iteration, their overall\ncontribution is O(n)time.\nThe running time of algorithm pre\ufb01x\n average3 is given by the sum of the four\nterms. The \ufb01rst is O(1)and the remaining three are O(n). By a simple application\nof Proposition 3.9, the running time of pre\ufb01x\n average3 isO(n), which is much\nbetter than the quadratic time of algorithms pre\ufb01x\n average1 andpre\ufb01x\n average2 .\nThree-Way Set Disjointness\nSuppose we are given three sequences of numbers, A,B,a n d C. We will assume\nthat no individual sequence contains duplicate values, but that there may be some\nnumbers that are in two or three of the sequences. The three-way set disjointness\nproblem is to determine if the intersection of the three sequences is empty, namely,that there is no element xsuch that x\u2208A,x\u2208B,a n d x\u2208C. A simple Python\nfunction to determine this property is given in Code Fragment 3.5.\n1defdisjoint1(A, B, C):\n2\u201d\u201d\u201dReturn True if there is no element common to all three lists.\u201d\u201d\u201d\n3forainA:\n4 forbinB:\n5 forcinC:\n6 ifa= =b= =c :\n7 return False # we found a common value\n8return True # if we reach this, sets are disjoint\nCode Fragment 3.5: Algorithm disjoint1 for testing three-way set disjointness.\nThis simple algorithm loops through each possible triple of values from the\nthree sets to see if those values are equivalent. If each of the original sets has size\nn, then the worst-case running time of this function is O(n3).", "3.3. Asymptotic Analysis 135\nWe can improve upon the asymptotic performance with a simple observation.\nOnce inside the body of the loop over B, if selected elements aandbdo not match\neach other, it is a waste of time to iterate through all values of Clooking for a\nmatching triple. An improved solution to this problem, taking advantage of this\nobservation, is presented in Code Fragment 3.6.\n1defdisjoint2(A, B, C):\n2\u201d\u201d\u201dReturn True if there is no element common to all three lists.\u201d\u201d\u201d\n3forainA:\n4 forbinB:\n5 ifa= =b : # only check C if we found match from A and B\n6 forcinC:\n7 ifa= =c # (and thus a == b == c)\n8 return False # we found a common value\n9return True # if we reach this, sets are disjoint\nCode Fragment 3.6: Algorithm disjoint2 for testing three-way set disjointness.\nIn the improved version, it is not simply that we save time if we get lucky. We\nclaim that the worst-case running time for disjoint2 isO(n2). There are quadrat-\nically many pairs (a,b)to consider. However, if AandBare each sets of distinct\nelements, there can be at most O(n)such pairs with aequal to b. Therefore, the\ninnermost loop, over C, executes at most ntimes.\nTo account for the overall running time, we examine the time spent executing\neach line of code. The management of the forloop over Arequires O(n)time.\nThe management of the forloop over Baccounts for a total of O(n2)time, since\nthat loop is executed ndifferent times. The test a= =b is evaluated O(n2)times.\nThe rest of the time spent depends upon how many matching (a,b)pairs exist. As\nwe have noted, there are at most nsuch pairs, and so the management of the loop\nover C, and the commands within the body of that loop, use at most O(n2)time.\nBy our standard application of Proposition 3.9, the total time spent is O(n2).\nElement Uniqueness\nA problem that is closely related to the three-way set disjointness problem is theelement uniqueness problem . In the former, we are given three collections and we\npresumed that there were no duplicates within a single collection. In the element\nuniqueness problem, we are given a single sequence Swith nelements and asked\nwhether all elements of that collection are distinct from each other.\nOur \ufb01rst solution to this problem uses a straightforward iterative algorithm.\nTheunique1 function, given in Code Fragment 3.7, solves the element uniqueness\nproblem by looping through all distinct pairs of indices j<k, checking if any of", "136 Chapter 3. Algorithm Analysis\n1defunique1(S):\n2\u201d\u201d\u201dReturn True if there are no duplicate elements in sequence S.\u201d\u201d\u201d\n3forjinrange(len(S)):\n4 forkinrange(j+1, len(S)):\n5 ifS[j] == S[k]:\n6 return False # found duplicate pair\n7return True # if we reach this, elements were unique\nCode Fragment 3.7: Algorithm unique1 for testing element uniqueness.\nthose pairs refer to elements that are equivalent to each other. It does this using two\nnested forloops, such that the \ufb01rst iteration of the outer loop causes n\u22121 iterations\nof the inner loop, the second iteration of the outer loop causes n\u22122 iterations of\nthe inner loop, and so on. Thus, the worst-case running time of this function is\nproportional to\n(n\u22121)+(n\u22122)+\u00b7\u00b7\u00b7+2+1,\nwhich we recognize as the familiar O(n2)summation from Proposition 3.3.\nUsing Sorting as a Problem-Solving Tool\nAn even better algorithm for the element uniqueness problem is based on using\nsorting as a problem-solving tool. In this case, by sorting the sequence of elements,\nwe are guaranteed that any duplicate elements will be placed next to each other.Thus, to determine if there are any duplicates, all we need to do is perform a sin-gle pass over the sorted sequence, looking for consecutive duplicates. A Python\nimplementation of this algorithm is as follows:\n1defunique2(S):\n2\u201d\u201d\u201dReturn True if there are no duplicate elements in sequence S.\u201d\u201d\u201d\n3temp = sorted(S) # create a sorted copy of S\n4forjinrange(1, len(temp)):\n5 ifS[j\u22121] == S[j]:\n6 return False # found duplicate pair\n7return True # if we reach this, elements were unique\nCode Fragment 3.8: Algorithm unique2 for testing element uniqueness.\nThe built-in function, sorted , as described in Section 1.5.2, produces a copy of\nthe original list with elements in sorted order. It guarantees a worst-case runningtime of O(nlogn); see Chapter 12 for a discussion of common sorting algorithms.\nOnce the data is sorted, the subsequent loop runs in O(n)time, and so the entire\nunique2 algorithm runs in O(nlogn)time.", "3.4. Simple Justi\ufb01cation Techniques 137\n3.4 Simple Justi\ufb01cation Techniques\nSometimes, we will want to make claims about an algorithm, such as showing that\nit is correct or that it runs fast. In order to rigorously make such claims, we must\nuse mathematical language, and in order to back up such claims, we must justify orprove our statements. Fortunately, there are several simple ways to do this.\n3.4.1 By Example\nSome claims are of the generic form, \u201cThere is an element xin a set Sthat has\nproperty P.\u201d To justify such a claim, we only need to produce a particular xinS\nthat has property P. Likewise, some hard-to-believe claims are of the generic form,\n\u201cEvery element xin a set Shas property P.\u201d To justify that such a claim is false, we\nonly need to produce a particular xfrom Sthat does not have property P.S u c ha n\ninstance is called a counterexample .\nExample 3.17: Professor Amongus claims that every number of the form 2i\u22121\nis a prime, when iis an integer greater than 1. Professor Amongus is wrong.\nJusti\ufb01cation: To prove Professor Amongus is wrong, we \ufb01nd a counterexample.\nFortunately, we need not look too far, for 24\u22121=15=3\u00b75.\n3.4.2 The \u201cContra\u201d Attack\nAnother set of justi\ufb01cation techniques involves the use of the negative. The twoprimary such methods are the use of the contrapositive and the contradiction .T h e\nuse of the contrapositive method is like looking through a negative mirror. To\njustify the statement \u201cif pis true, then qis true,\u201d we establish that \u201cif qis not true,\nthen pis not true\u201d instead. Logically, these two statements are the same, but the\nlatter, which is called the contrapositive of the \ufb01rst, may be easier to think about.\nExample 3.18:\nLetaandbbe integers. If abis even, then ais even or bis even.\nJusti\ufb01cation: To justify this claim, consider the contrapositive, \u201cIf ais odd and\nbis odd, then abis odd.\u201d So, suppose a=2j+1a n d b=2k+1, for some integers\njandk.T h e n ab=4jk+2j+2k+1=2(2jk+j+k)+1; hence, abis odd.\nBesides showing a use of the contrapositive justi\ufb01cation technique, the previous\nexample also contains an application of DeMorgan\u2019s Law . This law helps us deal\nwith negations, for it states that the negation of a statement of the form \u201c porq\u201di s\n\u201cnot pand not q.\u201d Likewise, it states that the negation of a statement of the form\n\u201cpandq\u201d is \u201cnot por not q.\u201d", "138 Chapter 3. Algorithm Analysis\nContradiction\nAnother negative justi\ufb01cation technique is justi\ufb01cation by contradiction ,w h i c h\nalso often involves using DeMorgan\u2019s Law. In applying the justi\ufb01cation by con-\ntradiction technique, we establish that a statement qis true by \ufb01rst supposing that\nqis false and then showing that this assumption leads to a contradiction (such as\n2/negationslash=2o r1 >3). By reaching such a contradiction, we show that no consistent sit-\nuation exists with qbeing false, so qmust be true. Of course, in order to reach this\nconclusion, we must be sure our situation is consistent before we assume qis false.\nExample 3.19: Letaandbbe integers. If abis odd, then ais odd and bis odd.\nJusti\ufb01cation: Letabbe odd. We wish to show that ais odd and bis odd. So,\nwith the hope of leading to a contradiction, let us assume the opposite, namely,\nsuppose ais even or bis even. In fact, without loss of generality, we can assume\nthatais even (since the case for bis symmetric). Then a=2jfor some integer\nj. Hence, ab=(2j)b=2(jb),t h a ti s , abis even. But this is a contradiction: ab\ncannot simultaneously be odd and even. Therefore, ais odd and bis odd.\n3.4.3 Induction and Loop Invariants\nMost of the claims we make about a running time or a space bound involve an inte-ger parameter n(usually denoting an intuitive notion of the \u201csize\u201d of the problem).\nMoreover, most of these claims are equivalent to saying some statement q(n)is true\n\u201cfor all n\u22651.\u201d Since this is making a claim about an in\ufb01nite set of numbers, we\ncannot justify this exhaustively in a direct fashion.\nInduction\nWe can often justify claims such as those above as true, however, by using thetechnique of induction . This technique amounts to showing that, for any particular\nn\u22651, there is a \ufb01nite sequence of implications that starts with something known\nto be true and ultimately leads to showing that q(n)is true. Speci\ufb01cally, we begin a\njusti\ufb01cation by induction by showing that q(n)is true for n=1 (and possibly some\nother values n=2,3,..., k, for some constant k). Then we justify that the inductive\n\u201cstep\u201d is true for n>k, namely, we show \u201cif q(j)is true for all j<n,t h e n q(n)is\ntrue.\u201d The combination of these two pieces completes the justi\ufb01cation by induction.", "3.4. Simple Justi\ufb01cation Techniques 139\nProposition 3.20: Consider the Fibonacci function F(n), which is de\ufb01ned such\nthatF(1)=1,F(2)=2,a n d F(n)=F(n\u22122)+F(n\u22121)forn>2. (See Sec-\ntion 1.8.) We claim that F(n)<2n.\nJusti\ufb01cation: We will show our claim is correct by induction.\nBase cases: (n\u22642).F(1)=1<2=21andF(2)=2<4=22.\nInduction step: (n>2). Suppose our claim is true for all n/prime<n. Consider F(n).\nSince n>2,F(n)=F(n\u22122)+F(n\u22121). Moreover, since both n\u22122a n d n\u22121a r e\nless than n, we can apply the inductive assumption (sometimes called the \u201cinductive\nhypothesis\u201d) to imply that F(n)<2n\u22122+2n\u22121,s i n c e\n2n\u22122+2n\u22121<2n\u22121+2n\u22121=2\u00b72n\u22121=2n.\nLet us do another inductive argument, this time for a fact we have seen before.\nProposition 3.21: (which is the same as Proposition 3.3)\nn\n\u2211\ni=1i=n(n+1)\n2.\nJusti\ufb01cation: We will justify this equality by induction.\nBase case: n=1. Trivial, for 1 =n(n+1)/2, ifn=1.\nInduction step: n\u22652. Assume the claim is true for n/prime<n. Consider n.\nn\n\u2211\ni=1i=n+n\u22121\n\u2211\ni=1i.\nBy the induction hypothesis, then\nn\n\u2211\ni=1i=n+(n\u22121)n\n2,\nwhich we can simplify as\nn+(n\u22121)n\n2=2n+n2\u2212n\n2=n2+n\n2=n(n+1)\n2.\nWe may sometimes feel overwhelmed by the task of justifying something true\nforall n\u22651. We should remember, however, the concreteness of the inductive tech-\nnique. It shows that, for any particular n, there is a \ufb01nite step-by-step sequence of\nimplications that starts with something true and leads to the truth about n. In short,\nthe inductive argument is a template for building a sequence of direct justi\ufb01cations.", "140 Chapter 3. Algorithm Analysis\nLoop Invariants\nThe \ufb01nal justi\ufb01cation technique we discuss in this section is the loop invariant .T o\nprove some statement Labout a loop is correct, de\ufb01ne Lin terms of a series of\nsmaller statements L0,L1,...,Lk,w h e r e :\n1. The initial claim, L0, is true before the loop begins.\n2. IfLj\u22121is true before iteration j,t h e nL jwill be true after iteration j.\n3. The \ufb01nal statement, Lk, implies the desired statement Lto be true.\nLet us give a simple example of using a loop-invariant argument to justify the\ncorrectness of an algorithm. In particular, we use a loop invariant to justify that\nthe function, \ufb01nd(see Code Fragment 3.9), \ufb01nds the smallest index at which ele-\nmentvaloccurs in sequence S.\n1def\ufb01nd(S, val):\n2\u201d\u201d\u201dReturn index j such that S[j] == val, or -1 if no such element.\u201d\u201d\u201d\n3n=l e n ( S )\n4j=0\n5while j<n:\n6 ifS[j] == val:\n7 return j # a match was found at index j\n8 j+ =1\n9return \u22121\nCode Fragment 3.9: Algorithm for \ufb01nding the \ufb01rst index at which a given element\noccurs in a Python list.\nTo show that \ufb01nd is correct, we inductively de\ufb01ne a series of statements, Lj,\nthat lead to the correctness of our algorithm. Speci\ufb01cally, we claim the following\nis true at the beginning of iteration jof the while loop:\nLj:valis not equal to any of the \ufb01rst jelements of S.\nThis claim is true at the beginning of the \ufb01rst iteration of the loop, because jis 0\nand there are no elements among the \ufb01rst 0 in S(this kind of a trivially true claim\nis said to hold vacuously ). In iteration j, we compare element valto element S[j]\nand return the index jif these two elements are equivalent, which is clearly correct\nand completes the algorithm in this case. If the two elements valandS[j]are not\nequal, then we have found one more element not equal to valand we increment\nthe index j. Thus, the claim Ljwill be true for this new value of j; hence, it is\ntrue at the beginning of the next iteration. If the while loop terminates without\never returning an index in S,t h e nw eh a v e j=n.T h a ti s , Lnis true\u2014there are no\nelements of Sequal to val. Therefore, the algorithm correctly returns \u22121t oi n d i c a t e\nthatvalis not in S.", "3.5. Exercises 141\n3.5 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-3.1 Graph the functions 8 n,4nlogn,2n2,n3,a n d2nusing a logarithmic scale\nfor the x-a n d y-axes; that is, if the function value f(n)isy,p l o tt h i sa sa\npoint with x-coordinate at log nandy-coordinate at log y.\nR-3.2 The number of operations executed by algorithms AandBis 8nlognand\n2n2, respectively. Determine n0such that Ais better than Bforn\u2265n0.\nR-3.3 The number of operations executed by algorithms AandBis 40 n2and\n2n3, respectively. Determine n0such that Ais better than Bforn\u2265n0.\nR-3.4 Give an example of a function that is plotted the same on a log-log scale\nas it is on a standard scale.\nR-3.5 Explain why the plot of the function ncis a straight line with slope con a\nlog-log scale.\nR-3.6 What is the sum of all the even numbers from 0 to 2 n, for any positive\ninteger n?\nR-3.7 Show that the following two statements are equivalent:\n(a) The running time of algorithm Ais always O(f(n)).\n(b) In the worst case, the running time of algorithm AisO(f(n)).\nR-3.8 Order the following functions by asymptotic growth rate.\n4nlogn+2n2102logn\n3n+100log n4n 2n\nn2+10nn3nlogn\nR-3.9 Show that if d(n)isO(f(n)),t h e n ad(n)isO(f(n)), for any constant\na>0.\nR-3.10 Show that if d(n)isO(f(n))ande(n)isO(g(n)), then the product d(n)e(n)\nisO(f(n)g(n)).\nR-3.11 Show that if d(n)isO(f(n))ande(n)isO(g(n)),t h e n d(n)+e(n)is\nO(f(n)+g(n)).\nR-3.12 Show that if d(n)isO(f(n))ande(n)isO(g(n)),t h e n d(n)\u2212e(n)isnot\nnecessarily O(f(n)\u2212g(n)).\nR-3.13 Show that if d(n)isO(f(n))andf(n)isO(g(n)),t h e n d(n)isO(g(n)).\nR\n-3.14 Show that O(max{f(n),g(n)})=O(f(n)+g(n)).", "142 Chapter 3. Algorithm Analysis\nR-3.15 Show that f(n)isO(g(n))if and only if g(n)is\u03a9(f(n)).\nR-3.16 Show that if p(n)is a polynomial in n, then log p(n)isO(logn).\nR-3.17 Show that (n+1)5isO(n5).\nR-3.18 Show that 2n+1isO(2n).\nR-3.19 Show that nisO(nlogn).\nR-3.20 Show that n2is\u03a9(nlogn).\nR-3.21 Show that nlognis\u03a9(n).\nR-3.22 Show that \u2308f(n)\u2309isO(f(n)),i ff(n)is a positive nondecreasing function\nthat is always greater than 1.\nR-3.23 Give a big-Oh characterization, in terms of n, of the running time of the\nexample1 function shown in Code Fragment 3.10.\nR-3.24 Give a big-Oh characterization, in terms of n, of the running time of the\nexample2 function shown in Code Fragment 3.10.\nR-3.25 Give a big-Oh characterization, in terms of n, of the running time of the\nexample3 function shown in Code Fragment 3.10.\nR-3.26 Give a big-Oh characterization, in terms of n, of the running time of the\nexample4 function shown in Code Fragment 3.10.\nR-3.27 Give a big-Oh characterization, in terms of n, of the running time of the\nexample5 function shown in Code Fragment 3.10.\nR-3.28 For each function f(n)and time tin the following table, determine the\nlargest size nof a problem Pthat can be solved in time tif the algorithm\nfor solving Ptakes f(n)microseconds (one entry is already completed).\n1 Second\n 1 Hour\n 1 Month\n 1C e n t u r y\nlogn\n\u224810300000\nn\nnlogn\nn2\n2n\nR-3.29 Algorithm Aexecutes an O(logn)-time computation for each entry of an\nn-element sequence. What is its worst-case running time?\nR-3.30 Given an n-element sequence S, Algorithm B chooses log nelements in\nSat random and executes an O(n)-time calculation for each. What is the\nworst-case running time of Algorithm B?\nR-3.31 Given an n-element sequence Sof integers, Algorithm C executes an\nO(n)-time computation for each even number in S,a n da n O(logn)-time\ncomputation for each odd number in S. What are the best-case and worst-\ncase running times of Algorithm C?", "3.5. Exercises 143\n1defexample1(S):\n2\u201d\u201d\u201dReturn the sum of the elements in sequence S.\u201d\u201d\u201d\n3n=l e n ( S )\n4total = 0\n5forjinrange(n): #l o o pf r o m0t on - 1\n6 total += S[j]\n7return total\n8\n9defexample2(S):\n10 \u201d\u201d\u201dReturn the sum of the elements with even index in sequence S.\u201d\u201d\u201d\n11 n=l e n ( S )\n12 total = 0\n13 forjinrange(0, n, 2): # note the increment of 2\n14 total += S[j]\n15 return total\n16\n17defexample3(S):\n18 \u201d\u201d\u201dReturn the sum of the pre\ufb01x sums of sequence S.\u201d\u201d\u201d\n19 n=l e n ( S )\n20 total = 0\n21 forjinrange(n): #l o o pf r o m0t on - 1\n22 forkinrange(1+j): #l o o pf r o m0t oj\n23 total += S[k]\n24 return total\n2526defexample4(S):\n27 \u201d\u201d\u201dReturn the sum of the pre\ufb01x sums of sequence S.\u201d\u201d\u201d\n28 n=l e n ( S )\n29 pre\ufb01x = 0\n30 total = 0\n31 forjinrange(n):\n32 pre\ufb01x += S[j]\n33 total += pre\ufb01x\n34 return total\n3536defexample5(A, B): # assume that A and B have equal length\n37 \u201d\u201d\u201dReturn the number of elements in B equal to the sum of pre\ufb01x sums in A.\u201d\u201d\u201d\n38 n=l e n ( A )\n39 count = 0\n40 fo\nriinrange(n): #l o o pf r o m0t on - 1\n41 total = 0\n42 forjinrange(n): #l o o pf r o m0t on - 1\n43 forkinrange(1+j): #l o o pf r o m0t oj\n44 total += A[k]\n45 ifB[i] == total:\n46 count += 1\n47 return count\nCode Fragment 3.10: Some sample algorithms for analysis.", "144 Chapter 3. Algorithm Analysis\nR-3.32 Given an n-element sequence S, Algorithm D calls Algorithm E on each\nelement S[i]. Algorithm E runs in O(i)time when it is called on element\nS[i]. What is the worst-case running time of Algorithm D?\nR-3.33 Al and Bob are arguing about their algorithms. Al claims his O(nlogn)-\ntime method is always faster than Bob\u2019s O(n2)-time method. To settle the\nissue, they perform a set of experiments. To Al\u2019s dismay, they \ufb01nd that if\nn<100, the O(n2)-time algorithm runs faster, and only when n\u2265100 is\ntheO(nlogn)-time one better. Explain how this is possible.\nR-3.34 There is a well-known city (which will go nameless here) whose inhabi-\ntants have the reputation of enjoying a meal only if that meal is the bestthey have ever experienced in their life. Otherwise, they hate it. Assum-ing meal quality is distributed uniformly across a person\u2019s life, describe\nthe expected number of times inhabitants of this city are happy with their\nmeals?\nCreativity\nC-3.35 Assuming it is possible to sort nnumbers in O(nlogn)time, show that it\nis possible to solve the three-way set disjointness problem in O(nlogn)\ntime.\nC-3.36 Describe an ef\ufb01cient algorithm for \ufb01nding the ten largest elements in asequence of size n. What is the running time of your algorithm?\nC-3.37 Give an example of a positive function f(n)such that f(n)is neither O(n)\nnor\u03a9(n).\nC-3.38 Show that \u2211\nn\ni=1i2isO(n3).\nC-3.39 Show that \u2211ni=1i/2i<2.(Hint: Try to bound this sum term by term with\na geometric progression.)\nC-3.40 Show that logbf(n)is\u0398(logf(n))ifb>1 is a constant.\nC-3.41 Describe an algorithm for \ufb01nding both the minimum and maximum of n\nnumbers using fewer than 3 n/2 comparisons. (Hint: First, construct a\ngroup of candidate minimums and a group of candidate maximums.)\nC-3.42 Bob built a Web site and gave the URL only to his nfriends, which he\nnumbered from 1 to n. He told friend number ithat he/she can visit the\nWeb site at most itimes. Now Bob has a counter, C, keeping track of the\ntotal number of visits to the site (but not the identities of who visits). What\nis the minimum value for Csuch that Bob can know that one of his friends\nhas visited his/her maximum allowed number of times?\nC-3.43 Draw a visual justi\ufb01cation of Proposition 3.3 analogous to that of Fig-\nure 3.3(b) for the case when nis odd.", "3.5. Exercises 145\nC-3.44 Communication security is extremely important in computer networks,\nand one way many network protocols achieve security is to encrypt mes-\nsages. Typical cryptographic schemes for the secure transmission of mes-\nsages over such networks are based on the fact that no ef\ufb01cient algorithmsare known for factoring large integers. Hence, if we can represent a secretmessage by a large prime number p, we can transmit, over the network,\nthe number r=p\u00b7q,w h e r e q>pis another large prime number that acts\nas the encryption key . An eavesdropper who obtains the transmitted num-\nberron the network would have to factor rin order to \ufb01gure out the secret\nmessage p.\nUsing factoring to \ufb01gure out a message is very dif\ufb01cult without knowingthe encryption key q. To understand why, consider the following naive\nfactoring algorithm:\nforpinrange(2,r):\nifr%p= =0 : # if p divides r\nreturn\nThe secret message is p!\na. Suppose that the eavesdropper uses the above algorithm and has a\ncomputer that can carry out in 1 microsecond (1 millionth of a sec-\nond) a division between two integers of up to 100 bits each. Give anestimate of the time that it will take in the worst case to decipher the\nsecret message pif the transmitted message rhas 100 bits.\nb. What is the worst-case time complexity of the above algorithm?\nSince the input to the algorithm is just one large number r, assume\nthat the input size nis the number of bytes needed to store r,t h a ti s ,\nn=\u230a(log\n2r)/8\u230b+1, and that each division takes time O(n).\nC-3.45 A sequence Scontains n\u22121 unique integers in the range [0,n\u22121],t h a t\nis, there is one number from this range that is not in S. Design an O(n)-\ntime algorithm for \ufb01nding that number. You are only allowed to use O(1)\nadditional space besides the sequence Sitself.\nC-3.46 Al says he can prove that all sheep in a \ufb02ock are the same color:\nBase case: One sheep. It is clearly the same color as itself.\nInduction step: A\ufb02 o c ko f nsheep. Take a sheep, a, out. The remaining\nn\u22121 are all the same color by induction. Now put sheep aback in and\ntake out a different sheep, b. By induction, the n\u22121 sheep (now with a)\nare all the same color. Therefore, all the sheep in the \ufb02ock are the same\ncolor. What is wrong with Al\u2019s \u201cjusti\ufb01cation\u201d?\nC-3.47 LetSbe a set of nlines in the plane such that no two are parallel and\nno three meet in the same point. Show, by induction, that the lines in S\ndetermine \u0398(n2)intersection points.", "146 Chapter 3. Algorithm Analysis\nC-3.48 Consider the following \u201cjusti\ufb01cation\u201d that the Fibonacci function, F(n)\n(see Proposition 3.20) is O(n):\nBase case (n\u22642):F(1)=1a n d F(2)=2.\nInduction step (n>2): Assume claim true for n/prime<n. Consider n.F(n)=\nF(n\u22122)+F(n\u22121). By induction, F(n\u22122)isO(n\u22122)andF(n\u22121)is\nO(n\u22121). Then, F(n)isO((n\u22122)+(n\u22121)), by the identity presented in\nExercise R-3.11. Therefore, F(n)isO(n).\nWhat is wrong with this \u201cjusti\ufb01cation\u201d?\nC-3.49 Consider the Fibonacci function, F(n)(see Proposition 3.20). Show by\ninduction that F(n)is\u03a9((3/2)n).\nC-3.50 Letp(x)be a polynomial of degree n,t h a ti s , p(x)=\u2211n\ni=0aixi.\n(a) Describe a simple O(n2)-time algorithm for computing p(x).\n(b) Describe an O(nlogn)-time algorithm for computing p(x), based upon\na more ef\ufb01cient calculation of xi.\n(c) Now consider a rewriting of p(x)as\np(x)=a0+x(a1+x(a2+x(a3+\u00b7\u00b7\u00b7+x(an\u22121+xan)\u00b7\u00b7\u00b7))),\nwhich is known as Horner\u2019s method . Using the big-Oh notation, charac-\nterize the number of arithmetic operations this method executes.\nC-3.51 Show that the summation \u2211n\ni=1logiisO(nlogn).\nC-3.52 Show that the summation \u2211n\ni=1logiis\u03a9(nlogn).\nC-3.53 An evil king has nbottles of wine, and a spy has just poisoned one of\nthem. Unfortunately, they do not know which one it is. The poison is very\ndeadly; just one drop diluted even a billion to one will still kill. Even so,it takes a full month for the poison to take effect. Design a scheme fordetermining exactly which one of the wine bottles was poisoned in justone month\u2019s time while expending O(logn)taste testers.\nC-3.54 A sequence Scontains nintegers taken from the interval [0,4n], with repe-\ntitions allowed. Describe an ef\ufb01cient algorithm for determining an integervalue kthat occurs the most often in S. What is the running time of your\nalgorithm?\nProjects\nP-3.55 Perform an experimental analysis of the three algorithms pre\ufb01x\n average1 ,\npre\ufb01x\n average2 ,a n dpre\ufb01x\n average3 , from Section 3.3.3. Visualize their\nrunning times as a function of the input size with a log-log chart.\nP-3.56 Perform an experimental analysis that compares the relative running times\nof the functions shown in Code Fragment 3.10.", "Chapter Notes 147\nP-3.57 Perform experimental analysis to test the hypothesis that Python\u2019s sorted\nmethod runs in O(nlogn)time on average.\nP-3.58 For each of the three algorithms, unique1 ,unique2 ,a n dunique3 ,w h i c h\nsolve the element uniqueness problem, perform an experimental analysis\nto determine the largest value of nsuch that the given algorithm runs in\none minute or less.\nChapter Notes\nThe big-Oh notation has prompted several comments about its proper use [19, 49, 63].\nKnuth [64, 63] de\ufb01nes it using the notation f(n)=O(g(n)), but says this \u201cequality\u201d is only\n\u201cone way.\u201d We have chosen to take a more standard view of equality and view the big-Oh\nnotation as a set, following Brassard [19]. The reader interested in studying average-case\nanalysis is referred to the book chapter by Vitter and Flajolet [101]. For some additional\nmathematical tools, please refer to Appendix B.", "Chapter\n4Recursion\nContents\n4 . 1 I l l u s t r a t i v eE x a m p l e s ...................... 1 5 0\n4 . 1 . 1 T h e F a c t o r i a l F u n c t i o n ...................1 5 0\n4 . 1 . 2 D r a w i n g a nE n g l i s h R u l e r..................1 5 24 . 1 . 3 B i n a r y S e a r c h ........................1 5 5\n4 . 1 . 4 F i l e S y s t e m s.........................1 5 7\n4 . 2 A n a l y z i n gR e c u r s i v eA l g o r i t h m s ............... 1 6 1\n4 . 3 R e c u r s i o nR u nA m o k ..................... 1 6 5\n4.3.1 Maximum Recursive Depth in Python . . . . . . . . . . . 168\n4 . 4 F u r t h e rE x a m p l e so fR e c u r s i o n................ 1 6 9\n4 . 4 . 1 L i n e a r R e c u r s i o n.......................1 6 9\n4 . 4 . 2 B i n a r y R e c u r s i o n ......................1 7 4\n4 . 4 . 3 M u l t i p l e R e c u r s i o n .....................1 7 5\n4 . 5 D e s i g n i n gR e c u r s i v eA l g o r i t h m s ............... 1 7 7\n4 . 6 E l i m i n a t i n gT a i lR e c u r s i o n .................. 1 7 8\n4 . 7 E x e r c i s e s ............................ 1 8 0\n", "149\nOne way to describe repetition within a computer program is the use of loops,\nsuch as Python\u2019s while -loop and for-loop constructs described in Section 1.4.2. An\nentirely different way to achieve repetition is through a process known as recursion .\nRecursion is a technique by which a function makes one or more calls to itself\nduring execution, or by which a data structure relies upon smaller instances of\nthe very same type of structure in its representation. There are many examples ofrecursion in art and nature. For example, fractal patterns are naturally recursive. Aphysical example of recursion used in art is in the Russian Matryoshka dolls. Each\ndoll is either made of solid wood, or is hollow and contains another Matryoshka\ndoll inside it.\nIn computing, recursion provides an elegant and powerful alternative for per-\nforming repetitive tasks. In fact, a few programming languages (e.g., Scheme,Smalltalk) do not explicitly support looping constructs and instead rely directlyon recursion to express repetition. Most modern programming languages supportfunctional recursion using the identical mechanism that is used to support tradi-\ntional forms of function calls. When one invocation of the function make a recur-\nsive call, that invocation is suspended until the recursive call completes.\nRecursion is an important technique in the study of data structures and algo-\nrithms. We will use it prominently in several later chapters of this book (mostnotably, Chapters 8 and 12). In this chapter, we begin with the following four il-lustrative examples of the use of recursion, providing a Python implementation foreach.\n\u2022Thefactorial function (commonly denoted as n!) is a classic mathematical\nfunction that has a natural recursive de\ufb01nition.\n\u2022AnEnglish ruler has a recursive pattern that is a simple example of a fractal\nstructure.\n\u2022Binary search is among the most important computer algorithms. It allows\nus to ef\ufb01ciently locate a desired value in a data set with upwards of billionsof entries.\n\u2022The\ufb01le system for a computer has a recursive structure in which directories\ncan be nested arbitrarily deeply within other directories. Recursive algo-\nrithms are widely used to explore and manage these \ufb01le systems.\nWe then describe how to perform a formal analysis of the running time of a\nrecursive algorithm and we discuss some potential pitfalls when de\ufb01ning recur-\nsions. In the balance of the chapter, we provide many more examples of recursive\nalgorithms, organized to highlight some common forms of design.", "150 Chapter 4. Recursion\n4.1 Illustrative Examples\n4.1.1 The Factorial Function\nTo demonstrate the mechanics of recursion, we begin with a simple mathematical\nexample of computing the value of the factorial function . The factorial of a posi-\ntive integer n, denoted n!, is de\ufb01ned as the product of the integers from 1 to n.I f\nn=0, then n! is de\ufb01ned as 1 by convention. More formally, for any integer n\u22650,\nn!=/braceleftbigg1i fn=0\nn\u00b7(n\u22121)\u00b7(n\u22122)\u00b7\u00b7\u00b73\u00b72\u00b71i f n\u22651.\nFor example, 5! =5\u00b74\u00b73\u00b72\u00b71=120. The factorial function is important because\nit is known to equal the number of ways in which ndistinct items can be arranged\ninto a sequence, that is, the number of permutations ofnitems. For example, the\nthree characters a,b,a n d ccan be arranged in 3! =3\u00b72\u00b71=6 ways: abc, acb,\nbac, bca, cab,a n d cba.\nThere is a natural recursive de\ufb01nition for the factorial function. To see this,\nobserve that 5! =5\u00b7(4\u00b73\u00b72\u00b71)=5\u00b74!. More generally, for a positive integer n,\nwe can de\ufb01ne n!t ob e n\u00b7(n\u22121)!. This recursive de\ufb01nition can be formalized as\nn!=/braceleftbigg1i fn=0\nn\u00b7(n\u22121)!i fn\u22651.\nThis de\ufb01nition is typical of many recursive de\ufb01nitions. First, it contains one\nor more base cases , which are de\ufb01ned nonrecursively in terms of \ufb01xed quantities.\nIn this case, n=0 is the base case. It also contains one or more recursive cases ,\nwhich are de\ufb01ned by appealing to the de\ufb01nition of the function being de\ufb01ned.\nA Recursive Implementation of the Factorial Function\nRecursion is not just a mathematical notation; we can use recursion to design a\nPython implementation of a factorial function, as shown in Code Fragment 4.1.\n1deffactorial(n):\n2ifn= =0 :\n3 return 1\n4else:\n5 return n\nfactorial(n \u22121)\nCode Fragment 4.1: A recursive implementation of the factorial function.", "4.1. Illustrative Examples 151\nThis function does not use any explicit loops. Repetition is provided by the\nrepeated recursive invocations of the function. There is no circularity in this de\ufb01ni-\ntion, because each time the function is invoked, its argument is smaller by one, andwhen a base case is reached, no further recursive calls are made.\nWe illustrate the execution of a recursive function using a recursion trace . Each\nentry of the trace corresponds to a recursive call. Each new recursive functioncall is indicated by a downward arrow to a new invocation. When the function\nreturns, an arrow showing this return is drawn and the return value may be indicated\nalongside this arrow. An example of such a trace for the factorial function is shownin Figure 4.1.\nreturn 4\n6=2 4\nfactorial(4)\nfactorial(1)\nfactorial(0)factorial(3)\nfactorial(2)\nreturn 1return 1\n1=1return 2\n1=2return 3\n2=6\nFigure 4.1: A recursion trace for the call factorial(5) .\nA recursion trace closely mirrors the programming language\u2019s execution of the\nrecursion. In Python, each time a function (recursive or otherwise) is called, a struc-ture known as an activation record orframe is created to store information about\nthe progress of that invocation of the function. This activation record includes anamespace for storing the function call\u2019s parameters and local variables (see Sec-tion 1.10 for a discussion of namespaces), and information about which command\nin the body of the function is currently executing.\nWhen the execution of a function leads to a nested function call, the execu-\ntion of the former call is suspended and its activation record stores the place in the\nsource code at which the \ufb02ow of control should continue upon return of the nestedcall. This process is used both in the standard case of one function calling a dif-ferent function, or in the recursive case in which a function invokes itself. The key\npoint is that there is a different activation record for each active call.", "152 Chapter 4. Recursion\n4.1.2 Drawing an English Ruler\nIn the case of computing a factorial, there is no compelling reason for preferring\nrecursion over a direct iteration with a loop. As a more complex example of the\nuse of recursion, consider how to draw the markings of a typical English ruler. For\neach inch, we place a tick with a numeric label. We denote the length of the tickdesignating a whole inch as the major tick length. Between the marks for whole\ninches, the ruler contains a series of minor ticks , placed at intervals of 1 /2 inch,\n1/4 inch, and so on. As the size of the interval decreases by half, the tick length\ndecreases by one. Figure 4.2 demonstrates several such rulers with varying majortick lengths (although not drawn to scale).\n---- 0 ----- 0 --- 0\n----- -- --\n---\n--- --- --- 1\n---\n-- -- --\n---\n---- 1 ---- --- 2\n---\n-- -- --\n------ --- --- 3\n--\n-- --\n--\n---- 2 ----- 1\n(a) (b) (c)\nFigure 4.2: Three sample outputs of an English ruler drawing: (a) a 2-inch ruler\nwith major tick length 4; (b) a 1-inch ruler with major tick length 5; (c) a 3-inchruler with major tick length 3.\nA Recursive Approach to Ruler Drawing\nThe English ruler pattern is a simple example of a fractal , that is, a shape that has\na self-recursive structure at various levels of magni\ufb01cation. Consider the rule withmajor tick length 5 shown in Figure 4.2(b). Ignoring the lines containing 0 and 1,let us consider how to draw the sequence of ticks lying between these lines. Thecentral tick (at 1/2 inch) has length 4. Observe that the two patterns of ticks above\nand below this central tick are identical, and each has a central tick of length 3.", "4.1. Illustrative Examples 153\nIn general, an interval with a central tick length L\u22651 is composed of:\n\u2022An interval with a central tick length L\u22121\n\u2022A single tick of length L\n\u2022An interval with a central tick length L\u22121\nAlthough it is possible to draw such a ruler using an iterative process (see Ex-\nercise P-4.25), the task is considerably easier to accomplish with recursion. Our\nimplementation consists of three functions, as shown in Code Fragment 4.2. Themain function, draw\nruler , manages the construction of the entire ruler. Its argu-\nments specify the total number of inches in the ruler and the major tick length. Theutility function, draw\nline, draws a single tick with a speci\ufb01ed number of dashes\n(and an optional string label, that is printed after the tick).\nThe interesting work is done by the recursive draw\ninterval function. This\nfunction draws the sequence of minor ticks within some interval, based upon thelength of the interval\u2019s central tick. We rely on the intuition shown at the top of this\npage, and with a base case when L=0 that draws nothing. For L\u22651, the \ufb01rst and\nlast steps are performed by recursively calling draw\ninterval (L\u22121). The middle\nstep is performed by calling the function draw\nline (L).\n1defdraw\nline(tick\n length, tick\n label=\n ):\n2\u201d\u201d\u201dDraw one line with given tick length (followed by optional label).\u201d\u201d\u201d\n3line =\n -\ntick\nlength\n4iftick\nlabel:\n5 line +=\n +t i c k\n label\n6print(line)\n78defdraw\ninterval(center\n length):\n9\u201d\u201d\u201dDraw tick interval based upon a central tick length.\u201d\u201d\u201d\n10ifcenter\n length >0: # stop when length drops to 0\n11 draw\ninterval(center\n length \u22121) # recursively draw top ticks\n12 draw\nline(center\n length) # draw center tick\n13 draw\ninterval(center\n length \u22121) # recursively draw bottom ticks\n14\n15defdraw\nruler(num\n inches, major\n length):\n16 \u201d\u201d\u201dDraw English ruler with given number of inches, major tick length.\u201d\u201d\u201d\n17 draw\nline(major\n length,\n 0\n) # draw inch 0 line\n18forjinrange(1, 1 + num\n inches):\n19 draw\ninterval(major\n length \u22121) # draw interior ticks for inch\n20 draw\nline(major\n length, str(j)) # draw inch j line and label\nCode Fragment 4.2: A recursive implementation of a function that draws a ruler.", "154 Chapter 4. Recursion\nIllustrating Ruler Drawing Using a Recursion Trace\nThe execution of the recursive draw\ninterval function can be visualized using a re-\ncursion trace. The trace for draw\ninterval is more complicated than in the factorial\nexample, however, because each instance makes two recursive calls. To illustrate\nthis, we will show the recursion trace in a form that is reminiscent of an outline for\na document. See Figure 4.3.\n(previous pattern repeats)draw\ninterval(3)\ndraw\ninterval(2)\ndraw\ninterval(1)\ndraw\ninterval(1)draw\ninterval(0)\ndraw\nline(1)\ndraw\ninterval(0)\ndraw\ninterval(0)\ndraw\nline(1)\ndraw\ninterval(0)\ndraw\nline(3)\ndraw\ninterval(2)draw\nline(2)Output\nFigure 4.3: A partial recursion trace for the call draw\ninterval(3) . The second\npattern of calls for draw\ninterval(2) is not shown, but it is identical to the \ufb01rst.", "4.1. Illustrative Examples 155\n4.1.3 Binary Search\nIn this section, we describe a classic recursive algorithm, binary search , that is used\nto ef\ufb01ciently locate a target value within a sorted sequence of nelements. This is\namong the most important of computer algorithms, and it is the reason that we so\noften store data in sorted order (as in Figure 4.4).\n375 01234 6789 1 0 1 1 1 2 1 3 1 4 1 5\n9 24578 1 2 1 4 1 7 1 9 2 2 2 5 2 7 2 8 3 3\nFigure 4.4: Values stored in sorted order within an indexable sequence, such as a\nPython list. The numbers at top are the indices.\nWhen the sequence is unsorted , the standard approach to search for a target\nvalue is to use a loop to examine every element, until either \ufb01nding the target orexhausting the data set. This is known as the sequential search algorithm. This\nalgorithm runs in O(n)time (i.e., linear time) since every element is inspected in\nthe worst case.\nWhen the sequence is sorted andindexable , there is a much more ef\ufb01cient\nalgorithm. (For intuition, think about how you would accomplish this task byhand!) For any index j, we know that all the values stored at indices 0 ,..., j\u22121\nare less than or equal to the value at index j, and all the values stored at indices\nj+1,..., n\u22121 are greater than or equal to that at index j. This observation allows\nus to quickly \u201chome in\u201d on a search target using a variant of the children\u2019s game\u201chigh-low.\u201d We call an element of the sequence a candidate if, at the current stage\nof the search, we cannot rule out that this item matches the target. The algorithmmaintains two parameters, lowandhigh, such that all the candidate entries have\nindex at least lowand at most high. Initially, low =0a n d high =n\u22121. We then\ncompare the target value to the median candidate, that is, the item data[mid] with\nindex\nmid =\u230a(low +high )/2\u230b.\nWe consider three cases:\n\u2022If the target equals data[mid] , then we have found the item we are looking\nfor, and the search terminates successfully.\n\u2022Iftarget <data[mid], then we recur on the \ufb01rst half of the sequence, that is,\non the interval of indices from lowtomid\u22121.\n\u2022Iftarget >data[mid] , then we recur on the second half of the sequence, that\nis,\non the interval of indices from mid +1t ohigh.\nAn unsuccessful search occurs if low>high, as the interval [low,high ]is empty.", "156 Chapter 4. Recursion\nThis algorithm is known as binary search . We give a Python implementation\nin Code Fragment 4.3, and an illustration of the execution of the algorithm in Fig-\nure 4.5. Whereas sequential search runs in O(n)time, the more ef\ufb01cient binary\nsearch runs in O(logn)time. This is a signi\ufb01cant improvement, given that if n\nis one billion, log nis only 30. (We defer our formal analysis of binary search\u2019s\nrunning time to Proposition 4.2 in Section 4.2.)\n1defbinary\n search(data, target, low, high):\n2\u201d\u201d\u201dReturn True if target is found in indicated portion of a Python list.\n34The search only considers the portion from data[low] to data[high] inclusive.\n5\u201d\u201d\u201d\n6iflow>high:\n7 return False # interval is empty; no match\n8else:\n9 mid = (low + high) // 2\n10 iftarget == data[mid]: # found a match\n11 return True\n12 eliftarget <data[mid]:\n13 # recur on the portion left of the middle\n14 return binary\nsearch(data, target, low, mid \u22121)\n15 else:\n16 # recur on the portion right of the middle\n17 return binary\n search(data, target, mid + 1, high)\nCode Fragment 4.3: An implementation of the binary search algorithm.\nmidhigh\nhigh low\nlow midlow mid\nlow=mid=highhigh14 19 22 25 27 28 33 376789 1 0 1 1 1 2 1 3 1 4 1 5\n7 5 4 29 8\n9 24578 1 2 1 4 1 737 33 28 27 25 22 19\n9 24578 1 2 1 4 1 7 1 9 2 2 2 5 2 7 2 8 3 3 3 7\n19 22 25 27 28 33 375 01234\n17 14 12\n9 24578 1 2 1 7\nFigure 4.5: Example of a binary search for target value 22.", "4.1. Illustrative Examples 157\n4.1.4 File Systems\nModern operating systems de\ufb01ne \ufb01le-system directories (which are also sometimes\ncalled \u201cfolders\u201d) in a recursive way. Namely, a \ufb01le system consists of a top-leveldirectory, and the contents of this directory consists of \ufb01les and other directories,\nwhich in turn can contain \ufb01les and other directories, and so on. The operating\nsystem allows directories to be nested arbitrarily deep (as long as there is enoughspace in memory), although there must necessarily be some base directories thatcontain only \ufb01les, not further subdirectories. A representation of a portion of sucha \ufb01le system is given in Figure 4.6.\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/hw1 hw2 hw3 pr1 pr2 pr3grades\nmarket buylow sellhighgrades\nFigure 4.6: A portion of a \ufb01le system demonstrating a nested organization.\nGiven the recursive nature of the \ufb01le-system representation, it should not come\nas a surprise that many common behaviors of an operating system, such as copying\na directory or deleting a directory, are implemented with recursive algorithms. Inthis section, we consider one such algorithm: computing the total disk usage for all\n\ufb01les and directories nested within a particular directory.\nFor illustration, Figure 4.7 portrays the disk space being used by all entries in\nour sample \ufb01le system. We differentiate between the immediate disk space used by\neach entry and the cumulative disk space used by that entry and all nested features.\nFor example, the cs016 directory uses only 2K of immediate space, but a total of\n249K of cumulative space.", "158 Chapter 4. Recursion\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/ hw1\n3Khw2\n2Khw3\n4Kpr1\n57Kpr2\n97Kpr3\n74Kgrades\n8K\nmarket\n4786Kbuylow\n26Ksellhigh\n55Kgrades\n3K2K 1K1K\n1K 1K 1K\n1K 1K10K 229K 4870K\n82K 4787K5124K\n249K 4874K\nFigure 4.7: The same portion of a \ufb01le system given in Figure 4.6, but with additional\nannotations to describe the amount of disk space that is used. Within the icon for\neach \ufb01le or directory is the amount of space directly used by that artifact. Above\nthe icon for each directory is an indication of the cumulative disk space used by\nthat directory and all its (recursive) contents.\nThe cumulative disk space for an entry can be computed with a simple recursive\nalgorithm. It is equal to the immediate disk space used by the entry plus the sumof the cumulative disk space usage of any entries that are stored directly within\nthe entry. For example, the cumulative disk space for cs016 is 249K because it\nuses 2K itself, 8K cumulatively in grades, 10K cumulatively in homeworks,a n d\n229K cumulatively in programs. Pseudo-code for this algorithm is given in Code\nFragment 4.4.\nAlgorithm DiskUsage(path) :\nInput: A string designating a path to a \ufb01le-system entry\nOutput: The cumulative disk space used by that entry and any nested entries\ntotal =size(path) {immediate disk space used by the entry }\nifpath represents a directory then\nforeachchild entry stored within directory path do\ntotal =total +DiskUsage (child ) {recursive call }\nreturn total\nCode Fragment 4.4: An algorithm for computing the cumulative disk space usage\nnested at a \ufb01le-system entry. Function sizereturns the immediate disk space of an\nentry.", "4.1. Illustrative Examples 159\nPython\u2019s os Module\nTo provide a Python implementation of a recursive algorithm for computing disk\nusage, we rely on Python\u2019s osmodule, which provides robust tools for interacting\nwith the operating system during the execution of a program. This is an extensive\nlibrary, but we will only need the following four functions:\n\u2022os.path.getsize(path)Return the immediate disk usage (measured in bytes) for the \ufb01le or directorythat is identi\ufb01ed by the string path (e.g., /user/rt/courses).\n\u2022os.path.isdir(path)Return True if entry designated by string path is a directory; False otherwise.\n\u2022os.listdir(path)Return a list of strings that are the names of all entries within a directory\ndesignated by string path. In our sample \ufb01le system, if the parameter is\n/user/rt/courses, this returns the list [\ncs016\n ,\ncs252\n ].\n\u2022os.path.join(path, \ufb01lename)\nCompose the path string and \ufb01lename string using an appropriate operating\nsystem separator between the two (e.g., the /character for a Unix/Linux\nsystem, and the \\character for Windows). Return the string that represents\nt h ef u l lp a t ht ot h e\ufb01 l e .\nPython Implementation\nWith use of the osmodule, we now convert the algorithm from Code Fragment 4.4\ninto the Python implementation of Code Fragment 4.5.\n1import os\n23defdisk\nusage(path):\n4\u201d\u201d\u201dReturn the number of bytes used by a \ufb01le/folder and any descendents.\u201d\u201d\u201d\n5total = os.path.getsize(path) # account for direct usage\n6ifos.path.isdir(path): # if this is a directory,\n7 for\ufb01lename inos.listdir(path): # then for each child:\n8 childpath = os.path.join(path, \ufb01lename) # compose full path to child\n9 total += disk\n usage(childpath) # add child\u2019s usage to total\n10\n11 print (\n {0:<7}\n .format(total), path) # descriptive output (optional)\n12return total # return the grand total\nCode Fragment 4.5: A recursive function for reporting disk usage of a \ufb01le system.", "160 Chapter 4. Recursion\nRecursion Trace\nTo produce a different form of a recursion trace, we have included an extraneous\nprint statement within our Python implementation (line 11 of Code Fragment 4.5).\nThe precise format of that output intentionally mirrors output that is produced bya classic Unix/Linux utility named du(for \u201cdisk usage\u201d). It reports the amount of\ndisk space used by a directory and all contents nested within, and can produce averbose report, as given in Figure 4.8.\nOur implementation of the disk\nusage function produces an identical result,\nwhen executed on the sample \ufb01le system portrayed in Figure 4.7. During the ex-ecution of the algorithm, exactly one recursive call is made for each entry in theportion of the \ufb01le system that is considered. Because the print statement is made\njust before returning from a recursive call, the output shown in Figure 4.8 re\ufb02ects\nthe order in which the recursive calls are completed. In particular, we begin and\nend a recursive call for each entry that is nested below another entry, computingthe nested cumulative disk space before we can compute and report the cumulativedisk space for the containing entry. For example, we do not know the cumulativetotal for entry /user/rt/courses/cs016 until after the recursive calls regarding\ncontained entries grades, homeworks,a n d programs complete.\n8 /user/rt/courses/cs016/grades3 /user/rt/courses/cs016/homeworks/hw12 /user/rt/courses/cs016/homeworks/hw24 /user/rt/courses/cs016/homeworks/hw310 /user/rt/courses/cs016/homeworks\n57 /user/rt/courses/cs016/programs/pr1\n97 /user/rt/courses/cs016/programs/pr274 /user/rt/courses/cs016/programs/pr3229 /user/rt/courses/cs016/programs\n249 /user/rt/courses/cs016\n26 /user/rt/courses/cs252/projects/papers/buylow55 /user/rt/courses/cs252/projects/papers/sellhigh82 /user/rt/courses/cs252/projects/papers4786 /user/rt/courses/cs252/projects/demos/market\n4787 /user/rt/courses/cs252/projects/demos\n4870 /user/rt/courses/cs252/projects3 /user/rt/courses/cs252/grades4874 /user/rt/courses/cs2525124 /user/rt/courses/\nFigure 4.8: A report of the disk usage for the \ufb01le system shown in Figure 4.7,\nas generated by the Unix/Linux utility du(with command-line options -ak), or\nequivalently by our disk\nusage function from Code Fragment 4.5.", "4.2. Analyzing Recursive Algorithms 161\n4.2 Analyzing Recursive Algorithms\nIn Chapter 3, we introduced mathematical techniques for analyzing the ef\ufb01ciency\nof an algorithm, based upon an estimate of the number of primitive operations that\nare executed by the algorithm. We use notations such as big-Oh to summarize therelationship between the number of operations and the input size for a problem. Inthis section, we demonstrate how to perform this type of running-time analysis torecursive algorithms.\nWith a recursive algorithm, we will account for each operation that is performed\nbased upon the particular activation of the function that manages the \ufb02ow of control\nat the time it is executed. Stated another way, for each invocation of the function,we only account for the number of operations that are performed within the body ofthat activation. We can then account for the overall number of operations that are\nexecuted as part of the recursive algorithm by taking the sum, over all activations,\nof the number of operations that take place during each individual activation. (Asan aside, this is also the way we analyze a nonrecursive function that calls otherfunctions from within its body.)\nTo demonstrate this style of analysis, we revisit the four recursive algorithms\npresented in Sections 4.1.1 through 4.1.4: factorial computation, drawing an En-glish ruler, binary search, and computation of the cumulative size of a \ufb01le system.In general, we may rely on the intuition afforded by a recursion trace in recogniz-\ning how many recursive activations occur, and how the parameterization of each\nactivation can be used to estimate the number of primitive operations that occur\nwithin the body of that activation. However, each of these recursive algorithms hasa unique structure and form.\nComputing Factorials\nIt is relatively easy to analyze the ef\ufb01ciency of our function for computing fac-torials, as described in Section 4.1.1. A sample recursion trace for our factorial\nfunction was given in Figure 4.1. To compute factorial(n) , we see that there are a\ntotal of n+1 activations, as the parameter decreases from nin the \ufb01rst call, to n\u22121\nin the second call, and so on, until reaching the base case with parameter 0.\nIt is also clear, given an examination of the function body in Code Fragment 4.1,\nthat each individual activation of factorial executes a constant number of opera-\ntions. Therefore, we conclude that the overall number of operations for computingfactorial(n) isO(n),a st h e r ea r e n+1 activations, each of which accounts for O(1)\noperations.", "162 Chapter 4. Recursion\nDrawing an English Ruler\nIn analyzing the English ruler application from Section 4.1.2, we consider the fun-\ndamental question of how many total lines of output are generated by an initial call\ntodraw\ninterval(c) ,w h e r e cdenotes the center length. This is a reasonable bench-\nmark for the overall ef\ufb01ciency of the algorithm as each line of output is based upona call to the draw\nlineutility, and each recursive call to draw\ninterval with nonzero\nparameter makes exactly one direct call to draw\nline.\nSome intuition may be gained by examining the source code and the recur-\nsion trace. We know that a call to draw\ninterval(c) forc>0 spawns two calls to\ndraw\ninterval(c \u22121)and a single call to draw\nline. We will rely on this intuition to\nprove the following claim.\nProposition 4.1: Forc\u22650, a call to draw\n interval(c) results in precisely 2c\u22121\nlines of output.\nJusti\ufb01cation: We provide a formal proof of this claim by induction (see Sec-\ntion 3.4.3). In fact, induction is a natural mathematical technique for proving the\ncorrectness and ef\ufb01ciency of a recursive process. In the case of the ruler, we\nnote that an application of draw\ninterval(0) generates no output, and that 20\u22121=\n1\u22121=0. This serves as a base case for our claim.\nMore generally, the number of lines printed by draw\ninterval(c) is one more\nthan twice the number generated by a call to draw\ninterval(c \u22121), as one center\nline is printed between two such recursive calls. By induction, we have that thenumber of lines is thus 1 +2\u00b7(2\nc\u22121\u22121)=1+2c\u22122=2c\u22121.\nThis proof is indicative of a more mathematically rigorous tool, known as a\nrecurrence equation that can be used to analyze the running time of a recursive\nalgorithm. That technique is discussed in Section 12.2.4, in the context of recursivesorting algorithms.\nPerforming a Binary Search\nConsidering the running time of the binary search algorithm, as presented in Sec-tion 4.1.3, we observe that a constant number of primitive operations are executedat each recursive call of method of a binary search. Hence, the running time isproportional to the number of recursive calls performed. We will show that at most\u230alogn\u230b+1 recursive calls are made during a binary search of a sequence having n\nelements, leading to the following claim.\nProposition 4.2:\nThe binary search algorithm runs in O(logn)time for a sorted\nsequence with nelements.", "4.2. Analyzing Recursive Algorithms 163\nJusti\ufb01cation: To prove this claim, a crucial fact is that with each recursive call\nthe number of candidate entries still to be searched is given by the value\nhigh\u2212low +1.\nMoreover, the number of remaining candidates is reduced by at least one half with\neach recursive call. Speci\ufb01cally, from the de\ufb01nition of mid, the number of remain-\ning candidates is either\n(mid\u22121)\u2212low +1=/floorleftbigglow +high\n2/floorrightbigg\n\u2212low\u2264high\u2212low +1\n2\nor\nhigh\u2212(mid +1)+1=high\u2212/floorleftbigglow +high\n2/floorrightbigg\n\u2264high\u2212low +1\n2.\nInitially, the number of candidates is n; after the \ufb01rst call in a binary search, it is at\nmost n/2; after the second call, it is at most n/4; and so on. In general, after the jth\ncall in a binary search, the number of candidate entries remaining is at most n/2j.\nIn the worst case (an unsuccessful search), the recursive calls stop when there are nomore candidate entries. Hence, the maximum number of recursive calls performed,is the smallest integer rsuch that\nn\n2r<1.\nIn other words (recalling that we omit a logarithm\u2019s base when it is 2), r>logn.\nThus, we have r=\u230alogn\u230b+1,\nwhich implies that binary search runs in O(logn)time.\nComputing Disk Space Usage\nOur \ufb01nal recursive algorithm from Section 4.1 was that for computing the overalldisk space usage in a speci\ufb01ed portion of a \ufb01le system. To characterize the \u201cprob-lem size\u201d for our analysis, we let ndenote the number of \ufb01le-system entries in the\nportion of the \ufb01le system that is considered. (For example, the \ufb01le system portrayed\nin Figure 4.6 has n=19 entries.)\nTo characterize the cumulative time spent for an initial call to the disk\nusage\nfunction, we must analyze the total number of recursive invocations that are made,\nas well as the number of operations that are executed within those invocations.\nWe begin by showing that there are precisely nrecursive invocations of the\nfunction, in particular, one for each entry in the relevant portion of the \ufb01le system.Intuitively, this is because a call to disk\nusage for a particular entry eof the \ufb01le\nsystem is only made from within the for loop of Code Fragment 4.5 when process-ing the entry for the unique directory that contains e, and that entry will only be\nexplored once.", "164 Chapter 4. Recursion\nTo formalize this argument, we can de\ufb01ne the nesting level of each entry such\nthat the entry on which we begin has nesting level 0, entries stored directly within\nit have nesting level 1, entries stored within those entries have nesting level 2, andso on. We can prove by induction that there is exactly one recursive invocation ofdisk\nusage upon each entry at nesting level k. As a base case, when k=0, the only\nrecursive invocation made is the initial one. As the inductive step, once we know\nthere is exactly one recursive invocation for each entry at nesting level k, we can\nclaim that there is exactly one invocation for each entry eat nesting level k,m a d e\nwithin the for loop for the entry at level kthat contains e.\nHaving established that there is one recursive call for each entry of the \ufb01le\nsystem, we return to the question of the overall computation time for the algorithm.It would be great if we could argue that we spend O(1)time in any single invocation\nof the function, but that is not the case. While there are a constant number of\nsteps re\ufb02ect in the call to os.path.getsize to compute the disk usage directly at that\nentry, when the entry is a directory, the body of the disk\nusage function includes a\nfor loop that iterates over all entries that are contained within that directory. In the\nworst case, it is possible that one entry includes n\u22121 others.\nBased on this reasoning, we could conclude that there are O(n)recursive calls,\neach of which runs in O(n)time, leading to an overall running time that is O(n2).\nWhile this upper bound is technically true, it is not a tight upper bound. Remark-\nably, we can prove the stronger bound that the recursive algorithm for disk\nusage\ncompletes in O(n)time! The weaker bound was pessimistic because it assumed\na worst-case number of entries for each directory. While it is possible that somedirectories contain a number of entries proportional to n, they cannot all contain\nthat many. To prove the stronger claim, we choose to consider the overall number\nof iterations of the for loop across all recursive calls. We claim there are preciselyn\u22121 such iteration of that loop overall. We base this claim on the fact that each\niteration of that loop makes a recursive call to disk\nusage , and yet we have already\nconcluded that there are a total of ncalls to disk\nusage (including the original call).\nWe therefore conclude that there are O(n)recursive calls, each of which uses O(1)\ntime outside the loop, and that the overall number of operations due to the loop\nisO(n). Summing all of these bounds, the overall number of operations is O(n).\nThe argument we have made is more advanced than with the earlier examples\nof recursion. The idea that we can sometimes get a tighter bound on a series ofoperations by considering the cumulative effect, rather than assuming that each\nachieves a worst case is a technique called amortization ; we will see a further\nexample of such analysis in Section 5.3. Furthermore, a \ufb01le system is an implicit\nexample of a data structure known as a tree, and our disk usage algorithm is really\na manifestation of a more general algorithm known as a tree traversal . Trees will\nbe the focus of Chapter 8, and our argument about the O(n)running time of the\ndisk usage algorithm will be generalized for tree traversals in Section 8.4.", "4.3. Recursion Run Amok 165\n4.3 Recursion Run Amok\nAlthough recursion is a very powerful tool, it can easily be misused in various ways.\nIn this section, we examine several problems in which a poorly implemented recur-\nsion causes drastic inef\ufb01ciency, and we discuss some strategies for recognizing andavoid such pitfalls.\nWe begin by revisiting the element uniqueness problem , de\ufb01ned on page 135\nof Section 3.3.3. We can use the following recursive formulation to determine ifallnelements of a sequence are unique. As a base case, when n=1, the elements\nare trivially unique. For n\u22652, the elements are unique if and only if the \ufb01rst n\u22121\nelements are unique, the last n\u22121 items are unique, and the \ufb01rst and last elements\nare different (as that is the only pair that was not already checked as a subcase). Arecursive implementation based on this idea is given in Code Fragment 4.6, namedunique3 (to differentiate it from unique1 andunique2 from Chapter 3).\n1defunique3(S, start, stop):\n2\u201d\u201d\u201dReturn True if there are no duplicate elements in slice S[start:stop].\u201d\u201d\u201d\n3ifstop\u2212start<=1 :return True #a tm o s to n ei t e m\n4elif not unique(S, start, stop \u22121):return False # \ufb01rst part has duplicate\n5elif not unique(S, start+1, stop): return False # second part has duplicate\n6else:return S[start] != S[stop \u22121] # do \ufb01rst and last di\ufb00er?\nCode Fragment 4.6: Recursive unique3 for testing element uniqueness.\nUnfortunately, this is a terribly inef\ufb01cient use of recursion. The nonrecursive\npart of each call uses O(1)time, so the overall running time will be proportional to\nthe total number of recursive invocations. To analyze the problem, we let ndenote\nthe number of entries under consideration, that is, let n= stop\u2212start .\nIfn=1, then the running time of unique3 isO(1), since there are no recursive\ncalls for this case. In the general case, the important observation is that a single calltounique3 for a problem of size nmay result in two recursive calls on problems of\nsizen\u22121. Those two calls with size n\u22121 could in turn result in four calls (two\neach) with a range of size n\u22122, and thus eight calls with size n\u22123 and so on.\nThus, in the worst case, the total number of function calls is given by the geometricsummation\n1+2+4+\u00b7\u00b7\u00b7+2\nn\u22121,\nwhich is equal to 2n\u22121 by Proposition 3.5. Thus, the running time of function\nunique3 isO(2n). This is an incredibly inef\ufb01cient function for solving the ele-\nment uniqueness problem. Its inef\ufb01ciency comes not from the fact that it usesrecursion\u2014it comes from the fact that it uses recursion poorly, which is something\nwe address in Exercise C-4.11.", "166 Chapter 4. Recursion\nAn Ine\ufb03cient Recursion for Computing Fibonacci Numbers\nIn Section 1.8, we introduced a process for generating the Fibonacci numbers,\nwhich can be de\ufb01ned recursively as follows:\nF0 =0\nF1 =1\nFn =Fn\u22122+Fn\u22121forn>1.\nIronically, a direct implementation based on this de\ufb01nition results in the functionbad\n\ufb01bonacci shown in Code Fragment 4.7, which computes the sequence of Fi-\nbonacci numbers by making two recursive calls in each non-base case.\n1defbad\n\ufb01bonacci(n):\n2\u201d\u201d\u201dReturn the nth Fibonacci number.\u201d\u201d\u201d\n3ifn<=1 :\n4 return n\n5else:\n6 return bad\n\ufb01bonacci(n \u22122) + bad\n \ufb01bonacci(n \u22121)\nCode Fragment 4.7: Computing the nthFibonacci number using binary recursion.\nUnfortunately, such a direct implementation of the Fibonacci formula results\nin a terribly inef\ufb01cient function. Computing the nthFibonacci number in this way\nrequires an exponential number of calls to the function. Speci\ufb01cally, let cndenote\nthe number of calls performed in the execution of bad\n\ufb01bonacci(n) . Then, we have\nthe following values for the cn\u2019s:\nc0 =1\nc1 =1\nc2 =1+c0+c1=1+1+1=3\nc3 =1+c1+c2=1+1+3=5\nc4 =1+c2+c3=1+3+5=9\nc5 =1+c3+c4=1+5+9=15\nc6 =1+c4+c5=1+9+15=25\nc7 =1+c5+c6=1+15+25=41\nc8 =1+c6+c7=1+25+41=67\nIf we follow the pattern forward, we see that the number of calls more than doublesfor each two consecutive indices. That is, c\n4is more than twice c2,c5is more than\ntwice c3,c6is more than twice c4, and so on. Thus, cn>2n/2, which means that\nbad\n\ufb01bonacci(n) makes a number of calls that is exponential in n.", "4.3. Recursion Run Amok 167\nAn E\ufb03cient Recursion for Computing Fibonacci Numbers\nWe were tempted into using the bad recursion formulation because of the way the\nnthFibonacci number, Fn, depends on the two previous values, Fn\u22122andFn\u22121.B u t\nnotice that after computing Fn\u22122, the call to compute Fn\u22121requires its own recursive\ncall to compute Fn\u22122, as it does not have knowledge of the value of Fn\u22122that was\ncomputed at the earlier level of recursion. That is duplicative work. Worse yet, both\nof those calls will need to (re)compute the value of Fn\u22123, as will the computation\nofFn\u22121. This snowballing effect is what leads to the exponential running time of\nbad\nrecursion .\nWe can compute Fnmuch more ef\ufb01ciently using a recursion in which each invo-\ncation makes only one recursive call. To do so, we need to rede\ufb01ne the expectationsof the function. Rather than having the function return a single value, which is then\nthFibonacci number, we de\ufb01ne a recursive function that returns a pair of con-\nsecutive Fibonacci numbers (Fn,Fn\u22121), using the convention F\u22121=0. Although\nit seems to be a greater burden to report two consecutive Fibonacci numbers in-\nstead of one, passing this extra information from one level of the recursion to the\nnext makes it much easier to continue the process. (It allows us to avoid havingto recompute the second value that was already known within the recursion.) Animplementation based on this strategy is given in Code Fragment 4.8.\n1defgood\n\ufb01bonacci(n):\n2\u201d\u201d\u201dReturn pair of Fibonacci numbers, F(n) and F(n-1).\u201d\u201d\u201d\n3ifn<=1 :\n4 return (n,0)\n5else:\n6 (a, b) = good\n \ufb01bonacci(n \u22121)\n7 return (a+b, a)\nCode Fragment 4.8: Computing the nthFibonacci number using linear recursion.\nIn terms of ef\ufb01ciency, the difference between the bad recursion and the good\nrecursion for this problem is like night and day. The bad\n\ufb01bonacci function uses\nexponential time. We claim that the execution of function good\n \ufb01bonacci (n)takes\nO(n)time. Each recursive call to good\n \ufb01bonacci decreases the argument nby 1;\ntherefore, a recursion trace includes a series of nfunction calls. Because the nonre-\ncursive work for each call uses constant time, the overall computation executes inO(n)time.", "168 Chapter 4. Recursion\n4.3.1 Maximum Recursive Depth in Python\nAnother danger in the misuse of recursion is known as in\ufb01nite recursion . If each\nrecursive call makes another recursive call, without ever reaching a base case, then\nwe have an in\ufb01nite series of such calls. This is a fatal error. An in\ufb01nite recursioncan quickly swamp computing resources, not only due to rapid use of the CPU,but because each successive call creates an activation record requiring additionalmemory. A blatant example of an ill-formed recursion is the following:\ndef\ufb01b(n):\nreturn \ufb01b(n) # \ufb01b(n) equals \ufb01b(n)\nHowever, there are far more subtle errors that can lead to an in\ufb01nite recursion.\nRevisiting our implementation of binary search in Code Fragment 4.3, in the \ufb01nalcase (line 17) we make a recursive call on the right portion of the sequence, inparticular going from index mid+1 tohigh. Had that line instead been written as\nreturn binary\nsearch(data, target, mid, high) # note the use of mid\nthis could result in an in\ufb01nite recursion. In particular, when searching a range of\ntwo elements, it becomes possible to make a recursive call on the identical range.\nA programmer should ensure that each recursive call is in some way progress-\ning toward a base case (for example, by having a parameter value that decreases\nwith each call). However, to combat against in\ufb01nite recursions, the designers ofPython made an intentional decision to limit the overall number of function acti-vations that can be simultaneously active. The precise value of this limit dependsupon the Python distribution, but a typical default value is 1000. If this limit is\nreached, the Python interpreter raises a RuntimeError with a message, maximum\nrecursion depth exceeded.\nFor many legitimate applications of recursion, a limit of 1000 nested function\ncalls suf\ufb01ces. For example, our binary\nsearch function (Section 4.1.3) has O(logn)\nrecursive depth, and so for the default recursive limit to be reached, there would\nneed to be 21000elements (far, far more than the estimated number of atoms in the\nuniverse). However, in the next section we discuss several algorithms that haverecursive depth proportional to n. Python\u2019s arti\ufb01cial limit on the recursive depth\ncould disrupt such otherwise legitimate computations.\nFortunately, the Python interpreter can be dynamically recon\ufb01gured to change\nthe default recursive limit. This is done through use of a module named sys,w h i c h\nsupports a getrecursionlimit function and a setrecursionlimit . Sample usage of\nthose functions is demonstrated as follows:\nimport sys\nold = sys.getrecursionlimit( ) # perhaps 1000 is typical\nsys.setrecursionlimit(1000000) # change to allow 1 million nested calls", "4.4. Further Examples of Recursion 169\n4.4 Further Examples of Recursion\nIn the remainder of this chapter, we provide additional examples of the use of re-\ncursion. We organize our presentation by considering the maximum number of\nrecursive calls that may be started from within the body of a single activation.\n\u2022If a recursive call starts at most one other, we call this a linear recursion .\n\u2022If a recursive call may start two others, we call this a binary recursion .\n\u2022If a recursive call may start three or more others, this is multiple recursion .\n4.4.1 Linear Recursion\nIf a recursive function is designed so that each invocation of the body makes at\nmost one new recursive call, this is know as linear recursion . Of the recursions we\nhave seen so far, the implementation of the factorial function (Section 4.1.1) andthegood\n\ufb01bonacci function (Section 4.3) are clear examples of linear recursion.\nMore interestingly, the binary search algorithm (Section 4.1.3) is also an example\noflinear recursion , despite the \u201cbinary\u201d terminology in the name. The code for\nbinary search (Code Fragment 4.3) includes a case analysis with two branches that\nlead to recursive calls, but only one of those calls can be reached during a particularexecution of the body.\nA consequence of the de\ufb01nition of linear recursion is that any recursion trace\nwill appear as a single sequence of calls, as we originally portrayed for the factorialfunction in Figure 4.1 of Section 4.1.1. Note that the linear recursion terminol-\nogy re\ufb02ects the structure of the recursion trace, not the asymptotic analysis of the\nrunning time; for example, we have seen that binary search runs in O(logn)time.\nSumming the Elements of a Sequence Recursively\nLinear recursion can be a useful tool for processing a data sequence, such as a\nPython list. Suppose, for example, that we want to compute the sum of a sequence,S,o fnintegers. We can solve this summation problem using linear recursion by\nobserving that the sum of all nintegers in Sis trivially 0, if n=0, and otherwise\nthat it is the sum of the \ufb01rst n\u22121i n t e g e r si n Splus the last element in S.( S e e\nFigure 4.9.)\n4362893285172835\n701234 6789 1 0 1 1 1 2 1 3 1 4 1 5\nFigure 4.9: Computing the sum of a sequence recursively, by adding the last number\nto the sum of the \ufb01rst n\u22121.", "170 Chapter 4. Recursion\nA recursive algorithm for computing the sum of a sequence of numbers based\non this intuition is implemented in Code Fragment 4.9.\n1deflinear\n sum(S, n):\n2\u201d\u201d\u201dReturn the sum of the \ufb01rst n numbers of sequence S.\u201d\u201d\u201d\n3ifn= =0 :\n4 return 0\n5else:\n6 return linear\n sum(S, n \u22121) + S[n \u22121]\nCode Fragment 4.9: Summing the elements of a sequence using linear recursion.\nA recursion trace of the linear\n sum function for a small example is given in\nFigure 4.10. For an input of size n,t h elinear\n sum algorithm makes n+1 function\ncalls. Hence, it will take O(n)time, because it spends a constant amount of time\nperforming the nonrecursive part of each call. Moreover, we can also see that the\nmemory space used by the algorithm (in addition to the sequence S)i sa l s o O(n),a s\nwe use a constant amount of memory space for each of the n+1 activation records\nin the trace at the time we make the \ufb01nal recursive call (with n=0).\nreturn 1 5+S [ 4 ]=1 5+8=2 3\nlinear\n sum(S, 5)\nlinear\n sum(S, 4)\nlinear\n sum(S, 3)\nlinear\n sum(S, 2)\nlinear\n sum(S, 1)\nlinear\n sum(S, 0)return 0return 0+S [ 0 ]=0+4=4return 4+S [ 1 ]=4+3=7return 7+S [ 2 ]=7+6=1 3return 1 3+S [ 3 ]=1 3+2=1 5\nFigure 4.10: Recursion trace for an execution of linear\n sum(S, 5) with input pa-\nrameter S=[ 4 ,3 ,6 ,2 ,8 ] .", "4.4. Further Examples of Recursion 171\nReversing a Sequence with Recursion\nNext, let us consider the problem of reversing the nelements of a sequence, S,s o\nthat the \ufb01rst element becomes the last, the second element becomes second to the\nlast, and so on. We can solve this problem using linear recursion, by observing thatthe reversal of a sequence can be achieved by swapping the \ufb01rst and last elementsand then recursively reversing the remaining elements. We present an implemen-\ntation of this algorithm in Code Fragment 4.10, using the convention that the \ufb01rst\ntime we call this algorithm we do so as reverse(S, 0, len(S)).\n1defreverse(S, start, stop):\n2\u201d\u201d\u201dReverse elements in implicit slice S[start:stop].\u201d\u201d\u201d\n3ifstart<stop\u22121: #i fa tl e a s t2e l e m e n t s :\n4 S[start], S[stop \u22121] = S[stop \u22121], S[start] # swap \ufb01rst and last\n5 reverse(S, start+1, stop \u22121) # recur on rest\nCode Fragment 4.10: Reversing the elements of a sequence using linear recursion.\nNote that there are two implicit base case scenarios: When start == stop ,t h e\nimplicit range is empty, and when start == stop \u22121, the implicit range has only\none element. In either of these cases, there is no need for action, as a sequence\nwith zero elements or one element is trivially equal to its reversal. When otherwise\ninvoking recursion, we are guaranteed to make progress towards a base case, asthe difference, stop\u2212start , decreases by two with each call (see Figure 4.11). If n\nis even, we will eventually reach the start == stop case, and if nis odd, we will\neventually reach the start == stop \u22121case.\nThe above argument implies that the recursive algorithm of Code Fragment 4.10\nis guaranteed to terminate after a total of 1 +/floorleftbig\nn\n2/floorrightbig\nrecursive calls. Since each call\ninvolves a constant amount of work, the entire process runs in O(n)time.\n6\n362895\n59826345362894\n598263459628345 01234\n4\nFigure 4.11: A trace of the recursion for reversing a sequence. The shaded portion\nhas yet to be reversed.", "172 Chapter 4. Recursion\nRecursive Algorithms for Computing Powers\nAs another interesting example of the use of linear recursion, we consider the prob-\nlem of raising a number xto an arbitrary nonnegative integer, n. That is, we wish\nto compute the power function ,d e \ufb01 n e da s power (x,n)=xn. (We use the name\n\u201cpower\u201d for this discussion, to differentiate from the built-in function pow that pro-\nvides such functionality.) We will consider two different recursive formulations forthe problem that lead to algorithms with very different performance.\nA trivial recursive de\ufb01nition follows from the fact that x\nn=x\u00b7xn\u22121forn>0.\npower (x,n)=/braceleftbigg1i fn=0\nx\u00b7power (x,n\u22121)otherwise.\nThis de\ufb01nition leads to a recursive algorithm shown in Code Fragment 4.11.\n1defpower(x, n):\n2\u201d\u201d\u201dCompute the value x\n n for integer n.\u201d\u201d\u201d\n3ifn= =0 :\n4 return 1\n5else:\n6 return x\npower(x, n \u22121)\nCode Fragment 4.11: Computing the power function using trivial recursion.\nA recursive call to this version of power (x,n)runs in O(n)time. Its recursion\ntrace has structure very similar to that of the factorial function from Figure 4.1,with the parameter decreasing by one with each call, and constant work performedat each of n+1 levels.\nHowever, there is a much faster way to compute the power function using an\nalternative de\ufb01nition that employs a squaring technique. Let k=/floorleftbig\nn\n2/floorrightbig\ndenote the\n\ufb02oor of the division (expressed as n/ /2 in Python). We consider the expression\n/parenleftbig\nxk/parenrightbig2.W h e n nis even,/floorleftbign\n2/floorrightbig\n=n\n2and therefore/parenleftbig\nxk/parenrightbig2=/parenleftBig\nxn\n2/parenrightBig2\n=xn.W h e n nis odd,\n/floorleftbign\n2/floorrightbig\n=n\u22121\n2and/parenleftbig\nxk/parenrightbig2=xn\u22121, and therefore xn=x\u00b7/parenleftbig\nxk/parenrightbig2,j u s ta s213=2\u00b726\u00b726.\nThis analysis leads to the following recursive de\ufb01nition:\npower (x,n)=\u23a7\n\u23aa\u23a8\n\u23aa\u23a91i fn=0\nx\u00b7/parenleftbig\npower/parenleftbig\nx,/floorleftbign\n2/floorrightbig/parenrightbig/parenrightbig2ifn>0 is odd/parenleftbig\npower/parenleftbig\nx,/floorleftbign\n2/floorrightbig/parenrightbig/parenrightbig2ifn>0i se v e n\nIf we were to implement this recursion making tworecursive calls to compute\npower (x,/floorleftbign\n2/floorrightbig\n)\u00b7power (x,/floorleftbign\n2/floorrightbig\n), a trace of the recursion would demonstrate O(n)\ncalls. We can perform signi\ufb01cantly fewer operations by computing power (x,/floorleftbign\n2/floorrightbig\n)\nas a partial result, and then multiplying it by itself. An implementation based on\nthis recursive de\ufb01nition is given in Code Fragment 4.12.", "4.4. Further Examples of Recursion 173\n1defpower(x, n):\n2\u201d\u201d\u201dCompute the value x\n n for integer n.\u201d\u201d\u201d\n3ifn= =0 :\n4 return 1\n5else:\n6 partial = power(x, n // 2) # rely on truncated division\n7 result = partial\n partial\n8 ifn%2= =1 : # if n odd, include extra factor of x\n9 result\n =x\n10 return result\nCode Fragment 4.12: Computing the power function using repeated squaring.\nTo illustrate the execution of our improved algorithm, Figure 4.12 provides a\nrecursion trace of the computation power(2, 13) .\nreturn 64\n64\n2 = 8192\npower(2, 13)\npower(2, 6)\npower(2, 3)\npower(2, 1)\npower(2, 0)return 1return 1\n1\n2=2return 2\n2\n2=8return 8\n8=6 4\nFigure 4.12: Recursion trace for an execution of power(2, 13) .\nTo analyze the running time of the revised algorithm, we observe that the expo-\nnent in each recursive call of function power(x,n) is at most half of the preceding\nexponent. As we saw with the analysis of binary search, the number of times that\nwe can divide nin half before getting to one or less is O(logn). Therefore, our new\nformulation of the power function results in O(logn)recursive calls. Each individ-\nual activation of the function uses O(1)operations (excluding the recursive calls),\nand so the total number of operations for computing power(x,n) isO(logn).T h i s\nis a signi\ufb01cant improvement over the original O(n)-time algorithm.\nThe improved version also provides signi\ufb01cant saving in reducing the memory\nusage. The \ufb01rst version has a recursive depth of O(n), and therefore O(n)activation\nrecords are simultaneous stored in memory. Because the recursive depth of the\nimproved version is O(logn), its memory usages is O(logn)as well.", "174 Chapter 4. Recursion\n4.4.2 Binary Recursion\nWhen a function makes two recursive calls, we say that it uses binary recursion .\nWe have already seen several examples of binary recursion, most notably when\ndrawing the English ruler (Section 4.1.2), or in the bad\n\ufb01bonacci function of Sec-\ntion 4.3. As another application of binary recursion, let us revisit the problem of\nsumming the nelements of a sequence, S, of numbers. Computing the sum of one\nor zero elements is trivial. With two or more elements, we can recursively com-\npute the sum of the \ufb01rst half, and the sum of the second half, and add these sumstogether. Our implementation of such an algorithm, in Code Fragment 4.13, isinitially invoked as binary\nsum(A, 0, len(A)).\n1defbinary\n sum(S, start, stop):\n2\u201d\u201d\u201dReturn the sum of the numbers in implicit slice S[start:stop].\u201d\u201d\u201d\n3ifstart>=s t o p : # zero elements in slice\n4 return 0\n5elifstart == stop \u22121: # one element in slice\n6 return S[start]\n7else: # two or more elements in slice\n8 mid = (start + stop) // 2\n9 return binary\n sum(S, start, mid) + binary\n sum(S, mid, stop)\nCode Fragment 4.13: Summing the elements of a sequence using binary recursion.\nTo analyze algorithm binary\n sum, we consider, for simplicity, the case where\nnis a power of two. Figure 4.13 shows the recursion trace of an execution of\nbinary\n sum(0, 8) . We label each box with the values of parameters start:stop\nfor that call. The size of the range is divided in half at each recursive call, andso the depth of the recursion is 1 +log\n2n. Therefore, binary\n sum uses O(logn)\namount of additional space, which is a big improvement over the O(n)space used\nby the linear\n sum function of Code Fragment 4.9. However, the running time of\nbinary\n sum isO(n),a st h e r ea r e2 n\u22121 function calls, each requiring constant time.\n0:1 1:2 2:3 4:5 6:7 7:8 3:4 5:60:2 4:6 6:8 2:40:4 4:80:8\nFigure 4.13: Recursion trace for the execution of binary\n sum(0, 8) .", "4.4. Further Examples of Recursion 175\n4.4.3 Multiple Recursion\nGeneralizing from binary recursion, we de\ufb01ne multiple recursion as a process in\nwhich a function may make more than two recursive calls. Our recursion for an-\nalyzing the disk space usage of a \ufb01le system (see Section 4.1.4) is an example of\nmultiple recursion, because the number of recursive calls made during one invoca-tion was equal to the number of entries within a given directory of the \ufb01le system.\nAnother common application of multiple recursion is when we want to enumer-\nate various con\ufb01gurations in order to solve a combinatorial puzzle. For example,\nthe following are all instances of what are known as summation puzzles :\npot +pan =bib\ndog +cat =pig\nboy +girl =baby\nTo solve such a puzzle, we need to assign a unique digit (that is, 0 ,1,..., 9) to each\nletter in the equation, in order to make the equation true. Typically, we solve such\na puzzle by using our human observations of the particular puzzle we are tryingto solve to eliminate con\ufb01gurations (that is, possible partial assignments of digits\nto letters) until we can work though the feasible con\ufb01gurations left, testing for the\ncorrectness of each one.\nIf the number of possible con\ufb01gurations is not too large, however, we can use\na computer to simply enumerate all the possibilities and test each one, withoutemploying any human observations. In addition, such an algorithm can use multiple\nrecursion to work through the con\ufb01gurations in a systematic way. We show pseudo-\ncode for such an algorithm in Code Fragment 4.14. To keep the description generalenough to be used with other puzzles, the algorithm enumerates and tests all k-\nlength sequences without repetitions of the elements of a given universe U.W e\nbuild the sequences of kelements by the following steps:\n1. Recursively generating the sequences of k\u22121e l e m e n t s\n2. Appending to each such sequence an element not already contained in it.\nThroughout the execution of the algorithm, we use a set Uto keep track of the\nelements not contained in the current sequence, so that an element ehas not been\nused yet if and only if eis in U.\nAnother way to look at the algorithm of Code Fragment 4.14 is that it enumer-\nates every possible size- kordered subset of U, and tests each subset for being a\npossible solution to our puzzle.\nFor summation puzzles, U={0,1,2,3,4,5,6,7,8,9}and each position in the\nsequence corresponds to a given letter. For example, the \ufb01rst position could stand\nforb, the second for o, the third for y, and so on.", "176 Chapter 4. Recursion\nAlgorithm PuzzleSolve(k,S,U) :\nInput: An integer k, sequence S, and set U\nOutput: An enumeration of all k-length extensions to Susing elements in U\nwithout repetitions\nforeacheinUdo\nAddeto the end of S\nRemove efromU {eis now being used }\nifk==1then\nTest whether Sis a con\ufb01guration that solves the puzzle\nifSsolves the puzzle then\nreturn \u201cSolution found: \u201d S\nelse\nPuzzleSolve(k \u22121,S,U) {a recursive call }\nRemove efrom the end of S\nAddeback to U {eis now considered as unused }\nCode Fragment 4.14: Solving a combinatorial puzzle by enumerating and testing\nall possible con\ufb01gurations.\nIn Figure 4.14, we show a recursion trace of a call to PuzzleSolve (3,S,U),\nwhere Sis empty and U={a,b,c}. During the execution, all the permutations\nof the three characters are generated and tested. Note that the initial call makes\nthree recursive calls, each of which in turn makes two more. If we had executedPuzzleSolve (3,S,U)on a set Uconsisting of four elements, the initial call would\nhave made four recursive calls, each of which would have a trace looking like the\none in Figure 4.14.\ninitial call\nPuzzleSolve(3, (), {a,b,c})\nPuzzleSolve(2, b, {a,c}) PuzzleSolve(2, c, {a,b})\nPuzzleSolve(1, ca, {b})PuzzleSolve(2, a, {b,c})\nPuzzleSolve(1, ab, {c}) PuzzleSolve(1, ba, {c})\nPuzzleSolve(1, bc, {a}) PuzzleSolve(1, ac, {b}) PuzzleSolve(1, cb, {a})\nacbabc bac cab\nbca cba\nFigure 4.14: Recursion trace for an execution of PuzzleSolve (3,S,U),w h e r e Sis\nempty and U={a,b,c}. This execution generates and tests all permutations of a,b,\nandc. We show the permutations generated directly below their respective boxes.", "4.5. Designing Recursive Algorithms 177\n4.5 Designing Recursive Algorithms\nIn general, an algorithm that uses recursion typically has the following form:\n\u2022Test for base cases. We begin by testing for a set of base cases (there should\nbe at least one). These base cases should be de\ufb01ned so that every possible\nchain of recursive calls will eventually reach a base case, and the handling ofeach base case should not use recursion.\n\u2022Recur. If not a base case, we perform one or more recursive calls. This recur-\nsive step may involve a test that decides which of several possible recursivecalls to make. We should de\ufb01ne each possible recursive call so that it makes\nprogress towards a base case.\nParameterizing a Recursion\nTo design a recursive algorithm for a given problem, it is useful to think of the\ndifferent ways we might de\ufb01ne subproblems that have the same general structureas the original problem. If one has dif\ufb01culty \ufb01nding the repetitive structure neededto design a recursive algorithm, it is sometimes useful to work out the problem on\na few concrete examples to see how the subproblems should be de\ufb01ned.\nA successful recursive design sometimes requires that we rede\ufb01ne the original\nproblem to facilitate similar-looking subproblems. Often, this involved reparam-\neterizing the signature of the function. For example, when performing a binarysearch in a sequence, a natural function signature for a caller would appear asbinary\nsearch(data, target). However, in Section 4.1.3, we de\ufb01ned our function\nwith calling signature binary\n search(data, target, low, high) , using the additional\nparameters to demarcate sublists as the recursion proceeds. This change in param-eterization is critical for binary search. If we had insisted on the cleaner signature,binary\nsearch(data, target), the only way to invoke a search on half the list would\nhave been to make a new list instance with only those elements to send as the \ufb01rstparameter. However, making a copy of half the list would already take O(n)time,\nnegating the whole bene\ufb01t of the binary search algorithm.\nIf we wished to provide a cleaner public interface to an algorithm like bi-\nnary search, without bothering a user with the extra parameters, a standard tech-nique is to make one function for public use with the cleaner interface, such asbinary\nsearch(data, target), and then having its body invoke a nonpublic utility\nfunction having the desired recursive parameters.\nYou will see that we similarly reparameterized the recursion in several other ex-\namples of this chapter (e.g., reverse ,linear\n sum,binary\n sum). We saw a different\napproach to rede\ufb01ning a recursion in our good\n \ufb01bonacci implementation, by in-\ntentionally strengthening the expectation of what is returned (in that case, returning\na pair of numbers rather than a single number).", "178 Chapter 4. Recursion\n4.6 Eliminating Tail Recursion\nThe main bene\ufb01t of a recursive approach to algorithm design is that it allows us to\nsuccinctly take advantage of a repetitive structure present in many problems. Bymaking our algorithm description exploit the repetitive structure in a recursive way,we can often avoid complex case analyses and nested loops. This approach can\nlead to more readable algorithm descriptions, while still being quite ef\ufb01cient.\nHowever, the usefulness of recursion comes at a modest cost. In particular, the\nPython interpreter must maintain activation records that keep track of the state ofeach nested call. When computer memory is at a premium, it is useful in some\ncases to be able to derive nonrecursive algorithms from recursive ones.\nIn general, we can use the stack data structure, which we will introduce in\nSection 6.1, to convert a recursive algorithm into a nonrecursive algorithm by man-aging the nesting of the recursive structure ourselves, rather than relying on theinterpreter to do so. Although this only shifts the memory usage from the inter-preter to our stack, we may be able to reduce the memory usage by storing only theminimal information necessary.\nEven better, some forms of recursion can be eliminated without any use of\naxillary memory. A notable such form is known as tail recursion . A recursion\nis a tail recursion if any recursive call that is made from one context is the very\nlast operation in that context, with the return value of the recursive call (if any)\nimmediately returned by the enclosing recursion. By necessity, a tail recursionmust be a linear recursion (since there is no way to make a second recursive call ifyou must immediately return the result of the \ufb01rst).\nOf the recursive functions demonstrated in this chapter, the binary\nsearch func-\ntion of Code Fragment 4.3 and the reverse function of Code Fragment 4.10 are\nexamples of tail recursion. Several others of our linear recursions are almost liketail recursion, but not technically so. For example, our factorial function of Code\nFragment 4.1 is nota tail recursion. It concludes with the command:\nreturn n\nfactorial(n \u22121)\nThis is not a tail recursion because an additional multiplication is performed after\nthe recursive call is completed. For similar reasons, the linear\n sum function of\nCode Fragment 4.9 and the good\n \ufb01bonacci function of Code Fragment 4.7 fail to\nbe tail recursions.\nAny tail recursion can be reimplemented nonrecursively by enclosing the body\nin a loop for repetition, and replacing a recursive call with new parameters by a\nreassignment of the existing parameters to those values. As a tangible example, ourbinary\nsearch function can be reimplemented as shown in Code Fragment 4.15.\nWe initialize variables lowandhigh, just prior to our while loop, to represent the\nfull extent of the sequence. Then, during each pass of the loop, we either \ufb01nd", "4.6. Eliminating Tail Recursion 179\n1defbinary\n search\n iterative(data, target):\n2\u201d\u201d\u201dReturn True if target is found in the given Python list.\u201d\u201d\u201d\n3low = 0\n4high = len(data) \u22121\n5while low<=h i g h :\n6 mid = (low + high) // 2\n7 iftarget == data[mid]: # found a match\n8 return True\n9 eliftarget <data[mid]:\n10 high = mid \u22121 # only consider values left of mid\n11 else:\n12 low = mid + 1 # only consider values right of mid\n13return False # loop ended without success\nCode Fragment 4.15: A nonrecursive implementation of binary search.\nthe target, or we narrow the range of the candidate subsequence. Where we made\nthe recursive call binary\n search(data, target, low, mid \u22121)in the original version,\nwe simply replace high = mid \u22121in our new version and then continue to the\nnext iteration of the loop. Our original base case condition of low>high has\nsimply been replaced by the opposite loop condition while low<=h i g h . In our\nnew implementation, we return False to designate a failed search if the while loop\nends (that is, without having ever returned True from within).\nWe can similarly develop a nonrecursive implementation (Code Fragment 4.16)\nof the original recursive reverse method of Code Fragment 4.10.\n1defreverse\n iterative(S):\n2\u201d\u201d\u201dReverse elements in sequence S.\u201d\u201d\u201d\n3start, stop = 0, len(S)\n4while start<stop\u22121:\n5 S[start], S[stop \u22121] = S[stop \u22121], S[start] # swap \ufb01rst and last\n6 start, stop = start + 1, stop \u22121 # narrow the range\nCode Fragment 4.16: Reversing the elements of a sequence using iteration.\nIn this new version, we update the values start andstop during each pass of the\nloop, exiting once we reach the case of having one or less elements in that range.\nMany other linear recursions can be expressed quite ef\ufb01ciently with iteration,\neven if they were not formally tail recursions. For example, there are trivial non-recursive implementations for computing factorials, summing elements of a se-quence, or computing Fibonacci numbers ef\ufb01ciently. In fact, our implementationof a Fibonacci generator, from Section 1.8, produces each subsequent value in O(1)\ntime, and thus takes O(n)time to generate the n\nthentry in the series.", "180 Chapter 4. Recursion\n4.7 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-4.1 Describe a recursive algorithm for \ufb01nding the maximum element in a se-\nquence, S,o fnelements. What is your running time and space usage?\nR-4.2 Draw the recursion trace for the computation of power (2,5),u s i n gt h e\ntraditional function implemented in Code Fragment 4.11.\nR-4.3 Draw the recursion trace for the computation of power (2,18),u s i n gt h e\nrepeated squaring algorithm, as implemented in Code Fragment 4.12.\nR-4.4 Draw the recursion trace for the execution of function reverse(S, 0, 5)\n(Code Fragment 4.10) on S=[ 4 ,3 ,6 ,2 ,6 ] .\nR-4.5 Draw the recursion trace for the execution of function PuzzleSolve (3,S,U)\n(Code Fragment 4.14), where Sis empty and U={a,b,c,d}.\nR-4.6 Describe a recursive function for computing the nthHarmonic number ,\nHn=\u2211n\ni=11/i.\nR-4.7 Describe a recursive function for converting a string of digits into the in-\nteger it represents. For example,\n 13531\n represents the integer 13 ,531.\nR-4.8 Isabel has an interesting way of summing up the values in a sequence Aof\nnintegers, where nis a power of two. She creates a new sequence Bof half\nthe size of Aand sets B[i]=A[2i]+A[2i+1],f o r i=0,1,..., (n/2)\u22121. If\nBhas size 1, then she outputs B[0]. Otherwise, she replaces Awith B,a n d\nrepeats the process. What is the running time of her algorithm?\nCreativity\nC-4.9 Write a short recursive Python function that \ufb01nds the minimum and max-imum values in a sequence without using any loops.\nC-4.10 Describe a recursive algorithm to compute the integer part of the base-twologarithm of nusing only addition and integer division.\nC-4.11 Describe an ef\ufb01cient recursive function for solving the element unique-\nness problem, which runs in time that is at most O(n\n2)in the worst case\nwithout using sorting.\nC-4.12 Give a recursive algorithm to compute the product of two positive integers,mandn, using only addition and subtraction.", "4.7. Exercises 181\nC-4.13 In Section 4.2 we prove by induction that the number of lines printed by\na call to draw\ninterval(c) is 2c\u22121. Another interesting question is how\nmany dashes are printed during that process. Prove by induction that the\nnumber of dashes printed by draw\ninterval(c) is 2c+1\u2212c\u22122.\nC-4.14 In the Towers of Hanoi puzzle, we are given a platform with three pegs, a,\nb,a n d c, sticking out of it. On peg ais a stack of ndisks, each larger than\nthe next, so that the smallest is on the top and the largest is on the bottom.\nThe puzzle is to move all the disks from peg ato peg c, moving one disk\nat a time, so that we never place a larger disk on top of a smaller one.See Figure 4.15 for an example of the case n=4. Describe a recursive\nalgorithm for solving the Towers of Hanoi puzzle for arbitrary n.( H i n t :\nConsider \ufb01rst the subproblem of moving all but the n\nthdisk from peg ato\nanother peg using the third as \u201ctemporary storage.\u201d)\nFigure 4.15: An illustration of the Towers of Hanoi puzzle.\nC-4.15 Write a recursive function that will output all the subsets of a set of n\nelements (without repeating any subsets).\nC-4.16 Write a short recursive Python function that takes a character string sand\noutputs its reverse. For example, the reverse of\n pots&pans\n would be\nsnap&stop\n .\nC-4.17 Write a short recursive Python function that determines if a string sis a\npalindrome, that is, it is equal to its reverse. For example,\n racecar\n and\ngohangasalamiimalasagnahog\n are palindromes.\nC-4.18 Use recursion to write a Python function for determining if a string shas\nmore vowels than consonants.\nC-4.19 Write a short recursive Python function that rearranges a sequence of in-teger values so that all the even values appear before all the odd values.\nC-4.20 Given an unsorted sequence, S, of integers and an integer k, describe a\nrecursive algorithm for rearranging the elements in Sso that all elements\nless than or equal to kcome before any elements larger than k.W h a t i s\nthe running time of your algorithm on a sequence of nvalues?", "182 Chapter 4. Recursion\nC-4.21 Suppose you are given an n-element sequence, S, containing distinct in-\ntegers that are listed in increasing order. Given a number k, describe a\nrecursive algorithm to \ufb01nd two integers in Sthat sum to k, if such a pair\nexists. What is the running time of your algorithm?\nC-4.22 Develop a nonrecursive implementation of the version of power from\nCode Fragment 4.12 that uses repeated squaring.\nProjects\nP-4.23 Implement a recursive function with signature \ufb01nd(path, \ufb01lename) that\nreports all entries of the \ufb01le system rooted at the given path having the\ngiven \ufb01le name.\nP-4.24 Write a program for solving summation puzzles by enumerating and test-\ning all possible con\ufb01gurations. Using your program, solve the three puz-zles given in Section 4.4.3.\nP-4.25 Provide a nonrecursive implementation of the draw\ninterval function for\nthe English ruler project of Section 4.1.2. There should be precisely 2c\u22121\nlines of output if crepresents the length of the center tick. If incrementing\na counter from 0 to 2c\u22122, the number of dashes for each tick line should\nbe exactly one more than the number of consecutive 1\u2019s at the end of thebinary representation of the counter.\nP-4.26 Write a program that can solve instances of the Tower of Hanoi problem(from Exercise C-4.14).\nP-4.27 Python\u2019s osmodule provides a function with signature walk(path) that\nis a generator yielding the tuple (dirpath, dirnames, \ufb01lenames) for each\nsubdirectory of the directory identi\ufb01ed by string path , such that string\ndirpath is the full path to the subdirectory, dirnames is a list of the names\nof the subdirectories within dirpath ,a n d\ufb01lenames is a list of the names\nof non-directory entries of dirpath . For example, when visiting the cs016\nsubdirectory of the \ufb01le system shown in Figure 4.6, the walk would yield\n(\n/user/rt/courses/cs016\n ,[\nhomeworks\n ,\nprograms\n ], [\ngrades\n ]).\nGive your own implementation of such a walk function.\nChapter Notes\nThe use of recursion in programs belongs to the folkore of computer science (for example,\nsee the article of Dijkstra [36]). It is also at the heart of functional programming languages(for example, see the classic book by Abelson, Sussman, and Sussman [1]). Interestingly,\nbinary search was \ufb01rst published in 1946, but was not published in a fully correct form\nuntil 1962. For further discussions on lessons learned, please see papers by Bentley [14]\nand Lesuisse [68].", "Chapter\n5Array-Based Sequences\nContents\n5 . 1 P y t h o n \u2019 sS e q u e n c eT y p e s................... 1 8 4\n5 . 2 L o w - L e v e lA r r a y s........................ 1 8 5\n5.2.1 Referential Arrays . . . . . . . . . . . . . . . . . . . . . . 187\n5.2.2 Compact Arrays in Python . . . . . . . . . . . . . . . . . 190\n5 . 3 D y n a m i cA r r a y sa n dA m o r t i z a t i o n.............. 1 9 2\n5.3.1 Implementing a Dynamic Array . . . . . . . . . . . . . . . 195\n5.3.2 Amortized Analysis of Dynamic Arrays . . . . . . . . . . . 197\n5 . 3 . 3 P y t h o n \u2019 s L i s t C l a s s .....................2 0 1\n5 . 4 E \ufb03 c i e n c yo fP y t h o n \u2019 sS e q u e n c eT y p e s ........... 2 0 2\n5 . 4 . 1 P y t h o n \u2019 s L i s t a n dT u p l e C l a s s e s ..............2 0 25 . 4 . 2 P y t h o n \u2019 s S t r i n g C l a s s....................2 0 8\n5 . 5 U s i n gA r r a y - B a s e dS e q u e n c e s ................ 2 1 0\n5 . 5 . 1 S t o r i n g H i g h S c o r e s f o r a G a m e ..............2 1 05 . 5 . 2 S o r t i n g aS e q u e n c e .....................2 1 45 . 5 . 3 S i m p l e C r y p t o g r a p h y ....................2 1 6\n5 . 6 M u l t i d i m e n s i o n a lD a t aS e t s .................. 2 1 95 . 7 E x e r c i s e s ............................ 2 2 4\n", "184 Chapter 5. Array-Based Sequences\n5.1 Python\u2019s Sequence Types\nIn this chapter, we explore Python\u2019s various \u201csequence\u201d classes, namely the built-\ninlist,tuple ,a n dstrclasses. There is signi\ufb01cant commonality between these\nclasses, most notably: each supports indexing to access an individual element of a\nsequence, using a syntax such as seq[k] , and each uses a low-level concept known\nas an array to represent the sequence. However, there are signi\ufb01cant differences in\nthe abstractions that these classes represent, and in the way that instances of these\nclasses are represented internally by Python. Because these classes are used so\nwidely in Python programs, and because they will become building blocks upon\nwhich we will develop more complex data structures, it is imperative that we estab-lish a clear understanding of both the public behavior and inner workings of theseclasses.\nPublic Behaviors\nA proper understanding of the outward semantics for a class is a necessity for agood programmer. While the basic usage of lists, strings, and tuples may seemstraightforward, there are several important subtleties regarding the behaviors as-sociated with these classes (such as what it means to make a copy of a sequence, or\nto take a slice of a sequence). Having a misunderstanding of a behavior can easily\nlead to inadvertent bugs in a program. Therefore, we establish an accurate men-tal model for each of these classes. These images will help when exploring moreadvanced usage, such as representing a multidimensional data set as a list of lists.\nImplementation Details\nA focus on the internal implementations of these classes seems to go against ourstated principles of object-oriented programming. In Section 2.1.2, we emphasizedthe principle of encapsulation , noting that the user of a class need not know about\nthe internal details of the implementation. While it is true that one only needs tounderstand the syntax and semantics of a class\u2019s public interface in order to be ableto write legal and correct code that uses instances of the class, the ef\ufb01ciency of aprogram depends greatly on the ef\ufb01ciency of the components upon which it relies.\nAsymptotic and Experimental Analyses\nIn describing the ef\ufb01ciency of various operations for Python\u2019s sequence classes,we will rely on the formal asymptotic analysis notations established in Chapter 3.\nWe will also perform experimental analyses of the primary operations to provide\nempirical evidence that is consistent with the more theoretical asymptotic analyses.", "5.2. Low-Level Arrays 185\n5.2 Low-Level Arrays\nTo accurately describe the way in which Python represents the sequence types,\nwe must \ufb01rst discuss aspects of the low-level computer architecture. The primary\nmemory of a computer is composed of bits of information, and those bits are typ-\nically grouped into larger units that depend upon the precise system architecture.Such a typical unit is a byte, which is equivalent to 8 bits.\nA computer system will have a huge number of bytes of memory, and to keep\ntrack of what information is stored in what byte, the computer uses an abstractionknown as a memory address . In effect, each byte of memory is associated with a\nunique number that serves as its address (more formally, the binary representationof the number serves as the address). In this way, the computer system can refer\nto the data in \u201cbyte #2150\u201d versus the data in \u201cbyte #2157,\u201d for example. Memory\naddresses are typically coordinated with the physical layout of the memory system,and so we often portray the numbers in sequential fashion. Figure 5.1 providessuch a diagram, with the designated memory address for each byte.\n2160 21452146214721482149215021512152215321542155215621572158 2144 2159\nFigure 5.1: A representation of a portion of a computer\u2019s memory, with individual\nbytes labeled with consecutive memory addresses.\nDespite the sequential nature of the numbering system, computer hardware is\ndesigned, in theory, so that any byte of the main memory can be ef\ufb01ciently accessed\nbased upon its memory address. In this sense, we say that a computer\u2019s main mem-\nory performs as random access memory (RAM) . That is, it is just as easy to retrieve\nbyte #8675309 as it is to retrieve byte #309. (In practice, there are complicatingfactors including the use of caches and external memory; we address some of thoseissues in Chapter 15.) Using the notation for asymptotic analysis, we say that any\nindividual byte of memory can be stored or retrieved in O(1)time.\nIn general, a programming language keeps track of the association between\nan identi\ufb01er and the memory address in which the associated value is stored. Forexample, identi\ufb01er xmight be associated with one value stored in memory, while y\nis associated with another value stored in memory. A common programming task\nis to keep track of a sequence of related objects. For example, we may want a videogame to keep track of the top ten scores for that game. Rather than use ten differentvariables for this task, we would prefer to use a single name for the group and use\nindex numbers to refer to the high scores in that group.", "186 Chapter 5. Array-Based Sequences\nA group of related variables can be stored one after another in a contiguous\nportion of the computer\u2019s memory. We will denote such a representation as an\narray . As a tangible example, a text string is stored as an ordered sequence of\nindividual characters. In Python, each character is represented using the Unicodecharacter set, and on most computing systems, Python internally represents eachUnicode character with 16 bits (i.e., 2 bytes). Therefore, a six-character string, such\nas\nSAMPLE\n , would be stored in 12 consecutive bytes of memory, as diagrammed\nin Figure 5.2.\n0M PLE A S2160 215021512152215321542155215621572158 2144 2159\n5 4 3 2 121452146214721482149\nFigure 5.2: A Python string embedded as an array of characters in the computer\u2019s\nmemory. We assume that each Unicode character of the string requires two bytes\nof memory. The numbers below the entries are indices into the string.\nW ed e s c r i b et h i sa sa n array of six characters , even though it requires 12 bytes\nof memory. We will refer to each location within an array as a cell, and will use an\ninteger index to describe its location within the array, with cells numbered starting\nwith 0, 1, 2, and so on. For example, in Figure 5.2, the cell of the array with index 4has contents Land is stored in bytes 2154 and 2155 of memory.\nEach cell of an array must use the same number of bytes. This requirement is\nwhat allows an arbitrary cell of the array to be accessed in constant time based onits index. In particular, if one knows the memory address at which an array starts\n(e.g., 2146 in Figure 5.2), the number of bytes per element (e.g., 2 for a Unicode\ncharacter), and a desired index within the array, the appropriate memory addresscan be computed using the calculation, start + cellsize\nindex . By this formula,\nthe cell at index 0 begins precisely at the start of the array, the cell at index 1 begins\nprecisely cellsize bytes beyond the start of the array, and so on. As an example,\ncell 4 of Figure 5.2 begins at memory location 2146 +2\u00b74=2146 +8=2154.\nOf course, the arithmetic for calculating memory addresses within an array can\nbe handled automatically. Therefore, a programmer can envision a more typical\nhigh-level abstraction of an array of characters as diagrammed in Figure 5.3.\n0ASM P L E\n345 12\nFigure 5.3: A higher-level abstraction for the string portrayed in Figure 5.2.", "5.2. Low-Level Arrays 187\n5.2.1 Referential Arrays\nAs another motivating example, assume that we want a medical information system\nto keep track of the patients currently assigned to beds in a certain hospital. If we\nassume that the hospital has 200 beds, and conveniently that those beds are num-\nbered from 0 to 199, we might consider using an array-based structure to maintainthe names of the patients currently assigned to those beds. For example, in Pythonwe might use a list of names, such as:\n[\nRene\n ,\nJoseph\n ,\nJanet\n ,\nJonas\n ,\nHelen\n ,\nVirginia\n , ... ]\nTo represent such a list with an array, Python must adhere to the requirement thateach cell of the array use the same number of bytes. Yet the elements are strings,and strings naturally have different lengths. Python could attempt to reserve enoughspace for each cell to hold the maximum length string (not just of currently stored\nstrings, but of any string we might ever want to store), but that would be wasteful.\nInstead, Python represents a list or tuple instance using an internal storage\nmechanism of an array of object references . At the lowest level, what is stored\nis a consecutive sequence of memory addresses at which the elements of the se-quence reside. A high-level diagram of such a list is shown in Figure 5.4.\n0 3 12 5 4\nRene\n Virginia\nJoseph\n Helen\nJonas\n Janet\nFigure 5.4: An array storing references to strings.\nAlthough the relative size of the individual elements may vary, the number of\nbits used to store the memory address of each element is \ufb01xed (e.g., 64-bits peraddress). In this way, Python can support constant-time access to a list or tupleelement based on its index.\nIn Figure 5.4, we characterize a list of strings that are the names of the patients\nin a hospital. It is more likely that a medical information system would managemore comprehensive information on each patient, perhaps represented as an in-stance of a Patient class. From the perspective of the list implementation, the same\nprinciple applies: The list will simply keep a sequence of references to those ob-jects. Note as well that a reference to the None object can be used as an element\nof the list to represent an empty bed in the hospital.", "188 Chapter 5. Array-Based Sequences\nThe fact that lists and tuples are referential structures is signi\ufb01cant to the se-\nmantics of these classes. A single list instance may include multiple references\nto the same object as elements of the list, and it is possible for a single object tobe an element of two or more lists, as those lists simply store references back tothat object. As an example, when you compute a slice of a list, the result is a newlist instance, but that new list has references to the same elements that are in the\noriginal list, as portrayed in Figure 5.5.\n3 4 5 6 7 012012\nprimes :temp :\n31 1 5 21 9 17 13 7\nFigure 5.5: The result of the command temp = primes[3:6] .\nWhen the elements of the list are immutable objects, as with the integer in-\nstances in Figure 5.5, the fact that the two lists share elements is not that signi\ufb01-\ncant, as neither of the lists can cause a change to the shared object. If, for example,the command temp[2] = 15 were executed from this con\ufb01guration, that does not\nchange the existing integer object; it changes the reference in cell 2 of the temp list\nto reference a different object. The resulting con\ufb01guration is shown in Figure 5.6.\n3 4 5 6 7 012012\nprimes :temp :\n13 11 5 215\n31 9 17 7\nFigure 5.6: The result of the command temp[2] = 15 upon the con\ufb01guration por-\ntrayed in Figure 5.5.\nThe same semantics is demonstrated when making a new list as a copy of an\nexisting one, with a syntax such as backup = list(primes) . This produces a new\nl i s tt h a ti sa shallow copy (see Section 2.6), in that it references the same elements\nas in the \ufb01rst list. With immutable elements, this point is moot. If the contents ofthe list were of a mutable type, a deep copy , meaning a new list with newelements,\ncan be produced by using the deepcopy function from the copy module.", "5.2. Low-Level Arrays 189\nAs a more striking example, it is a common practice in Python to initialize an\narray of integers using a syntax such as counters = [0]\n 8. This syntax produces\na list of length eight, with all eight elements being the value zero. Technically, all\neight cells of the list reference the same object, as portrayed in Figure 5.7.\n4567 012 3counters :0\nFigure 5.7: The result of the command data = [0]\n 8.\nAt \ufb01rst glance, the extreme level of aliasing in this con\ufb01guration may seem\nalarming. However, we rely on the fact that the referenced integer is immutable.Even a command such as counters[2] += 1 does not technically change the value\nof the existing integer instance. This computes a new integer, with value 0 +1, and\nsets cell 2 to reference the newly computed value. The resulting con\ufb01guration isshown in Figure 5.8.\n4567 012 3counters :01\nFigure 5.8: The result of command data[2] += 1 upon the list from Figure 5.7.\nAs a \ufb01nal manifestation of the referential nature of lists, we note that the extend\ncommand is used to add all elements from one list to the end of another list. Theextended list does not receive copies of those elements, it receives references to\nthose elements. Figure 5.9 portrays the effect of a call to extend .\n3 4 5 6 7 8 10 9 2 1 0012\nprimes :extras :\n29 31 71 9 31 7 13 11 25 23\nFigure 5.9: The effect of command primes.extend(extras) , shown in light gray.", "190 Chapter 5. Array-Based Sequences\n5.2.2 Compact Arrays in Python\nIn the introduction to this section, we emphasized that strings are represented using\nan array of characters (not an array of references). We will refer to this more directrepresentation as a compact array because the array is storing the bits that represent\nthe primary data (characters, in the case of strings).\n0ASM P L E\n345 12\nCompact arrays have several advantages over referential structures in terms\nof computing performance. Most signi\ufb01cantly, the overall memory usage will bemuch lower for a compact structure because there is no overhead devoted to the\nexplicit storage of the sequence of memory references (in addition to the primary\ndata). That is, a referential structure will typically use 64-bits for the memoryaddress stored in the array, on top of whatever number of bits are used to representthe object that is considered the element. Also, each Unicode character stored ina compact array within a string typically requires 2 bytes. If each character were\nstored independently as a one-character string, there would be signi\ufb01cantly more\nbytes used.\nAs another case study, suppose we wish to store a sequence of one million,\n64-bit integers. In theory, we might hope to use only 64 million bits. However, we\nestimate that a Python list will use four to \ufb01ve times as much memory . Each element\nof the list will result in a 64-bit memory address being stored in the primary array,\nand an intinstance being stored elsewhere in memory. Python allows you to query\nthe actual number of bytes being used for the primary storage of any object. Thisis done using the getsizeof function of the sysmodule. On our system, the size of\na typical intobject requires 14 bytes of memory (well beyond the 4 bytes needed\nfor representing the actual 64-bit number). In all, the list will be using 18 bytes perentry, rather than the 4 bytes that a compact list of integers would require.\nAnother important advantage to a compact structure for high-performance com-\nputing is that the primary data are stored consecutively in memory. Note well that\nthis is not the case for a referential structure. That is, even though a list maintains\ncareful ordering of the sequence of memory addresses, where those elements residein memory is not determined by the list. Because of the workings of the cache andmemory hierarchies of computers, it is often advantageous to have data stored in\nmemory near other data that might be used in the same computations.\nDespite the apparent inef\ufb01ciencies of referential structures, we will generally\nbe content with the convenience of Python\u2019s lists and tuples in this book. The only\nplace in which we consider alternatives will be in Chapter 15, which focuses onthe impact of memory usage on data structures and algorithms. Python provides\nseveral means for creating compact arrays of various types.", "5.2. Low-Level Arrays 191\nPrimary support for compact arrays is in a module named array . That module\nde\ufb01nes a class, also named array , providing compact storage for arrays of primitive\ndata types. A portrayal of such an array of integers is shown in Figure 5.10.\n3 4 5 6 7 01217 5 23 7 1 1 1 3 1 9\nFigure 5.10: Integers stored compactly as elements of a Python array .\nThe public interface for the array class conforms mostly to that of a Python list.\nHowever, the constructor for the array class requires a type code as a \ufb01rst parameter,\nwhich is a character that designates the type of data that will be stored in the array.\nAs a tangible example, the type code,\n i\n, designates an array of (signed) integers,\ntypically represented using at least 16-bits each. We can declare the array shown inFigure 5.10 as,\nprimes = array(\ni\n, [2, 3, 5, 7, 11, 13, 17, 19])\nThe type code allows the interpreter to determine precisely how many bits are\nneeded per element of the array. The type codes supported by the array module,\nas shown in Table 5.1, are formally based upon the native data types used by theC programming language (the language in which the the most widely used distri-\nbution of Python is implemented). The precise number of bits for the C data types\nis system-dependent, but typical ranges are shown in the table.\nCode\n C Data Type\n Typical Number of Bytes\nb\n signed char\n 1\nB\n unsigned char\n 1\nu\n Unicode char\n 2o r4\nh\n signed short int\n 2\nH\n unsigned short int\n 2\ni\n signed int\n 2o r4\nI\n unsigned int\n 2o r4\nl\n signed long int\n 4\nL\n unsigned long int\n 4\nf\n \ufb02oat\n 4\nd\n \ufb02oat\n 8\nTable 5.1: Type codes supported by the array module.\nThearray module does not provide support for making compact arrays of user-\nde\ufb01ned data types. Compact arrays of such structures can be created with the lower-\nlevel support of a module named ctypes . (See Section 5.3.1 for more discussion of\nthectypes module.)", "192 Chapter 5. Array-Based Sequences\n5.3 Dynamic Arrays and Amortization\nWhen creating a low-level array in a computer system, the precise size of that array\nmust be explicitly declared in order for the system to properly allocate a consecutivepiece of memory for its storage. For example, Figure 5.11 displays an array of 12bytes that might be stored in memory locations 2146 through 2157.\n2160 21452146214721482149215021512152215321542155215621572158 2144 2159\nFigure 5.11: An array of 12 bytes allocated in memory locations 2146 through 2157.\nBecause the system might dedicate neighboring memory locations to store otherdata, the capacity of an array cannot trivially be increased by expanding into sub-sequent cells. In the context of representing a Python tuple orstrinstance, this\nconstraint is no problem. Instances of those classes are immutable, so the correct\nsize for an underlying array can be \ufb01xed when the object is instantiated.\nPython\u2019s listclass presents a more interesting abstraction. Although a list has a\nparticular length when constructed, the class allows us to add elements to the list,\nwith no apparent limit on the overall capacity of the list. To provide this abstraction,\nPython relies on an algorithmic sleight of hand known as a dynamic array .\nThe \ufb01rst key to providing the semantics of a dynamic array is that a list instance\nmaintains an underlying array that often has greater capacity than the current lengthof the list. For example, while a user may have created a list with \ufb01ve elements,\nthe system may have reserved an underlying array capable of storing eight objectreferences (rather than only \ufb01ve). This extra capacity makes it easy to append anew element to the list by using the next available cell of the array.\nIf a user continues to append elements to a list, any reserved capacity will\neventually be exhausted. In that case, the class requests a new, larger array from thesystem, and initializes the new array so that its pre\ufb01x matches that of the existingsmaller array. At that point in time, the old array is no longer needed, so it is\nreclaimed by the system. Intuitively, this strategy is much like that of the hermit\ncrab, which moves into a larger shell when it outgrows its previous one.\nWe give empirical evidence that Python\u2019s listclass is based upon such a strat-\negy. The source code for our experiment is displayed in Code Fragment 5.1, and asample output of that program is given in Code Fragment 5.2. We rely on a func-tion named getsizeof that is available from the sysmodule. This function reports\nthe number of bytes that are being used to store an object in Python. For a list, itreports the number of bytes devoted to the array and other instance variables of the\nlist, but notany space devoted to elements referenced by the list.", "5.3. Dynamic Arrays and Amortization 193\n1import sys # provides getsizeof function\n2data = [ ]\n3forkinrange(n): # NOTE: must \ufb01x choice of n\n4a = len(data) # number of elements\n5b = sys.getsizeof(data) # actual size in bytes\n6print(\n Length: {0:3d}; Size in bytes: {1:4d}\n .format(a, b))\n7data.append( None) # increase length by one\nCode Fragment 5.1: An experiment to explore the relationship between a list\u2019s\nlength and its underlying size in Python.\nLength: 0; Size in bytes: 72\nLength: 1; Size in bytes: 104Length: 2; Size in bytes: 104Length: 3; Size in bytes: 104\nLength: 4; Size in bytes: 104\nLength: 5; Size in bytes: 136Length: 6; Size in bytes: 136Length: 7; Size in bytes: 136Length: 8; Size in bytes: 136\nLength: 9; Size in bytes: 200\nLength: 10; Size in bytes: 200Length: 11; Size in bytes: 200Length: 12; Size in bytes: 200\nLength: 13; Size in bytes: 200\nLength: 14; Size in bytes: 200Length: 15; Size in bytes: 200Length: 16; Size in bytes: 200Length: 17; Size in bytes: 272\nLength: 18; Size in bytes: 272\nLength: 19; Size in bytes: 272Length: 20; Size in bytes: 272Length: 21; Size in bytes: 272\nLength: 22; Size in bytes: 272\nLength: 23; Size in bytes: 272Length: 24; Size in bytes: 272Length: 25; Size in bytes: 272Length: 26; Size in bytes: 352\nCode Fragment 5.2: Sample output from the experiment of Code Fragment 5.1.", "194 Chapter 5. Array-Based Sequences\nIn evaluating the results of the experiment, we draw attention to the \ufb01rst line of\noutput from Code Fragment 5.2. We see that an empty list instance already requires\na certain number of bytes of memory (72 on our system). In fact, each object inPython maintains some state, for example, a reference to denote the class to whichit belongs. Although we cannot directly access private instance variables for a list,we can speculate that in some form it maintains state information akin to:\nnThe number of actual elements currently stored in the list.\ncapacity The maximum number of elements that could be stored in thecurrently allocated array.\nAThe reference to the currently allocated array (initially None ).\nAs soon as the \ufb01rst element is inserted into the list, we detect a change in the\nunderlying size of the structure. In particular, we see the number of bytes jumpfrom 72 to 104, an increase of exactly 32 bytes. Our experiment was run on a64-bit machine architecture, meaning that each memory address is a 64-bit number(i.e., 8 bytes). We speculate that the increase of 32 bytes re\ufb02ects the allocation of\nan underlying array capable of storing four object references. This hypothesis is\nconsistent with the fact that we do not see any underlying change in the memoryusage after inserting the second, third, or fourth element into the list.\nAfter the \ufb01fth element has been added to the list, we see the memory usage jump\nfrom 104 bytes to 136 bytes. If we assume the original base usage of 72 bytes forthe list, the total of 136 suggests an additional 64 =8\u00d78 bytes that provide capacity\nfor up to eight object references. Again, this is consistent with the experiment, asthe memory usage does not increase again until the ninth insertion. At that point,the 200 bytes can be viewed as the original 72 plus an additional 128-byte array tostore 16 object references. The 17\nthinsertion pushes the overall memory usage to\n272 =72+200 =72+25\u00d78, hence enough to store up to 25 element references.\nBecause a list is a referential structure, the result of getsizeof for a list instance\nonly includes the size for representing its primary structure; it does not account formemory used by the objects that are elements of the list. In our experiment, we\nrepeatedly append None to the list, because we do not care about the contents, but\nwe could append any type of object without affecting the number of bytes reported\nbygetsizeof(data) .\nIf we were to continue such an experiment for further iterations, we might try\nto discern the pattern for how large of an array Python creates each time the ca-\npacity of the previous array is exhausted (see Exercises R-5.2 and C-5.13). Beforeexploring the precise sequence of capacities used by Python, we continue in thissection by describing a general approach for implementing dynamic arrays and for\nperforming an asymptotic analysis of their performance.", "5.3. Dynamic Arrays and Amortization 195\n5.3.1 Implementing a Dynamic Array\nAlthough the Python listclass provides a highly optimized implementation of dy-\nnamic arrays, upon which we rely for the remainder of this book, it is instructive to\nsee how such a class might be implemented.\nThe key is to provide means to grow the array Athat stores the elements of a\nlist. Of course, we cannot actually grow that array, as its capacity is \ufb01xed. If anelement is appended to a list at a time when the underlying array is full, we perform\nthe following steps:\n1. Allocate a new array Bwith larger capacity.\n2. Set B[i]=A[i],f o r i=0,..., n\u22121, where ndenotes current number of items.\n3. Set A=B, that is, we henceforth use Bas the array supporting the list.\n4. Insert the new element in the new array.\nAn illustration of this process is shown in Figure 5.12.\nBA A\nB A\n(a) (b) (c)\nFigure 5.12: An illustration of the three steps for \u201cgrowing\u201d a dynamic array: (a)\ncreate new array B; (b) store elements of AinB; (c) reassign reference Ato the new\narray. Not shown is the future garbage collection of the old array, or the insertion\nof the new element.\nThe remaining issue to consider is how large of a new array to create. A com-\nmonly used rule is for the new array to have twice the capacity of the existing array\nthat has been \ufb01lled. In Section 5.3.2, we will provide a mathematical analysis to\njustify such a choice.\nIn Code Fragment 5.3, we offer a concrete implementation of dynamic arrays\nin Python. Our DynamicArray class is designed using ideas described in this sec-\ntion. While consistent with the interface of a Python listclass, we provide only\nlimited functionality in the form of an append method, and accessors\n len\n and\ngetitem\n . Support for creating low-level arrays is provided by a module named\nctypes . Because we will not typically use such a low-level structure in the remain-\nder of this book, we omit a detailed explanation of the ctypes module. Instead,\nwe wrap the necessary command for declaring the raw array within a private util-ity method\nmake\n array . The hallmark expansion procedure is performed in our\nnonpublic\n resize method.", "196 Chapter 5. Array-Based Sequences\n1import ctypes # provides low-level arrays\n2\n3classDynamicArray:\n4\u201d\u201d\u201dA dynamic array class akin to a simpli\ufb01ed Python list.\u201d\u201d\u201d\n5\n6def\n init\n(self):\n7 \u201d\u201d\u201dCreate an empty array.\u201d\u201d\u201d\n8 self.\nn=0 # count actual elements\n9 self.\ncapacity = 1 # default array capacity\n10 self.\nA=self.\nmake\n array(self .\ncapacity) # low-level array\n11\n12def\n len\n(self):\n13 \u201d\u201d\u201dReturn number of elements stored in the array.\u201d\u201d\u201d\n14 return self .\nn\n15\n16def\n getitem\n (self,k ) :\n17 \u201d\u201d\u201dReturn element at index k.\u201d\u201d\u201d\n18 if not 0<=k<self.\nn:\n19 raiseIndexError(\n invalid index\n )\n20 return self .\nA[k] # retrieve from array\n2122defappend( self,o b j ) :\n23 \u201d\u201d\u201dAdd object to end of the array.\u201d\u201d\u201d\n24 if self.\nn= =self.\ncapacity: # not enough room\n25 self.\nresize(2\n self.\ncapacity) # so double capacity\n26 self.\nA[self.\nn] = obj\n27 self.\nn+ =1\n2829def\nresize(self,c ) : # nonpublic utitity\n30 \u201d\u201d\u201dResize internal array to capacity c.\u201d\u201d\u201d\n31 B=self.\nmake\n array(c) # new (bigger) array\n32 forkinrange(self.\nn): # for each existing value\n33 B[k] = self.\nA[k]\n34 self.\nA=B # use the bigger array\n35 self.\ncapacity = c\n3637def\nmake\n array(self ,c ) : # nonpublic utitity\n38 \u201d\u201d\u201dReturn new array with capacity c.\u201d\u201d\u201d\n39 return (c\nctypes.py\n object)( ) # see ctypes documentation\nCode Fragment 5.3: An implementation of a DynamicArray class, using a raw array\nfrom the ctypes module as storage.", "5.3. Dynamic Arrays and Amortization 197\n5.3.2 Amortized Analysis of Dynamic Arrays\nIn this section, we perform a detailed analysis of the running time of operations on\ndynamic arrays. We use the big-Omega notation introduced in Section 3.3.1 to givean asymptotic lower bound on the running time of an algorithm or step within it.\nThe strategy of replacing an array with a new, larger array might at \ufb01rst seem\nslow, because a single append operation may require \u03a9(n)time to perform, where\nnis the current number of elements in the array. However, notice that by doubling\nthe capacity during an array replacement, our new array allows us to add nnew\nelements before the array must be replaced again. In this way, there are manysimple append operations for each expensive one (see Figure 5.13). This fact allowsus to show that performing a series of operations on an initially empty dynamic\narray is ef\ufb01cient in terms of its total running time.\nUsing an algorithmic design pattern called amortization , we can show that per-\nforming a sequence of such append operations on a dynamic array is actually quite\nef\ufb01cient. To perform an amortized analysis , we use an accounting technique where\nwe view the computer as a coin-operated appliance that requires the payment of\nonecyber-dollar for a constant amount of computing time. When an operation\nis executed, we should have enough cyber-dollars available in our current \u201cbank\naccount\u201d to pay for that operation\u2019s running time. Thus, the total amount of cyber-dollars spent for any computation will be proportional to the total time spent on thatcomputation. The beauty of using this analysis method is that we can overcharge\nsome operations in order to save up cyber-dollars to pay for others.\nprimitive operations for an append\ncurrent number of elements13 10 12 5678 1 1 1 4 1 5 1 6 1234 9\nFigure 5.13: Running times of a series of append operations on a dynamic array.", "198 Chapter 5. Array-Based Sequences\nProposition 5.1: LetSbe a sequence implemented by means of a dynamic array\nwith initial capacity one, using the strategy of doubling the array size when full.\nThe total time to perform a series of nappend operations in S, starting from Sbeing\nempty, is O(n).\nJusti\ufb01cation: Let us assume that one cyber-dollar is enough to pay for the execu-\ntion of each append operation in S, excluding the time spent for growing the array.\nAlso, let us assume that growing the array from size kto size 2 krequires kcyber-\ndollars for the time spent initializing the new array. We shall charge each append\noperation three cyber-dollars. Thus, we overcharge each append operation that doesnot cause an over\ufb02ow by two cyber-dollars. Think of the two cyber-dollars pro\ufb01ted\nin an insertion that does not grow the array as being \u201cstored\u201d with the cell in which\nthe element was inserted. An over\ufb02ow occurs when the array Shas 2\nielements, for\nsome integer i\u22650, and the size of the array used by the array representing Sis 2i.\nThus, doubling the size of the array will require 2icyber-dollars. Fortunately, these\ncyber-dollars can be found stored in cells 2i\u22121through 2i\u22121. (See Figure 5.14.)\nNote that the previous over\ufb02ow occurred when the number of elements becamelarger than 2\ni\u22121for the \ufb01rst time, and thus the cyber-dollars stored in cells 2i\u22121\nthrough 2i\u22121 have not yet been spent. Therefore, we have a valid amortization\nscheme in which each operation is charged three cyber-dollars and all the comput-\ning time is paid for. That is, we can pay for the execution of nappend operations\nusing 3 ncyber-dollars. In other words, the amortized running time of each append\noperation is O(1); hence, the total running time of nappend operations is O(n).\n(a)\n0 2 45673 1$$$$$$$$\n(b)\n0 2 456789 1 13 1 0 1 21 31 41 5 1$$\nFigure 5.14: Illustration of a series of append operations on a dynamic array: (a) an\n8-cell array is full, with two cyber-dollars \u201cstored\u201d at cells 4 through 7; (b) an\nappend operation causes an over\ufb02ow and a doubling of capacity. Copying the eightold elements to the new array is paid for by the cyber-dollars already stored in thetable. Inserting the new element is paid for by one of the cyber-dollars charged to\nthe current append operation, and the two cyber-dollars pro\ufb01ted are stored at cell 8.", "5.3. Dynamic Arrays and Amortization 199\nGeometric Increase in Capacity\nAlthough the proof of Proposition 5.1 relies on the array being doubled each time\nwe expand, the O(1)amortized bound per operation can be proven for any geo-\nmetrically increasing progression of array sizes (see Section 2.4.2 for discussion ofgeometric progressions). When choosing the geometric base, there exists a trade-off between run-time ef\ufb01ciency and memory usage. With a base of 2 (i.e., doublingthe array), if the last insertion causes a resize event, the array essentially ends uptwice as large as it needs to be. If we instead increase the array by only 25% of\nits current size (i.e., a geometric base of 1.25), we do not risk wasting as much\nmemory in the end, but there will be more intermediate resize events along theway. Still it is possible to prove an O(1)amortized bound, using a constant factor\ngreater than the 3 cyber-dollars per operation used in the proof of Proposition 5.1\n(see Exercise C-5.15). The key to the performance is that the amount of additional\nspace is proportional to the current size of the array.\nBeware of Arithmetic Progression\nTo avoid reserving too much space at once, it might be tempting to implement adynamic array with a strategy in which a constant number of additional cells arereserved each time an array is resized. Unfortunately, the overall performance of\nsuch a strategy is signi\ufb01cantly worse. At an extreme, an increase of only one cell\ncauses each append operation to resize the array, leading to a familiar 1 +2+3+\n\u00b7\u00b7\u00b7+nsummation and \u03a9(n\n2)overall cost. Using increases of 2 or 3 at a time is\nslightly better, as portrayed in Figure 5.13, but the overall cost remains quadratic.\nprimitive operations for an append\ncurrent number of elements13 10 12 5678 1 1 1 4 1 5 1 6 1234 9\nprimitive operations for an append\ncurrent number of elements13 10 12 5 6 7 8 11 14 15 16 1234 9\n(a) (b)\nFigure 5.15: Running times of a series of append operations on a dynamic array\nusing arithmetic progression of sizes. (a) Assumes increase of 2 in size of thearray, while (b) assumes increase of 3.", "200 Chapter 5. Array-Based Sequences\nUsing a \ufb01xed increment for each resize, and thus an arithmetic progression of\nintermediate array sizes, results in an overall time that is quadratic in the number\nof operations, as shown in the following proposition. Intuitively, even an increasein 1000 cells per resize will become insigni\ufb01cant for large data sets.\nProposition 5.2:\nPerforming a series of nappend operations on an initially empty\ndynamic array using a \ufb01xed increment with each resize takes \u03a9(n2)time.\nJusti\ufb01cation: Letc>0 represent the \ufb01xed increment in capacity that is used for\neach resize event. During the series of nappend operations, time will have been\nspent initializing arrays of size c,2c,3c,..., mcform=\u2308n/c\u2309, and therefore, the\noverall time would be proportional to c+2c+3c+\u00b7\u00b7\u00b7+mc. By Proposition 3.3,\nthis sum is\nm\n\u2211\ni=1ci=c\u00b7m\n\u2211\ni=1i=cm(m+1)\n2\u2265cn\nc(n\nc+1)\n2\u2265n2\n2c.\nTherefore, performing the nappend operations takes \u03a9(n2)time.\nA lesson to be learned from Propositions 5.1 and 5.2 is that a subtle difference in\nan algorithm design can produce drastic differences in the asymptotic performance,\nand that a careful analysis can provide important insights into the design of a data\nstructure.\nMemory Usage and Shrinking an Array\nAnother consequence of the rule of a geometric increase in capacity when append-\ning to a dynamic array is that the \ufb01nal array size is guaranteed to be proportional tothe overall number of elements. That is, the data structure uses O(n)memory. This\nis a very desirable property for a data structure.\nIf a container, such as a Python list, provides operations that cause the removal\nof one or more elements, greater care must be taken to ensure that a dynamic arrayguarantees O(n)memory usage. The risk is that repeated insertions may cause the\nunderlying array to grow arbitrarily large, and that there will no longer be a propor-tional relationship between the actual number of elements and the array capacity\nafter many elements are removed.\nA robust implementation of such a data structure will shrink the underlying\narray, on occasion, while maintaining the O(1)amortized bound on individual op-\nerations. However, care must be taken to ensure that the structure cannot rapidlyoscillate between growing and shrinking the underlying array, in which case the\namortized bound would not be achieved. In Exercise C-5.16, we explore a strategyin which the array capacity is halved whenever the number of actual element fallsbelow one fourth of that capacity, thereby guaranteeing that the array capacity is atmost four times the number of elements; we explore the amortized analysis of such\na strategy in Exercises C-5.17 and C-5.18.", "5.3. Dynamic Arrays and Amortization 201\n5.3.3 Python\u2019s List Class\nThe experiments of Code Fragment 5.1 and 5.2, at the beginning of Section 5.3,\nprovide empirical evidence that Python\u2019s listclass is using a form of dynamic arrays\nfor its storage. Yet, a careful examination of the intermediate array capacities (see\nExercises R-5.2 and C-5.13) suggests that Python is not using a pure geometric\nprogression, nor is it using an arithmetic progression.\nWith that said, it is clear that Python\u2019s implementation of the append method\nexhibits amortized constant-time behavior. We can demonstrate this fact experi-\nmentally. A single append operation typically executes so quickly that it would be\ndif\ufb01cult for us to accurately measure the time elapsed at that granularity, althoughwe should notice some of the more expensive operations in which a resize is per-formed. We can get a more accurate measure of the amortized cost per operationby performing a series of nappend operations on an initially empty list and deter-\nmining the average cost of each. A function to perform that experiment is given in\nCode Fragment 5.4.\n1fromtimeimport time # import time function from time module\n2defcompute\naverage(n):\n3\u201d\u201d\u201dPerform n appends to an empty list and return average time elapsed.\u201d\u201d\u201d\n4data = [ ]\n5start = time( ) # record the start time (in seconds)\n6forkinrange(n):\n7 data.append( None)\n8end = time( ) # record the end time (in seconds)\n9return (end\u2212start) / n # compute average per operation\nCode Fragment 5.4: Measuring the amortized cost of append for Python\u2019s listclass.\nTechnically, the time elapsed between the start and end includes the time to\nmanage the iteration of the for loop, in addition to the append calls. The empirical\nresults of the experiment, for increasingly large values of n,a r es h o w ni nT a b l e5 . 2 .\nWe see higher average cost for the smaller data sets, perhaps in part due to the over-head of the loop range. There is also natural variance in measuring the amortizedcost in this way, because of the impact of the \ufb01nal resize event relative to n. Taken\nas a whole, there seems clear evidence that the amortized time for each append is\nindependent of n.\nn\n 100\n 1,000\n 10,000\n 100,000\n 1,000,000\n 10,000,000\n 100,000,000\n\u03bcs\n0.219\n 0.158\n 0.164\n 0.151\n 0.147\n 0.147\n 0.149\nTable 5.2: Average running time of append , measured in microseconds, as observed\nover a sequence of ncalls, starting with an empty list.", "202 Chapter 5. Array-Based Sequences\n5.4 E\ufb03ciency of Python\u2019s Sequence Types\nIn the previous section, we began to explore the underpinnings of Python\u2019s list\nclass, in terms of implementation strategies and ef\ufb01ciency. We continue in this\nsection by examining the performance of all of Python\u2019s sequence types.\n5.4.1 Python\u2019s List and Tuple Classes\nThenonmutating behaviors of the listclass are precisely those that are supported\nby the tuple class. We note that tuples are typically more memory ef\ufb01cient than\nlists because they are immutable; therefore, there is no need for an underlying\ndynamic array with surplus capacity. We summarize the asymptotic ef\ufb01ciency ofthe nonmutating behaviors of the listandtuple classes in Table 5.3. An explanation\nof this analysis follows.\nOperation\n Running Time\nlen(data)\n O(1)\ndata[j]\n O(1)\ndata.count(value)\n O(n)\ndata.index(value)\n O(k+1)\nvalueindata\n O(k+1)\ndata1 == data2\nO(k+1)\n(similarly !=,<,<=,>,>=)\ndata[j:k]\n O(k\u2212j+1)\ndata1 + data2\n O(n1+n2)\nc\ndata\n O(cn)\nTable 5.3: Asymptotic performance of the nonmutating behaviors of the listand\ntuple classes. Identi\ufb01ers data,data1 ,a n ddata2 designate instances of the listor\ntuple class, and n,n1,a n d n2their respective lengths. For the containment check\nandindex method, krepresents the index of the leftmost occurrence (with k=nif\nthere is no occurrence). For comparisons between two sequences, we let kdenote\nthe leftmost index at which they disagree or else k=min (n1,n2).\nConstant-Time Operations\nThe length of an instance is returned in constant time because an instance explicitlymaintains such state information. The constant-time ef\ufb01ciency of syntax data[j] is\nassured by the underlying access into an array.", "5.4. Ef\ufb01ciency of Python\u2019s Sequence Types 203\nSearching for Occurrences of a Value\nEach of the count ,index ,a n d\n contains\n methods proceed through iteration\nof the sequence from left to right. In fact, Code Fragment 2.14 of Section 2.4.3\ndemonstrates how those behaviors might be implemented. Notably, the loop for\ncomputing the count must proceed through the entire sequence, while the loopsfor checking containment of an element or determining the index of an elementimmediately exit once they \ufb01nd the leftmost occurrence of the desired value, ifone exists. So while count always examines the nelements of the sequence,\nindex and\ncontains\n examine nelements in the worst case, but may be faster.\nEmpirical evidence can be found by setting data = list(range(10000000)) and\nthen comparing the relative ef\ufb01ciency of the test, 5indata, relative to the test,\n9999995 indata,o re v e nt h ef a i l e dt e s t , \u22125indata.\nLexicographic Comparisons\nComparisons between two sequences are de\ufb01ned lexicographically. In the worstcase, evaluating such a condition requires an iteration taking time proportionalto the length of the shorter of the two sequences (because when one sequence\nends, the lexicographic result can be determined). However, in some cases the\nresult of the test can be evaluated more ef\ufb01ciently. For example, if evaluating\n[7, 3, ...] <[7, 5, ...] , it is clear that the result is True without examining the re-\nmainders of those lists, because the second element of the left operand is strictlyless than the second element of the right operand.\nCreating New Instances\nThe \ufb01nal three behaviors in Table 5.3 are those that construct a new instance basedon one or more existing instances. In all cases, the running time depends on theconstruction and initialization of the new result, and therefore the asymptotic be-havior is proportional to the length of the result. Therefore, we \ufb01nd that slice\ndata[6000000:6000008] can be constructed almost immediately because it has only\neight elements, while slice data[6000000:7000000] has one million elements, and\nthus is more time-consuming to create.\nMutating Behaviors\nThe ef\ufb01ciency of the mutating behaviors of the listclass are described in Table 5.3.\nThe simplest of those behaviors has syntax data[j] = val , and is supported by the\nspecial\n setitem\n method. This operation has worst-case O(1)running time be-\ncause it simply replaces one element of a list with a new value. No other elementsare affected and the size of the underlying array does not change. The more inter-\nesting behaviors to analyze are those that add or remove elements from the list.", "204 Chapter 5. Array-Based Sequences\nOperation\n Running Time\ndata[j] = val\n O(1)\ndata.append(value)\n O(1)\u2217\ndata.insert(k, value)\n O(n\u2212k+1)\u2217\ndata.pop()\n O(1)\u2217\ndata.pop(k)\nO(n\u2212k)\u2217\ndeldata[k]\ndata.remove(value)\n O(n)\u2217\ndata1.extend(data2)\nO(n2)\u2217\ndata1 += data2\ndata.reverse()\n O(n)\ndata.sort()\n O(nlogn)\n\u2217amortized\nTable 5.4: Asymptotic performance of the mutating behaviors of the listclass. Iden-\nti\ufb01ers data,data1 ,a n ddata2 designate instances of the listclass, and n,n1,a n d n2\ntheir respective lengths.\nAdding Elements to a List\nIn Section 5.3 we fully explored the append method. In the worst case, it requires\n\u03a9(n)time because the underlying array is resized, but it uses O(1)time in the amor-\ntized sense. Lists also support a method, with signature insert(k, value) , that inserts\na given value into the list at index 0 \u2264k\u2264nwhile shifting all subsequent elements\nback one slot to make room. For the purpose of illustration, Code Fragment 5.5 pro-\nvides an implementation of that method, in the context of our DynamicArray class\nintroduced in Code Fragment 5.3. There are two complicating factors in analyzingthe ef\ufb01ciency of such an operation. First, we note that the addition of one element\nmay require a resizing of the dynamic array. That portion of the work requires \u03a9(n)\nworst-case time but only O(1)amortized time, as per append . The other expense\nforinsert is the shifting of elements to make room for the new item. The time for\n1definsert(self,k ,v a l u e ) :\n2 \u201d\u201d\u201dInsert value at index k, shifting subsequent values rightward.\u201d\u201d\u201d\n3 # (for simplicity, we assume 0 <=k<= n in this verion)\n4 if self.\nn= =self.\ncapacity: # not enough room\n5 self.\nresize(2\n self.\ncapacity) # so double capacity\n6 forjinrange(self.\nn, k,\u22121): # shift rightmost \ufb01rst\n7 self.\nA[j] = self.\nA[j\u22121]\n8 self.\nA[k] = value # store newest element\n9 self.\nn+ =1\nCode Fragment 5.5: Implementation of insert for our DynamicArray class.", "5.4. Ef\ufb01ciency of Python\u2019s Sequence Types 205\nk 2 1 0 n\u22121\nFigure 5.16: Creating room to insert a new element at index kof a dynamic array.\nthat process depends upon the index of the new element, and thus the number of\nother elements that must be shifted. That loop copies the reference that had beenat index n\u22121t oi n d e x n, then the reference that had been at index n\u22122t on\u22121,\ncontinuing until copying the reference that had been at index ktok+1, as illus-\ntrated in Figure 5.16. Overall this leads to an amortized O(n\u2212k+1)performance\nfor inserting at index k.\nWhen exploring the ef\ufb01ciency of Python\u2019s append method in Section 5.3.3,\nwe performed an experiment that measured the average cost of repeated calls onvarying sizes of lists (see Code Fragment 5.4 and Table 5.2). We have repeated thatexperiment with the insert method, trying three different access patterns:\n\u2022In the \ufb01rst case, we repeatedly insert at the beginning of a list,\nforninrange(N):\ndata.insert(0, None)\n\u2022In a second case, we repeatedly insert near the middle of a list,\nforninrange(N):\ndata.insert(n // 2, None)\n\u2022In a third case, we repeatedly insert at the end of the list,\nforninrange(N):\ndata.insert(n, None)\nThe results of our experiment are given in Table 5.5, reporting the average time per\noperation (not the total time for the entire loop). As expected, we see that insertingat the beginning of a list is most expensive, requiring linear time per operation.Inserting at the middle requires about half the time as inserting at the beginning,yet is still \u03a9(n)time. Inserting at the end displays O(1)behavior, akin to append.\nN\n100\n 1,000\n 10,000\n 100,000\n 1,000,000\nk=0\n 0.482\n 0.765\n 4.014\n 36.643\n 351.590\nk=n//2\n0.451\n 0.577\n 2.191\n 17.873\n 175.383\nk=n\n 0.420\n 0.422\n 0.395\n 0.389\n 0.397\nTable 5.5: Average running time of insert(k, val) , measured in microseconds, as\nobserved over a sequence of Ncalls, starting with an empty list.W e l e t ndenote\nthe size of the current list (as opposed to the \ufb01nal list).", "206 Chapter 5. Array-Based Sequences\nRemoving Elements from a List\nPython\u2019s listclass offers several ways to remove an element from a list. A call to\npop() removes the last element from a list. This is most ef\ufb01cient, because all other\nelements remain in their original location. This is effectively an O(1)operation,\nbut the bound is amortized because Python will occasionally shrink the underlying\ndynamic array to conserve memory.\nThe parameterized version, pop(k) , removes the element that is at index k<n\nof a list, shifting all subsequent elements leftward to \ufb01ll the gap that results from\nthe removal. The ef\ufb01ciency of this operation is O(n\u2212k), as the amount of shifting\ndepends upon the choice of index k, as illustrated in Figure 5.17. Note well that this\nimplies that pop(0) is the most expensive call, using \u03a9(n)time. (see experiments\nin Exercise R-5.8.)\nk 2 1 0 n\u22121\nFigure 5.17: Removing an element at index kof a dynamic array.\nThelistclass offers another method, named remove , that allows the caller to\nspecify the value that should be removed (not the index at which it resides). For-\nmally, it removes only the \ufb01rst occurrence of such a value from a list, or raises a\nValueError if no such value is found. An implementation of such behavior is given\nin Code Fragment 5.6, again using our DynamicArray class for illustration.\nInterestingly, there is no \u201cef\ufb01cient\u201d case for remove ; every call requires \u03a9(n)\ntime. One part of the process searches from the beginning until \ufb01nding the value atindex k, while the rest iterates from kto the end in order to shift elements leftward.\nThis linear behavior can be observed experimentally (see Exercise C-5.24).\n1defremove( self,v a l u e ) :\n2 \u201d\u201d\u201dRemove \ufb01rst occurrence of value (or raise ValueError).\u201d\u201d\u201d\n3 # note: we do not consider shrinking the dynamic array in this version\n4 forkinrange(self.\nn):\n5 if self.\nA[k] == value: # found a match!\n6 forjinrange(k, self.\nn\u22121): # shift others to \ufb01ll gap\n7 self.\nA[j] = self.\nA[j+1]\n8 self.\nA[self.\nn\u22121] =None # help garbage collection\n9 self.\nn\u2212=1 # we have one less item\n10 return # exit immediately\n11 raiseValueError(\n value not found\n ) # only reached if no match\nCode Fragment 5.6: Implementation of remove for our DynamicArray class.", "5.4. Ef\ufb01ciency of Python\u2019s Sequence Types 207\nExtending a List\nPython provides a method named extend that is used to add all elements of one list\nto the end of a second list. In effect, a call to data.extend(other) produces the same\noutcome as the code,\nforelement inother:\ndata.append(element)\nIn either case, the running time is proportional to the length of the other list, and\namortized because the underlying array for the \ufb01rst list may be resized to accom-\nmodate the additional elements.\nIn practice, the extend method is preferable to repeated calls to append because\nthe constant factors hidden in the asymptotic analysis are signi\ufb01cantly smaller. The\ngreater ef\ufb01ciency of extend is threefold. First, there is always some advantage to\nusing an appropriate Python method, because those methods are often implementednatively in a compiled language (rather than as interpreted Python code). Second,there is less overhead to a single function call that accomplishes all the work, versusmany individual function calls. Finally, increased ef\ufb01ciency of extend comes from\nthe fact that the resulting size of the updated list can be calculated in advance. If the\nsecond data set is quite large, there is some risk that the underlying dynamic array\nmight be resized multiple times when using repeated calls to append . With a single\ncall to extend , at most one resize operation will be performed. Exercise C-5.22\nexplores the relative ef\ufb01ciency of these two approaches experimentally.\nConstructing New Lists\nThere are several syntaxes for constructing new lists. In almost all cases, the asymp-totic ef\ufb01ciency of the behavior is linear in the length of the list that is created. How-\never, as with the case in the preceding discussion of extend , there are signi\ufb01cant\ndifferences in the practical ef\ufb01ciency.\nSection 1.9.2 introduces the topic of list comprehension , using an example\nsuch as squares = [ k\nkforkinrange(1, n+1) ] as a shorthand for\nsquares = [ ]\nforkinrange(1, n+1):\nsquares.append(k\n k)\nExperiments should show that the list comprehension syntax is signi\ufb01cantly fasterthan building the list by repeatedly appending (see Exercise C-5.23).\nSimilarly, it is a common Python idiom to initialize a list of constant values\nusing the multiplication operator, as in [0]\nnto produce a list of length nwith\nall values equal to zero. Not only is this succinct for the programmer; it is moreef\ufb01cient than building such a list incrementally.", "208 Chapter 5. Array-Based Sequences\n5.4.2 Python\u2019s String Class\nStrings are very important in Python. We introduced their use in Chapter 1, with\na discussion of various operator syntaxes in Section 1.3. A comprehensive sum-mary of the named methods of the class is given in Tables A.1 through A.4 of\nAppendix A. We will not formally analyze the ef\ufb01ciency of each of those behav-\niors in this section, but we do wish to comment on some notable issues. In general,we let ndenote the length of a string. For operations that rely on a second string as\na pattern, we let mdenote the length of that pattern string.\nThe analysis for many behaviors is quite intuitive. For example, methods that\nproduce a new string (e.g., capitalize ,center ,strip) require time that is linear in\nthe length of the string that is produced. Many of the behaviors that test Booleanconditions of a string (e.g., islower )t a k e O(n)time, examining all ncharacters in the\nworst case, but short circuiting as soon as the answer becomes evident (e.g., islower\ncan immediately return False if the \ufb01rst character is uppercased). The comparison\noperators (e.g., ==,<) fall into this category as well.\nPattern Matching\nSome of the most interesting behaviors, from an algorithmic point of view, are thosethat in some way depend upon \ufb01nding a string pattern within a larger string; this\ngoal is at the heart of methods such as\ncontains\n ,\ufb01nd,index ,count ,replace ,\nandsplit. String algorithms will be the topic of Chapter 13, and this particular\nproblem known as pattern matching will be the focus of Section 13.2. A naive im-\nplementation runs in O(mn)time case, because we consider the n\u2212m+1 possible\nstarting indices for the pattern, and we spend O(m)time at each starting position,\nchecking if the pattern matches. However, in Section 13.2, we will develop an al-\ngorithm for \ufb01nding a pattern of length mwithin a longer string of length ninO(n)\ntime.\nComposing Strings\nFinally, we wish to comment on several approaches for composing large strings. Asan academic exercise, assume that we have a large string named document , and our\ngoal is to produce a new string, letters , that contains only the alphabetic characters\nof the original string (e.g., with spaces, numbers, and punctuation removed). It maybe tempting to compose a result through repeated concatenation, as follows.\n# WARNING: do not do thisletters =\n# start with empty string\nforcindocument:\nifc.isalpha():\nletters += c # concatenate alphabetic character", "5.4. Ef\ufb01ciency of Python\u2019s Sequence Types 209\nWhile the preceding code fragment accomplishes the goal, it may be terribly\ninef\ufb01cient. Because strings are immutable, the command, letters += c , would\npresumably compute the concatenation, letters + c , as a new string instance and\nthen reassign the identi\ufb01er, letters , to that result. Constructing that new string\nwould require time proportional to its length. If the \ufb01nal result has ncharacters, the\nseries of concatenations would take time proportional to the familiar sum 1 +2+\n3+\u00b7\u00b7\u00b7+n, and therefore O(n2)time.\nInef\ufb01cient code of this type is widespread in Python, perhaps because of the\nsomewhat natural appearance of the code, and mistaken presumptions about how\nthe+= operator is evaluated with strings. Some later implementations of the\nPython interpreter have developed an optimization to allow such code to completein linear time, but this is not guaranteed for all Python implementations. The op-\ntimization is as follows. The reason that a command, letters += c , causes a new\nstring instance to be created is that the original string must be left unchanged if\nanother variable in a program refers to that string. On the other hand, if Pythonknew that there were no other references to the string in question, it could imple-\nment+=more ef\ufb01ciently by directly mutating the string (as a dynamic array). As\nit happens, the Python interpreter already maintains what are known as reference\ncounts for each object; this count is used in part to determine if an object can be\ngarbage collected. (See Section 15.1.2.) But in this context, it provides a means to\ndetect when no other references exist to a string, thereby allowing the optimization.\nA more standard Python idiom to guarantee linear time composition of a string\nis to use a temporary list to store individual pieces, and then to rely on the join\nmethod of the strclass to compose the \ufb01nal result. Using this technique with our\nprevious example would appear as follows:\ntemp = [ ] # start with empty list\nforcindocument:\nifc.isalpha():\ntemp.append(c) # append alphabetic character\nletters =\n.join(temp) # compose overall result\nThis approach is guaranteed to run in O(n)time. First, we note that the series of\nup to nappend calls will require a total of O(n)time, as per the de\ufb01nition of the\namortized cost of that operation. The \ufb01nal call to joinalso guarantees that it takes\ntime that is linear in the \ufb01nal length of the composed string.\nAs we discussed at the end of the previous section, we can further improve\nthe practical execution time by using a list comprehension syntax to build up thetemporary list, rather than by repeated calls to append. That solution appears as,\nletters =\n.join([c forcindocument ifc.isalpha()])\nBetter yet, we can entirely avoid the temporary list with a generator comprehension:\nletters =\n .join(c forcindocument ifc.isalpha())", "210 Chapter 5. Array-Based Sequences\n5.5 Using Array-Based Sequences\n5.5.1 Storing High Scores for a Game\nThe \ufb01rst application we study is storing a sequence of high score entries for a video\ngame. This is representative of many applications in which a sequence of objects\nmust be stored. We could just as easily have chosen to store records for patients ina hospital or the names of players on a football team. Nevertheless, let us focus onstoring high score entries, which is a simple application that is already rich enoughto present some important data-structuring concepts.\nTo begin, we consider what information to include in an object representing a\nhigh score entry. Obviously, one component to include is an integer representing\nthe score itself, which we identify as\nscore . Another useful thing to include is\nthe name of the person earning this score, which we identify as\n name . We could\ngo on from here, adding \ufb01elds representing the date the score was earned or game\nstatistics that led to that score. However, we omit such details to keep our example\nsimple. A Python class, GameEntry , representing a game entry, is given in Code\nFragment 5.7.\n1classGameEntry:\n2\u201d\u201d\u201dRepresents one entry of a list of high scores.\u201d\u201d\u201d\n3\n4def\n init\n(self,n a m e ,s c o r e ) :\n5 self.\nname = name\n6 self.\nscore = score\n78defget\nname(self):\n9 return self .\nname\n1011defget\nscore(self):\n12 return self .\nscore\n13\n14def\n str\n(self):\n15 return\n ({0}, {1})\n .format( self.\nname, self.\nscore) # e.g.,\n (Bob, 98)\nCode Fragment 5.7: Python code for a simple GameEntry class. We include meth-\nods for returning the name and score for a game entry object, as well as a method\nfor returning a string representation of this entry.", "5.5. Using Array-Based Sequences 211\nA Class for High Scores\nTo maintain a sequence of high scores, we develop a class named Scoreboard .A\nscoreboard is limited to a certain number of high scores that can be saved; once that\nlimit is reached, a new score only quali\ufb01es for the scoreboard if it is strictly higherthan the lowest \u201chigh score\u201d on the board. The length of the desired scoreboard maydepend on the game, perhaps 10, 50, or 500. Since that limit may vary depending onthe game, we allow it to be speci\ufb01ed as a parameter to our Scoreboard constructor.\nInternally, we will use a Python listnamed\nboard in order to manage the\nGameEntry instances that represent the high scores. Since we expect the score-\nboard to eventually reach full capacity, we initialize the list to be large enough tohold the maximum number of scores, but we initially set all entries to None .B y\nallocating the list with maximum capacity initially, it never needs to be resized. Asentries are added, we will maintain them from highest to lowest score, starting atindex 0 of the list. We illustrate a typical state of the data structure in Figure 5.18.\n3 2 01 9 78 6 5 4660\nMike 1105 Paul 720 Rose 590Rob 750 Anna Jack 510\nFigure 5.18: An illustration of an ordered list of length ten, storing references to six\nGameEntry objects in the cells from index 0 to 5, with the rest being None .\nA complete Python implementation of the Scoreboard class is given in Code\nFragment 5.8. The constructor is rather simple. The command\nself.\nboard = [ None]\ncapacity\ncreates a list with the desired length, yet all entries equal to None . We maintain\nan additional instance variable,\n n, that represents the number of actual entries\ncurrently in our table. For convenience, our class supports the\n getitem\n method\nto retrieve an entry at a given index with a syntax board[i] (orNone if no such entry\nexists), and we support a simple\n str\n method that returns a string representation\nof the entire scoreboard, with one entry per line.", "212 Chapter 5. Array-Based Sequences\n1classScoreboard:\n2\u201d\u201d\u201dFixed-length sequence of high scores in nondecreasing order.\u201d\u201d\u201d\n3\n4def\n init\n(self, capacity=10):\n5 \u201d\u201d\u201dInitialize scoreboard with given maximum capacity.\n67 All entries are initially None.\n8 \u201d\u201d\u201d\n9 self.\nboard = [ None]\ncapacity # reserve space for future scores\n10 self.\nn=0 # number of actual entries\n1112def\ngetitem\n (self,k ) :\n13 \u201d\u201d\u201dReturn entry at index k.\u201d\u201d\u201d\n14 return self .\nboard[k]\n15\n16def\n str\n(self):\n17 \u201d\u201d\u201dReturn string representation of the high score list.\u201d\u201d\u201d\n18 return\n \\n\n.join(str(self.\nboard[j]) forjinrange(self.\nn))\n1920defadd(self, entry):\n21 \u201d\u201d\u201dConsider adding entry to high scores.\u201d\u201d\u201d\n22 score = entry.get\nscore()\n23\n24 # Does new entry qualify as a high score?\n25 # answer is yes if board not full or score is higher than last entry\n26 good = self.\nn<len(self.\nboard) orscore>self.\nboard[ \u22121].get\n score()\n2728 ifgood:\n29 if self.\nn<len(self.\nboard): # no score drops from list\n30 self.\nn+ =1 # so overall number increases\n3132 # shift lower scores rightward to make room for new entry\n33 j=self.\nn\u22121\n34 while j>0and self .\nboard[j \u22121].get\n score( ) <score:\n35 self.\nboard[j] = self.\nboard[j \u22121] # shift entry from j-1 to j\n36 j\u2212=1 # and decrement j\n37 self.\nboard[j] = entry # when done, add new entry\nCode Fragment 5.8: Python code for a Scoreboard class that maintains an ordered\nseries of scores as GameEntry objects.", "5.5. Using Array-Based Sequences 213\nAdding an Entry\nThe most interesting method of the Scoreboard class is add, which is responsible\nfor considering the addition of a new entry to the scoreboard. Keep in mind that\nevery entry will not necessarily qualify as a high score. If the board is not yet full,any new entry will be retained. Once the board is full, a new entry is only retained\nif it is strictly better than one of the other scores, in particular, the last entry of the\nscoreboard, which is the lowest of the high scores.\nWhen a new score is considered, we begin by determining whether it quali\ufb01es\nas a high score. If so, we increase the count of active scores,\nn, unless the board\nis already at full capacity. In that case, adding a new high score causes some otherentry to be dropped from the scoreboard, so the overall number of entries remainsthe same.\nTo correctly place a new entry within the list, the \ufb01nal task is to shift any in-\nferior scores one spot lower (with the least score being dropped entirely when thescoreboard is full). This process is quite similar to the implementation of the insert\nmethod of the listclass, as described on pages 204\u2013205. In the context of our score-\nboard, there is no need to shift any None references that remain near the end of the\narray, so the process can proceed as diagrammed in Figure 5.19.\n01 9 8 7 6 5 4 3 2Mike 1105Rob 750\nPaul 720 Rose 590Anna 660 Jack 510740 Jill\nFigure 5.19: Adding a new GameEntry for Jill to the scoreboard. In order to make\nroom for the new reference, we have to shift the references for game entries withsmaller scores than the new one to the right by one cell. Then we can insert the newentry with index 2.\nTo implement the \ufb01nal stage, we begin by considering index j=s e l f .\nn\u22121,\nwhich is the index at which the last GameEntry instance will reside, after complet-\ning the operation. Either jis the correct index for the newest entry, or one or more\nimmediately before it will have lesser scores. The while loop at line 34 checks thecompound condition, shifting references rightward and decrementing j, as long as\nthere is another entry at index j\u22121 with a score less than the new score.", "214 Chapter 5. Array-Based Sequences\n5.5.2 Sorting a Sequence\nIn the previous subsection, we considered an application for which we added an ob-\nject to a sequence at a given position while shifting other elements so as to keep theprevious order intact. In this section, we use a similar technique to solve the sorting\nproblem, that is, starting with an unordered sequence of elements and rearrangingthem into nondecreasing order.\nThe Insertion-Sort Algorithm\nWe study several sorting algorithms in this book, most of which are described inChapter 12. As a warm-up, in this section we describe a nice, simple sorting al-\ngorithm known as insertion-sort . The algorithm proceeds as follows for an array-\nbased sequence. We start with the \ufb01rst element in the array. One element by itself\nis already sorted. Then we consider the next element in the array. If it is smallerthan the \ufb01rst, we swap them. Next we consider the third element in the array. Weswap it leftward until it is in its proper order with the \ufb01rst two elements. We then\nconsider the fourth element, and swap it leftward until it is in the proper order with\nthe \ufb01rst three. We continue in this manner with the \ufb01fth element, the sixth, and soon, until the whole array is sorted. We can express the insertion-sort algorithm inpseudo-code, as shown in Code Fragment 5.9.\nAlgorithm InsertionSort(A) :\nInput: An array Aofncomparable elements\nOutput: The array Awith elements rearranged in nondecreasing order\nforkfrom1ton\u22121do\nInsert A[k] at its proper location within A[0],A[1],...,A[k].\nCode Fragment 5.9: High-level description of the insertion-sort algorithm.\nThis is a simple, high-level description of insertion-sort. If we look back to\nCode Fragment 5.8 of Section 5.5.1, we see that the task of inserting a new en-\ntry into the list of high scores is almost identical to the task of inserting a newly\nconsidered element in insertion-sort (except that game scores were ordered from\nhigh to low). We provide a Python implementation of insertion-sort in Code Frag-ment 5.10, using an outer loop to consider each element in turn, and an innerloop that moves a newly considered element to its proper location relative to the\n(sorted) subarray of elements that are to its left. We illustrate an example run of the\ninsertion-sort algorithm in Figure 5.20.\nThe nested loops of insertion-sort lead to an O(n\n2)running time in the worst\ncase. The most work is done if the array is initially in reverse order. On the otherhand, if the initial array is nearly sorted or perfectly sorted, insertion-sort runs in\nO(n)time because there are few or no iterations of the inner loop.", "5.5. Using Array-Based Sequences 215\n1definsertion\n sort(A):\n2\u201d\u201d\u201dSort list of comparable elements into nondecreasing order.\u201d\u201d\u201d\n3forkinrange(1, len(A)): # from 1 to n-1\n4 cur = A[k] # current element to be inserted\n5 j=k # \ufb01nd correct index j for current\n6 while j>0andA[j\u22121]>cur: # element A[j-1] must be after current\n7 A[j] = A[j \u22121]\n8 j\u2212=1\n9 A[j] = cur # cur is now in the right place\nCode Fragment 5.10: Python code for performing insertion-sort on a list.\ninsertinsertinsert\n00\n0\n0\n0\n000 0\n000\nDone!0C AEHGF\nBC AEHGFD\nB EHGF CD\nA HGF BCDE\nA F BCDE HA GF BCDEHEHGFD C B B EHGFD C\nA F BCDE H\nBCDE H G\nB AABCDEG H ABC\nFFGA\nGHEAD\nCDE H\nH G F E DG\nCAB D\nno move\n2345671234567\n1234567\n1234567\n1234567\n123456712345671234567 1234567\n1234567 1234567 1234567cur\n1234567move move moveno moveno move\nno move\nno move\nmove no move\nmove move\n1\nFigure 5.20: Execution of the insertion-sort algorithm on an array of eight charac-\nters. Each row corresponds to an iteration of the outer loop, and each copy of the\nsequence in a row corresponds to an iteration of the inner loop. The current elementthat is being inserted is highlighted in the array, and shown as the curvalue.", "216 Chapter 5. Array-Based Sequences\n5.5.3 Simple Cryptography\nAn interesting application of strings and lists is cryptography , the science of secret\nmessages and their applications. This \ufb01eld studies ways of performing encryp-\ntion, which takes a message, called the plaintext , and converts it into a scrambled\nmessage, called the ciphertext . Likewise, cryptography also studies corresponding\nways of performing decryption , which takes a ciphertext and turns it back into its\noriginal plaintext.\nArguably the earliest encryption scheme is the Caesar cipher , which is named\nafter Julius Caesar, who used this scheme to protect important military messages.\n(All of Caesar\u2019s messages were written in Latin, of course, which already makesthem unreadable for most of us!) The Caesar cipher is a simple way to obscure amessage written in a language that forms words with an alphabet.\nThe Caesar cipher involves replacing each letter in a message with the letter that\nis a certain number of letters after it in the alphabet. So, in an English message, wemight replace each A with D, each B with E, each C with F, and so on, if shifting bythree characters. We continue this approach all the way up to W, which is replacedwith Z. Then, we let the substitution pattern wrap around , so that we replace X\nwith A, Y with B, and Z with C.\nConverting Between Strings and Character Lists\nGiven that strings are immutable, we cannot directly edit an instance to encrypt it.Instead, our goal will be to generate a new string. A convenient technique for per-forming string transformations is to create an equivalent list of characters, edit the\nlist, and then reassemble a (new) string based on the list. The \ufb01rst step can be per-\nformed by sending the string as a parameter to the constructor of the listclass. For\nexample, the expression list(\nbird\n )produces the result [\nb\n,\ni\n,\nr\n,\nd\n].\nConversely, we can use a list of characters to build a string by invoking the join\nmethod on an empty string, with the list of characters as the parameter. For exam-\nple, the call\n .join([\n b\n,\ni\n,\nr\n,\nd\n])returns the string\n bird\n .\nUsing Characters as Array Indices\nIf we were to number our letters like array indices, so that A is 0, B is 1, C is 2,\nand so on, then we can write the Caesar cipher with a rotation of ras a simple\nformula: Replace each letter iwith the letter (i+r)mod 26, where mod is the\nmodulo operator, which returns the remainder after performing an integer division.\nThis operator is denoted with %in Python, and it is exactly the operator we need\nto easily perform the wrap around at the end of the alphabet. For 26 mod 26 is0, 27 mod 26 is 1, and 28 mod 26 is 2. The decryption algorithm for the Caesarcipher is just the opposite\u2014we replace each letter with the one rplaces before it,\nwith wrap around (that is, letter iis replaced by letter (i\u2212r)mod 26).", "5.5. Using Array-Based Sequences 217\nWe can represent a replacement rule using another string to describe the trans-\nlation. As a concrete example, suppose we are using a Caesar cipher with a three-\ncharacter rotation. We can precompute a string that represents the replacementsthat should be used for each character from A to Z. For example, A should be re-placed by D, B replaced by E, and so on. The 26 replacement characters in orderare\nDEFGHIJKLMNOPQRSTUVWXYZABC\n . We can subsequently use this translation\nstring as a guide to encrypt a message. The remaining challenge is how to quicklylocate the replacement for each character of the original message.\nFortunately, we can rely on the fact that characters are represented in Unicode\nby integer code points, and the code points for the uppercase letters of the Latin\nalphabet are consecutive (for simplicity, we restrict our encryption to uppercase\nletters). Python supports functions that convert between integer code points andone-character strings. Speci\ufb01cally, the function ord(c) takes a one-character string\nas a parameter and returns the integer code point for that character. Conversely, thefunction chr(j) takes an integer and returns its associated one-character string.\nIn order to \ufb01nd a replacement for a character in our Caesar cipher, we need to\nmap the characters\nA\nto\nZ\nto the respective numbers 0 to 25. The formula for\ndoing that conversion is j=o r d ( c ) \u2212ord(\n A\n). As a sanity check, if character c\nis\nA\n,w eh a v et h a t j=0. When cis\nB\n, we will \ufb01nd that its ordinal value is pre-\ncisely one more than that for\n A\n, so their difference is 1. In general, the integer j\nthat results from such a calculation can be used as an index into our precomputedtranslation string, as illustrated in Figure 5.21.\n10 24 23 22 21 20 19 18 17 16 15 14 13 12 1125 9 8 7 6 5 4 3 2 1 0M OPQRSTUV W XYZABC DEFGH I JKL N\nHere is the\nreplacement for\n T\nord(\n T\n)\u2212ord(\n A\n)\n84 \u2212 65\n19 ==In UnicodeUsing\n T\nas an indexencoder array\nFigure 5.21: Illustrating the use of uppercase characters as indices, in this case to\nperform the replacement rule for Caesar cipher encryption.\nIn Code Fragment 5.11, we develop a Python class for performing the Caesar\ncipher with an arbitrary rotational shift, and demonstrate its use. When we run this\nprogram (to perform a simple test), we get the following output.\nSecret: WKH HDJOH LV LQ SODB; PHHW DW MRH\u2019V.Message: THE EAGLE IS IN PLAY; MEET AT JOE\u2019S.\nThe constructor for the class builds the forward and backward translation strings forthe given rotation. With those in hand, the encryption and decryption algorithmsare essentially the same, and so we perform both by means of a nonpublic utility\nmethod named\ntransform .", "218 Chapter 5. Array-Based Sequences\n1classCaesarCipher:\n2\u201d\u201d\u201dClass for doing encryption and decryption using a Caesar cipher.\u201d\u201d\u201d\n3\n4def\n init\n(self,s h i f t ) :\n5 \u201d\u201d\u201dConstruct Caesar cipher using given integer shift for rotation.\u201d\u201d\u201d\n6 encoder = [ None]\n26 # temp array for encryption\n7 decoder = [ None]\n26 # temp array for decryption\n8 forkinrange(26):\n9 encoder[k] = chr((k + shift) % 26 + ord(\n A\n))\n10 decoder[k] = chr((k \u2212shift) % 26 + ord(\n A\n))\n11 self.\nforward =\n .join(encoder) # will store as string\n12 self.\nbackward =\n .join(decoder) # since \ufb01xed\n1314defencrypt( self, message):\n15 \u201d\u201d\u201dReturn string representing encripted message.\u201d\u201d\u201d\n16 return self .\ntransform(message, self.\nforward)\n1718defdecrypt( self, secret):\n19 \u201d\u201d\u201dReturn decrypted message given encrypted secret.\u201d\u201d\u201d\n20 return self .\ntransform(secret, self.\nbackward)\n2122def\ntransform( self, original, code):\n23 \u201d\u201d\u201dUtility to perform transformation based on given code string.\u201d\u201d\u201d\n24 msg = list(original)\n25 forkinrange(len(msg)):\n26 ifmsg[k].isupper():\n27 j=o r d ( m s g [ k ] ) \u2212ord(\n A\n) # index from 0 to 25\n28 msg[k] = code[j] # replace this character\n29 return\n .join(msg)\n3031if\nname\n ==\n __main__\n :\n32 cipher = CaesarCipher(3)\n33 message = \"THE EAGLE IS IN PLAY; MEET AT JOE\n S.\"\n34 coded = cipher.encrypt(message)\n35 print(\n Secret:\n ,c o d e d )\n36 answer = cipher.decrypt(coded)\n37 print(\n Message:\n , answer)\nCode Fragment 5.11: A complete Python class for the Caesar cipher.", "5.6. Multidimensional Data Sets 219\n5.6 Multidimensional Data Sets\nLists, tuples, and strings in Python are one-dimensional. We use a single index to\naccess each element of the sequence. Many computer applications involve mul-tidimensional data sets. For example, computer graphics are often modeled in\neither two or three dimensions. Geographic information may be naturally repre-\nsented in two dimensions, medical imaging may provide three-dimensional scansof a patient, and a company\u2019s valuation is often based upon a high number of in-dependent \ufb01nancial measures that can be modeled as multidimensional data. Atwo-dimensional array is sometimes also called a matrix . We may use two indices,\nsayiand j, to refer to the cells in the matrix. The \ufb01rst index usually refers to a\nrow number and the second to a column number, and these are traditionally zero-indexed in computer science. Figure 5.22 illustrates a two-dimensional data setwith integer values. This data might, for example, represent the number of stores\nin various regions of Manhattan.\n22 18 709 5 33 10 4 56 82 440\n45 32 830 120 750 660 13 77 20 105\n4 880 45 66 61 28 650 7 510 67\n940 12 36 3 20 100 306 590 0 500\n50 65 42 49 88 25 70 126 83 288\n398 233 5 83 59 232 49 8 365 90\n33 58 632 87 94 5 59 204 120 82962 394 3 4 102 140 183 390 16 268\n0\n1\n234\n5\n6701234567 9\nFigure 5.22: Illustration of a two-dimensional integer data set, which has 8 rows\nand 10 columns. The rows and columns are zero-indexed. If this data set were\nnamed stores ,t h ev a l u eo f stores[3][5] is 100 and the value of stores[6][2] is 632.\nA common representation for a two-dimensional data set in Python is as a list\nof lists. In particular, we can represent a two-dimensional array as a list of rows,with each row itself being a list of values. For example, the two-dimensional data\n22 18 709 5 3345 32 830 120 750\n4 880 45 66 61\nmight be stored in Python as follows.\ndata = [ [22, 18, 709, 5, 33], [45, 32, 830, 120, 750], [4, 880, 45, 66, 61] ]\nAn advantage of this representation is that we can naturally use a syntax suchasdata[1][3] to represent the value that has row index 1 and column index 3, as\ndata[1] , the second entry in the outer list, is itself a list, and thus indexable.", "220 Chapter 5. Array-Based Sequences\nConstructing a Multidimensional List\nTo quickly initialize a one-dimensional list, we generally rely on a syntax such as\ndata = [0]\n nto create a list of nzeros. On page 189, we emphasized that from\na technical perspective, this creates a list of length nwith all entries referencing\nthe same integer instance, but that there was no meaningful consequence of such\naliasing because of the immutability of the intclass in Python.\nWe have to be considerably more careful when creating a list of lists. If our\ngoal were to create the equivalent of a two-dimensional list of integers, with rrows\nandccolumns, and to initialize all values to zero, a \ufb02awed approach might be to\ntry the command\ndata = ([0]\n c)\nr # Warning: this is a mistake\nWhile([0]\n c)is indeed a list of czeros, multiplying that list by runfortunately cre-\nates a single list with length r\u00b7c,j u s ta s [2,4,6]\n 2results in list [ 2 ,4 ,6 ,2 ,4 ,6 ] .\nA better, yet still \ufb02awed attempt is to make a list that contains the list of czeros\nas its only element, and then to multiply that list by r. That is, we could try the\ncommand\ndata = [ [0]\n c]\nr # Warning: still a mistake\nThis is much closer, as we actually do have a structure that is formally a list of lists.\nThe problem is that all rentries of the list known as data are references to the same\ninstance of a list of czeros. Figure 5.23 provides a portrayal of such aliasing.\n0\n000 0000\n12345\n12data:\nFigure 5.23: A \ufb02awed representation of a 3 \u00d76 data set as a list of lists, created with\nthe command data = [ [0]\n 6]\n3. (For simplicity, we overlook the fact that the\nvalues in the secondary list are referential.)\nThis is truly a problem. Setting an entry such as data[2][0] = 100 would change\nthe \ufb01rst entry of the secondary list to reference a new value, 100. Yet that cell of\nthe secondary list also represents the value data[0][0] , because \u201crow\u201d data[0] and\n\u201crow\u201d data[2] refer to the same secondary list.", "5.6. Multidimensional Data Sets 221\n00 0\n00 0000 00 0 00 0 00 0 0 00 0\n3 12 2345 12 45\n12data:345 1\nFigure 5.24: A valid representation of a 3 \u00d76 data set as a list of lists. (For simplic-\nity, we overlook the fact that the values in the secondary lists are referential.)\nTo properly initialize a two-dimensional list, we must ensure that each cell of\nthe primary list refers to an independent instance of a secondary list. This can be\naccomplished through the use of Python\u2019s list comprehension syntax.\ndata = [ [0]\n cforjinrange(r) ]\nThis command produces a valid con\ufb01guration, similar to the one shown in Fig-\nure 5.24. By using list comprehension, the expression [0]\ncis reevaluated for\neach pass of the embedded for loop. Therefore, we get rdistinct secondary lists, as\ndesired. (We note that the variable jin that command is irrelevant; we simply need\na for loop that iterates rtimes.)\nTwo-Dimensional Arrays and Positional Games\nMany computer games, be they strategy games, simulation games, or \ufb01rst-personcon\ufb02ict games, involve objects that reside in a two-dimensional space. Software forsuch positional games need a way of representing such a two-dimensional \u201cboard,\u201d\nand in Python the list of lists is a natural choice.\nTic-Tac-Toe\nAs most school children know, Tic-Tac-Toe is a game played in a three-by-three\nboard. Two players\u2014X and O\u2014alternate in placing their respective marks in the\ncells of this board, starting with player X. If either player succeeds in getting three\nof his or her marks in a row, column, or diagonal, then that player wins.\nThis is admittedly not a sophisticated positional game, and it\u2019s not even that\nmuch fun to play, since a good player O can always force a tie. Tic-Tac-Toe\u2019s savinggrace is that it is a nice, simple example showing how two-dimensional arrays canbe used for positional games. Software for more sophisticated positional games,such as checkers, chess, or the popular simulation games, are all based on the same\napproach we illustrate here for using a two-dimensional array for Tic-Tac-Toe.", "222 Chapter 5. Array-Based Sequences\nOur representation of a 3 \u00d73 board will be a list of lists of characters, with\nX\nor\nO\ndesignating a player\u2019s move, or\n designating an empty space. For\nexample, the board con\ufb01guration\nXOO\nOXX\nwill be stored internally as\n[[\nO\n,\nX\n,\nO\n], [\n ,\nX\n,\n], [\n ,\nO\n,\nX\n]]\nWe develop a complete Python class for maintaining a Tic-Tac-Toe board for\ntwo players. That class will keep track of the moves and report a winner, but it\ndoes not perform any strategy or allow someone to play Tic-Tac-Toe against thecomputer. The details of such a program are beyond the scope of this chapter, but\nit might nonetheless make a good course project (see Exercise P-8.68).\nBefore presenting the implementation of the class, we demonstrate its public\ninterface with a simple test in Code Fragment 5.12.\n1game = TicTacToe()\n2#Xm o v e s : #Om o v e s :\n3game.mark(1, 1); game.mark(0, 2)\n4game.mark(2, 2); game.mark(0, 0)\n5game.mark(0, 1); game.mark(2, 1)\n6game.mark(1, 2); game.mark(1, 0)\n7game.mark(2, 0)\n8\n9print(game)\n10 winner = game.winner()\n11ifwinner is None :\n12 print(\nTie\n)\n13else:\n14 print(winner,\n wins\n )\nCode Fragment 5.12: A simple test for our Tic-Tac-Toe class.\nThe basic operations are that a new game instance represents an empty board,\nthat the mark(i,j) method adds a mark at the given position for the current player\n(with the software managing the alternating of turns), and that the game board canbe printed and the winner determined. The complete source code for the TicTacToe\nclass is given in Code Fragment 5.13. Our mark method performs error checking\nto make sure that valid indices are sent, that the position is not already occupied,and that no further moves are made after someone wins the game.", "5.6. Multidimensional Data Sets 223\n1classTicTacToe:\n2\u201d\u201d\u201dManagement of a Tic-Tac-Toe game (does not do strategy).\u201d\u201d\u201d\n3\n4def\n init\n(self):\n5 \u201d\u201d\u201dStart a new game.\u201d\u201d\u201d\n6 self.\nboard = [ [\n ]\n3forjinrange(3) ]\n7 self.\nplayer =\n X\n8\n9defmark(self,i ,j ) :\n10 \u201d\u201d\u201dPut an X or O mark at position (i,j) for next player\n s turn.\u201d\u201d\u201d\n11 if not (0<=i<=2and 0<=j<=2 ) :\n12 raiseValueError(\n Invalid board position\n )\n13 if self.\nboard[i][j] !=\n :\n14 raiseValueError(\n Board position occupied\n )\n15 if self.winner( ) is not None :\n16 raiseValueError(\n Game is already complete\n )\n17 self.\nboard[i][j] = self.\nplayer\n18 if self.\nplayer ==\n X\n:\n19 self.\nplayer =\n O\n20 else:\n21 self.\nplayer =\n X\n22\n23 def\nis\nwin(self,m a r k ) :\n24 \u201d\u201d\u201dCheck whether the board con\ufb01guration is a win for the given player.\u201d\u201d\u201d\n25 board = self.\nboard # local variable for shorthand\n26 return (mark == board[0][0] == board[0][1] == board[0][2] or #r o w0\n27 mark == board[1][0] == board[1][1] == board[1][2] or #r o w1\n28 mark == board[2][0] == board[2][1] == board[2][2] or #r o w2\n29 mark == board[0][0] == board[1][0] == board[2][0] or #c o l u m n0\n30 mark == board[0][1] == board[1][1] == board[2][1] or #c o l u m n1\n31 mark == board[0][2] == board[1][2] == board[2][2] or #c o l u m n2\n32 mark == board[0][0] == board[1][1] == board[2][2] or # diagonal\n33 mark == board[0][2] == board[1][1] == board[2][0]) #r e vd i a g\n34\n35 defwinner( self):\n36 \u201d\u201d\u201dReturn mark of winning player, or None to indicate a tie.\u201d\u201d\u201d\n37 formarkin\nXO\n:\n38 if self.\nis\nwin(mark):\n39 return mark\n40 return None\n41\n42 def\n str\n(self):\n43 \u201d\u201d\u201dReturn string representation of current game board.\u201d\u201d\u201d\n44 rows = [\n |\n.join(self.\nboard[r]) forrinrange(3)]\n45 return\n \\n-----\\n\n .join(rows)\nCode Fragment 5.13: A complete Python class for managing a Tic-Tac-Toe game.", "224 Chapter 5. Array-Based Sequences\n5.7 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-5.1 Execute the experiment from Code Fragment 5.1 and compare the results\non your system to those we report in Code Fragment 5.2.\nR-5.2 In Code Fragment 5.1, we perform an experiment to compare the length ofa Python list to its underlying memory usage. Determining the sequence\nof array sizes requires a manual inspection of the output of that program.\nRedesign the experiment so that the program outputs only those values ofkat which the existing capacity is exhausted. For example, on a system\nconsistent with the results of Code Fragment 5.2, your program shouldoutput that the sequence of array capacities are 0, 4, 8, 16, 25, ....\nR-5.3 Modify the experiment from Code Fragment 5.1 in order to demonstratethat Python\u2019s listclass occasionally shrinks the size of its underlying array\nwhen elements are popped from a list.\nR-5.4 OurDynamicArray class, as given in Code Fragment 5.3, does not support\nuse of negative indices with\ngetitem\n . Update that method to better\nmatch the semantics of a Python list.\nR-5.5 Redo the justi\ufb01cation of Proposition 5.1 assuming that the the cost ofgrowing the array from size kto size 2 kis 3kcyber-dollars. How much\nshould each append operation be charged to make the amortization work?\nR-5.6 Our implementation of insert for the DynamicArray class, as given in\nCode Fragment 5.5, has the following inef\ufb01ciency. In the case when a re-size occurs, the resize operation takes time to copy all the elements froman old array to a new array, and then the subsequent loop in the body of\ninsert shifts many of those elements. Give an improved implementation\nof the insert method, so that, in the case of a resize, the elements are\nshifted into their \ufb01nal position during that operation, thereby avoiding the\nsubsequent shifting.\nR-5.7 LetAbe an array of size n\u22652 containing integers from 1 to n\u22121, inclu-\nsive, with exactly one repeated. Describe a fast algorithm for \ufb01nding theinteger in Athat is repeated.\nR-5.8 Experimentally evaluate the ef\ufb01ciency of the popmethod of Python\u2019s list\nclass when using varying indices as a parameter, as we did for insert on\npage 205. Report your results akin to Table 5.5.", "5.7. Exercises 225\nR-5.9 Explain the changes that would have to be made to the program of Code\nFragment 5.11 so that it could perform the Caesar cipher for messages\nthat are written in an alphabet-based language other than English, such asGreek, Russian, or Hebrew.\nR-5.10 The constructor for the CaesarCipher class in Code Fragment 5.11 can\nbe implemented with a two-line body by building the forward and back-\nward strings using a combination of the joinmethod and an appropriate\ncomprehension syntax. Give such an implementation.\nR-5.11 Use standard control structures to compute the sum of all numbers in an\nn\u00d7ndata set, represented as a list of lists.\nR-5.12 Describe how the built-in sum function can be combined with Python\u2019s\ncomprehension syntax to compute the sum of all numbers in an n\u00d7ndata\nset, represented as a list of lists.\nCreativity\nC-5.13 In the experiment of Code Fragment 5.1, we begin with an empty list. Ifdata were initially constructed with nonempty length, does this affect the\nsequence of values at which the underlying array is expanded? Performyour own experiments, and comment on any relationship you see betweenthe initial length and the expansion sequence.\nC-5.14 Theshu\ufb04e method, supported by the random module, takes a Python\nlist and rearranges it so that every possible ordering is equally likely.\nImplement your own version of such a function. You may rely on the\nrandrange(n) function of the random module, which returns a random\nnumber between 0 and n\u22121i n c l u s i v e .\nC-5.15 Consider an implementation of a dynamic array, but instead of copyingthe elements into an array of double the size (that is, from Nto 2N)w h e n\nits capacity is reached, we copy the elements into an array with \u2308N/4\u2309\nadditional cells, going from capacity Nto capacity N+\u2308N/4\u2309. Prove that\nperforming a sequence of nappend operations still runs in O(n)time in\nthis case.\nC-5.16 Implement a popmethod for the DynamicArray class, given in Code Frag-\nment 5.3, that removes the last element of the array, and that shrinks the\ncapacity, N, of the array by half any time the number of elements in the\narray goes below N/4.\nC-5.17 Prove that when using a dynamic array that grows and shrinks as in the\nprevious exercise, the following series of 2 noperations takes O(n)time:\nnappend operations on an initially empty array, followed by npopoper-\nations.", "226 Chapter 5. Array-Based Sequences\nC-5.18 Give a formal proof that any sequence of nappend orpopoperations on\nan initially empty dynamic array takes O(n)time, if using the strategy\ndescribed in Exercise C-5.16.\nC-5.19 Consider a variant of Exercise C-5.16, in which an array of capacity Nis\nresized to capacity precisely that of the number of elements, any time the\nnumber of elements in the array goes strictly below N/4. Give a formal\nproof that any sequence of nappend orpop operations on an initially\nempty dynamic array takes O(n)time.\nC-5.20 Consider a variant of Exercise C-5.16, in which an array of capacity N,i s\nresized to capacity precisely that of the number of elements, any time thenumber of elements in the array goes strictly below N/2. Show that there\nexists a sequence of noperations that requires \u03a9(n\n2)time to execute.\nC-5.21 In Section 5.4.2, we described four different ways to compose a long\nstring: (1) repeated concatenation, (2) appending to a temporary list and\nthen joining, (3) using list comprehension with join, and (4) using genera-\ntor comprehension with join. Develop an experiment to test the ef\ufb01ciencyof all four of these approaches and report your \ufb01ndings.\nC-5.22 Develop an experiment to compare the relative ef\ufb01ciency of the extend\nmethod of Python\u2019s listclass versus using repeated calls to append to\naccomplish the equivalent task.\nC-5.23 Based on the discussion of page 207, develop an experiment to comparethe ef\ufb01ciency of Python\u2019s list comprehension syntax versus the construc-\ntion of a list by means of repeated calls to append.\nC-5.24 Perform experiments to evaluate the ef\ufb01ciency of the remove method of\nPython\u2019s listclass, as we did for insert on page 205. Use known values so\nthat all removals occur either at the beginning, middle, or end of the list.\nReport your results akin to Table 5.5.\nC-5.25 The syntax data.remove(value) for Python list data removes only the \ufb01rst\noccurrence of element value from the list. Give an implementation of a\nfunction, with signature remove\nall(data, value) , that removes alloccur-\nrences of value from the given list, such that the worst-case running time\nof the function is O(n)on a list with nelements. Not that it is not ef\ufb01cient\nenough in general to rely on repeated calls to remove .\nC-5.26 LetBbe an array of size n\u22656 containing integers from 1 to n\u22125, inclu-\nsive, with exactly \ufb01ve repeated. Describe a good algorithm for \ufb01nding the\n\ufb01ve integers in Bthat are repeated.\nC-5.27 Given a Python list Lofnpositive integers, each represented with k=\n\u2308logn\u2309+1 bits, describe an O(n)-time method for \ufb01nding a k-bit integer\nnot in L.\nC-5.28 Argue why any solution to the previous problem must run in \u03a9(n)time.", "Chapter Notes 227\nC-5.29 A useful operation in databases is the natural join . If we view a database\nas a list of ordered pairs of objects, then the natural join of databases A\nandBis the list of all ordered triples (x,y,z)such that the pair (x,y)is in\nAand the pair (y,z)is in B. Describe and analyze an ef\ufb01cient algorithm\nfor computing the natural join of a list Aofnpairs and a list Bofmpairs.\nC-5.30 When Bob wants to send Alice a message Mon the Internet, he breaks M\nintondata packets , numbers the packets consecutively, and injects them\ninto the network. When the packets arrive at Alice\u2019s computer, they may\nbe out of order, so Alice must assemble the sequence of npackets in order\nbefore she can be sure she has the entire message. Describe an ef\ufb01cient\nscheme for Alice to do this, assuming that she knows the value of n.W h a t\nis the running time of this algorithm?\nC-5.31 Describe a way to use recursion to add all the numbers in an n\u00d7ndata\nset, represented as a list of lists.\nProjects\nP-5.32 Write a Python function that takes two three-dimensional numeric datasets and adds them componentwise.\nP-5.33 Write a Python program for a matrix class that can add and multiply two-dimensional arrays of numbers, assuming the dimensions agree appropri-ately for the operation.\nP-5.34 Write a program that can perform the Caesar cipher for English messagesthat include both upper- and lowercase characters.\nP-5.35 Implement a class, SubstitutionCipher , with a constructor that takes a\nstring with the 26 uppercase letters in an arbitrary order and uses that forthe forward mapping for encryption (akin to the self.\nforward string in\nourCaesarCipher class of Code Fragment 5.11). You should derive the\nbackward mapping from the forward version.\nP-5.36 Redesign the CaesarCipher class as a subclass of the SubstitutionCipher\nfrom the previous problem.\nP-5.37 Design a RandomCipher class as a subclass of the SubstitutionCipher\nfrom Exercise P-5.35, so that each instance of the class relies on a random\npermutation of letters for its mapping.\nChapter Notes\nThe fundamental data structures of arrays belong to th e folklore of computer science. They\nwere \ufb01rst chronicled in the computer science literature by Knuth in his seminal book on\nFundamental Algorithms [64].", "Chapter\n6Stacks, Queues, and Deques\nContents\n6 . 1 S t a c k s .............................. 2 2 9\n6 . 1 . 1 T h e S t a c k A b s t r a c t D a t a T y p e...............2 3 0\n6.1.2 Simple Array-Based Stack Implementation . . . . . . . . . 231\n6 . 1 . 3 R e v e r s i n g D a t a U s i n g a S t a c k ...............2 3 5\n6.1.4 Matching Parentheses and HTML Tags . . . . . . . . . . 236\n6 . 2 Q u e u e s ............................. 2 3 9\n6 . 2 . 1 T h e Q u e u e A b s t r a c t D a t a T y p e ..............2 4 0\n6.2.2 Array-Based Queue Implementation . . . . . . . . . . . . 241\n6 . 3 D o u b l e - E n d e dQ u e u e s..................... 2 4 7\n6 . 3 . 1 T h e D e q u e A b s t r a c t D a t a T y p e ..............2 4 7\n6.3.2 Implementing a Deque with a Circular Array . . . . . . . . 248\n6.3.3 Deques in the Python Collections Module . . . . . . . . . 249\n6 . 4 E x e r c i s e s ............................ 2 5 0\n", "6.1. Stacks 229\n6.1 Stacks\nAstack is a collection of objects that are inserted and removed according to the\nlast-in, \ufb01rst-out (LIFO ) principle. A user may insert objects into a stack at any\ntime, but may only access or remove the most recently inserted object that remains\n(at the so-called \u201ctop\u201d of the stack). The name \u201cstack\u201d is derived from the metaphorof a stack of plates in a spring-loaded, cafeteria plate dispenser. In this case, thefundamental operations involve the \u201cpushing\u201d and \u201cpopping\u201d of plates on the stack.\nWhen we need a new plate from the dispenser, we \u201cpop\u201d the top plate off the stack,\nand when we add a plate, we \u201cpush\u201d it down on the stack to become the new topplate. Perhaps an even more amusing example is a PEZ\n\u00aecandy dispenser, which\nstores mint candies in a spring-loaded container that \u201cpops\u201d out the topmost candyin the stack when the top of the dispenser is lifted (see Figure 6.1). Stacks are\na fundamental data structure. They are used in many applications, including the\nfollowing.\nExample 6.1:\nInternet Web browsers store the addresses of recently visited sites\nin a stack. Each time a user visits a new site, that site\u2019s address is \u201cpushed\u201d onto the\nstack of addresses. The browser then allows the user to \u201cpop\u201d back to previously\nvisited sites using the \u201cback\u201d button.\nExample 6.2: Text editors usually provide an \u201cundo\u201d mechanism that cancels re-\ncent editing operations and reverts to former states of a document. This undo oper-\nation can be accomplished by keeping text changes in a stack.\nFigure 6.1: A schematic drawing of a PEZ\u00aedispenser; a physical implementation\nof the stack ADT. (PEZ\u00aeis a registered trademark of PEZ Candy, Inc.)", "230 Chapter 6. Stacks, Queues, and Deques\n6.1.1 The Stack Abstract Data Type\nStacks are the simplest of all data structures, yet they are also among the most\nimportant. They are used in a host of different applications, and as a tool for manymore sophisticated data structures and algorithms. Formally, a stack is an abstract\ndata type (ADT) such that an instance Ssupports the following two methods:\nS.push(e) :Add element eto the top of stack S.\nS.pop() :Remove and return the top element from the stack S;\nan error occurs if the stack is empty.\nAdditionally, let us de\ufb01ne the following accessor methods for convenience:\nS.top() :Return a reference to the top element of stack S, without\nremoving it; an error occurs if the stack is empty.\nS.is\nempty() :Return True if stack Sdoes not contain any elements.\nlen(S): Return the number of elements in stack S; in Python, we\nimplement this with the special method\n len\n .\nBy convention, we assume that a newly created stack is empty, and that there is no\na priori bound on the capacity of the stack. Elements added to the stack can havearbitrary type.\nExample 6.3:\nThe following table shows a series of stack operations and their\neffects on an initially empty stack Sof integers.\nOperation\n Return Value\n Stack Contents\nS.push(5)\n \u2013\n [5]\nS.push(3)\n \u2013\n [5, 3]\nlen(S)\n 2\n [5, 3]\nS.pop()\n 3\n [5]\nS.is\nempty()\n False\n [5]\nS.pop()\n 5\n []\nS.is\nempty()\n True\n []\nS.pop()\n \u201cerror\u201d\n []\nS.push(7)\n \u2013\n [7]\nS.push(9)\n \u2013\n [7, 9]\nS.top()\n 9\n [7, 9]\nS.push(4)\n \u2013\n [7, 9, 4]\nlen(S)\n 3\n [7, 9, 4]\nS.pop()\n 4\n [7, 9]\nS.push(6)\n \u2013\n [7, 9, 6]\nS.push(8)\n \u2013\n [ 7 ,9 ,6 ,8 ]\nS.pop()\n 8\n [7, 9, 6]\n", "6.1. Stacks 231\n6.1.2 Simple Array-Based Stack Implementation\nWe can implement a stack quite easily by storing its elements in a Python list. The\nlistclass already supports adding an element to the end with the append method,\nand removing the last element with the popmethod, so it is natural to align the top\nof the stack at the end of the list, as shown in Figure 6.2.\n0M BCDEFG KLA\n1 2 top\nFigure 6.2: Implementing a stack with a Python list, storing the top element in the\nrightmost cell.\nAlthough a programmer could directly use the listclass in place of a formal\nstack class, lists also include behaviors (e.g., adding or removing elements fromarbitrary positions) that would break the abstraction that the stack ADT represents.\nAlso, the terminology used by the listclass does not precisely align with traditional\nnomenclature for a stack ADT, in particular the distinction between append and\npush . Instead, we demonstrate how to use a list for internal storage while providing\na public interface consistent with a stack.\nThe Adapter Pattern\nThe adapter design pattern applies to any context where we effectively want to\nmodify an existing class so that its methods match those of a related, but different,\nclass or interface. One general way to apply the adapter pattern is to de\ufb01ne a new\nclass in such a way that it contains an instance of the existing class as a hidden\ufb01eld, and then to implement each method of the new class using methods of thishidden instance variable. By applying the adapter pattern in this way, we havecreated a new class that performs some of the same functions as an existing class,\nbut repackaged in a more convenient way. In the context of the stack ADT, we can\nadapt Python\u2019s list class using the correspondences shown in Table 6.1.\nStack Method\n Realization with Python list\nS.push(e)\n L.append(e)\nS.pop()\n L.pop()\nS.top()\n L[\u22121]\nS.is\nempty()\n len(L) == 0\nlen(S)\n len(L)\nTable 6.1: Realization of a stack Sas an adaptation of a Python list L.", "232 Chapter 6. Stacks, Queues, and Deques\nImplementing a Stack Using a Python List\nWe use the adapter design pattern to de\ufb01ne an ArrayStack class that uses an un-\nderlying Python list for storage. (We choose the name ArrayStack to emphasize\nthat the underlying storage is inherently array based.) One question that remains is\nwhat our code should do if a user calls pop ortopwhen the stack is empty. Our\nADT suggests that an error occurs, but we must decide what type of error. Whenpopis called on an empty Python list, it formally raises an IndexError , as lists are\nindex-based sequences. That choice does not seem appropriate for a stack, sincethere is no assumption of indices. Instead, we can de\ufb01ne a new exception class thatis more appropriate. Code Fragment 6.1 de\ufb01nes such an Empty class as a trivial\nsubclass of the Python Exception class.\nclassEmpty(Exception):\n\u201d\u201d\u201dError attempting to access an element from an empty container.\u201d\u201d\u201d\npass\nCode Fragment 6.1: De\ufb01nition for an Empty exception class.\nThe formal de\ufb01nition for our ArrayStack class is given in Code Fragment 6.2.\nThe constructor establishes the member self.\ndata as an initially empty Python list,\nfor internal storage. The rest of the public stack behaviors are implemented, using\nthe corresponding adaptation that was outlined in Table 6.1.\nExample Usage\nBelow, we present an example of the use of our ArrayStack class, mirroring the\noperations at the beginning of Example 6.3 on page 230.\nS = ArrayStack( ) #c o n t e n t s :[]\nS.push(5) #c o n t e n t s :[ 5 ]\nS.push(3) #c o n t e n t s :[ 5 ,3 ]\nprint(len(S)) # contents: [5, 3]; outputs 2\nprint(S.pop()) # contents: [5]; outputs 3\nprint(S.is\n empty()) # contents: [5]; outputs False\nprint(S.pop()) # contents: [ ]; outputs 5\nprint(S.is\n empty()) # contents: [ ]; outputs True\nS.push(7) #c o n t e n t s :[ 7 ]\nS.push(9) #c o n t e n t s :[ 7 ,9 ]\nprint(S.top()) # contents: [7, 9]; outputs 9\nS.push(4) #c o n t e n t s :[ 7 ,9 ,4 ]\nprint(len(S)) # contents: [7, 9, 4]; outputs 3\nprint(S.pop()) # contents: [7, 9]; outputs 4\nS.push(6) #c o n t e n t s :[ 7 ,9 ,6 ]", "6.1. Stacks 233\n1classArrayStack:\n2\u201d\u201d\u201dLIFO Stack implementation using a Python list as underlying storage.\u201d\u201d\u201d\n3\n4def\n init\n(self):\n5 \u201d\u201d\u201dCreate an empty stack.\u201d\u201d\u201d\n6 self.\ndata = [ ] # nonpublic list instance\n78def\nlen\n(self):\n9 \u201d\u201d\u201dReturn the number of elements in the stack.\u201d\u201d\u201d\n10 return len(self.\ndata)\n1112defis\nempty( self):\n13 \u201d\u201d\u201dReturn True if the stack is empty.\u201d\u201d\u201d\n14 return len(self.\ndata) == 0\n1516defpush(self,e ) :\n17 \u201d\u201d\u201dAdd element e to the top of the stack.\u201d\u201d\u201d\n18 self.\ndata.append(e) # new item stored at end of list\n1920deftop(self):\n21 \u201d\u201d\u201dReturn (but do not remove) the element at the top of the stack.\n2223 Raise Empty exception if the stack is empty.\n24 \u201d\u201d\u201d\n25 if self.is\nempty():\n26 raiseEmpty(\n Stack is empty\n )\n27 return self .\ndata[\u22121] # the last item in the list\n2829defpop(self):\n30 \u201d\u201d\u201dRemove and return the element from the top of the stack (i.e., LIFO).\n3132 Raise Empty exception if the stack is empty.\n33 \u201d\u201d\u201d\n34 if self.is\nempty():\n35 raiseEmpty(\n Stack is empty\n )\n36 return self .\ndata.pop( ) #r e m o v el a s ti t e mf r o ml i s t\nCode Fragment 6.2: Implementing a stack using a Python list as storage.", "234 Chapter 6. Stacks, Queues, and Deques\nAnalyzing the Array-Based Stack Implementation\nTable 6.2 shows the running times for our ArrayStack methods. The analysis di-\nrectly mirrors the analysis of the listclass given in Section 5.3. The implementa-\ntions for top,is\nempty ,a n dlenuse constant time in the worst case. The O(1)time\nforpush andpopareamortized bounds (see Section 5.3.2); a typical call to either\nof these methods uses constant time, but there is occasionally an O(n)-time worst\ncase, where nis the current number of elements in the stack, when an operation\ncauses the list to resize its internal array. The space usage for a stack is O(n).\nOperation\n Running Time\nS.push(e)\n O(1)\u2217\nS.pop()\n O(1)\u2217\nS.top()\n O(1)\nS.is\nempty()\n O(1)\nlen(S)\n O(1)\n\u2217amortized\nTable 6.2: Performance of our array-based stack implementation. The bounds for\npush andpop are amortized due to similar bounds for the listclass. The space\nusage is O(n),w h e r e nis the current number of elements in the stack.\nAvoiding Amortization by Reserving Capacity\nIn some contexts, there may be additional knowledge that suggests a maximum size\nthat a stack will reach. Our implementation of ArrayStack from Code Fragment 6.2\nbegins with an empty list and expands as needed. In the analysis of lists from\nSection 5.4.1, we emphasized that it is more ef\ufb01cient in practice to construct a listwith initial length nthan it is to start with an empty list and append nitems (even\nthough both approaches run in O(n)time).\nAs an alternate model for a stack, we might wish for the constructor to accept\na parameter specifying the maximum capacity of a stack and to initialize the\ndata\nmember to a list of that length. Implementing such a model requires signi\ufb01cantchanges relative to Code Fragment 6.2. The size of the stack would no longer besynonymous with the length of the list, and pushes and pops of the stack would notrequire changing the length of the list. Instead, we suggest maintaining a separateinteger as an instance variable that denotes the current number of elements in the\nstack. Details of such an implementation are left as Exercise C-6.17.", "6.1. Stacks 235\n6.1.3 Reversing Data Using a Stack\nAs a consequence of the LIFO protocol, a stack can be used as a general tool to\nreverse a data sequence. For example, if the values 1, 2, and 3 are pushed onto astack in that order, they will be popped from the stack in the order 3, 2, and then 1.\nThis idea can be applied in a variety of settings. For example, we might wish\nto print lines of a \ufb01le in reverse order in order to display a data set in decreasingorder rather than increasing order. This can be accomplished by reading each lineand pushing it onto a stack, and then writing the lines in the order they are popped.\nAn implementation of such a process is given in Code Fragment 6.3.\n1defreverse\n\ufb01le(\ufb01lename):\n2\u201d\u201d\u201dOverwrite given \ufb01le with its contents line-by-line reversed.\u201d\u201d\u201d\n3S = ArrayStack()\n4original = open(\ufb01lename)\n5forlineinoriginal:\n6 S.push(line.rstrip(\n \\n\n)) # we will re-insert newlines when writing\n7original.close()\n8\n9# now we overwrite with contents in LIFO order\n10 output = open(\ufb01lename,\n w\n)# reopening \ufb01le overwrites original\n11while not S.is\nempty():\n12 output.write(S.pop( ) +\n \\n\n)# re-insert newline characters\n13 output.close()\nCode Fragment 6.3: A function that reverses the order of lines in a \ufb01le.\nOne technical detail worth noting is that we intentionally strip trailing newlines\nfrom lines as they are read, and then re-insert newlines after each line when writing\nthe resulting \ufb01le. Our reason for doing this is to handle a special case in which the\noriginal \ufb01le does not have a trailing newline for the \ufb01nal line. If we exactly echoedthe lines read from the \ufb01le in reverse order, then the original last line would be fol-lowed (without newline) by the original second-to-last line. In our implementation,\nwe ensure that there will be a separating newline in the result.\nThe idea of using a stack to reverse a data set can be applied to other types of\nsequences. For example, Exercise R-6.5 explores the use of a stack to provide yet\nanother solution for reversing the contents of a Python list (a recursive solution forthis goal was discussed in Section 4.4.1). A more challenging task is to reverse\nthe order in which elements are stored within a stack. If we were to move them\nfrom one stack to another, they would be reversed, but if we were to then replacethem into the original stack, they would be reversed again, thereby reverting to theiroriginal order. Exercise C-6.18 explores a solution for this task.", "236 Chapter 6. Stacks, Queues, and Deques\n6.1.4 Matching Parentheses and HTML Tags\nIn this subsection, we explore two related applications of stacks, both of which\ninvolve testing for pairs of matching delimiters. In our \ufb01rst application, we considerarithmetic expressions that may contain various pairs of grouping symbols, such as\n\u2022Parentheses: \u201c (\u201da n d\u201c )\u201d\n\u2022Braces: \u201c {\u201da n d\u201c }\u201d\n\u2022Brackets: \u201c [\u201da n d\u201c ]\u201d\nEach opening symbol must match its corresponding closing symbol. For example, aleft bracket, \u201c[,\u201d must match a corresponding right bracket, \u201c],\u201d as in the expression[(5+x)-(y+z)]. The following examples further illustrate this concept:\n\u2022Correct: ( )(( )) {([( )])}\n\u2022Correct: ((( )(( )) {([( )])}))\n\u2022Incorrect: )(( )){([( )])}\n\u2022Incorrect: ({[] )}\n\u2022Incorrect: (\nWe leave the precise de\ufb01nition of a matching group of symbols to Exercise R-6.6.\nAn Algorithm for Matching Delimiters\nAn important task when processing arithmetic expressions is to make sure theirdelimiting symbols match up correctly. Code Fragment 6.4 presents a Python im-plementation of such an algorithm. A discussion of the code follows.\n1defis\nmatched(expr):\n2\u201d\u201d\u201dReturn True if all delimiters are properly match; False otherwise.\u201d\u201d\u201d\n3lefty =\n ({[\n # opening delimiters\n4righty =\n )}]\n # respective closing delims\n5S = ArrayStack()\n6forcinexpr:\n7 ifcinlefty:\n8 S.push(c) # push left delimiter on stack\n9 elifcinrighty:\n10 ifS.is\nempty():\n11 return False # nothing to match with\n12 ifrighty.index(c) != lefty.index(S.pop()):\n13 return False #m i s m a t c h e d\n14return S.is\nempty( ) # were all symbols matched?\nCode Fragment 6.4: Function for matching delimiters in an arithmetic expression.", "6.1. Stacks 237\nWe assume the input is a sequence of characters, such as\n [(5+x)-(y+z)]\n .\nWe perform a left-to-right scan of the original sequence, using a stack Sto facilitate\nthe matching of grouping symbols. Each time we encounter an opening symbol,\nwe push that symbol onto S, and each time we encounter a closing symbol, we pop\na symbol from the stack S(assuming Sis not empty), and check that these two\nsymbols form a valid pair. If we reach the end of the expression and the stack is\nempty, then the original expression was properly matched. Otherwise, there must\nbe an opening delimiter on the stack without a matching symbol.\nIf the length of the original expression is n, the algorithm will make at most\nncalls to push andncalls to pop. Those calls run in a total of O(n)time, even con-\nsidering the amortized nature of the O(1)time bound for those methods. Given that\nour selection of possible delimiters, ({[, has constant size, auxiliary tests such as\nci nl e f t y andrighty.index(c) each run in O(1)time. Combining these operations,\nthe matching algorithm on a sequence of length nruns in O(n)time.\nMatching Tags in a Markup Language\nAnother application of matching delimiters is in the validation of markup languagessuch as HTML or XML. HTML is the standard format for hyperlinked documents\non the Internet and XML is an extensible markup language used for a variety of\nstructured data sets. We show a sample HTML document and a possible renderingin Figure 6.3.\n<body>\n<center>\n<h1> The Little Boat </h1>\n</center>\n<p> The storm tossed the little\nboat like a cheap sneaker in an\nold washing machine. The three\ndrunken fishermen were used tosuch treatment, of course, but\nnot the tree salesman, who even as\na stowaway now felt that hehad overpaid for the voyage. </p>\n<ol>\n<li> Will the salesman die? </li>\n<li> What color is the boat? </li>\n<li> And what about Naomi? </li>\n</ol>\n</body>The Little Boat\nThe storm tossed the little boat\nlike a cheap sneaker in anold washing machine. The threedrunken \ufb01shermen were used tosuch treatment, of course, but not\nthe tree salesman, who even as\na stowaway now felt that he hadoverpaid for the voyage.\n1. Will the salesman die?2. What color is the boat?3. And what about Naomi?\n(a) (b)\nFigure 6.3: Illustrating HTML tags. (a) An HTML document; (b) its rendering.", "238 Chapter 6. Stacks, Queues, and Deques\nIn an HTML document, portions of text are delimited by HTML tags .As i m p l e\nopening HTML tag has the form \u201c <name>\u201d and the corresponding closing tag has\nthe form \u201c </name>\u201d. For example, we see the <body> tag on the \ufb01rst line of\nFigure 6.3(a), and the matching </body> tag at the close of that document. Other\ncommonly used HTML tags that are used in this example include:\n\u2022body : document body\n\u2022h1: section header\n\u2022center : center justify\n\u2022p: paragraph\n\u2022ol: numbered (ordered) list\n\u2022li: list item\nIdeally, an HTML document should have matching tags, although most browsers\ntolerate a certain number of mismatching tags. In Code Fragment 6.5, we give a\nPython function that matches tags in a string representing an HTML document. We\nmake a left-to-right pass through the raw string, using index jto track our progress\nand the \ufb01ndmethod of the strclass to locate the\n <\nand\n >\ncharacters that de\ufb01ne\nthe tags. Opening tags are pushed onto the stack, and matched against closing tagsas they are popped from the stack, just as we did when matching delimiters in Code\nFragment 6.4. By similar analysis, this algorithm runs in O(n)time, where nis the\nnumber of characters in the raw HTML source.\n1defis\nmatched\n html(raw):\n2\u201d\u201d\u201dReturn True if all HTML tags are properly match; False otherwise.\u201d\u201d\u201d\n3S = ArrayStack()\n4j = raw.\ufb01nd(\n <\n) #\ufb01 n d\ufb01 r s t\u2019 <\u2019 character (if any)\n5while j! =\u22121:\n6 k = raw.\ufb01nd(\n >\n,j + 1 ) # \ufb01nd next \u2019 >\u2019 character\n7 ifk= = \u22121:\n8 return False # invalid tag\n9 tag = raw[j+1:k] # strip away <>\n10 if not tag.startswith(\n /\n): # this is opening tag\n11 S.push(tag)\n12 else: # this is closing tag\n13 ifS.is\nempty():\n14 return False # nothing to match with\n15 iftag[1:] != S.pop():\n16 return False # mismatched delimiter\n17 j = raw.\ufb01nd(\n <\n,k + 1 ) # \ufb01nd next \u2019 <\u2019 character (if any)\n18return S.is\nempty( ) # were all opening tags matched?\nCode Fragment 6.5: Function for testing if an HTML document has matching tags.", "6.2. Queues 239\n6.2 Queues\nAnother fundamental data structure is the queue . It is a close \u201ccousin\u201d of the stack,\nas a queue is a collection of objects that are inserted and removed according to the\n\ufb01rst-in, \ufb01rst-out (FIFO ) principle. That is, elements can be inserted at any time,\nbut only the element that has been in the queue the longest can be next removed.\nWe usually say that elements enter a queue at the back and are removed from\nthe front. A metaphor for this terminology is a line of people waiting to get on anamusement park ride. People waiting for such a ride enter at the back of the line\nand get on the ride from the front of the line. There are many other applications\nof queues (see Figure 6.4). Stores, theaters, reservation centers, and other similarservices typically process customer requests according to the FIFO principle. Aqueue would therefore be a logical choice for a data structure to handle calls to acustomer service center, or a wait-list at a restaurant. FIFO queues are also used by\nmany computing devices, such as a networked printer, or a Web server responding\nto requests.\nTickets\n(a)\nCall Center\nCall Queue\n(b)\nFigure 6.4: Real-world examples of a \ufb01rst-in, \ufb01rst-out queue. (a) People waiting in\nline to purchase tickets; (b) phone calls being routed to a customer service center.", "240 Chapter 6. Stacks, Queues, and Deques\n6.2.1 The Queue Abstract Data Type\nFormally, the queue abstract data type de\ufb01nes a collection that keeps objects in a\nsequence, where element access and deletion are restricted to the \ufb01rst element in\nthe queue, and element insertion is restricted to the back of the sequence. Thisrestriction enforces the rule that items are inserted and deleted in a queue accord-\ning to the \ufb01rst-in, \ufb01rst-out (FIFO) principle. The queue abstract data type (ADT)\nsupports the following two fundamental methods for a queue Q:\nQ.enqueue(e) :Add element eto the back of queue Q.\nQ.dequeue() :Remove and return the \ufb01rst element from queue Q;\nan error occurs if the queue is empty.\nThe queue ADT also includes the following supporting methods (with \ufb01rst being\nanalogous to the stack\u2019s topmethod):\nQ.\ufb01rst() :Return a reference to the element at the front of queue Q,\nwithout removing it; an error occurs if the queue is empty.\nQ.is\nempty() :Return True if queue Qdoes not contain any elements.\nlen(Q): Return the number of elements in queue Q; in Python,\nwe implement this with the special method\n len\n .\nBy convention, we assume that a newly created queue is empty, and that there\nis no a priori bound on the capacity of the queue. Elements added to the queue can\nhave arbitrary type.\nExample 6.4: The following table shows a series of queue operations and their\neffects on an initially empty queue Qof integers.\nOperation\n Return Value\n \ufb01rst\u2190Q\u2190last\nQ.enqueue(5)\n \u2013\n [5]\nQ.enqueue(3)\n \u2013\n [5, 3]\nlen(Q)\n 2\n [5, 3]\nQ.dequeue()\n 5\n [3]\nQ.is\nempty()\n False\n [3]\nQ.dequeue()\n 3\n []\nQ.is\nempty()\n True\n []\nQ.dequeue()\n \u201cerror\u201d\n []\nQ.enqueue(7)\n \u2013\n [7]\nQ.enqueue(9)\n \u2013\n [7, 9]\nQ.\ufb01rst()\n 7\n [7, 9]\nQ.enqueue(4)\n \u2013\n [7, 9, 4]\nlen(Q)\n 3\n [7, 9, 4]\nQ.dequeue()\n 7\n [9, 4]\n", "6.2. Queues 241\n6.2.2 Array-Based Queue Implementation\nFor the stack ADT, we created a very simple adapter class that used a Python list\nas the underlying storage. It may be very tempting to use a similar approach forsupporting the queue ADT. We could enqueue element eby calling append(e) to\nadd it to the end of the list. We could use the syntax pop(0) , as opposed to pop() ,\nto intentionally remove the \ufb01rstelement from the list when dequeuing.\nAs easy as this would be to implement, it is tragically inef\ufb01cient. As we dis-\ncussed in Section 5.4.1, when pop is called on a list with a non-default index, a\nloop is executed to shift all elements beyond the speci\ufb01ed index to the left, so as to\n\ufb01ll the hole in the sequence caused by the pop. Therefore, a call to pop(0) always\ncauses the worst-case behavior of \u0398(n)time.\nWe can improve on the above strategy by avoiding the call to pop(0) entirely.\nWe can replace the dequeued entry in the array with a reference to None ,a n dm a i n -\ntain an explicit variable fto store the index of the element that is currently at the\nfront of the queue. Such an algorithm for dequeue would run in O(1)time. After\nseveral dequeue operations, this approach might lead to the con\ufb01guration portrayed\nin Figure 6.5.\n0EFG KL M\n12 f\nFigure 6.5: Allowing the front of the queue to drift away from index 0.\nUnfortunately, there remains a drawback to the revised approach. In the case\nof a stack, the length of the list was precisely equal to the size of the stack (even if\nthe underlying array for the list was slightly larger). With the queue design that weare considering, the situation is worse. We can build a queue that has relatively few\nelements, yet which are stored in an arbitrarily large list. This occurs, for example,\nif we repeatedly enqueue a new element and then dequeue another (allowing thefront to drift rightward). Over time, the size of the underlying list would grow toO(m)where mis the total number of enqueue operations since the creation of the\nqueue, rather than the current number of elements in the queue.\nThis design would have detrimental consequences in applications in which\nqueues have relatively modest size, but which are used for long periods of time.For example, the wait-list for a restaurant might never have more than 30 entriesat one time, but over the course of a day (or a week), the overall number of entries\nwould be signi\ufb01cantly larger.", "242 Chapter 6. Stacks, Queues, and Deques\nUsing an Array Circularly\nIn developing a more robust queue implementation, we allow the front of the queue\nto drift rightward, and we allow the contents of the queue to \u201cwrap around\u201d the end\nof an underlying array. We assume that our underlying array has \ufb01xed length N\nthat is greater that the actual number of elements in the queue. New elementsare enqueued toward the \u201cend\u201d of the current queue, progressing from the front to\nindex N\u22121 and continuing at index 0, then 1. Figure 6.6 illustrates such a queue\nwith \ufb01rst element Eand last element M.\n0MF GH IJ K L E\n12 f N\u22121\nFigure 6.6: Modeling a queue with a circular array that wraps around the end.\nImplementing this circular view is not dif\ufb01cult. When we dequeue an element\nand want to \u201cadvance\u201d the front index, we use the arithmetic f=( f+1 )%N .R e -\nc a l lt h a tt h e %operator in Python denotes the modulo operator, which is computed\nby taking the remainder after an integral division. For example, 14divided by 3has\na quotient of 4with remainder 2,t h a ti s ,14\n3=42\n3. So in Python, 14 // 3 evaluates\nto the quotient 4, while 14 % 3 evaluates to the remainder 2. The modulo operator\nis ideal for treating an array circularly. As a concrete example, if we have a list\nof length 10, and a front index 7, we can advance the front by formally computing(7+1) % 10 , which is simply 8, as 8 divided by 10 is 0 with a remainder of 8.\nSimilarly, advancing index 8 results in index 9. But when we advance from index 9(the last one in the array), we compute (9+1) % 10 , which evaluates to index 0 (as\n10 divided by 10 has a remainder of zero).\nA Python Queue Implementation\nA complete implementation of a queue ADT using a Python list in circular fashionis presented in Code Fragments 6.6 and 6.7. Internally, the queue class maintainsthe following three instance variables:\ndata :is a reference to a listinstance with a \ufb01xed capacity.\nsize:is an integer representing the current number of elements storedin the queue (as opposed to the length of the\ndata list).\nfront :is an integer that represents the index within\n data of the \ufb01rst\nelement of the queue (assuming the queue is not empty).\nWe initially reserve a list of moderate size for storing data, although the queue\nformally has size zero. As a technicality, we initialize the\n front index to zero.\nWhen front ordequeue are called with no elements in the queue, we raise an\ninstance of the Empty exception, de\ufb01ned in Code Fragment 6.1 for our stack.", "6.2. Queues 243\n1classArrayQueue:\n2\u201d\u201d\u201dFIFO queue implementation using a Python list as underlying storage.\u201d\u201d\u201d\n3DEFAULT\n CAPACITY = 10 # moderate capacity for all new queues\n4\n5def\n init\n(self):\n6 \u201d\u201d\u201dCreate an empty queue.\u201d\u201d\u201d\n7 self.\ndata = [ None]\nArrayQueue.DEFAULT\n CAPACITY\n8 self.\nsize = 0\n9 self.\nfront = 0\n1011def\nlen\n(self):\n12 \u201d\u201d\u201dReturn the number of elements in the queue.\u201d\u201d\u201d\n13 return self .\nsize\n14\n15defis\nempty( self):\n16 \u201d\u201d\u201dReturn True if the queue is empty.\u201d\u201d\u201d\n17 return self .\nsize == 0\n1819def\ufb01rst(self):\n20 \u201d\u201d\u201dReturn (but do not remove) the element at the front of the queue.\n21\n22 Raise Empty exception if the queue is empty.\n23 \u201d\u201d\u201d\n24 if self.is\nempty():\n25 raiseEmpty(\n Queue is empty\n )\n26 return self .\ndata[self.\nfront]\n2728defdequeue( self):\n29 \u201d\u201d\u201dRemove and return the \ufb01rst element of the queue (i.e., FIFO).\n3031 Raise Empty exception if the queue is empty.\n32 \u201d\u201d\u201d\n33 if self.is\nempty():\n34 raiseEmpty(\n Queue is empty\n )\n35 answer = self.\ndata[self.\nfront]\n36 self.\ndata[self.\nfront] = None # help garbage collection\n37 self.\nfront = ( self.\nfront + 1) % len( self.\ndata)\n38 self.\nsize\u2212=1\n39 return answer\nCode Fragment 6.6: Array-based implementation of a queue (continued in Code\nFragment 6.7).", "244 Chapter 6. Stacks, Queues, and Deques\n40defenqueue( self,e ) :\n41 \u201d\u201d\u201dAdd an element to the back of queue.\u201d\u201d\u201d\n42 if self.\nsize == len( self.\ndata):\n43 self.\nresize(2\n len(self.data)) # double the array size\n44 avail = ( self.\nfront + self.\nsize) % len( self.\ndata)\n45 self.\ndata[avail] = e\n46 self.\nsize += 1\n47\n48def\nresize(self,c a p ) : # we assume cap >= len(self)\n49 \u201d\u201d\u201dResize to a new list of capacity >= len(self).\u201d\u201d\u201d\n50 old =self.\ndata # keep track of existing list\n51 self.\ndata = [ None]\ncap # allocate list with new capacity\n52 walk = self.\nfront\n53 forkinrange(self.\nsize): # only consider existing elements\n54 self.\ndata[k] = old[walk] # intentionally shift indices\n55 walk = (1 + walk) % len(old) # use old size as modulus\n56 self.\nfront = 0 # front has been realigned\nCode Fragment 6.7: Array-based implementation of a queue (continued from Code\nFragment 6.6).\nThe implementation of\n len\n andis\nempty are trivial, given knowledge of\nthe size. The implementation of the front method is also simple, as the\n front\nindex tells us precisely where the desired element is located within the\n data list,\nassuming that list is not empty.\nAdding and Removing Elements\nThe goal of the enqueue method is to add a new element to the back of the queue.\nWe need to determine the proper index at which to place the new element. Althoughwe do not explicitly maintain an instance variable for the back of the queue, wecompute the location of the next opening based on the formula:\navail = (self.\nfront + self.\n size) % len(self.\n data)\nNote that we are using the size of the queue as it exists prior to the addition of the\nnew element. For example, consider a queue with capacity 10, current size 3, and\n\ufb01rst element at index 5. The three elements of such a queue are stored at indices 5,\n6, and 7. The new element should be placed at index (front +size )=8. In a case\nwith wrap-around, the use of the modular arithmetic achieves the desired circularsemantics. For example, if our hypothetical queue had 3 elements with the \ufb01rst atindex 8, our computation of (8+3) % 10 evaluates to 1, which is perfect since the\nthree existing elements occupy indices 8, 9, and 0.", "6.2. Queues 245\nWhen the dequeue method is called, the current value of self.\nfront designates\nthe index of the value that is to be removed and returned. We keep a local refer-\nence to the element that will be returned, setting answer = self.\n data[self.\n front]\njust prior to removing the reference to that object from the list, with the assignmentself.\ndata[self.\n front] = None . Our reason for the assignment to None relates to\nPython\u2019s mechanism for reclaiming unused space. Internally, Python maintains a\ncount of the number of references that exist to each object. If that count reaches\nzero, the object is effectively inaccessible, thus the system may reclaim that mem-ory for future use. (For more details, see Section 15.1.2.) Since we are no longerresponsible for storing a dequeued element, we remove the reference to it from our\nlist so as to reduce that element\u2019s reference count.\nThe second signi\ufb01cant responsibility of the dequeue method is to update the\nvalue of\nfront to re\ufb02ect the removal of the element, and the presumed promotion\nof the second element to become the new \ufb01rst. In most cases, we simply want\nto increment the index by one, but because of the possibility of a wrap-around\ncon\ufb01guration, we rely on modular arithmetic as originally described on page 242.\nResizing the Queue\nWhen enqueue is called at a time when the size of the queue equals the size of the\nunderlying list, we rely on a standard technique of doubling the storage capacity of\nthe underlying list. In this way, our approach is similar to the one used when we\nimplemented a DynamicArray in Section 5.3.1.\nHowever, more care is needed in the queue\u2019s\n resize utility than was needed in\nthe corresponding method of the DynamicArray class. After creating a temporary\nreference to the old list of values, we allocate a new list that is twice the size and\ncopy references from the old list to the new list. While transferring the contents, we\nintentionally realign the front of the queue with index 0 in the new array, as shown\nin Figure 6.7. This realignment is not purely cosmetic. Since the modular arith-metic depends on the size of the array, our state would be \ufb02awed had we transferredeach element to its same index in the new array.\nEG H IJ K\nEFG HI JKFf\n12 f=0\nFigure 6.7: Resizing the queue, while realigning the front element with index 0.", "246 Chapter 6. Stacks, Queues, and Deques\nShrinking the Underlying Array\nA desirable property of a queue implementation is to have its space usage be \u0398(n)\nwhere nis the current number of elements in the queue. Our ArrayQueue imple-\nmentation, as given in Code Fragments 6.6 and 6.7, does not have this property.\nIt expands the underlying array when enqueue is called with the queue at full ca-\npacity, but the dequeue implementation never shrinks the underlying array. As a\nconsequence, the capacity of the underlying array is proportional to the maximumnumber of elements that have ever been stored in the queue, not the current numberof elements.\nWe discussed this very issue on page 200, in the context of dynamic arrays, and\nin subsequent Exercises C-5.16 through C-5.20 of that chapter. A robust approachis to reduce the array to half of its current size, whenever the number of elementsstored in it falls below one fourth of its capacity. We can implement this strategy by\nadding the following two lines of code in our dequeue method, just after reducing\nself.\nsizeat line 38 of Code Fragment 6.6, to re\ufb02ect the loss of an element.\nif0<self.\nsize<len(self.\ndata) // 4:\nself.\nresize(len( self.\ndata) // 2)\nAnalyzing the Array-Based Queue Implementation\nTable 6.3 describes the performance of our array-based implementation of the queue\nADT, assuming the improvement described above for occasionally shrinking the\nsize of the array. With the exception of the\n resize utility, all of the methods rely\non a constant number of statements involving arithmetic operations, comparisons,and assignments. Therefore, each method runs in worst-case O(1)time, except\nforenqueue anddequeue , which have amortized bounds of O(1)time, for reasons\nsimilar to those given in Section 5.3.\nOperation\n Running Time\nQ.enqueue(e)\n O(1)\u2217\nQ.dequeue()\n O(1)\u2217\nQ.\ufb01rst()\n O(1)\nQ.is\nempty()\n O(1)\nlen(Q)\n O(1)\n\u2217amortized\nTable 6.3: Performance of an array-based implementation of a queue. The bounds\nforenqueue anddequeue are amortized due to the resizing of the array. The space\nusage is O(n),w h e r e nis the current number of elements in the queue.", "6.3. Double-Ended Queues 247\n6.3 Double-Ended Queues\nWe next consider a queue-like data structure that supports insertion and deletion\nat both the front and the back of the queue. Such a structure is called a double-\nended queue ,o rdeque , which is usually pronounced \u201cdeck\u201d to avoid confusion\nwith the dequeue method of the regular queue ADT, which is pronounced like the\nabbreviation \u201cD.Q.\u201d\nThe deque abstract data type is more general than both the stack and the queue\nADTs. The extra generality can be useful in some applications. For example, wedescribed a restaurant using a queue to maintain a waitlist. Occassionally, the \ufb01rstperson might be removed from the queue only to \ufb01nd that a table was not available;typically, the restaurant will re-insert the person at the \ufb01rstposition in the queue. It\nmay also be that a customer at the end of the queue may grow impatient and leave\nthe restaurant. (We will need an even more general data structure if we want to\nmodel customers leaving the queue from other positions.)\n6.3.1 The Deque Abstract Data Type\nTo provide a symmetrical abstraction, the deque ADT is de\ufb01ned so that deque D\nsupports the following methods:\nD.add\n \ufb01rst(e) :Add element eto the front of deque D.\nD.add\n last(e) :Add element eto the back of deque D.\nD.delete\n \ufb01rst(): Remove and return the \ufb01rst element from deque D;\nan error occurs if the deque is empty.\nD.delete\n last() :Remove and return the last element from deque D;\nan error occurs if the deque is empty.\nAdditionally, the deque ADT will include the following accessors:\nD.\ufb01rst() :Return (but do not remove) the \ufb01rst element of deque D;\nan error occurs if the deque is empty.\nD.last() :Return (but do not remove) the last element of deque D;\nan error occurs if the deque is empty.\nD.is\nempty() :Return True if deque Ddoes not contain any elements.\nlen(D): Return the number of elements in deque D; in Python,\nwe implement this with the special method\n len\n .", "248 Chapter 6. Stacks, Queues, and Deques\nExample 6.5: The following table shows a series of operations and their effects\non an initially empty deque Dof integers.\nOperation\n Return Value\n Deque\nD.add\n last(5)\n \u2013\n [5]\nD.add\n \ufb01rst(3)\n \u2013\n [3, 5]\nD.add\n \ufb01rst(7)\n \u2013\n [ 7 ,3 ,5 ]\nD.\ufb01rst()\n 7\n [ 7 ,3 ,5 ]\nD.delete\n last()\n 5\n [7, 3]\nlen(D)\n 2\n [7, 3]\nD.delete\n last()\n 3\n [7]\nD.delete\n last()\n 7\n []\nD.add\n \ufb01rst(6)\n \u2013\n [6]\nD.last()\n 6\n [6]\nD.add\n \ufb01rst(8)\n \u2013\n [8, 6]\nD.is\nempty()\n False\n [8, 6]\nD.last()\n 6\n [8, 6]\n6.3.2 Implementing a Deque with a Circular Array\nWe can implement the deque ADT in much the same way as the ArrayQueue class\nprovided in Code Fragments 6.6 and 6.7 of Section 6.2.2 (so much so that we leave\nthe details of an ArrayDeque implementation to Exercise P-6.32). We recommend\nmaintaining the same three instance variables:\n data,\nsize,a n d\n front . Whenever\nwe need to know the index of the back of the deque, or the \ufb01rst available slotbeyond the back of the deque, we use modular arithmetic for the computation. Forexample, our implementation of the last() method uses the index\nback = (self.\nfront + self.\n size\u22121) % len(self.\n data)\nOur implementation of the ArrayDeque.add\n lastmethod is essentially the same\nas that for ArrayQueue.enqueue , including the reliance on a\n resize utility. Like-\nwise, the implementation of the ArrayDeque.delete\n \ufb01rst method is the same as\nArrayQueue.dequeue . Implementations of add\n\ufb01rst anddelete\n lastuse similar\ntechniques. One subtlety is that a call to add\n\ufb01rst may need to wrap around the\nbeginning of the array, so we rely on modular arithmetic to circularly decrement\nthe index, as\nself.\nfront = (self.\n front\u22121) % len(self.\n data) # cyclic shift\nThe ef\ufb01ciency of an ArrayDeque i ss i m i l a rt ot h a to fa n ArrayQueue, with all\noperations having O(1)running time, but with that bound being amortized for op-\nerations that may change the size of the underlying list.", "6.3. Double-Ended Queues 249\n6.3.3 Deques in the Python Collections Module\nAn implementation of a deque class is available in Python\u2019s standard collections\nmodule. A summary of the most commonly used behaviors of the collections.deque\nclass is given in Table 6.4. It uses more asymmetric nomenclature than our ADT.\nOur Deque ADT\n collections.deque\n Description\nlen(D)\n len(D)\n number of elements\nD.add\n \ufb01rst()\n D.appendleft()\n add to beginning\nD.add\n last()\n D.append()\n add to end\nD.delete\n \ufb01rst()\n D.popleft()\n remove from beginning\nD.delete\n last()\n D.pop()\n remove from end\nD.\ufb01rst()\n D[0]\n access \ufb01rst element\nD.last()\n D[\u22121]\n access last element\nD[j]\n access arbitrary entry by index\nD[j] = val\n modify arbitrary entry by index\nD.clear()\n clear all contents\nD.rotate(k)\n circularly shift rightward k steps\nD.remove(e)\n remove \ufb01rst matching element\nD.count(e)\n count number of matches for e\nTable 6.4: Comparison of our deque ADT and the collections.deque class.\nThecollections.deque interface was chosen to be consistent with established\nnaming conventions of Python\u2019s listclass, for which append andpopare presumed\nto act at the end of the list. Therefore, appendleft andpopleft designate an opera-\ntion at the beginning of the list. The library deque also mimics a list in that it is an\nindexed sequence, allowing arbitrary access or modi\ufb01cation using the D[j]syntax.\nThe library deque constructor also supports an optional maxlen parameter to\nforce a \ufb01xed-length deque. However, if a call to append at either end is invoked\nwhen the deque is full, it does not throw an error; instead, it causes one element tobe dropped from the opposite side. That is, calling appendleft when the deque is\nfull causes an implicit popfrom the right side to make room for the new element.\nThe current Python distribution implements collections.deque with a hybrid ap-\nproach that uses aspects of circular arrays, but organized into blocks that are them-selves organized in a doubly linked list (a data structure that we will introduce inthe next chapter). The deque class is formally documented to guarantee O(1)-time\noperations at either end, but O(n)-time worst-case operations when using index\nnotation near the middle of the deque.", "250 Chapter 6. Stacks, Queues, and Deques\n6.4 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-6.1 What values are returned during the following series of stack operations, if\nexecuted upon an initially empty stack? push(5) ,push(3) ,pop() ,push(2) ,\npush(8) ,pop() ,pop() ,push(9) ,push(1) ,pop() ,push(7) ,push(6) ,pop() ,\npop() ,push(4) ,pop() ,pop() .\nR-6.2 Suppose an initially empty stack Shas executed a total of 25 push opera-\ntions, 12 topoperations, and 10 popoperations, 3 of which raised Empty\nerrors that were caught and ignored. What is the current size of S?\nR-6.3 Implement a function with signature transfer(S, T) that transfers all ele-\nments from stack Sonto stack T, so that the element that starts at the top\nofSis the \ufb01rst to be inserted onto T, and the element at the bottom of S\nends up at the top of T.\nR-6.4 Give a recursive method for removing all the elements from a stack.\nR-6.5 Implement a function that reverses a list of elements by pushing them onto\na stack in one order, and writing them back to the list in reversed order.\nR-6.6 Give a precise and complete de\ufb01nition of the concept of matching forgrouping symbols in an arithmetic expression. Your de\ufb01nition may be\nrecursive.\nR-6.7 What values are returned during the following sequence of queue opera-\ntions, if executed on an initially empty queue? enqueue(5) ,enqueue(3) ,\ndequeue() ,enqueue(2) ,enqueue(8) ,dequeue() ,dequeue() ,enqueue(9) ,\nenqueue(1) ,dequeue() ,enqueue(7) ,enqueue(6) ,dequeue() ,dequeue() ,\nenqueue(4) ,dequeue() ,dequeue() .\nR-\n6.8Suppose an initially empty queue Qhas executed a total of 32 enqueue\noperations, 10 \ufb01rst operations, and 15 dequeue operations, 5 of which\nraised Empty errors that were caught and ignored. What is the current\nsize of Q?\nR-6.9 Had the queue of the previous problem been an instance of ArrayQueue\nthat used an initial array of capacity 30, and had its size never been greaterthan 30, what would be the \ufb01nal value of the\nfront instance variable?\nR-6.10 Consider what happens if the loop in the ArrayQueue.\n resize method at\nlines 53\u201355 of Code Fragment 6.7 had been implemented as:\nforkinrange(self.\nsize):\nself.\ndata[k] = old[k] # rather than old[walk]\nGive a clear explanation of what could go wrong.", "6.4. Exercises 251\nR-6.11 Give a simple adapter that implements our queue ADT while using a\ncollections.deque instance for storage.\nR-6.12 What values are returned during the following sequence of deque ADT op-\nerations, on initially empty deque? add\n\ufb01rst(4) ,add\nlast(8) ,add\nlast(9) ,\nadd\n\ufb01rst(5) ,back() ,delete\n \ufb01rst() ,delete\n last() ,add\nlast(7) ,\ufb01rst() ,\nlast() ,add\nlast(6) ,delete\n \ufb01rst() ,delete\n \ufb01rst() .\nR-6.13 Suppose you have a deque Dcontaining the numbers (1,2,3,4,5,6,7,8),\nin this order. Suppose further that you have an initially empty queue Q.\nGive a code fragment that uses only DandQ(and no other variables) and\nresults in Dstoring the elements in the order (1,2,3,5,4,6,7,8).\nR-6.14 Repeat the previous problem using the deque Dand an initially empty\nstack S.\nCreativity\nC-6.15 Suppose Alice has picked three distinct integers and placed them into astack Sin random order. Write a short, straight-line piece of pseudo-code\n(with no loops or recursion) that uses only one comparison and only one\nvariable x, yet that results in variable xstoring the largest of Alice\u2019s three\nintegers with probability 2 /3. Argue why your method is correct.\nC-6.16 Modify the ArrayStack implementation so that the stack\u2019s capacity is lim-\nited to maxlen elements, where maxlen is an optional parameter to the\nconstructor (that defaults to None ). Ifpush is called when the stack is at\nfull capacity, throw a Fullexception (de\ufb01ned similarly to Empty ).\nC-6.17 In the previous exercise, we assume that the underlying list is initially\nempty. Redo that exercise, this time preallocating an underlying list withlength equal to the stack\u2019s maximum capacity.\nC-6.18 Show how to use the transfer function, described in Exercise R-6.3, and\ntwo temporary stacks, to replace the contents of a given stack Swith those\nsame elements, but in reversed order.\nC-6.19 In Code Fragment 6.5 we assume that opening tags in HTML have form\n<name>, as with <li>. More generally, HTML allows optional attributes\nto be expressed as part of an opening tag. The general form used is\n<name attribute1=\"value1\" attribute2=\"value2\"> ; for example,\na table can be given a border and additional padding by using an opening\ntag of <table border=\"3\" cellpadding=\"5\"> . Modify Code Frag-\nment 6.5 so that it can properly match tags, even when an opening tag\nmay include one or more such attributes.\nC-6.20 Describe a nonrecursive algorithm for enumerating all permutations of the\nnumbers {1,2,..., n}using an explicit stack.", "252 Chapter 6. Stacks, Queues, and Deques\nC-6.21 Show how to use a stack Sand a queue Qto generate all possible subsets\nof an n-element set Tnonrecursively.\nC-6.22 Post\ufb01x notation is an unambiguous way of writing an arithmetic expres-\nsion without parentheses. It is de\ufb01ned so that if \u201c (exp1)op(exp2)\u201di sa\nnormal, fully parenthesized expression whose operation is op, the post\ufb01x\nversion of this is \u201c pexp1pexp2op\u201d, where pexp1is the post\ufb01x version of\nexp1andpexp2is the post\ufb01x version of exp2. The post\ufb01x version of a sin-\ngle number or variable is just that number or variable. For example, the\npost\ufb01x version of \u201c ((5+2)\u2217(8\u22123))/4\u201d is \u201c5 2 +83\u2212\u2217 4/\u201d. Describe\na nonrecursive way of evaluating an expression in post\ufb01x notation.\nC-6.23 Suppose you have three nonempty stacks R,S,a n d T. Describe a sequence\nof operations that results in Sstoring all elements originally in Tbelow all\nofS\u2019s original elements, with both sets of those elements in their original\norder. The \ufb01nal con\ufb01guration for Rshould be the same as its original\ncon\ufb01guration. For example, if R=[1,2,3],S=[4,5],a n d T=[6,7,8,9],\nthe \ufb01nal con\ufb01guration should have R=[1,2,3]andS=[6,7,8,9,4,5].\nC-6.24 Describe how to implement the stack ADT using a single queue as aninstance variable, and only constant additional local memory within themethod bodies. What is the running time of the push() ,pop() ,a n dtop()\nm\nethods for your design?\nC-6.25 Describe how to implement the queue ADT using two stacks as instancevariables, such that all queue operations execute in amortized O(1)time.\nGive a formal proof of the amortized bound.\nC-6.26 Describe how to implement the double-ended queue ADT using two stacksas instance variables. What are the running times of the methods?\nC-6.27 Suppose you have a stack Scontaining nelements and a queue Qthat is\ninitially empty. Describe how you can use Qto scan Sto see if it contains a\ncertain element x, with the additional constraint that your algorithm must\nreturn the elements back to Sin their original order. You may only use S,\nQ, and a constant number of other variables.\nC-6.28 Modify the ArrayQueue implementation so that the queue\u2019s capacity is\nlimited to maxlen elements, where maxlen is an optional parameter to the\nconstructor (that defaults to None ). Ifenqueue is called when the queue\nis at full capacity, throw a Fullexception (de\ufb01ned similarly to Empty ).\nC-6.29 In certain applications of the queue ADT, it is common to repeatedly\ndequeue an element, process it in some way, and then immediately en-\nqueue the same element. Modify the ArrayQueue implementation to in-\nclude a rotate() method that has semantics identical to the combina-\ntion,Q.enqueue(Q.dequeue()) . However, your implementation should\nbe more ef\ufb01cient than making two separate calls (for example, because\nthere is no need to modify\nsize).", "6.4. Exercises 253\nC-6.30 Alice has two queues, QandR, which can store integers. Bob gives Alice\n50 odd integers and 50 even integers and insists that she store all 100\nintegers in QandR. They then play a game where Bob picks QorR\nat random and then applies the round-robin scheduler, described in thechapter, to the chosen queue a random number of times. If the last numberto be processed at the end of this game was odd, Bob wins. Otherwise,\nAlice wins. How can Alice allocate integers to queues to optimize her\nchances of winning? What is her chance of winning?\nC-6.31 Suppose Bob has four cows that he wants to take across a bridge, but onlyone yoke, which can hold up to two cows, side by side, tied to the yoke.The yoke is too heavy for him to carry across the bridge, but he can tie(and untie) cows to it in no time at all. Of his four cows, Mazie can crossthe bridge in 2 minutes, Daisy can cross it in 4 minutes, Crazy can cross\nit in 10 minutes, and Lazy can cross it in 20 minutes. Of course, when\ntwo cows are tied to the yoke, they must go at the speed of the slower cow.Describe how Bob can get all his cows across the bridge in 34 minutes.\nProjects\nP-6.32 Give a complete ArrayDeque implementation of the double-ended queue\nADT as sketched in Section 6.3.2.\nP-6.33 Give an array-based implementation of a double-ended queue supportingall of the public behaviors shown in Table 6.4 for the collections.deque\nclass, including use of the maxlen optional parameter. When a length-\nlimited deque is full, provide semantics similar to the collections.deque\nclass, whereby a call to insert an element on one end of a deque causes anelement to be lost from the opposite side.\nP-6.34 Implement a program that can input an expression in post\ufb01x notation (see\nExercise C-6.22) and output its value.\nP-6.35 The introduction of Section 6.1 notes that stacks are often used to provide\u201cundo\u201d support in applications like a Web browser or text editor. While\nsupport for undo can be implemented with an unbounded stack, manyapplications provide only limited support for such an undo history, with a\n\ufb01xed-capacity stack. When push is invoked with the stack at full capacity,\nrather than throwing a Fullexception (as described in Exercise C-6.16),\na more typical semantic is to accept the pushed element at the top while\u201cleaking\u201d the oldest element from the bottom of the stack to make room.\nGive an implementation of such a LeakyStack abstraction, using a circular\narray with appropriate storage capacity. This class should have a public\ninterface similar to the bounded-capacity stack in Exercise C-6.16, but\nwith the desired leaky semantics when full.", "254 Chapter 6. Stacks, Queues, and Deques\nP-6.36 When a share of common stock of some company is sold, the capital\ngain (or, sometimes, loss) is the difference between the share\u2019s selling\nprice and the price originally paid to buy it. This rule is easy to under-\nstand for a single share, but if we sell multiple shares of stock boughtover a long period of time, then we must identify the shares actually be-ing sold. A standard accounting principle for identifying which shares of\na stock were sold in such a case is to use a FIFO protocol\u2014the shares\nsold are the ones that have been held the longest (indeed, this is the de-fault method built into several personal \ufb01nance software packages). Forexample, suppose we buy 100 shares at $20 each on day 1, 20 shares at\n$24 on day 2, 200 shares at $36 on day 3, and then sell 150 shares on day\n4 at $30 each. Then applying the FIFO protocol means that of the 150shares sold, 100 were bought on day 1, 20 were bought on day 2, and 30were bought on day 3. The capital gain in this case would therefore be100\u00b710+20\u00b76+30\u00b7(\u22126), or $940. Write a program that takes as input\na sequence of transactions of the form \u201c buy xshare(s) at\nyeach\u201d\nor \u201csell xshare(s) at\n yeach,\u201d assuming that the transactions oc-\ncur on consecutive days and the values xandyare integers. Given this\ninput sequence, the output should be the total capital gain (or loss) for theentire sequence, using the FIFO protocol to identify shares.\nP-6.37 Design an ADT for a two-color, double-stack ADT that consists of twostacks\u2014one \u201cred\u201d and one \u201cblue\u201d\u2014and has as its operations color-coded\nversions of the regular stack ADT operations. For example, this ADT\nshould support both a red push operation and a blue push operation. Give\nan ef\ufb01cient implementation of this ADT using a single array whose ca-pacity is set at some value Nthat is assumed to always be larger than the\nsizes of the red and blue stacks combined.\nChapter Notes\nWe were introduced to the approach of de\ufb01nin g data structures \ufb01rst in terms of their ADTs\nand then in terms of concrete implementati ons by the classic books by Aho, Hopcroft, and\nUllman [5, 6]. Exercises C-6.30, and C-6.31 are similar to interview questions said to be\nfrom a well-known software company. For furthe r study of abstract data types, see Liskov\nand Guttag [71], Cardelli and Wegner [23], or Demurjian [33].", "Chapter\n7Linked Lists\nContents\n7 . 1 S i n g l yL i n k e dL i s t s....................... 2 5 6\n7.1.1 Implementing a Stack with a Singly Linked List . . . . . . 261\n7.1.2 Implementing a Queue with a Singly Linked List . . . . . . 264\n7 . 2 C i r c u l a r l yL i n k e dL i s t s..................... 2 6 6\n7.2.1 Round-Robin Schedulers . . . . . . . . . . . . . . . . . . 267\n7.2.2 Implementing a Queue with a Circularly Linked List . . . . 268\n7 . 3 D o u b l yL i n k e dL i s t s ...................... 2 7 0\n7.3.1 Basic Implementation of a Doubly Linked List . . . . . . . 2737.3.2 Implementing a Deque with a Doubly Linked List . . . . . 275\n7 . 4 T h eP o s i t i o n a lL i s tA D T ................... 2 7 7\n7 . 4 . 1 T h e P o s i t i o n a l L i s t A b s t r a c t D a t aT y p e ..........2 7 9\n7.4.2 Doubly Linked List Implementation . . . . . . . . . . . . . 281\n7 . 5 S o r t i n gaP o s i t i o n a lL i s t.................... 2 8 5\n7 . 6 C a s eS t u d y :M a i n t a i n i n gA c c e s sF r e q u e n c i e s........ 2 8 6\n7 . 6 . 1 U s i n g a S o r t e d L i s t .....................2 8 6\n7.6.2 Using a List with the Move-to-Front Heuristic . . . . . . . 289\n7 . 7 L i n k - B a s e dv s .A r r a y - B a s e dS e q u e n c e s ........... 2 9 27 . 8 E x e r c i s e s ............................ 2 9 4\n", "256 Chapter 7. Linked Lists\nIn Chapter 5 we carefully examined Python\u2019s array-based listclass, and in\nChapter 6 we demonstrated use of that class in implementing the classic stack,\nqueue, and dequeue ADTs. Python\u2019s listclass is highly optimized, and often a\ngreat choice for storage. With that said, there are some notable disadvantages:\n1. The length of a dynamic array might be longer than the actual number of\nelements that it stores.\n2. Amortized bounds for operations may be unacceptable in real-time systems.\n3. Insertions and deletions at interior positions of an array are expensive.\nIn this chapter, we introduce a data structure known as a linked list ,w h i c h\nprovides an alternative to an array-based sequence (such as a Python list). Both\narray-based sequences and linked lists keep elements in a certain order, but us-\ning a very different style. An array provides the more centralized representation,with one large chunk of memory capable of accommodating references to many\nelements. A linked list, in contrast, relies on a more distributed representation in\nwhich a lightweight object, known as a node , is allocated for each element. Each\nnode maintains a reference to its element and one or more references to neighboringnodes in order to collectively represent the linear order of the sequence.\nWe will demonstrate a trade-off of advantages and disadvantages when con-\ntrasting array-based sequences and linked lists. Elements of a linked list cannot beef\ufb01ciently accessed by a numeric index k, and we cannot tell just by examining a\nnode if it is the second, \ufb01fth, or twentieth node in the list. However, linked listsavoid the three disadvantages noted above for array-based sequences.\n7.1 Singly Linked Lists\nAsingly linked list , in its simplest form, is a collection of nodes that collectively\nform a linear sequence. Each node stores a reference to an object that is an elementof the sequence, as well as a reference to the next node of the list (see Figures 7.1and 7.2).\nnextMSP\nelement\nFigure 7.1: Example of a node instance that forms part of a singly linked list. The\nnode\u2019s element member references an arbitrary object that is an element of the se-\nquence (the airport code MSP , in this example), while the next member references\nthe subsequent node of the linked list (or None if there is no further node).", "7.1. Singly Linked Lists 257\ntailMSP BOS ATL\nheadLAX\nFigure 7.2: Example of a singly linked list whose elements are strings indicating\nairport codes. The list instance maintains a member named head that identi\ufb01es\nthe \ufb01rst node of the list, and in some applications another member named tailthat\nidenti\ufb01es the last node of the list. The None object is denoted as \u00d8.\nThe \ufb01rst and last node of a linked list are known as the head andtailof the\nlist, respectively. By starting at the head, and moving from one node to another\nby following each node\u2019s next reference, we can reach the tail of the list. We can\nidentify the tail as the node having None as its next reference. This process is\ncommonly known as traversing the linked list. Because the next reference of a\nnode can be viewed as a link orpointer to another node, the process of traversing\na list is also known as link hopping orpointer hopping .\nA linked list\u2019s representation in memory relies on the collaboration of many\nobjects. Each node is represented as a unique object, with that instance storing areference to its element and a reference to the next node (or None ). Another object\nrepresents the linked list as a whole. Minimally, the linked list instance must keepa reference to the head of the list. Without an explicit reference to the head, there\nwould be no way to locate that node (or indirectly, any others). There is not an\nabsolute need to store a direct reference to the tail of the list, as it could otherwisebe located by starting at the head and traversing the rest of the list. However,storing an explicit reference to the tail node is a common convenience to avoidsuch a traversal. In similar regard, it is common for the linked list instance to keep\na count of the total number of nodes that comprise the list (commonly described as\nthesizeof the list), to avoid the need to traverse the list to count the nodes.\nFor the remainder of this chapter, we continue to illustrate nodes as objects,\nand each node\u2019s \u201cnext\u201d reference as a pointer. However, for the sake of simplicity,we illustrate a node\u2019s element embedded directly within the node structure, eventhough the element is, in fact, an independent object. For example, Figure 7.3 is amore compact illustration of the linked list from Figure 7.2.\nLAX MSP BOS ATL\nhead tail\nFigure 7.3: A compact illustration of a singly linked list, with elements embedded\nin the nodes (rather than more accurately drawn as references to external objects).", "258 Chapter 7. Linked Lists\nInserting an Element at the Head of a Singly Linked List\nAn important property of a linked list is that it does not have a predetermined \ufb01xed\nsize; it uses space proportionally to its current number of elements. When using a\nsingly linked list, we can easily insert an element at the head of the list, as shown in\nFigure 7.4, and described with pseudo-code in Code Fragment 7.1. The main ideais that we create a new node, set its element to the new element, set its next link to\nrefer to the current head , and then set the list\u2019s head to point to the new node.\nATL BOS MSPhead\n(a)\nBOSnewest\nMSP ATLhead\nLAX\n(b)\nLAX MSP ATL BOShead newest\n(c)\nFigure 7.4: Insertion of an element at the head of a singly linked list: (a) before\nthe insertion; (b) after creation of a new node; (c) after reassignment of the headreference.\nAlgorithm add\n\ufb01rst(L,e):\nnewest =Node (e){create new node instance storing reference to element e}\nnewest .next =L.head {set new node\u2019s next to reference the old head node }\nL.head =newest {set variable head to reference the new node }\nL.size =L.size +1 {increment the node count }\nCode Fragment 7.1: Inserting a new element at the beginning of a singly linked\nlistL. Note that we set the next pointer of the new node before we reassign variable\nL.head to it. If the list were initially empty (i.e., L.head isNone ), then a natural\nconsequence is that the new node has its next reference set to None .", "7.1. Singly Linked Lists 259\nInserting an Element at the Tail of a Singly Linked List\nWe can also easily insert an element at the tail of the list, provided we keep a\nreference to the tail node, as shown in Figure 7.5. In this case, we create a new\nnode, assign its next reference to None , set the next reference of the tail to point to\nthis new node, and then update the tailreference itself to this new node. We give\nthe details in Code Fragment 7.2.\nATL BOS MSPtail\n(a)\nMIA ATL BOS MSPtail newest\n(b)\nMSP MIAtail newest\nATL BOS\n(c)\nFigure 7.5: Insertion at the tail of a singly linked list: (a) before the insertion;\n(b) after creation of a new node; (c) after reassignment of the tail reference. Notethat we must set the next link of the tailin (b) before we assign the tailvariable to\npoint to the new node in (c).\nAlgorithm add\nlast(L,e):\nnewest =Node (e){create new node instance storing reference to element e}\nnewest .next =None {set new node\u2019s next to reference the None object}\nL.tail.next =newest {make old tail node point to new node }\nL.tail =newest {set variable tailto reference the new node }\nL.size =L.size +1 {increment the node count }\nCode Fragment 7.2: Inserting a new node at the end of a singly linked list. Note\nthat we set the next pointer for the old tail node before we make variable tailpoint\nto the new node. This code would need to be adjusted for inserting onto an empty\nlist, since there would not be an existing tail node.", "260 Chapter 7. Linked Lists\nRemoving an Element from a Singly Linked List\nRemoving an element from the head of a singly linked list is essentially the reverse\noperation of inserting a new element at the head. This operation is illustrated in\nFigure 7.6 and given in detail in Code Fragment 7.3.\nhead\nMSP ATL BOS LAX\n(a)\nBOShead\nMSP ATL LAX\n(b)\nATL BOS MSPhead\n(c)\nFigure 7.6: Removal of an element at the head of a singly linked list: (a) before the\nremoval; (b) after \u201clinking out\u201d the old head; (c) \ufb01nal con\ufb01guration.\nAlgorithm remove\n \ufb01rst(L):\nifL.head isNone then\nIndicate an error: the list is empty.\nL.head =L.head .next {makehead point to next node (or None) }\nL.size =L.size\u22121 {decrement the node count }\nCode Fragment 7.3: Removing the node at the beginning of a singly linked list.\nUnfortunately, we cannot easily delete the last node of a singly linked list. Even\nif we maintain a tailreference directly to the last node of the list, we must be able\nto access the node before the last node in order to remove the last node. But we\ncannot reach the node before the tail by following next links from the tail. The only\nway to access this node is to start from the head of the list and search all the way\nthrough the list. But such a sequence of link-hopping operations could take a longtime. If we want to support such an operation ef\ufb01ciently, we will need to make ourlistdoubly linked (as we do in Section 7.3).", "7.1. Singly Linked Lists 261\n7.1.1 Implementing a Stack with a Singly Linked List\nIn this section, we demonstrate use of a singly linked list by providing a complete\nPython implementation of the stack ADT (see Section 6.1). In designing such animplementation, we need to decide whether to model the top of the stack at the head\nor at the tail of the list. There is clearly a best choice here; we can ef\ufb01ciently insert\nand delete elements in constant time only at the head. Since all stack operationsaffect the top, we orient the top of the stack at the head of our list.\nTo represent individual nodes of the list, we develop a lightweight\nNode class.\nThis class will never be directly exposed to the user of our stack class, so we willformally de\ufb01ne it as a nonpublic, nested class of our eventual LinkedStack class\n(see Section 2.5.1 for discussion of nested classes). The de\ufb01nition of the\nNode\nclass is shown in Code Fragment 7.4.\nclass\n Node:\n\u201d\u201d\u201dLightweight, nonpublic class for storing a singly linked node.\u201d\u201d\u201d\nslots\n =\n_element\n ,\n_next\n # streamline memory usage\ndef\n init\n(self,e l e m e n t ,n e x t ) : # initialize node\u2019s \ufb01elds\nself.\nelement = element # reference to user\u2019s element\nself.\nnext = next # reference to next node\nCode Fragment 7.4: A lightweight\n Node class for a singly linked list.\nA node has only two instance variables:\n element and\nnext. We intentionally\nde\ufb01ne\n slots\n to streamline the memory usage (see page 99 of Section 2.5.1 for\ndiscussion), because there may potentially be many node instances in a single list.The constructor of the\nNode class is designed for our convenience, allowing us to\nspecify initial values for both \ufb01elds of a newly created node.\nA complete implementation of our LinkedStack class is given in Code Frag-\nments 7.5 and 7.6. Each stack instance maintains two variables. The\n head mem-\nber is a reference to the node at the head of the list (or None , if the stack is empty).\nWe keep track of the current number of elements with the\n sizeinstance variable,\nfor otherwise we would be forced to traverse the entire list to count the number ofelements when reporting the size of the stack.\nThe implementation of push essentially mirrors the pseudo-code for insertion\nat the head of a singly linked list as outlined in Code Fragment 7.1. When we pusha new element eonto the stack, we accomplish the necessary changes to the linked\nstructure by invoking the constructor of the\nNode class as follows:\nself.\nhead = self.\nNode(e, self.\nhead) # create and link a new node\nNote that the\n next \ufb01eld of the new node is set to the existing top node, and then\nself.\nhead is reassigned to the new node.", "262 Chapter 7. Linked Lists\n1classLinkedStack:\n2\u201d\u201d\u201dLIFO Stack implementation using a singly linked list for storage.\u201d\u201d\u201d\n3\n4#-------------------------- nested\n Node class --------------------------\n5class\n Node:\n6 \u201d\u201d\u201dLightweight, nonpublic class for storing a singly linked node.\u201d\u201d\u201d\n7\n slots\n =\n_element\n ,\n_next\n # streamline memory usage\n89 def\ninit\n(self,e l e m e n t ,n e x t ) : # initialize node\u2019s \ufb01elds\n10 self.\nelement = element # reference to user\u2019s element\n11 self.\nnext = next # reference to next node\n1213 #------------------------------- stack methods -------------------------------\n14def\ninit\n(self):\n15 \u201d\u201d\u201dCreate an empty stack.\u201d\u201d\u201d\n16 self.\nhead = None # reference to the head node\n17 self.\nsize = 0 # number of stack elements\n1819def\nlen\n(self):\n20 \u201d\u201d\u201dReturn the number of elements in the stack.\u201d\u201d\u201d\n21 return self .\nsize\n2223defis\nempty( self):\n24 \u201d\u201d\u201dReturn True if the stack is empty.\u201d\u201d\u201d\n25 return self .\nsize == 0\n2627defpush(self,e ) :\n28 \u201d\u201d\u201dAdd element e to the top of the stack.\u201d\u201d\u201d\n29 self.\nhead = self.\nNode(e, self.\nhead) # create and link a new node\n30 self.\nsize += 1\n3132deftop(self):\n33 \u201d\u201d\u201dReturn (but do not remove) the element at the top of the stack.\n3435 Raise Empty exception if the stack is empty.\n36 \u201d\u201d\u201d\n37 if self.is\nempty():\n38 raiseEmpty(\n Stack is empty\n )\n39 return self .\nhead.\n element # top of stack is at head of list\nCode Fragment 7.5: Implementation of a stack ADT using a singly linked list (con-\ntinued in Code Fragment 7.6).", "7.1. Singly Linked Lists 263\n40defpop(self):\n41 \u201d\u201d\u201dRemove and return the element from the top of the stack (i.e., LIFO).\n42\n43 Raise Empty exception if the stack is empty.\n44 \u201d\u201d\u201d\n45 if self.is\nempty():\n46 raiseEmpty(\n Stack is empty\n )\n47 answer = self.\nhead.\n element\n48 self.\nhead = self.\nhead.\n next # bypass the former top node\n49 self.\nsize\u2212=1\n50 return answer\nCode Fragment 7.6: Implementation of a stack ADT using a singly linked list (con-\ntinued from Code Fragment 7.5).\nWhen implementing the topmethod, the goal is to return the element that is\nat the top of the stack. When the stack is empty, we raise an Empty exception, as\noriginally de\ufb01ned in Code Fragment 6.1 of Chapter 6. When the stack is nonempty,self.\nhead is a reference to the \ufb01rst node of the linked list. The top element can be\nidenti\ufb01ed as self.\nhead.\n element .\nOur implementation of popessentially mirrors the pseudo-code given in Code\nFragment 7.3, except that we maintain a local reference to the element that is storedat the node that is being removed, and we return that element to the caller of pop.\nThe analysis of our LinkedStack operations is given in Table 7.1. We see that\nall of the methods complete in worst-case constant time. This is in contrast to the\namortized bounds for the ArrayStack that were given in Table 6.2.\nOperation\n Running Time\nS.push(e)\n O(1)\nS.pop()\n O(1)\nS.top()\n O(1)\nlen(S)\n O(1)\nS.is\nempty()\n O(1)\nTable 7.1: Performance of our LinkedStack implementation. All bounds are worst-\ncase and our space usage is O(n),w h e r e nis the current number of elements in the\nstack.", "264 Chapter 7. Linked Lists\n7.1.2 Implementing a Queue with a Singly Linked List\nAs we did for the stack ADT, we can use a singly linked list to implement the\nqueue ADT while supporting worst-case O(1)-time for all operations. Because we\nneed to perform operations on both ends of the queue, we will explicitly maintain\nboth a\n head reference and a\n tailreference as instance variables for each queue.\nThe natural orientation for a queue is to align the front of the queue with the head of\nthe list, and the back of the queue with the tail of the list, because we must be ableto enqueue elements at the back, and dequeue them from the front. (Recall fromthe introduction of Section 7.1 that we are unable to ef\ufb01ciently remove elements\nfrom the tail of a singly linked list.) Our implementation of a LinkedQueue class is\ngiven in Code Fragments 7.7 and 7.8.\n1classLinkedQueue:\n2\u201d\u201d\u201dFIFO queue implementation using a singly linked list for storage.\u201d\u201d\u201d\n3\n4class\nNode:\n5 \u201d\u201d\u201dLightweight, nonpublic class for storing a singly linked node.\u201d\u201d\u201d\n6 (omitted here; identical to that of LinkedStack.\n Node)\n78def\ninit\n(self):\n9 \u201d\u201d\u201dCreate an empty queue.\u201d\u201d\u201d\n10 self.\nhead = None\n11 self.\ntail = None\n12 self.\nsize = 0 # number of queue elements\n1314def\nlen\n(self):\n15 \u201d\u201d\u201dReturn the number of elements in the queue.\u201d\u201d\u201d\n16 return self .\nsize\n1718defis\nempty( self):\n19 \u201d\u201d\u201dReturn True if the queue is empty.\u201d\u201d\u201d\n20 return self .\nsize == 0\n2122def\ufb01rst(self):\n23 \u201d\u201d\u201dReturn (but do not remove) the element at the front of the queue.\u201d\u201d\u201d\n24 if self.is\nempty():\n25 raiseEmpty(\n Queue is empty\n )\n26 return self .\nhead.\n element # front aligned with head of list\nCode Fragment 7.7: Implementation of a queue ADT using a singly linked list\n(continued in Code Fragment 7.8).", "7.1. Singly Linked Lists 265\n27defdequeue( self):\n28 \u201d\u201d\u201dRemove and return the \ufb01rst element of the queue (i.e., FIFO).\n29\n30 Raise Empty exception if the queue is empty.\n31 \u201d\u201d\u201d\n32 if self.is\nempty():\n33 raiseEmpty(\n Queue is empty\n )\n34 answer = self.\nhead.\n element\n35 self.\nhead = self.\nhead.\n next\n36 self.\nsize\u2212=1\n37 if self.is\nempty(): # special case as queue is empty\n38 self.\ntail = None # removed head had been the tail\n39 return answer\n40\n41defenqueue( self,e ) :\n42 \u201d\u201d\u201dAdd an element to the back of queue.\u201d\u201d\u201d\n43 newest = self.\nNode(e, None) # node will be new tail node\n44 if self.is\nempty():\n45 self.\nhead = newest # special case: previously empty\n46 else:\n47 self.\ntail.\nnext = newest\n48 self.\ntail = newest # update reference to tail node\n49 self.\nsize += 1\nCode Fragment 7.8: Implementation of a queue ADT using a singly linked list\n(continued from Code Fragment 7.7).\nMany aspects of our implementation are similar to that of the LinkedStack\nclass, such as the de\ufb01nition of the nested\n Node class. Our implementation of\ndequeue forLinkedQueue is similar to that of popforLinkedStack , as both remove\nthe head of the linked list. However, there is a subtle difference because our queuemust accurately maintain the\ntailreference (no such variable was maintained for\nour stack). In general, an operation at the head has no effect on the tail, but when\ndequeue is invoked on a queue with one element, we are simultaneously removing\nthe tail of the list. We therefore set self.\ntailtoNone for consistency.\nThere is a similar complication in our implementation of enqueue . The newest\nnode always becomes the new tail. Yet a distinction is made depending on whetherthat new node is the only node in the list. In that case, it also becomes the new head;otherwise the new node must be linked immediately after the existing tail node.\nIn terms of performance, the LinkedQueue is similar to the LinkedStack in that\nall operations run in worst-case constant time, and the space usage is linear in the\ncurrent number of elements.", "266 Chapter 7. Linked Lists\n7.2 Circularly Linked Lists\nIn Section 6.2.2, we introduced the notion of a \u201ccircular\u201d array and demonstrated\nits use in implementing the queue ADT. In reality, the notion of a circular array was\narti\ufb01cial, in that there was nothing about the representation of the array itself that\nwas circular in structure. It was our use of modular arithmetic when \u201cadvancing\u201dan index from the last slot to the \ufb01rst slot that provided such an abstraction.\nIn the case of linked lists, there is a more tangible notion of a circularly linked\nlist, as we can have the tail of the list use its next reference to point back to the headof the list, as shown in Figure 7.7. We call such a structure a circularly linked list .\nBOS\nhead tailLAX MSP ATL\nFigure 7.7: Example of a singly linked list with circular structure.\nA circularly linked list provides a more general model than a standard linked\nlist for data sets that are cyclic, that is, which do not have any particular notion of abeginning and end. Figure 7.8 provides a more symmetric illustration of the samecircular list structure as Figure 7.7.\nATLBOS\ncurrentMSPLAX\nFigure 7.8: Example of a circular linked list, with current denoting a reference to a\nselect node.\nA circular view similar to Figure 7.8 could be used, for example, to describe\nthe order of train stops in the Chicago loop, or the order in which players take turnsduring a game. Even though a circularly linked list has no beginning or end, perse, we must maintain a reference to a particular node in order to make use of thelist. We use the identi\ufb01er current to describe such a designated node. By setting\ncurrent = current.next , we can effectively advance through the nodes of the list.", "7.2. Circularly Linked Lists 267\n7.2.1 Round-Robin Schedulers\nTo motivate the use of a circularly linked list, we consider a round-robin scheduler,\nwhich iterates through a collection of elements in a circular fashion and \u201cservices\u201d\neach element by performing a given action on it. Such a scheduler is used, for\nexample, to fairly allocate a resource that must be shared by a collection of clients.\nFor instance, round-robin scheduling is often used to allocate slices of CPU time tovarious applications running concurrently on a computer.\nA round-robin scheduler could be implemented with the general queue ADT,\nby repeatedly performing the following steps on queue Q(see Figure 7.9):\n1.e=Q.dequeue ()\n2. Service element e\n3.Q.enqueue (e)\nThe Queue\nShared\nService1. Deque the\nnext element3. Enqueue the\nserviced element2. Service the\nnext element\nFigure 7.9: The three iterative steps for round-robin scheduling using a queue.\nIf we use of the LinkedQueue class of Section 7.1.2 for such an application,\nthere is unnecessary effort in the combination of a dequeue operation followed soonafter by an enqueue of the same element. One node is removed from the list, with\nappropriate adjustments to the head of the list and the size decremented, and then a\nnew node is created to reinsert at the tail of the list and the size is incremented.\nIf using a circularly linked list, the effective transfer of an item from the \u201chead\u201d\nof the list to the \u201ctail\u201d of the list can be accomplished by advancing a reference\nthat marks the boundary of the queue. We will next provide an implementation\nof aCircularQueue class that supports the entire queue ADT, together with an ad-\nditional method, rotate() , that moves the \ufb01rst element of the queue to the back.\n(A similar method is supported by the deque class of Python\u2019s collections module;\nsee Table 6.4.) With this operation, a round-robin schedule can more ef\ufb01ciently be\nimplemented by repeatedly performing the following steps:\n1. Service element Q.front ()\n2.Q.rotate ()", "268 Chapter 7. Linked Lists\n7.2.2 Implementing a Queue with a Circularly Linked List\nTo implement the queue ADT using a circularly linked list, we rely on the intuition\nof Figure 7.7, in which the queue has a head and a tail, but with the next reference ofthe tail linked to the head. Given such a model, there is no need for us to explicitly\nstore references to both the head and the tail; as long as we keep a reference to the\ntail, we can always \ufb01nd the head by following the tail\u2019s next reference.\nCode Fragments 7.9 and 7.10 provide an implementation of a CircularQueue\nclass based on this model. The only two instance variables are\ntail,w h i c hi sa\nreference to the tail node (or None when empty), and\n size, which is the current\nnumber of elements in the queue. When an operation involves the front of thequeue, we recognize self.\ntail.\nnext as the head of the queue. When enqueue is\ncalled, a new node is placed just after the tail but before the current head, and thenthe new node becomes the tail.\nIn addition to the traditional queue operations, the CircularQueue class supports\narotate method that more ef\ufb01ciently enacts the combination of removing the front\nelement and reinserting it at the back of the queue. With the circular representation,we simply set self.\ntail = self.\n tail.\nnext to make the old head become the new tail\n(with the node after the old head becoming the new head).\n1classCircularQueue:\n2\u201d\u201d\u201dQueue implementation using circularly linked list for storage.\u201d\u201d\u201d\n34class\nNode:\n5 \u201d\u201d\u201dLightweight, nonpublic class for storing a singly linked node.\u201d\u201d\u201d\n6 (omitted here; identical to that of LinkedStack.\n Node)\n78def\ninit\n(self):\n9 \u201d\u201d\u201dCreate an empty queue.\u201d\u201d\u201d\n10 self.\ntail = None # will represent tail of queue\n11 self.\nsize = 0 # number of queue elements\n1213def\nlen\n(self):\n14 \u201d\u201d\u201dReturn the number of elements in the queue.\u201d\u201d\u201d\n15 return self .\nsize\n1617defis\nempty( self):\n18 \u201d\u201d\u201dReturn True if the queue is empty.\u201d\u201d\u201d\n19 return self .\nsize == 0\nCode Fragment 7.9: Implementation of a CircularQueue class, using a circularly\nlinked list as storage (continued in Code Fragment 7.10).", "7.2. Circularly Linked Lists 269\n20def\ufb01rst(self):\n21 \u201d\u201d\u201dReturn (but do not remove) the element at the front of the queue.\n22\n23 Raise Empty exception if the queue is empty.\n24 \u201d\u201d\u201d\n25 if self.is\nempty():\n26 raiseEmpty(\n Queue is empty\n )\n27 head = self.\ntail.\nnext\n28 return head.\n element\n29\n30defdequeue( self):\n31 \u201d\u201d\u201dRemove and return the \ufb01rst element of the queue (i.e., FIFO).\n3233 Raise Empty exception if the queue is empty.\n34 \u201d\u201d\u201d\n35 if self.is\nempty():\n36 raiseEmpty(\n Queue is empty\n )\n37 oldhead = self.\ntail.\nnext\n38 if self.\nsize == 1: # removing only element\n39 self.\ntail = None # queue becomes empty\n40 else:\n41 self.\ntail.\nnext = oldhead.\n next #b y p a s st h eo l dh e a d\n42 self.\nsize\u2212=1\n43 return oldhead.\n element\n4445defenqueue( self,e ) :\n46 \u201d\u201d\u201dAdd an element to the back of queue.\u201d\u201d\u201d\n47 newest = self.\nNode(e, None) # node will be new tail node\n48 if self.is\nempty():\n49 newest.\n next = newest # initialize circularly\n50 else:\n51 newest.\n next = self.\ntail.\nnext # new node points to head\n52 self.\ntail.\nnext = newest # old tail points to new node\n53 self.\ntail = newest # new node becomes the tail\n54 self.\nsize += 1\n5556defrotate( self):\n57 \u201d\u201d\u201dRotate front element to the back of the queue.\u201d\u201d\u201d\n58 if self.\nsize>0:\n59 self.\ntail = self.\ntail.\nnext # old head becomes new tail\nCode Fragment 7.10: Implementation of a CircularQueue class, using a circularly\nlinked list as storage (continued from Code Fragment 7.9).", "270 Chapter 7. Linked Lists\n7.3 Doubly Linked Lists\nIn a singly linked list, each node maintains a reference to the node that is immedi-\nately after it. We have demonstrated the usefulness of such a representation when\nmanaging a sequence of elements. However, there are limitations that stem from\nthe asymmetry of a singly linked list. In the opening of Section 7.1, we empha-sized that we can ef\ufb01ciently insert a node at either end of a singly linked list, andcan delete a node at the head of a list, but we are unable to ef\ufb01ciently delete a node\nat the tail of the list. More generally, we cannot ef\ufb01ciently delete an arbitrary node\nfrom an interior position of the list if only given a reference to that node, becausewe cannot determine the node that immediately precedes the node to be deleted\n(yet, that node needs to have its next reference updated).\nTo provide greater symmetry, we de\ufb01ne a linked list in which each node keeps\nan explicit reference to the node before it and a reference to the node after it. Sucha structure is known as a doubly linked list . These lists allow a greater variety of\nO(1)-time update operations, including insertions and deletions at arbitrary posi-\ntions within the list. We continue to use the term \u201cnext\u201d for the reference to thenode that follows another, and we introduce the term \u201cprev\u201d for the reference to thenode that precedes it.\nHeader and Trailer Sentinels\nIn order to avoid some special cases when operating near the boundaries of a doublylinked list, it helps to add special nodes at both ends of the list: a header node at the\nbeginning of the list, and a trailer node at the end of the list. These \u201cdummy\u201d nodes\nare known as sentinels (or guards), and they do not store elements of the primary\nsequence. A doubly linked list with such sentinels is shown in Figure 7.10.\nSFO JFK PVDnext next next\nprev prev prev prevheader trailernext\nFigure 7.10: A doubly linked list representing the sequence {JFK, PVD, SFO },\nusing sentinels header andtrailer to demarcate the ends of the list.\nWhen using sentinel nodes, an empty list is initialized so that the next \ufb01eld of\nthe header points to the trailer, and the prev \ufb01eld of the trailer points to the header;\nthe remaining \ufb01elds of the sentinels are irrelevant (presumably None , in Python).\nFor a nonempty list, the header\u2019s next will refer to a node containing the \ufb01rst real\nelement of a sequence, just as the trailer\u2019s prev references the node containing the\nlast element of a sequence.", "7.3. Doubly Linked Lists 271\nAdvantage of Using Sentinels\nAlthough we could implement a doubly linked list without sentinel nodes (as we\ndid with our singly linked list in Section 7.1), the slight extra space devoted to the\nsentinels greatly simpli\ufb01es the logic of our operations. Most notably, the header andtrailer nodes never change\u2014only the nodes between them change. Furthermore,we can treat all insertions in a uni\ufb01ed manner, because a new node will always beplaced between a pair of existing nodes. In similar fashion, every element that is to\nbe deleted is guaranteed to be stored in a node that has neighbors on each side.\nFor contrast, look back at our LinkedQueue implementation from Section 7.1.2.\nItsenqueue method, given in Code Fragment 7.8, adds a new node to the end of\nthe list. However, its implementation required a conditional to manage the specialcase of inserting into an empty list. In the general case, the new node was linked\nafter the existing tail. But when adding to an empty list, there is no existing tail;instead it is necessary to reassign self.\nhead to reference the new node. The use of\na sentinel node in that implementation would eliminate the special case, as there\nwould always be an existing node (possibly the header) before a new node.\nInserting and Deleting with a Doubly Linked List\nEvery insertion into our doubly linked list representation will take place betweena pair of existing nodes, as diagrammed in Figure 7.11. For example, when a new\nelement is inserted at the front of the sequence, we will simply add the new nodebetween the header and the node that is currently after the header. (See Figure 7.12.)\nJFK BWI SFOtrailer header\n(a)\nBWI PVD SFO JFKtrailer header\n(b)\nBWI PVD SFO JFKtrailer header\n(c)\nFigure 7.11: Adding an element to a doubly linked list with header and trailer sen-\ntinels: (a) before the operation; (b) after creating the new node; (c) after linking theneighbors to the new node.", "272 Chapter 7. Linked Lists\nJFK BWI SFOtrailer header\n(a)\nPVD BWI JFK SFOtrailer header\n(b)\nPVD JFK SFO BWItrailer header\n(c)\nFigure 7.12: Adding an element to the front of a sequence represented by a dou-\nbly linked list with header and trailer sentinels: (a) before the operation; (b) after\ncreating the new node; (c) after linking the neighbors to the new node.\nThe deletion of a node, portrayed in Figure 7.13, proceeds in the opposite fash-\nion of an insertion. The two neighbors of the node to be deleted are linked directlyto each other, thereby bypassing the original node. As a result, that node will no\nlonger be considered part of the list and it can be reclaimed by the system. Because\nof our use of sentinels, the same implementation can be used when deleting the \ufb01rstor the last element of a sequence, because even such an element will be stored at anode that lies between two others.\nBWI PVD SFO JFKtrailer header\n(a)\nBWI PVD SFO JFKtrailer header\n(b)\nJFK BWI SFOtrailer header\n(c)\nFigure 7.13: Removing the element PVD from a doubly linked list: (a) before\nthe removal; (b) after linking out the old node; (c) after the removal (and garbagecollection).", "7.3. Doubly Linked Lists 273\n7.3.1 Basic Implementation of a Doubly Linked List\nWe begin by providing a preliminary implementation of a doubly linked list, in the\nform of a class named\n DoublyLinkedBase. We intentionally name the class with\na leading underscore because we do not intend for it to provide a coherent publicinterface for general use. We will see that linked lists can support general insertionsand deletions in O(1)worst-case time, but only if the location of an operation\ncan be succinctly identi\ufb01ed. With array-based sequences, an integer index was aconvenient means for describing a position within a sequence. However, an index\nis not convenient for linked lists as there is no ef\ufb01cient way to \ufb01nd the j\nthelement;\nit would seem to require a traversal of a portion of the list.\nWhen working with a linked list, the most direct way to describe the location\nof an operation is by identifying a relevant node of the list. However, we prefer\nto encapsulate the inner workings of our data structure to avoid having users di-rectly access nodes of a list. In the remainder of this chapter, we will develop\ntwo public classes that inherit from our\nDoublyLinkedBase class to provide more\ncoherent abstractions. Speci\ufb01cally, in Section 7.3.2, we provide a LinkedDeque\nclass that implements the double-ended queue ADT introduced in Section 6.3; that\nclass only supports operations at the ends of the queue, so there is no need for a\nuser to identify an interior position within the list. In Section 7.4, we introduce a\nnewPositionalList abstraction that provides a public interface that allows arbitrary\ninsertions and deletions from a list.\nOur low-level\n DoublyLinkedBase class relies on the use of a nonpublic\n Node\nclass that is similar to that for a singly linked list, as given in Code Fragment 7.4,except that the doubly linked version includes a\nprev attribute, in addition to the\nnext and\nelement attributes, as shown in Code Fragment 7.11.\nclass\n Node:\n\u201d\u201d\u201dLightweight, nonpublic class for storing a doubly linked node.\u201d\u201d\u201d\nslots\n =\n_element\n ,\n_prev\n ,\n_next\n # streamline memory\ndef\n init\n(self, element, prev, next): # initialize node\u2019s \ufb01elds\nself.\nelement = element # user\u2019s element\nself.\nprev = prev # previous node reference\nself.\nnext = next # next node reference\nCode Fragment 7.11: A Python\n Node class for use in a doubly linked list.\nThe remainder of our\n DoublyLinkedBase class is given in Code Fragment 7.12.\nThe constructor instantiates the two sentinel nodes and links them directly to eachother. We maintain a\nsizemember and provide public support for\n len\n and\nis\nempty so that these behaviors can be directly inherited by the subclasses.", "274 Chapter 7. Linked Lists\n1class\n DoublyLinkedBase:\n2\u201d\u201d\u201dA base class providing a doubly linked list representation.\u201d\u201d\u201d\n3\n4class\n Node:\n5 \u201d\u201d\u201dLightweight, nonpublic class for storing a doubly linked node.\u201d\u201d\u201d\n6 (omitted here; see previous code fragment)\n78def\ninit\n(self):\n9 \u201d\u201d\u201dCreate an empty list.\u201d\u201d\u201d\n10 self.\nheader = self.\nNode(None,None,None)\n11 self.\ntrailer = self.\nNode(None,None,None)\n12 self.\nheader.\n next = self.\ntrailer # trailer is after header\n13 self.\ntrailer.\n prev = self.\nheader # header is before trailer\n14 self.\nsize = 0 # number of elements\n15\n16def\n len\n(self):\n17 \u201d\u201d\u201dReturn the number of elements in the list.\u201d\u201d\u201d\n18 return self .\nsize\n1920defis\nempty( self):\n21 \u201d\u201d\u201dReturn True if list is empty.\u201d\u201d\u201d\n22 return self .\nsize == 0\n2324def\ninsert\n between( self, e, predecessor, successor):\n25 \u201d\u201d\u201dAdd element e between two existing nodes and return new node.\u201d\u201d\u201d\n26 newest = self.\nNode(e, predecessor, successor) # linked to neighbors\n27 predecessor.\n next = newest\n28 successor.\n prev = newest\n29 self.\nsize += 1\n30 return newest\n3132def\ndelete\n node(self,n o d e ) :\n33 \u201d\u201d\u201dDelete nonsentinel node from the list and return its element.\u201d\u201d\u201d\n34 predecessor = node.\n prev\n35 successor = node.\n next\n36 predecessor.\n next = successor\n37 successor.\n prev = predecessor\n38 self.\nsize\u2212=1\n39 element = node.\n element # record deleted element\n40 node.\n prev = node.\n next = node.\n element = None # deprecate node\n41 return element # return deleted element\nCode Fragment 7.12: A base class for managing a doubly linked list.", "7.3. Doubly Linked Lists 275\nThe other two methods of our class are the nonpublic utilities,\n insert\n between\nand\ndelete\n node . These provide generic support for insertions and deletions, re-\nspectively, but require one or more node references as parameters. The implemen-\ntation of the\n insert\n between method is modeled upon the algorithm that was previ-\nously portrayed in Figure 7.11. It creates a new node, with that node\u2019s \ufb01elds initial-ized to link to the speci\ufb01ed neighboring nodes. Then the \ufb01elds of the neighboring\nnodes are updated to include the newest node in the list. For later convenience, the\nmethod returns a reference to the newly created node.\nThe implementation of the\ndelete\n node method is modeled upon the algorithm\nportrayed in Figure 7.13. The neighbors of the node to be deleted are linked directly\nto each other, thereby bypassing the deleted node from the list. As a formality,\nwe intentionally reset the\n prev,\nnext,a n d\n element \ufb01elds of the deleted node to\nNone (after recording the element to be returned). Although the deleted node will\nbe ignored by the rest of the list, setting its \ufb01elds to None is advantageous as it\nmay help Python\u2019s garbage collection, since unnecessary links to the other nodes\nand the stored element are eliminated. We will also rely on this con\ufb01guration to\nrecognize a node as \u201cdeprecated\u201d when it is no longer part of the list.\n7.3.2 Implementing a Deque with a Doubly Linked List\nThe double-ended queue (deque) ADT was introduced in Section 6.3. With an\narray-based implementation, we achieve all operations in amortized O (1)time, due\nto the occasional need to resize the array. With an implementation based upon a\ndoubly linked list, we can achieve all deque operation in worst-case O (1)time.\nWe provide an implementation of a LinkedDeque class (Code Fragment 7.13)\nthat inherits from the\n DoublyLinkedBase class of the preceding section. We do\nnot provide an explicit\n init\n method for the LinkedDeque class, as the inherited\nversion of that method suf\ufb01ces to initialize a new instance. We also rely on theinherited methods\nlen\n andis\nempty in meeting the deque ADT.\nWith the use of sentinels, the key to our implementation is to remember that\nthe header does not store the \ufb01rst element of the deque\u2014it is the node just after the\nheader that stores the \ufb01rst element (assuming the deque is nonempty). Similarly,the node just before the trailer stores the last element of the deque.\nWe use the inherited\ninsert\n between method to insert at either end of the\ndeque. To insert an element at the front of the deque, we place it immediately\nbetween the header and the node just after the header. An insertion at the end of\ndeque is placed immediately before the trailer node. Note that these operationssucceed, even when the deque is empty; in such a situation, the new node is placedbetween the two sentinels. When deleting an element from a nonempty deque, werely upon the inherited\ndelete\n node method, knowing that the designated node is\nassured to have neighbors on each side.", "276 Chapter 7. Linked Lists\n1classLinkedDeque(\n DoublyLinkedBase): # note the use of inheritance\n2\u201d\u201d\u201dDouble-ended queue implementation based on a doubly linked list.\u201d\u201d\u201d\n3\n4def\ufb01rst(self):\n5 \u201d\u201d\u201dReturn (but do not remove) the element at the front of the deque.\u201d\u201d\u201d\n6 if self.is\nempty():\n7 raiseEmpty( \"Deque is empty\" )\n8 return self .\nheader.\n next.\n element # real item just after header\n9\n10deflast(self):\n11 \u201d\u201d\u201dReturn (but do not remove) the element at the back of the deque.\u201d\u201d\u201d\n12 if self.is\nempty():\n13 raiseEmpty( \"Deque is empty\" )\n14 return self .\ntrailer.\n prev.\nelement # real item just before trailer\n15\n16definsert\n \ufb01rst(self,e ) :\n17 \u201d\u201d\u201dAdd an element to the front of the deque.\u201d\u201d\u201d\n18 self.\ninsert\n between(e, self.\nheader, self.\nheader.\n next) # after header\n1920definsert\nlast(self,e ) :\n21 \u201d\u201d\u201dAdd an element to the back of the deque.\u201d\u201d\u201d\n22 self.\ninsert\n between(e, self.\ntrailer.\n prev,self.\ntrailer) # before trailer\n2324defdelete\n\ufb01rst(self):\n25 \u201d\u201d\u201dRemove and return the element from the front of the deque.\n26\n27 Raise Empty exception if the deque is empty.\n28 \u201d\u201d\u201d\n29 if self.is\nempty():\n30 raiseEmpty( \"Deque is empty\" )\n31 return self .\ndelete\n node(self.\nheader.\n next) # use inherited method\n32\n33defdelete\n last(self):\n34 \u201d\u201d\u201dRemove and return the element from the back of the deque.\n35\n36 Raise Empty exception if the deque is empty.\n37 \u201d\u201d\u201d\n38 if self.is\nempty():\n39 raiseEmpty( \"Deque is empty\" )\n40 return self .\ndelete\n node(self.\ntrailer.\n prev) # use inherited method\nCode Fragment 7.13: Implementation of a LinkedDeque class that inherits from the\nDoublyLinkedBase class.", "7.4. The Positional List ADT 277\n7.4 The Positional List ADT\nThe abstract data types that we have considered thus far, namely stacks, queues,\nand double-ended queues, only allow update operations that occur at one end of a\nsequence or the other. We wish to have a more general abstraction. For example,\nalthough we motivated the FIFO semantics of a queue as a model for customerswho are waiting to speak with a customer service representative, or fans who arewaiting in line to buy tickets to a show, the queue ADT is too limiting. What ifa waiting customer decides to hang up before reaching the front of the customer\nservice queue? Or what if someone who is waiting in line to buy tickets allows a\nfriend to \u201ccut\u201d into line at that position? We would like to design an abstract datatype that provides a user a way to refer to elements anywhere in a sequence, and toperform arbitrary insertions and deletions.\nWhen working with array-based sequences (such as a Python list), integer in-\ndices provide an excellent means for describing the location of an element, or thelocation at which an insertion or deletion should take place. However, numeric in-dices are not a good choice for describing positions within a linked list because wecannot ef\ufb01ciently access an entry knowing only its index; \ufb01nding an element at agiven index within a linked list requires traversing the list incrementally from its\nbeginning or end, counting elements as we go.\nFurthermore, indices are not a good abstraction for describing a local position\nin some applications, because the index of an entry changes over time due to inser-\ntions or deletions that happen earlier in the sequence. For example, it may not beconvenient to describe the location of a person waiting in line by knowing precisely\nhow far away that person is from the front of the line. We prefer an abstraction, as\ncharacterized in Figure 7.14, in which there is some other means for describinga position. We then wish to model situations such as when an identi\ufb01ed personleaves the line before reaching the front, or in which a new person is added to a lineimmediately behind another identi\ufb01ed person.\nTicketsme\nFigure 7.14: We wish to be able to identify the position of an element in a sequence\nwithout the use of an integer index.", "278 Chapter 7. Linked Lists\nAs another example, a text document can be viewed as a long sequence of\ncharacters. A word processor uses the abstraction of a cursor to describe a position\nwithin the document without explicit use of an integer index, allowing operations\nsuch as \u201cdelete the character at the cursor\u201d or \u201cinsert a new character just after thecursor.\u201d Furthermore, we may be able to refer to an inherent position within a doc-ument, such as the beginning of a particular section, without relying on a character\nindex (or even a section number) that may change as the document evolves.\nA Node Reference as a Position?\nOne of the great bene\ufb01ts of a linked list structure is that it is possible to perform\nO(1)-time insertions and deletions at arbitrary positions of the list, as long as we\nare given a reference to a relevant node of the list. It is therefore very tempting todevelop an ADT in which a node reference serves as the mechanism for describinga position. In fact, our\nDoublyLinkedBase class of Section 7.3.1 has methods\ninsert\n between and\ndelete\n node that accept node references as parameters.\nHowever, such direct use of nodes would violate the object-oriented design\nprinciples of abstraction and encapsulation that were introduced in Chapter 2. There\nare several reasons to prefer that we encapsulate the nodes of a linked list, for both\nour sake and for the bene\ufb01t of users of our abstraction.\n\u2022It will be simpler for users of our data structure if they are not bothered withunnecessary details of our implementation, such as low-level manipulationof nodes, or our reliance on the use of sentinel nodes. Notice that to use the\ninsert\n between method of our\n DoublyLinkedBase class to add a node at\nthe beginning of a sequence, the header sentinel must be sent as a parameter.\n\u2022We can provide a more robust data structure if we do not permit users todirectly access or manipulate the nodes. In that way, we ensure that users\ncannot invalidate the consistency of a list by mismanaging the linking of\nnodes. A more subtle problem arises if a user were allowed to call the\ninsert\n between or\ndelete\n node method of our\n DoublyLinkedBase class,\nsending a node that does not belong to the given list as a parameter. (Go back\nand look at that code and see why it causes a problem!)\n\u2022By better encapsulating the internal details of our implementation, we havegreater \ufb02exibility to redesign the data structure and improve its performance.\nIn fact, with a well-designed abstraction, we can provide a notion of a non-numeric position, even if using an array-based sequence.\nFor these reasons, instead of relying directly on nodes, we introduce an inde-\npendent position abstraction to denote the location of an element within a list, and\nthen a complete positional list ADT that can encapsulate a doubly linked list (or\neven an array-based sequence; see Exercise P-7.46).", "7.4. The Positional List ADT 279\n7.4.1 The Positional List Abstract Data Type\nTo provide for a general abstraction of a sequence of elements with the ability to\nidentify the location of an element, we de\ufb01ne a positional list ADT as well as a\nsimpler position abstract data type to describe a location within a list. A position\nacts as a marker or token within the broader positional list. A position pis unaf-\nfected by changes elsewhere in a list; the only way in which a position becomesinvalid is if an explicit command is issued to delete it.\nA position instance is a simple object, supporting only the following method:\np.element() :Return the element stored at position p.\nIn the context of the positional list ADT, positions serve as parameters to some\nmethods and as return values from other methods. In describing the behaviors of a\npositional list, we being by presenting the accessor methods supported by a list L:\nL.\ufb01rst() :Return the position of the \ufb01rst element of L,o rNone ifLis empty.\nL.last() :Return the position of the last element of L,o rNone ifLis empty.\nL.before(p) :Return the position of Limmediately before position p,o rNone\nifpis the \ufb01rst position.\nL.after(p) :Return the position of Limmediately after position p,o rNone if\npis the last position.\nL.is\nempty() :Return True if listLdoes not contain any elements.\nlen(L) :Return the number of elements in the list.\niter(L) :Return a forward iterator for the elements of the list. See Sec-\ntion 1.8 for discussion of iterators in Python.\nThe positional list ADT also includes the following update methods:\nL.add\n \ufb01rst(e) :Insert a new element eat the front of L, returning the position\nof the new element.\nL.add\n last(e) :Insert a new element eat the back of L, returning the position\nof the new element.\nL.add\n before(p, e) :Insert a new element ejust before position pinL, returning\nthe position of the new element.\nL.add\n after(p, e) :Insert a new element ejust after position pinL, returning\nthe position of the new element.\nL.replace(p, e) :Replace the element at position pwith element e, returning\nthe element formerly at position p.\nL.delete(p) :Remove and return the element at position pinL, invalidat-\ning the position.\nFor those methods of the ADT that accept a position pas a parameter, an error\noccurs if pis not a valid position for list L.", "280 Chapter 7. Linked Lists\nNote well that the \ufb01rst() andlast() methods of the positional list ADT return\nthe associated positions , not the elements . (This is in contrast to the corresponding\n\ufb01rst andlastmethods of the deque ADT.) The \ufb01rst element of a positional list\ncan be determined by subsequently invoking the element method on that position,\nasL.\ufb01rst().element() . The advantage of receiving a position as a return value is\nthat we can use that position to navigate the list. For example, the following code\nfragment prints all elements of a positional list named data.\ncursor = data.\ufb01rst()\nwhile cursor is not None :\nprint(cursor.element()) # print the element stored at the position\ncursor = data.after(cursor) # advance to the next position (if any)\nThis code relies on the stated convention that the None object is returned when\nafter is called upon the last position. That return value is clearly distinguishable\nfrom any legitimate position. The positional list ADT similarly indicates that theNone value is returned when the before method is invoked at the front of the list, or\nwhen\ufb01rstorlastmethods are called upon an empty list. Therefore, the above code\nfragment works correctly even if the data list is empty.\nBecause the ADT includes support for Python\u2019s iterfunction, users may rely\non the traditional for-loop syntax for such a forward traversal of a list named data.\nforeindata:\nprint(e)\nMore general navigational and update methods of the positional list ADT are shownin the following example.\nExample 7.1:\nThe following table shows a series of operations on an initially\nempty positional list L. To identify position instances, we use variables such as p\nandq. For ease of exposition, when displaying the list contents, we use subscript\nnotation to denote its positions.\nOperation\n Return Value\n L\nL.add\n last(8)\n p\n 8p\nL.\ufb01rst()\n p\n 8p\nL.add\n after(p, 5)\n q\n 8p,5q\nL.before(q)\n p\n 8p,5q\nL.add\n before(q, 3)\n r\n 8p,3r,5q\nr.element()\n 3\n 8p,3r,5q\nL.after(p)\n r\n 8p,3r,5q\nL.before(p)\n None\n 8p,3r,5q\nL.add\n \ufb01rst(9)\n s\n 9s,8p,3r,5q\nL.delete(L.last())\n 5\n 9s,8p,3r\nL.replace(p, 7)\n 8\n 9s,7p,3r\n", "7.4. The Positional List ADT 281\n7.4.2 Doubly Linked List Implementation\nIn this section, we present a complete implementation of a PositionalList class\nusing a doubly linked list that satis\ufb01es the following important proposition.\nProposition 7.2: Each method of the positional list ADT runs in worst-case O(1)\ntime when implemented with a doubly linked list.\nWe rely on the\n DoublyLinkedBase class from Section 7.3.1 for our low-level\nrepresentation; the primary responsibility of our new class is to provide a public\ninterface in accordance with the positional list ADT. We begin our class de\ufb01nitionin Code Fragment 7.14 with the de\ufb01nition of the public Position class, nested within\nourPositionalList class. Position instances will be used to represent the locations\nof elements within the list. Our various PositionalList methods may end up creating\nredundant Position instances that reference the same underlying node (for example,\nwhen\ufb01rst andlastare the same). For that reason, our Position class de\ufb01nes the\neq\n and\n ne\n special methods so that a test such as p= =q evaluates to\nTrue when two positions refer to the same node.\nValidating Positions\nEach time a method of the PositionalList class accepts a position as a parameter,\nwe want to verify that the position is valid, and if so, to determine the underlyingnode associated with the position. This functionality is implemented by a non-public method named\nvalidate . Internally, a position maintains a reference to the\nassociated node of the linked list, and also a reference to the list instance that con-tains the speci\ufb01ed node. With the container reference, we can robustly detect whena caller sends a position instance that does not belong to the indicated list.\nWe are also able to detect a position instance that belongs to the list, but that\nrefers to a node that is no longer part of that list. Recall that the\ndelete\n node of\nthe base class sets the previous and next references of a deleted node to None ;w e\ncan recognize that condition to detect a deprecated node.\nAccess and Update Methods\nThe access methods of the PositionalList class are given in Code Fragment 7.15\nand the update methods are given in Code Fragment 7.16. All of these methods\ntrivially adapt the underlying doubly linked list implementation to support the pub-\nlic interface of the positional list ADT. Those methods rely on the\n validate utility\nto \u201cunwrap\u201d any position that is sent. They also rely on a\n make\n position utility\nto \u201cwrap\u201d nodes as Position instances to return to the user, making sure never to\nreturn a position referencing a sentinel. For convenience, we have overridden the\ninherited\n insert\n between utility method so that ours returns a position associated\nwith the newly created node (whereas the inherited version returns the node itself).", "282 Chapter 7. Linked Lists\n1classPositionalList(\n DoublyLinkedBase):\n2\u201d\u201d\u201dA sequential container of elements allowing positional access.\u201d\u201d\u201d\n3\n4#-------------------------- nested Position class --------------------------\n5classPosition:\n6 \u201d\u201d\u201dAn abstraction representing the location of a single element.\u201d\u201d\u201d\n78 def\ninit\n(self,c o n t a i n e r ,n o d e ) :\n9 \u201d\u201d\u201dConstructor should not be invoked by user.\u201d\u201d\u201d\n10 self.\ncontainer = container\n11 self.\nnode = node\n1213 defelement( self):\n14 \u201d\u201d\u201dReturn the element stored at this Position.\u201d\u201d\u201d\n15 return self .\nnode.\n element\n1617 def\neq\n(self,o t h e r ) :\n18 \u201d\u201d\u201dReturn True if other is a Position representing the same location.\u201d\u201d\u201d\n19 return type(other) istype(self)andother.\n nodeis self .\nnode\n2021 def\nne\n(self,o t h e r ) :\n22 \u201d\u201d\u201dReturn True if other does not represent the same location.\u201d\u201d\u201d\n23 return not (self== other) #o p p o s i t eo f\n eq\n2425 #------------------------------- utility method -------------------------------\n26def\nvalidate( self,p ) :\n27 \u201d\u201d\u201dReturn position\n s node, or raise appropriate error if invalid.\u201d\u201d\u201d\n28 if not isinstance(p, self.Position):\n29 raiseTypeError(\n p must be proper Position type\n )\n30 ifp.\ncontainer is not self :\n31 raiseValueError(\n p does not belong to this container\n )\n32 ifp.\nnode.\n nextis None : # convention for deprecated nodes\n33 raiseValueError(\n p is no longer valid\n )\n34 return p.\nnode\nCode Fragment 7.14: APositionalList class based on a doubly linked list. (Contin-\nues in Code Fragments 7.15 and 7.16.)", "7.4. The Positional List ADT 283\n35 #------------------------------- utility method -------------------------------\n36def\nmake\n position( self,n o d e ) :\n37 \u201d\u201d\u201dReturn Position instance for given node (or None if sentinel).\u201d\u201d\u201d\n38 ifnodeis self .\nheader ornodeis self .\ntrailer:\n39 return None # boundary violation\n40 else:\n41 return self .Position( self,n o d e ) # legitimate position\n42\n43 #------------------------------- accessors -------------------------------\n44def\ufb01rst(self):\n45 \u201d\u201d\u201dReturn the \ufb01rst Position in the list (or None if list is empty).\u201d\u201d\u201d\n46 return self .\nmake\n position( self.\nheader.\n next)\n4748deflast(self):\n49 \u201d\u201d\u201dReturn the last Position in the list (or None if list is empty).\u201d\u201d\u201d\n50 return self .\nmake\n position( self.\ntrailer.\n prev)\n5152defbefore( self,p ) :\n53 \u201d\u201d\u201dReturn the Position just before Position p (or None if p is \ufb01rst).\u201d\u201d\u201d\n54 node = self.\nvalidate(p)\n55 return self .\nmake\n position(node.\n prev)\n5657defafter(self,p ) :\n58 \u201d\u201d\u201dReturn the Position just after Position p (or None if p is last).\u201d\u201d\u201d\n59 node = self.\nvalidate(p)\n60 return self .\nmake\n position(node.\n next)\n61\n62def\n iter\n(self):\n63 \u201d\u201d\u201dGenerate a forward iteration of the elements of the list.\u201d\u201d\u201d\n64 cursor = self.\ufb01rst()\n65 while cursor is not None :\n66 yieldcursor.element()\n67 cursor = self.after(cursor)\nCode Fragment 7.15: APositionalList class based on a doubly linked list. (Contin-\nued from Code Fragment 7.14; continues in Code Fragment 7.16.)", "284 Chapter 7. Linked Lists\n68 #------------------------------- mutators -------------------------------\n69 # override inherited version to return Position, rather than Node\n70def\ninsert\n between( self, e, predecessor, successor):\n71 \u201d\u201d\u201dAdd element between existing nodes and return new Position.\u201d\u201d\u201d\n72 node = super().\ninsert\n between(e, predecessor, successor)\n73 return self .\nmake\n position(node)\n74\n75defadd\n\ufb01rst(self,e ) :\n76 \u201d\u201d\u201dInsert element e at the front of the list and return new Position.\u201d\u201d\u201d\n77 return self .\ninsert\n between(e, self.\nheader, self.\nheader.\n next)\n78\n79defadd\nlast(self,e ) :\n80 \u201d\u201d\u201dInsert element e at the back of the list and return new Position.\u201d\u201d\u201d\n81 return self .\ninsert\n between(e, self.\ntrailer.\n prev,self.\ntrailer)\n8283defadd\nbefore( self,p ,e ) :\n84 \u201d\u201d\u201dInsert element e into list before Position p and return new Position.\u201d\u201d\u201d\n85 original = self.\nvalidate(p)\n86 return self .\ninsert\n between(e, original.\n prev, original)\n8788defadd\nafter(self,p ,e ) :\n89 \u201d\u201d\u201dInsert element e into list after Position p and return new Position.\u201d\u201d\u201d\n90 original = self.\nvalidate(p)\n91 return self .\ninsert\n between(e, original, original.\n next)\n9293defdelete( self,p ) :\n94 \u201d\u201d\u201dRemove and return the element at Position p.\u201d\u201d\u201d\n95 original = self.\nvalidate(p)\n96 return self .\ndelete\n node(original) # inherited method returns element\n97\n98defreplace( self,p ,e ) :\n99 \u201d\u201d\u201dReplace the element at Position p with e.\n100\n101 Return the element formerly at Position p.\n102 \u201d\u201d\u201d\n103 original = self.\nvalidate(p)\n104 old\nvalue = original.\n element # temporarily store old element\n105 original.\n element = e # replace with new element\n106 return old\nvalue # return the old element value\nCode Fragment 7.16: APositionalList class based on a doubly linked list. (Contin-\nued from Code Fragments 7.14 and 7.15.)", "7.5. Sorting a Positional List 285\n7.5 Sorting a Positional List\nIn Section 5.5.2, we introduced the insertion-sort algorithm, in the context of an\narray-based sequence. In this section, we develop an implementation that operates\non aPositionalList , relying on the same high-level algorithm in which each element\nis placed relative to a growing collection of previously sorted elements.\nWe maintain a variable named marker that represents the rightmost position of\nthe currently sorted portion of a list. During each pass, we consider the position justpast the marker as the pivot and consider where the pivot\u2019s element belongs relative\nto the sorted portion; we use another variable, named walk, to move leftward from\nthe marker, as long as there remains a preceding element with value larger than thepivot\u2019s. A typical con\ufb01guration of these variables is diagrammed in Figure 7.15. APython implementation of this strategy is given in Code 7.17.\n15 22 25 29 36 23 53 11 42\nmarkerpivot walk\nFigure 7.15: Overview of one step of our insertion-sort algorithm. The shaded\nelements, those up to and including marker , have already been sorted. In this step,\nthe pivot\u2019s element should be relocated immediately before the walk position.\n1definsertion\n sort(L):\n2\u201d\u201d\u201dSort PositionalList of comparable elements into nondecreasing order.\u201d\u201d\u201d\n3iflen(L) >1: # otherwise, no need to sort it\n4 marker = L.\ufb01rst()\n5 while marker != L.last():\n6 pivot = L.after(marker) # next item to place\n7 value = pivot.element()\n8 ifvalue>marker.element(): # pivot is already sorted\n9 marker = pivot # pivot becomes new marker\n10 else: #m u s tr e l o c a t ep i v o t\n11 walk = marker # \ufb01nd leftmost item greater than value\n12 while walk != L.\ufb01rst( ) andL.before(walk).element( ) >value:\n13 walk = L.before(walk)\n14 L.delete(pivot)\n15 L.add\n before(walk, value) # reinsert value before walk\nCode Fragment 7.17: Python code for performing insertion-sort on a positional list.", "286 Chapter 7. Linked Lists\n7.6 Case Study: Maintaining Access Frequencies\nThe positional list ADT is useful in a number of settings. For example, a program\nthat simulates a game of cards could model each person\u2019s hand as a positional list(Exercise P-7.47). Since most people keep cards of the same suit together, inserting\nand removing cards from a person\u2019s hand could be implemented using the methods\nof the positional list ADT, with the positions being determined by a natural orderof the suits. Likewise, a simple text editor embeds the notion of positional insertionand deletion, since such editors typically perform all updates relative to a cursor ,\nwhich represents the current position in the list of characters of text being edited.\nIn this section, we consider maintaining a collection of elements while keeping\ntrack of the number of times each element is accessed. Keeping such access countsallows us to know which elements are among the most popular. Examples of suchscenarios include a Web browser that keeps track of a user\u2019s most accessed URLs,\nor a music collection that maintains a list of the most frequently played songs for\na user. We model this with a new favorites list ADT that supports the lenand\nis\nempty methods as well as the following:\naccess (e):Access the element e, incrementing its access count, and\nadding it to the favorites list if it is not already present.\nremove (e):Remove element efrom the favorites list, if present.\ntop (k):Return an iteration of the kmost accessed elements.\n7.6.1 Using a Sorted List\nOur \ufb01rst approach for managing a list of favorites is to store elements in a linkedlist, keeping them in nonincreasing order of access counts. We access or removean element by searching the list from the most frequently accessed to the leastfrequently accessed. Reporting the top kmost accessed elements is easy, as they\nare the \ufb01rst kentries of the list.\nTo maintain the invariant that elements are stored in nonincreasing order of\naccess counts, we must consider how a single access operation may affect the order.The accessed element\u2019s count increases by one, and so it may become larger than\none or more of its preceding neighbors in the list, thereby violating the invariant.\nFortunately, we can reestablish the sorted invariant using a technique similar to\na single pass of the insertion-sort algorithm, introduced in the previous section. We\ncan perform a backward traversal of the list, starting at the position of the elementwhose access count has increased, until we locate a valid position after which the\nelement can be relocated.", "7.6. Case Study: Maintaining Access Frequencies 287\nUsing the Composition Pattern\nWe wish to implement a favorites list by making use of a PositionalList for storage.\nIf elements of the positional list were simply elements of the favorites list, we\nwould be challenged to maintain access counts and to keep the proper count with\nthe associated element as the contents of the list are reordered. We use a generalobject-oriented design pattern, the composition pattern , in which we de\ufb01ne a single\nobject that is composed of two or more other objects. Speci\ufb01cally, we de\ufb01ne anonpublic nested class,\nItem , that stores the element and its access count as a\nsingle instance. We then maintain our favorites list as a PositionalList ofitem\ninstances, so that the access count for a user\u2019s element is embedded alongside it inour representation. (An\nItem is never exposed to a user of a FavoritesList .)\n1classFavoritesList:\n2\u201d\u201d\u201dList of elements ordered from most frequently accessed to least.\u201d\u201d\u201d\n3\n4#------------------------------ nested\n Item class ------------------------------\n5class\n Item:\n6\n slots\n =\n_value\n ,\n_count\n # streamline memory usage\n7 def\n init\n(self,e ) :\n8 self.\nvalue = e # the user\n se l e m e n t\n9 self.\ncount = 0 # access count initially zero\n10\n11 #------------------------------- nonpublic utilities -------------------------------\n12def\n\ufb01nd\nposition( self,e ) :\n13 \u201d\u201d\u201dSearch for element e and return its Position (or None if not found).\u201d\u201d\u201d\n14 walk = self.\ndata.\ufb01rst()\n15 while walkis not None and walk.element().\n value != e:\n16 walk = self.\ndata.after(walk)\n17 return walk\n1819def\nmove\n up(self,p ) :\n20 \u201d\u201d\u201dMove item at Position p earlier in the list based on access count.\u201d\u201d\u201d\n21 ifp! =self.\ndata.\ufb01rst(): # consider moving...\n22 cnt = p.element().\n count\n23 walk = self.\ndata.before(p)\n24 ifcnt>walk.element().\n count: # must shift forward\n25 while (walk != self.\ndata.\ufb01rst( ) and\n26 cnt>self.\ndata.before(walk).element().\n count):\n27 walk = self.\ndata.before(walk)\n28 self.\ndata.add\n before(walk, self.\ndata.delete(p)) # delete/reinsert\nCode Fragment 7.18: ClassFavoritesList . (Continues in Code Fragment 7.19.)", "288 Chapter 7. Linked Lists\n29 #------------------------------- public methods -------------------------------\n30def\n init\n(self):\n31 \u201d\u201d\u201dCreate an empty list of favorites.\u201d\u201d\u201d\n32 self.\ndata = PositionalList( ) # will be list of\n Item instances\n33\n34def\n len\n(self):\n35 \u201d\u201d\u201dReturn number of entries on favorites list.\u201d\u201d\u201d\n36 return len(self.\ndata)\n37\n38defis\nempty( self):\n39 \u201d\u201d\u201dReturn True if list is empty.\u201d\u201d\u201d\n40 return len(self.\ndata) == 0\n41\n42defaccess( self,e ) :\n43 \u201d\u201d\u201dAccess element e, thereby increasing its access count.\u201d\u201d\u201d\n44 p=self.\n\ufb01nd\nposition(e) # try to locate existing element\n45 ifpis None :\n46 p=self.\ndata.add\n last(self.\nItem(e)) # if new, place at end\n47 p.element().\n count += 1 # always increment count\n48 self.\nmove\n up(p) # consider moving forward\n4950defremove( self,e ) :\n51 \u201d\u201d\u201dRemove element e from the list of favorites.\u201d\u201d\u201d\n52 p=self.\n\ufb01nd\nposition(e) # try to locate existing element\n53 ifpis not None :\n54 self.\ndata.delete(p) # delete, if found\n55\n56deftop(self,k ) :\n57 \u201d\u201d\u201dGenerate sequence of top k elements in terms of access count.\u201d\u201d\u201d\n58 if not 1<=k<=l e n (self):\n59 raiseValueError(\n Illegal value for k\n )\n60 walk = self.\ndata.\ufb01rst()\n61 forjinrange(k):\n62 item = walk.element( ) # element of list is\n Item\n63 yielditem.\n value # report user\u2019s element\n64 walk = self.\ndata.after(walk)\nCode Fragment 7.19: ClassFavoritesList . (Continued from Code Fragment 7.18.)", "7.6. Case Study: Maintaining Access Frequencies 289\n7.6.2 Using a List with the Move-to-Front Heuristic\nThe previous implementation of a favorites list performs the access(e) method in\ntime proportional to the index of ein the favorites list. That is, if eis the kthmost\npopular element in the favorites list, then accessing it takes O(k)time. In many\nreal-life access sequences (e.g., Web pages visited by a user), once an element is\naccessed it is more likely to be accessed again in the near future. Such scenariosare said to possess locality of reference .\nAheuristic , or rule of thumb, that attempts to take advantage of the locality of\nreference that is present in an access sequence is the move-to-front heuristic .T o\napply this heuristic, each time we access an element we move it all the way to thefront of the list. Our hope, of course, is that this element will be accessed again inthe near future. Consider, for example, a scenario in which we have nelements and\nthe following series of n\n2accesses:\n\u2022element 1 is accessed ntimes\n\u2022element 2 is accessed ntimes\n\u2022 \u00b7\u00b7\u00b7\n\u2022element nis accessed ntimes.\nIf we store the elements sorted by their access counts, inserting each element the\n\ufb01rst time it is accessed, then\n\u2022each access to element 1 runs in O(1)time\n\u2022each access to element 2 runs in O(2)time\n\u2022 \u00b7\u00b7\u00b7\n\u2022each access to element nruns in O(n)time.\nThus, the total time for performing the series of accesses is proportional to\nn+2n+3n+\u00b7\u00b7\u00b7+n\u00b7n=n(1+2+3+\u00b7\u00b7\u00b7+n)=n\u00b7n(n+1)\n2,\nwhich is O(n3).\nOn the other hand, if we use the move-to-front heuristic, inserting each element\nthe \ufb01rst time it is accessed, then\n\u2022each subsequent access to element 1 takes O(1)time\n\u2022each subsequent access to element 2 takes O(1)time\n\u2022 \u00b7\u00b7\u00b7\u2022each subsequent access to element nruns in O(1)time.\nSo the running time for performing all the accesses in this case is O(n\n2). Thus,\nthe move-to-front implementation has faster access times for this scenario. Still,\nthe move-to-front approach is just a heuristic, for there are access sequences whereusing the move-to-front approach is slower than simply keeping the favorites list\nordered by access counts.", "290 Chapter 7. Linked Lists\nThe Trade-O\ufb00s with the Move-to-Front Heuristic\nIf we no longer maintain the elements of the favorites list ordered by their access\ncounts, when we are asked to \ufb01nd the kmost accessed elements, we need to search\nfor them. We will implement the top (k)method as follows:\n1. We copy all entries of our favorites list into another list, named temp.\n2. We scan the temp listktimes. In each scan, we \ufb01nd the entry with the largest\naccess count, remove this entry from temp , and report it in the results.\nThis implementation of method toptakes O(kn)time. Thus, when kis a constant,\nmethod topruns in O(n)time. This occurs, for example, when we want to get the\n\u201ctop ten\u201d list. However, if kis proportional to n,t h e ntop runs in O(n2)time. This\noccurs, for example, when we want a \u201ctop 25%\u201d list.\nIn Chapter 9 we will introduce a data structure that will allow us to implement\ntopinO(n+klogn)time (see Exercise P-9.54), and more advanced techniques\ncould be used to perform topinO(n+klogk)time.\nWe could easily achieve O(nlogn)time if we use a standard sorting algorithm\nto reorder the temporary list before reporting the top k(see Chapter 12); this ap-\nproach would be preferred to the original in the case that kis\u03a9(logn). (Recall\nthe big-Omega notation introduced in Section 3.3.1 to give an asymptotic lower\nbound on the running time of an algorithm.) There is a more specialized sorting\nalgorithm (see Section 12.4.2) that can take advantage of the fact that access countsare integers in order to achieve O(n)time for top, for any value of k.\nImplementing the Move-to-Front Heuristic in Python\nWe give an implementation of a favorites list using the move-to-front heuristic in\nCode Fragment 7.20. The new FavoritesListMTF class inherits most of its func-\ntionality from the original FavoritesList as a base class.\nBy our original design, the access method of the original class relies on a non-\npublic utility named\n move\n upto enact the potential shifting of an element forward\nin the list, after its access count had been incremented. Therefore, we implementthe move-to-front heuristic by simply overriding the\nmove\n upmethod so that each\naccessed element is moved directly to the front of the list (if not already there). This\naction is easily implemented by means of the positional list ADT.\nThe more complex portion of our FavoritesListMTF class is the new de\ufb01nition\nfor the topmethod. We rely on the \ufb01rst of the approaches outlined above, inserting\ncopies of the items into a temporary list and then repeatedly \ufb01nding, reporting, and\nremoving an element that has the largest access count of those remaining.", "7.6. Case Study: Maintaining Access Frequencies 291\n1classFavoritesListMTF(FavoritesList):\n2\u201d\u201d\u201dList of elements ordered with move-to-front heuristic.\u201d\u201d\u201d\n3\n4# we override\n move\n up to provide move-to-front semantics\n5def\nmove\n up(self,p ) :\n6 \u201d\u201d\u201dMove accessed item at Position p to front of list.\u201d\u201d\u201d\n7 ifp! =self.\ndata.\ufb01rst():\n8 self.\ndata.add\n \ufb01rst(self.\ndata.delete(p)) # delete/reinsert\n9\n10 # we override top because list is no longer sorted\n11deftop(self,k ) :\n12 \u201d\u201d\u201dGenerate sequence of top k elements in terms of access count.\u201d\u201d\u201d\n13 if not 1<=k<=l e n (self):\n14 raiseValueError(\n Illegal value for k\n )\n15\n16 # we begin by making a copy of the original list\n17 temp = PositionalList()\n18 foritemin self .\ndata: # positional lists support iteration\n19 temp.add\n last(item)\n2021 # we repeatedly \ufb01nd, report, and remove element with largest count\n22 forjinrange(k):\n23 # \ufb01nd and report next highest from temp\n24 highPos = temp.\ufb01rst()\n25 walk = temp.after(highPos)\n26 while walkis not None :\n27 ifwalk.element().\ncount >highPos.element().\n count:\n28 highPos = walk\n29 walk = temp.after(walk)\n30 # we have found the element with highest count\n31 yieldhighPos.element().\n value # report element to user\n32 temp.delete(highPos) # remove from temp list\nCode Fragment 7.20: Class FavoritesListMTF implementing the move-to-front\nheuristic. This class extends FavoritesList (Code Fragments 7.18 and 7.19) and\noverrides methods\n move\n upandtop.", "292 Chapter 7. Linked Lists\n7.7 Link-Based vs. Array-Based Sequences\nWe close this chapter by re\ufb02ecting on the relative pros and cons of array-based\nand link-based data structures that have been introduced thus far. The dichotomybetween these approaches presents a common design decision when choosing an\nappropriate implementation of a data structure. There is not a one-size-\ufb01ts-all so-\nlution, as each offers distinct advantages and disadvantages.\nAdvantages of Array-Based Sequences\n\u2022Arrays provide O (1)-time access to an element based on an integer index.\nThe ability to access the kthelement for any kinO(1)time is a hallmark\nadvantage of arrays (see Section 5.2). In contrast, locating the kthelement\nin a linked list requires O(k)time to traverse the list from the beginning,\nor possibly O(n\u2212k)time, if traversing backward from the end of a doubly\nlinked list.\n\u2022Operations with equivalent asymptotic bounds typically run a constant factor\nmore ef\ufb01ciently with an array-based structure versus a linked structure. As\nan example, consider the typical enqueue operation for a queue. Ignoring\nthe issue of resizing an array, this operation for the ArrayQueue class (see\nCode Fragment 6.7) involves an arithmetic calculation of the new index, an\nincrement of an integer, and storing a reference to the element in the array.\nIn contrast, the process for a LinkedQueue (see Code Fragment 7.8) requires\nthe instantiation of a node, appropriate linking of nodes, and an increment\nof an integer. While this operation completes in O(1)time in either model,\nthe actual number of CPU operations will be more in the linked version,\nespecially given the instantiation of the new node.\n\u2022Array-based representations typically use proportionally less memory than\nlinked structures. This advantage may seem counterintuitive, especially given\nthat the length of a dynamic array may be longer than the number of elements\nthat it stores. Both array-based lists and linked lists are referential structures,\nso the primary memory for storing the actual objects that are elements is thesame for either structure. What differs is the auxiliary amounts of memorythat are used by the two structures. For an array-based container of nele-\nments, a typical worst case may be that a recently resized dynamic array has\nallocated memory for 2 nobject references. With linked lists, memory must\nbe devoted not only to store a reference to each contained object, but also\nexplicit references that link the nodes. So a singly linked list of length n\nalready requires 2 nreferences (an element reference and next reference for\neach node). With a doubly linked list, there are 3 nreferences.", "7.7. Link-Based vs. Array-Based Sequences 293\nAdvantages of Link-Based Sequences\n\u2022Link-based structures provide worst-case time bounds for their operations.\nThis is in contrast to the amortized bounds associated with the expansion or\ncontraction of a dynamic array (see Section 5.3).\nWhen many individual operations are part of a larger computation, and we\nonly care about the total time of that computation, an amortized bound is as\ngood as a worst-case bound precisely because it gives a guarantee on the sum\nof the time spent on the individual operations.\nHowever, if data structure operations are used in a real-time system that is de-\nsigned to provide more immediate responses (e.g., an operating system, Webserver, air traf\ufb01c control system), a long delay caused by a single (amortized)operation may have an adverse effect.\n\u2022Link-based structures support O (1)-time insertions and deletions at arbi-\ntrary positions. The ability to perform a constant-time insertion or deletion\nwith the PositionalList class, by using a Position to ef\ufb01ciently describe the\nlocation of the operation, is perhaps the most signi\ufb01cant advantage of thelinked list.\nThis is in stark contrast to an array-based sequence. Ignoring the issue of\nresizing an array, inserting or deleting an element from the end of an array-based list can be done in constant time. However, more general insertions anddeletions are expensive. For example, with Python\u2019s array-based listclass, a\ncall to insert orpopwith index kuses O(n\u2212k+1)time because of the loop\nto shift all subsequent elements (see Section 5.4).\nAs an example application, consider a text editor that maintains a document\nas a sequence of characters. Although users often add characters to the end\nof the document, it is also possible to use the cursor to insert or delete one or\nmore characters at an arbitrary position within the document. If the charac-ter sequence were stored in an array-based sequence (such as a Python list),\neach such edit operation may require linearly many characters to be shifted,\nleading to O(n)performance for each edit operation. With a linked-list rep-\nresentation, an arbitrary edit operation (insertion or deletion of a character\nat the cursor) can be performed in O(1)worst-case time, assuming we are\ngiven a position that represents the location of the cursor.", "294 Chapter 7. Linked Lists\n7.8 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-7.1 Give an algorithm for \ufb01nding the second-to-last node in a singly linked\nlist in which the last node is indicated by a next reference of None .\nR-7.2 Describe a good algorithm for concatenating two singly linked lists Land\nM, given only references to the \ufb01rst node of each list, into a single list L/prime\nthat contains all the nodes of Lfollowed by all the nodes of M.\nR-7.3 Describe a recursive algorithm that counts the number of nodes in a singlylinked list.\nR-7.4 Describe in detail how to swap two nodes xandy(and not just their con-\ntents) in a singly linked list Lgiven references only to xandy. Repeat\nthis exercise for the case when Lis a doubly linked list. Which algorithm\ntakes more time?\nR-7.5 Implement a function that counts the number of nodes in a circularlylinked list.\nR-7.6 Suppose that xandyare references to nodes of circularly linked lists,\nalthough not necessarily the same list. Describe a fast algorithm for telling\nifxandybelong to the same list.\nR-7.7 OurCircularQueue class of Section 7.2.2 provides a rotate() method that\nhas semantics equivalent to Q.enqueue(Q.dequeue()) , for a nonempty\nqueue. Implement such a method for the LinkedQueue class of Sec-\ntion 7.1.2 without the creation of any new nodes.\nR-7.8 Describe a nonrecursive method for \ufb01nding, by link hopping, the middlenode of a doubly linked list with header and trailer sentinels. In the case\nof an even number of nodes, report the node slightly left of center as the\u201cmiddle.\u201d (Note: This method must only use link hopping; it cannot use acounter.) What is the running time of this method?\nR-7.9 Give a fast algorithm for concatenating two doubly linked lists LandM,\nwith header and trailer sentinel nodes, into a single list L\n/prime.\nR-7.10 There seems to be some redundancy in the repertoire of the positionallist ADT, as the operation L.add\n\ufb01rst(e) could be enacted by the alter-\nnative L.add\n before(L.\ufb01rst(), e). Likewise, L.add\n last(e) might be per-\nformed as L.add\n after(L.last(), e) . Explain why the methods add\n\ufb01rst\nandadd\nlastare necessary.", "7.8. Exercises 295\nR-7.11 Implement a function, with calling syntax max(L) , that returns the max-\nimum element from a PositionalList instance Lcontaining comparable\nelements.\nR-7.12 Redo the previously problem with max as a method of the PositionalList\nclass, so that calling syntax L.max() is supported.\nR-7.13 Update the PositionalList class to support an additional method \ufb01nd(e) ,\nwhich returns the position of the (\ufb01rst occurrence of) element ein the list\n(orNone if not found).\nR-7.14 Repeat the previous process using recursion. Your method should not\ncontain any loops. How much space does your method use in addition to\nthe space used for L?\nR-7.15 Provide support for a\n reversed\n method of the PositionalList class that\nis similar to the given\n iter\n , but that iterates the elements in reversed\norder.\nR-7.16 Describe an implementation of the PositionalList methods add\nlastand\nadd\nbefore realized by using only methods in the set {is\nempty ,\ufb01rst,last,\nprev,next,add\nafter ,a n dadd\n\ufb01rst} .\nR-7.17 In theFavoritesListMTF class, we rely on public methods of the positional\nlist ADT to move an element of a list at position pto become the \ufb01rst ele-\nment of the list, while keeping the relative order of the remaining elements\nunchanged. Internally, that combination of operations causes one node tobe removed and a new node to be inserted. Augment the PositionalList\nclass to support a new method, move\nto\nfront(p) , that accomplishes this\ngoal more directly, by relinking the existing node.\nR-7.18 Given the set of element {a,b,c,d,e,f}stored in a list, show the \ufb01nal state\nof the list, assuming we use the move-to-front heuristic and access the el-ements according to the following sequence: (a,b,c,d,e,f,a,c,f,b,d,e).\nR-7.19 Suppose that we have made kntotal accesses to the elements in a list Lof\nnelements, for some integer k\u22651. What are the minimum and maximum\nnumber of elements that have been accessed fewer than ktimes?\nR-7.20 LetLbe a list of nitems maintained according to the move-to-front heuris-\ntic. Describe a series of O(n)accesses that will reverse L.\nR-7.21 Suppose we have an n-element list Lmaintained according to the move-\nto-front heuristic. Describe a sequence of n\n2accesses that is guaranteed\nto take \u03a9(n3)time to perform on L.\nR-7.22 Implement a clear() method for the FavoritesList class that returns the list\nto empty.\nR-7.23 Implement a reset\ncounts() method for the FavoritesList class that resets\nall elements\u2019 access counts to zero (while leaving the order of the listunchanged).", "296 Chapter 7. Linked Lists\nCreativity\nC-7.24 Give a complete implementation of the stack ADT using a singly linked\nlist that includes a header sentinel.\nC-7.25 Give a complete implementation of the queue ADT using a singly linkedlist that includes a header sentinel.\nC-7.26 Implement a method, concatenate(Q2) for the LinkedQueue class that\ntakes all elements of LinkedQueue Q2 and appends them to the end of the\noriginal queue. The operation should run in O(1)time and should result\ninQ2being an empty queue.\nC-7.27 Give a recursive implementation of a singly linked list class, such that aninstance of a nonempty list stores its \ufb01rst element and a reference to a listof remaining elements.\nC-7.28 Describe a fast recursive algorithm for reversing a singly linked list.\nC-7.29 Describe in detail an algorithm for reversing a singly linked list Lusing\nonly a constant amount of additional space and not using any recursion.\nC-7.30 Exercise P-6.35 describes a LeakyStack abstraction. Implement that ADT\nusing a singly linked list for storage.\nC-7.31 Design a forward list ADT that abstracts the operations on a singly linked\nlist, much as the positional list ADT abstracts the use of a doubly linked\nlist. Implement a ForwardList class that supports such an ADT.\nC-7.32 Design a circular positional list ADT that abstracts a circularly linked listin the same way that the positional list ADT abstracts a doubly linked list,\nwith a notion of a designated \u201ccursor\u201d position within the list.\nC-7.33 Modify the\nDoublyLinkedBase class to include a reverse method that re-\nverses the order of the list, yet without creating or destroying any nodes.\nC-7.34 Modify the PositionalList class to support a method swap(p, q) that causes\nthe underlying nodes referenced by positions pandqto be exchanged for\neach other. Relink the existing nodes; do not create any new nodes.\nC-7.35 To implement the itermethod of the PositionalList class, we relied on the\nconvenience of Python\u2019s generator syntax and the yield statement. Give\nan alternative implementation of iterby designing a nested iterator class.\n(See Section 2.3.4 for discussion of iterators.)\nC-7.36 Give a complete implementation of the positional list ADT using a doublylinked list that does not include any sentinel nodes.\nC-7.37 Implement a function that accepts a PositionalList Lofnintegers sorted\nin nondecreasing order, and another value V, and determines in O(n)time\nif there are two elements of Lthat sum precisely to V. The function should\nreturn a pair of positions of such elements, if found, or None otherwise.", "7.8. Exercises 297\nC-7.38 There is a simple, but inef\ufb01cient, algorithm, called bubble-sort , for sorting\na list Lofncomparable elements. This algorithm scans the list n\u22121 times,\nwhere, in each scan, the algorithm compares the current element with the\nnext one and swaps them if they are out of order. Implement a bubble\n sort\nfunction that takes a positional list Las a parameter. What is the running\ntime of this algorithm, assuming the positional list is implemented with a\ndoubly linked list?\nC-7.39 To better model a FIFO queue in which entries may be deleted beforereaching the front, design a PositionalQueue class that supports the com-\nplete queue ADT, yet with enqueue returning a position instance and sup-\nport for a new method, delete(p) , that removes the element associated\nwith position pfrom the queue. You may use the adapter design pattern\n(Section 6.1.2), using a PositionalList as your storage.\nC-7.40 Describe an ef\ufb01cient method for maintaining a favorites list L, with move-\nto-front heuristic, such that elements that have not been accessed in themost recent naccesses are automatically purged from the list.\nC-7.41 Exercise C-5.29 introduces the notion of a natural join of two databases.\nDescribe and analyze an ef\ufb01cient algorithm for computing the natural joinof a linked list Aofnpairs and a linked list Bofmpairs.\nC-7.42 Write a Scoreboard class that maintains the top 10 scores for a game ap-\nplication using a singly linked list, rather than the array that was used inSection 5.5.1.\nC-7.43 Describe a method for performing a card shuf\ufb02e o fal i s to f2 nelements,\nby converting it into two lists. A card shuf\ufb02e is a permutation where a list\nLis cut into two lists, L\n1andL2,w h e r e L1is the \ufb01rst half of LandL2is the\nsecond half of L, and then these two lists are merged into one by taking\nthe \ufb01rst element in L1, then the \ufb01rst element in L2, followed by the second\nelement in L1, the second element in L2, and so on.\nProjects\nP-7.44 Write a simple text editor that stores and displays a string of charactersusing the positional list ADT, together with a cursor object that highlights\na position in this string. A simple interface is to print the string and then\nto use a second line of output to underline the position of the cursor. Youreditor should support the following operations:\n\u2022left: Move cursor left one character (do nothing if at beginning).\n\u2022right : Move cursor right one character (do nothing if at end).\n\u2022insert c: Insert the character cjust after the cursor.\n\u2022delete : Delete the character just after the cursor (do nothing at end).", "298 Chapter 7. Linked Lists\nP-7.45 An array Aissparse if most of its entries are empty (i.e., None ). A list\nLcan be used to implement such an array ef\ufb01ciently. In particular, for\neach nonempty cell A[i], we can store an entry (i,e)inL,w h e r e eis the\nelement stored at A[i]. This approach allows us to represent Ausing O(m)\nstorage, where mis the number of nonempty entries in A. Provide such\naSparseArray class that minimally supports methods\n getitem\n (j)and\nsetitem\n (j, e) to provide standard indexing operations. Analyze the\nef\ufb01ciency of these methods.\nP-7.46 Although we have used a doubly linked list to implement the positional\nlist ADT, it is possible to support the ADT with an array-based imple-\nmentation. The key is to use the composition pattern and store a sequenceof position items, where each item stores an element as well as that ele-ment\u2019s current index in the array. Whenever an element\u2019s place in the array\nis changed, the recorded index in the position must be updated to match.\nGiven a complete class providing such an array-based implementation ofthe positional list ADT. What is the ef\ufb01ciency of the various operations?\nP-7.47 Implement a CardHand class that supports a person arranging a group of\ncards in his or her hand. The simulator should represent the sequence ofcards using a single positional list ADT so that cards of the same suit arekept together. Implement this strategy by means of four \u201c\ufb01ngers\u201d into thehand, one for each of the suits of hearts, clubs, spades, and diamonds,\nso that adding a new card to the person\u2019s hand or playing a correct card\nfrom the hand can be done in constant time. The class should support thefollowing methods:\n\u2022add\ncard(r, s) : Add a new card with rank rand suit sto the hand.\n\u2022play(s) : Remove and return a card of suit sfrom the player\u2019s hand;\nif there is no card of suit s, then remove and return an arbitrary card\nfrom the hand.\n\u2022\niter\n(): Iterate through all cards currently in the hand.\n\u2022all\nof\nsuit(s) : Iterate through all cards of suit sthat are currently in\nthe hand.\nChapter Notes\nA view of data structures as collections (and other principles of object-oriented design)\ncan be found in object-oriented design books by Booch [17], Budd [20], Goldberg and\nRobson [42], and Liskov and Guttag [71]. Our positional list ADT is derived from the\n\u201cposition\u201d abstraction int roduced by Aho, Hopcroft, and Ullman [6], and the list ADT of\nWood [104]. Implementations of linked lists are discussed by Knuth [64].", "Chapter\n8Trees\nContents\n8 . 1 G e n e r a lT r e e s ......................... 3 0 0\n8 . 1 . 1 T r e e D e \ufb01 n i t i o n s a n d P r o p e r t i e s...............3 0 1\n8 . 1 . 2 T h e T r e e A b s t r a c t D a t a T y p e ...............3 0 5\n8 . 1 . 3 C o m p u t i n g D e p t h a n dH e i g h t ................3 0 8\n8 . 2 B i n a r yT r e e s .......................... 3 1 1\n8 . 2 . 1 T h e B i n a r y T r e e A b s t r a c t D a t a T y p e............3 1 38 . 2 . 2 P r o p e r t i e s o f B i n a r y T r e e s .................3 1 5\n8 . 3 I m p l e m e n t i n gT r e e s ...................... 3 1 7\n8 . 3 . 1 L i n k e d S t r u c t u r e f o r B i n a r y T r e e s ..............3 1 78.3.2 Array-Based Representation of a Binary Tree . . . . . . . 325\n8 . 3 . 3 L i n k e d S t r u c t u r e f o r G e n e r a lT r e e s.............3 2 7\n8 . 4 T r e eT r a v e r s a lA l g o r i t h m s................... 3 2 8\n8.4.1 Preorder and Postorder Traversals of General Trees . . . . 328\n8 . 4 . 2 B r e a d t h - F i r s t T r e e T r a v e r s a l ................3 3 0\n8 . 4 . 3 I n o r d e r T r a v e r s a lo f aB i n a r y T r e e .............3 3 1\n8.4.4 Implementing Tree Traversals in Python . . . . . . . . . . 333\n8 . 4 . 5 A p p l i c a t i o n s o f T r e e T r a v e r s a l s...............3 3 78.4.6 Euler Tours and the Template Method Pattern\n\u22c6.....3 4 1\n8 . 5 C a s eS t u d y :A nE x p r e s s i o nT r e e............... 3 4 88 . 6 E x e r c i s e s ............................ 3 5 2\n", "300 Chapter 8. Trees\n8.1 General Trees\nProductivity experts say that breakthroughs come by thinking \u201cnonlinearly.\u201d In\nthis chapter, we discuss one of the most important nonlinear data structures incomputing\u2014 trees . Tree structures are indeed a breakthrough in data organization,\nfor they allow us to implement a host of algorithms much faster than when usinglinear data structures, such as array-based lists or linked lists. Trees also provide anatural organization for data, and consequently have become ubiquitous structuresin \ufb01le systems, graphical user interfaces, databases, Web sites, and other computer\nsystems.\nIt is not always clear what productivity experts mean by \u201cnonlinear\u201d thinking,\nbut when we say that trees are \u201cnonlinear,\u201d we are referring to an organizational\nrelationship that is richer than the simple \u201cbefore\u201d and \u201cafter\u201d relationships be-tween objects in sequences. The relationships in a tree are hierarchical , with some\nobjects being \u201cabove\u201d and some \u201cbelow\u201d others. Actually, the main terminologyfor tree data structures comes from family trees, with the terms \u201cparent,\u201d \u201cchild,\u201d\u201cancestor,\u201d and \u201cdescendant\u201d being the most common words used to describe rela-tionships. We show an example of a family tree in Figure 8.1.\nEldaahNebaioth\nKedar\nAdbeel\nMibsam\nMishma\nDumah\nMassa\nHadad\nTema\nJetur\nNaphish\nKedemahIshmael\nGadNaphtaliDanJudahLeviSimeon\nAsher\nIssachar\nZebulun\nDinah\nJoseph\nBenjaminEliphaz\nReuel\nJeush\nJalam\nReubenKorah\nJacob (Israel)Esau\nIsaac\nZimran\nJokshan\nMedan\nMidian\nIshbak\nShuahAbraham\nSheba\nDedan\nEphah\nEpher\nHanoch\nAbida\nFigure 8.1: A family tree showing some descendants of Abraham, as recorded in\nGenesis, chapters 25\u201336.", "8.1. General Trees 301\n8.1.1 Tree De\ufb01nitions and Properties\nAtreeis an abstract data type that stores elements hierarchically. With the excep-\ntion of the top element, each element in a tree has a parent element and zero or\nmore children elements. A tree is usually visualized by placing elements inside\novals or rectangles, and by drawing the connections between parents and children\nwith straight lines. (See Figure 8.2.) We typically call the top element the root\nof the tree, but it is drawn as the highest element, with the other elements being\nconnected below (just the opposite of a botanical tree).\nEurope Asia Africa AustraliaCanada Overseas S. AmericaDomestic International TV CD TunerSales Purchasing Manufacturing R&DElectronics R\u2019Us\nFigure 8.2: A tree with 17 nodes representing the organization of a \ufb01ctitious cor-\nporation. The root stores Electronics R\u2019Us . The children of the root store R&D,\nSales ,Purchasing ,a n d Manufacturing . The internal nodes store Sales ,Interna-\ntional ,Overseas ,Electronics R\u2019Us ,a n d Manufacturing .\nFormal Tree De\ufb01nition\nFormally, we de\ufb01ne a treeTas a set of nodes storing elements such that the nodes\nhave a parent-child relationship that satis\ufb01es the following properties:\n\u2022IfTis nonempty, it has a special node, called the rootofT, that has no parent.\n\u2022Each node vofTdifferent from the root has a unique parent node w;e v e r y\nnode with parent wis achild ofw.\nNote that according to our de\ufb01nition, a tree can be empty, meaning that it does not\nhave any nodes. This convention also allows us to de\ufb01ne a tree recursively suchthat a tree Tis either empty or consists of a node r, called the root of T,a n da\n(possibly empty) set of subtrees whose roots are the children of r.", "302 Chapter 8. Trees\nOther Node Relationships\nTwo nodes that are children of the same parent are siblings . A node visexternal\nifvhas no children. A node visinternal if it has one or more children. External\nnodes are also known as leaves .\nExample 8.1: In Section 4.1.4, we discussed the hierarchical relationship be-\ntween \ufb01les and directories in a computer\u2019s \ufb01le system, although at the time we\ndid not emphasize the nomenclature of a \ufb01le system as a tree. In Figure 8.3, we\nrevisit an earlier example. We see that the internal nodes of the tree are associ-\nated with directories and the leaves are associated with regular \ufb01les. In the UNIX\nand Linux operating systems, the root of the tree is appropriately called the \u201croot\ndirectory,\u201d and is represented by the symbol \u201c /.\u201d\n/user/rt/courses/\ncs016/ cs252/\nprograms/ homeworks/ projects/\npapers/ demos/hw1 hw2 hw3 pr1 pr2 pr3grades\nmarket buylow sellhighgrades\nFigure 8.3: Tree representing a portion of a \ufb01le system.\nA node uis an ancestor of a node vifu=voruis an ancestor of the parent\nofv. Conversely, we say that a node vis adescendant of a node uifuis an ancestor\nofv. For example, in Figure 8.3, cs252/ is an ancestor of papers/ ,a n dpr3is a\ndescendant of cs016/ .T h e subtree ofTrooted at a node vis the tree consisting of\nall the descendants of vinT(including vitself). In Figure 8.3, the subtree rooted at\ncs016/ consists of the nodes cs016/ ,grades ,homeworks/ ,programs/ ,hw1,hw2,\nhw3,pr1,pr2,a n dpr3.\nEdges and Paths in Trees\nAnedge of tree Tis a pair of nodes (u,v)such that uis the parent of v,o rv i c e\nversa. A path ofTis a sequence of nodes such that any two consecutive nodes in\nthe sequence form an edge. For example, the tree in Figure 8.3 contains the path\n(cs252/ ,projects/ ,demos/ ,market ).", "8.1. General Trees 303\nExample 8.2: The inheritance relation between classes in a Python program forms\na tree when single inheritance is used. For example, in Section 2.4 we provided a\nsummary of the hierarchy for Python\u2019s exception types, as portrayed in Figure 8.4\n(originally Figure 2.5). The BaseException class is the root of that hierarchy, while\nall user-de\ufb01ned exception classes should conventionally be declared as descendants\nof the more speci\ufb01c Exception class. (See, for example, the Empty class we intro-\nduced in Code Fragment 6.1 of Chapter 6.)\nValueErrorException KeyboardInterrupt SystemExitBaseException\nIndexError KeyError ZeroDivisionErrorLookupError ArithmeticError\nFigure 8.4: A portion of Python\u2019s hierarchy of exception types.\nIn Python, all classes are organized into a single hierarchy, as there exists a\nbuilt-in class named object as the ultimate base class. It is a direct or indirect base\nclass of all other types in Python (even if not declared as such when de\ufb01ning a new\nclass). Therefore, the hierarchy pictured in Figure 8.4 is only a portion of Python\u2019s\ncomplete class hierarchy.\nAs a preview of the remainder of this chapter, Figure 8.5 portrays our own\nhierarchy of classes for representing various forms of a tree.\nArrayBinaryTree LinkedBinaryTreeTree\nLinkedTree BinaryTree\nFigure 8.5: Our own inheritance hierarchy for modeling various abstractions and\nimplementations of tree data structures. In the remainder of this chapter, we provideimplementations of Tree,BinaryTree ,a n d LinkedBinaryTree classes, and high-\nlevel sketches for how LinkedTree andArrayBinaryTree might be designed.", "304 Chapter 8. Trees\nOrdered Trees\nA tree is ordered if there is a meaningful linear order among the children of each\nnode; that is, we purposefully identify the children of a node as being the \ufb01rst,\nsecond, third, and so on. Such an order is usually visualized by arranging siblings\nleft to right, according to their order.\nExample 8.3: The components of a structured document, such as a book, are hier-\narchically organized as a tree whose internal nodes are parts, chapters, and sections,\nand whose leaves are paragraphs, tables, \ufb01gures, and so on. (See Figure 8.6.) The\nroot of the tree corresponds to the book itself. We could, in fact, consider expanding\nthe tree further to show paragraphs consisting of sentences, sentences consisting of\nwords, and words consisting of characters. Such a tree is an example of an ordered\ntree, because there is a well-de\ufb01ned order among the children of each node.\n... ... \u00b6 \u00b6 ...\u00b6\u00b6Book\nPart A Part B References Preface\n... ... ... ... Ch. 1 Ch. 5 Ch. 6 Ch. 9 \u00b6\u00b6 \u00b6\u00b6\n... ... ... ... \u00a7 1.4 \u00a7 1.1 \u00a7 5.7 \u00a7 5.1 \u00a7 9.6 \u00a7 9.1 \u00a7 6.5 \u00a7 6.1\nFigure 8.6: An ordered tree associated with a book.\nLet\u2019s look back at the other examples of trees that we have described thus far,\nand consider whether the order of children is signi\ufb01cant. A family tree that de-\nscribes generational relationships, as in Figure 8.1, is often modeled as an orderedtree, with siblings ordered according to their birth.\nIn contrast, an organizational chart for a company, as in Figure 8.2, is typically\nconsidered an unordered tree. Likewise, when using a tree to describe an inher-itance hierarchy, as in Figure 8.4, there is no particular signi\ufb01cance to the orderamong the subclasses of a parent class. Finally, we consider the use of a tree inmodeling a computer\u2019s \ufb01le system, as in Figure 8.3. Although an operating systemoften displays entries of a directory in a particular order (e.g., alphabetical, chrono-\nlogical), such an order is not typically inherent to the \ufb01le system\u2019s representation.", "8.1. General Trees 305\n8.1.2 The Tree Abstract Data Type\nAs we did with positional lists in Section 7.4, we de\ufb01ne a tree ADT using the\nconcept of a position as an abstraction for a node of a tree. An element is stored\nat each position, and positions satisfy parent-child relationships that de\ufb01ne the tree\nstructure. A position object for a tree supports the method:\np.element() :Return the element stored at position p.\nThe tree ADT then supports the following accessor methods , allowing a user to\nnavigate the various positions of a tree:\nT.root() :Return the position of the root of tree T,\norNone ifTis empty.\nT.is\nroot(p): Return True if position pis the root of Tree T.\nT.parent(p): Return the position of the parent of position p,\norNone ifpis the root of T.\nT.num\n children(p) :Return the number of children of position p.\nT.children(p) :Generate an iteration of the children of position p.\nT.is\nleaf(p) :Return True if position pdoes not have any children.\nlen(T) :Return the number of positions (and hence elements) thatare contained in tree T.\nT.is\nempty() :Return True if tree Tdoes not contain any positions.\nT.positions() :Generate an iteration of all positions of tree T.\niter(T) :Generate an iteration of all elements stored within tree T.\nAny of the above methods that accepts a position as an argument should generate aValueError if that position is invalid for T.\nIf a tree Tis ordered, then T.children(p) reports the children of pin the natural\norder. If pis a leaf, then T.children(p) generates an empty iteration. In similar\nregard, if tree Tis empty, then both T.positions() anditer(T) generate empty iter-\nations. We will discuss general means for iterating through all positions of a tree inSections 8.4.\nWe do not de\ufb01ne any methods for creating or modifying trees at this point.\nWe prefer to describe different tree update methods in conjunction with speci\ufb01c\nimplementations of the tree interface, and speci\ufb01c applications of trees.", "306 Chapter 8. Trees\nA Tree Abstract Base Class in Python\nIn discussing the object-oriented design principle of abstraction in Section 2.1.2, we\nnoted that a public interface for an abstract data type is often managed in Python viaduck typing . For example, we de\ufb01ned the notion of the public interface for a queue\nADT in Section 6.2, and have since presented several classes that implement thequeue interface (e.g., ArrayQueue in Section 6.2.2, LinkedQueue in Section 7.1.2,\nCircularQueue in Section 7.2.2). However, we never gave any formal de\ufb01nition of\nthe queue ADT in Python; all of the concrete implementations were self-containedclasses that just happen to adhere to the same public interface. A more formalmechanism to designate the relationships between different implementations of the\nsame abstraction is through the de\ufb01nition of one class that serves as an abstract\nbase class , via inheritance, for one or more concrete classes . (See Section 2.4.3.)\nWe choose to de\ufb01ne a Tree class, in Code Fragment 8.1, that serves as an ab-\nstract base class corresponding to the tree ADT. Our reason for doing so is that thereis quite a bit of useful code that we can provide, even at this level of abstraction, al-\nlowing greater code reuse in the concrete tree implementations we later de\ufb01ne. TheTree class provides a de\ufb01nition of a nested Position class (which is also abstract),\nand declarations of many of the accessor methods included in the tree ADT.\nHowever, our Tree class does not de\ufb01ne any internal representation for stor-\ning a tree, and \ufb01ve of the methods given in that code fragment remain abstract\n(root,parent ,num\nchildren ,children ,a n d\n len\n ); each of these methods raises a\nNotImplementedError . (A more formal approach for de\ufb01ning abstract base classes\nand abstract methods, using Python\u2019s abcmodule, is described in Section 2.4.3.)\nThe subclasses are responsible for overriding abstract methods, such as children ,t o\nprovide a working implementation for each behavior, based on their chosen internalrepresentation.\nAlthough the Tree class is an abstract base class, it includes several concrete\nmethods with implementations that rely on calls to the abstract methods of the class.In de\ufb01ning the tree ADT in the previous section, we declare ten accessor methods.Five of those are the ones we left as abstract, in Code Fragment 8.1. The other \ufb01ve\ncan be implemented based on the former. Code Fragment 8.2 provides concrete\nimplementations for methods is\nroot,is\nleaf,a n dis\nempty . In Section 8.4, we will\nexplore general algorithms for traversing a tree that can be used to provide concreteimplementations of the positions and\niter\n methods within the Tree class. The\nbeauty of this design is that the concrete methods de\ufb01ned within the Tree abstract\nbase class will be inherited by all subclasses. This promotes greater code reuse, asthere will be no need for those subclasses to reimplement such behaviors.\nWe note that, with the Tree class being abstract, there is no reason to create a\ndirect instance of it, nor would such an instance be useful. The class exists to serve\nas a base for inheritance, and users will create instances of concrete subclasses.", "8.1. General Trees 307\n1classTree:\n2\u201d\u201d\u201dAbstract base class representing a tree structure.\u201d\u201d\u201d\n3\n4#------------------------------- nested Position class -------------------------------\n5classPosition:\n6 \u201d\u201d\u201dAn abstraction representing the location of a single element.\u201d\u201d\u201d\n78 defelement( self):\n9 \u201d\u201d\u201dReturn the element stored at this Position.\u201d\u201d\u201d\n10 raiseNotImplementedError(\nmust be implemented by subclass\n )\n11\n12 def\n eq\n(self,o t h e r ) :\n13 \u201d\u201d\u201dReturn True if other Position represents the same location.\u201d\u201d\u201d\n14 raiseNotImplementedError(\n must be implemented by subclass\n )\n15\n16 def\n ne\n(self,o t h e r ) :\n17 \u201d\u201d\u201dReturn True if other does not represent the same location.\u201d\u201d\u201d\n18 return not (self== other) #o p p o s i t eo f\n eq\n1920 # ---------- abstract methods that concrete subclass must support ----------\n21defroot(self):\n22 \u201d\u201d\u201dReturn Position representing the tree\ns root (or None if empty).\u201d\u201d\u201d\n23 raiseNotImplementedError(\n must be implemented by subclass\n )\n2425defparent( self,p ) :\n26 \u201d\u201d\u201dReturn Position representing p\ns parent (or None if p is root).\u201d\u201d\u201d\n27 raiseNotImplementedError(\n must be implemented by subclass\n )\n2829defnum\nchildren( self,p ) :\n30 \u201d\u201d\u201dReturn the number of children that Position p has.\u201d\u201d\u201d\n31 raiseNotImplementedError(\n must be implemented by subclass\n )\n3233defchildren( self,p ) :\n34 \u201d\u201d\u201dGenerate an iteration of Positions representing p\ns children.\u201d\u201d\u201d\n35 raiseNotImplementedError(\n must be implemented by subclass\n )\n3637def\nlen\n(self):\n38 \u201d\u201d\u201dReturn the total number of elements in the tree.\u201d\u201d\u201d\n39 raiseNotImplementedError(\n must be implemented by subclass\n )\nCode Fragment 8.1: A portion of our Tree abstract base class (continued in Code\nFragment 8.2).", "308 Chapter 8. Trees\n40 # ---------- concrete methods implemented in this class ----------\n41defis\nroot(self,p ) :\n42 \u201d\u201d\u201dReturn True if Position p represents the root of the tree.\u201d\u201d\u201d\n43 return self . r o o t ()= =p\n44\n45defis\nleaf(self ,p ) :\n46 \u201d\u201d\u201dReturn True if Position p does not have any children.\u201d\u201d\u201d\n47 return self .num\n children(p) == 0\n4849defis\nempty( self):\n50 \u201d\u201d\u201dReturn True if the tree is empty.\u201d\u201d\u201d\n51 return len(self)= =0\nCode Fragment 8.2: Some concrete methods of our Tree abstract base class.\n8.1.3 Computing Depth and Height\nLetpbe the position of a node of a tree T.T h e depth ofpis the number of\nancestors of p, excluding pitself. For example, in the tree of Figure 8.2, the node\nstoring International has depth 2. Note that this de\ufb01nition implies that the depth of\nthe root of Tis 0. The depth of pcan also be recursively de\ufb01ned as follows:\n\u2022Ifpis the root, then the depth of pis 0.\n\u2022Otherwise, the depth of pis one plus the depth of the parent of p.\nBased on this de\ufb01nition, we present a simple, recursive algorithm, depth , in Code\nFragment 8.3, for computing the depth of a position pinTree T. This method calls\nitself recursively on the parent of p, and adds 1 to the value returned.\n52defdepth(self,p ) :\n53 \u201d\u201d\u201dReturn the number of levels separating Position p from the root.\u201d\u201d\u201d\n54 if self.is\nroot(p):\n55 return 0\n56 else:\n57 return 1+self.depth( self.parent(p))\nCode Fragment 8.3: Method depth of the Tree class.\nThe running time of T.depth (p) for position pisO(dp+1),w h e r e dpdenotes\nthe depth of pin the tree T, because the algorithm performs a constant-time recur-\nsive step for each ancestor of p. Thus, algorithm T.depth (p) runs in O(n)worst-\ncase time, where nis the total number of positions of T, because a position of T\nmay have depth n\u22121 if all nodes form a single branch. Although such a running\ntime is a function of the input size, it is more informative to characterize the running\ntime in terms of the parameter dp, as this parameter may be much smaller than n.", "8.1. General Trees 309\nHeight\nTheheight of a position pin a tree Tis also de\ufb01ned recursively:\n\u2022Ifpis a leaf, then the height of pis 0.\n\u2022Otherwise, the height of pis one more than the maximum of the heights of\np\u2019s children.\nTheheight of a nonempty tree Tis the height of the root of T. For example, the\ntree of Figure 8.2 has height 4. In addition, height can also be viewed as follows.\nProposition 8.4: The height of a nonempty tree Tis equal to the maximum of\nthe depths of its leaf positions.\nWe leave the justi\ufb01cation of this fact to an exercise (R-8.3). We present an\nalgorithm, height1 , implemented in Code Fragment 8.4 as a nonpublic method\nheight1 of the Tree class. It computes the height of a nonempty tree Tbased on\nProposition 8.4 and the algorithm depth from Code Fragment 8.3.\n58def\nheight1( self): # works, but O(n\u02c62) worst-case time\n59 \u201d\u201d\u201dReturn the height of the tree.\u201d\u201d\u201d\n60 return max(self.depth(p) forpin self .positions( ) if self.is\n leaf(p))\nCode Fragment 8.4: Method\n height1 of the Tree class. Note that this method calls\nthedepth method.\nUnfortunately, algorithm height1 is not very ef\ufb01cient. We have not yet de\ufb01ned\nthepositions() method; we will see that it can be implemented to run in O(n)time,\nwhere nis the number of positions of T. Because height1 calls algorithm depth (p)\non each leaf of T, its running time is O(n+\u2211p\u2208L(dp+1)),w h e r e Lis the set of\nleaf positions of T. In the worst case, the sum \u2211p\u2208L(dp+1)is proportional to n2.\n(See Exercise C-8.33.) Thus, algorithm height1 runs in O(n2)worst-case time.\nWe can compute the height of a tree more ef\ufb01ciently, in O(n)worst-case time,\nby relying instead on the original recursive de\ufb01nition. To do this, we will param-\neterize a function based on a position within the tree, and calculate the height ofthe subtree rooted at that position. Algorithm height2 , shown as nonpublic method\nheight2 in Code Fragment 8.5, computes the height of tree Tin this way.\n61def\nheight2( self,p ) : # time is linear in size of subtree\n62 \u201d\u201d\u201dReturn the height of the subtree rooted at Position p.\u201d\u201d\u201d\n63 if self.is\nleaf(p):\n64 return 0\n65 else:\n66 return 1+m a x ( self.\nheight2(c) forcin self .children(p))\nCode Fragment 8.5: Method\n height2 for computing the height of a subtree rooted\nat a position pof aTree.", "310 Chapter 8. Trees\nIt is important to understand why algorithm height2 is more ef\ufb01cient than\nheight1 . The algorithm is recursive, and it progresses in a top-down fashion. If\nthe method is initially called on the root of T, it will eventually be called once for\neach position of T. This is because the root eventually invokes the recursion on\neach of its children, which in turn invokes the recursion on each of their children,\na n ds oo n .\nWe can determine the running time of the height2 algorithm by summing, over\nall the positions, the amount of time spent on the nonrecursive part of each call.(Review Section 4.2 for analyses of recursive processes.) In our implementation,there is a constant amount of work per position, plus the overhead of computing the\nmaximum over the iteration of children. Although we do not yet have a concrete\nimplementation of children(p) , we assume that such an iteration is generated in\nO(c\np+1)time, where cpdenotes the number of children of p. Algorithm height2\nspends O(cp+1)time at each position pto compute the maximum, and its overall\nrunning time is O(\u2211p(cp+1)) = O(n+\u2211pcp). In order to complete the analysis,\nwe make use of the following property.\nProposition 8.5: LetTbe a tree with npositions, and let cpdenote the number of\nchildren of a position pofT. Then, summing over the positions of T,\u2211pcp=n\u22121.\nJusti\ufb01cation: Each position of T, with the exception of the root, is a child of\nanother position, and thus contributes one unit to the above sum.\nBy Proposition 8.5, the running time of algorithm height2 , when called on the\nroot of T,i sO(n),w h e r e nis the number of positions of T.\nRevisiting the public interface for our Tree class, the ability to compute heights\nof subtrees is bene\ufb01cial, but a user might expect to be able to compute the height\nof the entire tree without explicitly designating the tree root. We can wrap the non-\npublic\n height2 in our implementation with a public height method that provides\na default interpretation when invoked on tree Twith syntax T.height() .S u c h a n\nimplementation is given in Code Fragment 8.6.\n67defheight( self,p =None):\n68 \u201d\u201d\u201dReturn the height of the subtree rooted at Position p.\n6970 If p is None, return the height of the entire tree.\n71 \u201d\u201d\u201d\n72 ifpis None :\n73 p=self.root()\n74 return self .\nheight2(p) #s t a r t\n height2 recursion\nCode Fragment 8.6: Public method Tree.height that computes the height of the\nentire tree by default, or a subtree rooted at given position, if speci\ufb01ed.", "8.2. Binary Trees 311\n8.2 Binary Trees\nAbinary tree is an ordered tree with the following properties:\n1. Every node has at most two children.\n2. Each child node is labeled as being either a left child or aright child .\n3. A left child precedes a right child in the order of children of a node.\nThe subtree rooted at a left or right child of an internal node vis called a left subtree\norright subtree , respectively, of v. A binary tree is proper if each node has either\nzero or two children. Some people also refer to such trees as being fullbinary\ntrees. Thus, in a proper binary tree, every internal node has exactly two children.\nA binary tree that is not proper is improper .\nExample 8.6: An important class of binary trees arises in contexts where we wish\nto represent a number of different outcomes that can result from answering a series\nof yes-or-no questions. Each internal node is associated with a question. Starting at\nthe root, we go to the left or right child of the current node, depending on whether\nthe answer to the question is \u201cYes\u201d or \u201cNo.\u201d With each decision, we follow an\nedge from a parent to a child, eventually tracing a path in the tree from the root\nto a leaf. Such binary trees are known as decision trees , because a leaf position p\nin such a tree represents a decision of what to do if the questions associated with\np\u2019s ancestors are answered in a way that leads to p. A decision tree is a proper\nbinary tree. Figure 8.7 illustrates a decision tree that provides recommendations to\na prospective investor.\nYes\nYes\nYes NoNoNoAre you nervous?\nWill you need to access most of the\nmoney within the next 5 years?\nAre you willing to accept risks inexchange for higher expected returns?Money market fund.\nStock portfolio.Savings account.\nDiversi\ufb01ed portfolio with stocks,\nbonds, and short-term instruments.\nFigure 8.7: A decision tree providing investment advice.", "312 Chapter 8. Trees\nExample 8.7: An arithmetic expression can be represented by a binary tree whose\nleaves are associated with variables or constants, and whose internal nodes are\nassociated with one of the operators +,\u2212,\u00d7,a n d/. (See Figure 8.8.) Each node\nin such a tree has a value associated with it.\n\u2022If a node is leaf, then its value is that of its variable or constant.\n\u2022If a node is internal, then its value is de\ufb01ned by applying its operation to the\nvalues of its children.\nAn arithmetic expression tree is a proper binary tree, since each operator +,\u2212,\u00d7,\nand/takes exactly two operands. Of course, if we were to allow unary operators,\nlike negation ( \u2212), as in \u201c \u2212x,\u201d then we could have an improper binary tree.\n/\n3 1+\u00d7\n3 2+\n9 5\u2212 3\u00d7\n4 7\u22126+\u2212\nFigure 8.8: A binary tree representing an arithmetic expression. This tree represents\nthe expression ((((3+1)\u00d73)/((9\u22125)+2))\u2212((3\u00d7(7\u22124)) + 6)).T h e v a l u e\nassociated with the internal node labeled \u201c /\u201di s2 .\nA Recursive Binary Tree De\ufb01nition\nIncidentally, we can also de\ufb01ne a binary tree in a recursive way such that a binary\ntree is either empty or consists of:\n\u2022A node r, called the root of T, that stores an element\n\u2022A binary tree (possibly empty), called the left subtree of T\n\u2022A binary tree (possibly empty), called the right subtree of T", "8.2. Binary Trees 313\n8.2.1 The Binary Tree Abstract Data Type\nAs an abstract data type, a binary tree is a specialization of a tree that supports three\nadditional accessor methods:\nT.left(p): Return the position that represents the left child of p,\nor None if phas no left child.\nT.right(p) :Return the position that represents the right child of p,\nor None if phas no right child.\nT.sibling(p) :Return the position that represents the sibling of p,\nor None if phas no sibling.\nJust as in Section 8.1.2 for the tree ADT, we do not de\ufb01ne specialized update meth-ods for binary trees here. Instead, we will consider some possible update methodswhen we describe speci\ufb01c implementations and applications of binary trees.\nT h eB i n a r y T r e eA b s t r a c tB a s eC l a s si nP y t h o n\nJust as Tree was de\ufb01ned as an abstract base class in Section 8.1.2, we de\ufb01ne a\nnewBinaryTree class associated with the binary tree ADT. We rely on inheritance\nto de\ufb01ne the BinaryTree class based upon the existing Tree class. However, our\nBinaryTree class remains abstract , as we still do not provide complete speci\ufb01ca-\ntions for how such a structure will be represented internally, nor implementationsfor some necessary behaviors.\nOur Python implementation of the BinaryTree class is given in Code Frag-\nment 8.7. By using inheritance, a binary tree supports all the functionality that wasde\ufb01ned for general trees (e.g., parent ,is\nleaf,root). Our new class also inherits the\nnested Position class that was originally de\ufb01ned within the Tree class de\ufb01nition.\nIn addition, the new class provides declarations for new abstract methods leftand\nright that should be supported by concrete subclasses of BinaryTree .\nOur new class also provides two concrete implementations of methods. The\nnewsibling method is derived from the combination of left,right ,a n dparent . Typ-\nically, we identify the sibling of a position pas the \u201cother\u201d child of p\u2019s parent.\nHowever, if pis the root, it has no parent, and thus no sibling. Also, pmay be the\nonly child of its parent, and thus does not have a sibling.\nFinally, Code Fragment 8.7 provides a concrete implementation of the children\nmethod; this method is abstract in the Tree class. Although we have still not speci-\n\ufb01ed how the children of a node will be stored, we derive a generator for the ordered\nchildren based upon the implied behavior of abstract methods leftandright .", "314 Chapter 8. Trees\n1classBinaryTree(Tree):\n2\u201d\u201d\u201dAbstract base class representing a binary tree structure.\u201d\u201d\u201d\n3\n4# --------------------- additional abstract methods ---------------------\n5defleft(self,p ) :\n6 \u201d\u201d\u201dReturn a Position representing p\n s left child.\n78 Return None if p does not have a left child.\n9 \u201d\u201d\u201d\n10 raiseNotImplementedError(\nmust be implemented by subclass\n )\n1112defright(self,p ) :\n13 \u201d\u201d\u201dReturn a Position representing p\ns right child.\n14\n15 Return None if p does not have a right child.\n16 \u201d\u201d\u201d\n17 raiseNotImplementedError(\n must be implemented by subclass\n )\n18\n19 # ---------- concrete methods implemented in this class ----------\n20defsibling( self,p ) :\n21 \u201d\u201d\u201dReturn a Position representing p\n s sibling (or None if no sibling).\u201d\u201d\u201d\n22 parent = self.parent(p)\n23 ifparent is None : #pm u s tb et h er o o t\n24 return None # root has no sibling\n25 else:\n26 ifp= =self.left(parent):\n27 return self .right(parent) # possibly None\n28 else:\n29 return self .left(parent) # possibly None\n3031defchildren( self,p ) :\n32 \u201d\u201d\u201dGenerate an iteration of Positions representing p\ns children.\u201d\u201d\u201d\n33 if self.left(p) is not None :\n34 yield self .left(p)\n35 if self.right(p) is not None :\n36 yield self .right(p)\nCode Fragment 8.7: ABinaryTree abstract base class that extends the existing Tree\nabstract base class from Code Fragments 8.1 and 8.2.", "8.2. Binary Trees 315\n8.2.2 Properties of Binary Trees\nBinary trees have several interesting properties dealing with relationships between\ntheir heights and number of nodes. We denote the set of all nodes of a tree Tat the\nsame depth daslevel dofT. In a binary tree, level 0 has at most one node (the\nroot), level 1 has at most two nodes (the children of the root), level 2 has at mostfour nodes, and so on. (See Figure 8.9.) In general, level dhas at most 2\ndnodes.\n...0 ......1\n2\n31 ...2\n4\n8Level Nodes\nFigure 8.9: Maximum number of nodes in the levels of a binary tree.\nWe can see that the maximum number of nodes on the levels of a binary tree\ngrows exponentially as we go down the tree. From this simple observation, we can\nderive the following properties relating the height of a binary tree Twith its number\nof nodes. A detailed justi\ufb01cation of these properties is left as Exercise R-8.8.\nProposition 8.8: LetTbe a nonempty binary tree, and let n,nE,nIandhdenote\nthe number of nodes, number of external nodes, number of internal nodes, and\nheight of T, respectively. Then Thas the following properties:\n1.h+1\u2264n\u22642h+1\u22121\n2. 1\u2264nE\u22642h\n3.h\u2264nI\u22642h\u22121\n4. log (n+1)\u22121\u2264h\u2264n\u22121\nAlso, if Tis proper, then Thas the following properties:\n1. 2h+1\u2264n\u22642h+1\u22121\n2.h+1\u2264nE\u22642h\n3.h\u2264nI\u22642h\u22121\n4. log (n+1)\u22121\u2264h\u2264(n\u22121)/2", "316 Chapter 8. Trees\nRelating Internal Nodes to External Nodes in a Proper Binary Tree\nIn addition to the earlier binary tree properties, the following relationship exists\nbetween the number of internal nodes and external nodes in a proper binary tree.\nProposition 8.9: In a nonempty proper binary tree T, with nEexternal nodes and\nnIinternal nodes, we have nE=nI+1.\nJusti\ufb01cation: We justify this proposition by removing nodes from Tand divid-\ning them up into two \u201cpiles,\u201d an internal-node pile and an external-node pile, until\nTbecomes empty. The piles are initially empty. By the end, we will show that the\nexternal-node pile has one more node than the internal-node pile. We consider twocases:Case 1: IfThas only one node v, we remove vand place it on the external-node\npile. Thus, the external-node pile has one node and the internal-node pile isempty.\nCase 2: Otherwise ( Thas more than one node), we remove from Tan (arbitrary)\nexternal node wand its parent v, which is an internal node. We place won\nthe external-node pile and von the internal-node pile. If vhas a parent u,\nthen we reconnect uwith the former sibling zofw, as shown in Figure 8.10.\nThis operation, removes one internal node and one external node, and leavesthe tree being a proper binary tree.Repeating this operation, we eventually are left with a \ufb01nal tree consistingof a single node. Note that the same number of external and internal nodeshave been removed and placed on their respective piles by the sequence of\noperations leading to this \ufb01nal tree. Now, we remove the node of the \ufb01nal\ntree and we place it on the external-node pile. Thus, the the external-nodepile has one more node than the internal-node pile.\nvu\nw zu\nzu\nz\n(a) (b) (c)\nFigure 8.10: Operation that removes an external node and its parent node, used in\nthe justi\ufb01cation of Proposition 8.9.\nNote that the above relationship does not hold, in general, for improper binary\ntrees and nonbinary trees, although there are other interesting relationships that dohold. (See Exercises C-8.32 through C-8.34.)", "8.3. Implementing Trees 317\n8.3 Implementing Trees\nTheTree andBinaryTree classes that we have de\ufb01ned thus far in this chapter are\nboth formally abstract base classes . Although they provide a great deal of support,\nneither of them can be directly instantiated. We have not yet de\ufb01ned key imple-\nmentation details for how a tree will be represented internally, and how we can\neffectively navigate between parents and children. Speci\ufb01cally, a concrete imple-\nmentation of a tree must provide methods root,parent ,num\nchildren ,children ,\nlen\n , and in the case of BinaryTree , the additional accessors leftandright .\nThere are several choices for the internal representation of trees. We describe\nthe most common representations in this section. We begin with the case of abinary tree , since its shape is more narrowly de\ufb01ned.\n8.3.1 Linked Structure for Binary Trees\nA natural way to realize a binary tree Tis to use a linked structure , with a node\n(see Figure 8.11a) that maintains references to the element stored at a position p\nand to the nodes associated with the children and parent of p.I fpis the root of\nT, then the parent \ufb01eld of pisNone . Likewise, if pdoes not have a left child\n(respectively, right child), the associated \ufb01eld is None . The tree itself maintains an\ninstance variable storing a reference to the root node (if any), and a variable, called\nsize, that represents the overall number of nodes of T. We show such a linked\nstructure representation of a binary tree in Figure 8.11b.\nparent\nelementright leftroot\n\u2205\u2205\n\u2205 \u2205 \u2205\u2205\n\u2205\nBaltimore Chicago New York Providence Seattlesize5\n(a) (b)\nFigure 8.11: A linked structure for representing: (a) a single node; (b) a binary tree.", "318 Chapter 8. Trees\nPython Implementation of a Linked Binary Tree Structure\nIn this section, we de\ufb01ne a concrete LinkedBinaryTree class that implements the\nbinary tree ADT by subclassing the BinaryTree class. Our general approach is very\nsimilar to what we used when developing the PositionalList in Section 7.4: We\nde\ufb01ne a simple, nonpublic\n Node class to represent a node, and a public Position\nclass that wraps a node. We provide a\n validate utility for robustly checking the\nvalidity of a given position instance when unwrapping it, and a\n make\n position\nutility for wrapping a node as a position to return to a caller.\nThose de\ufb01nitions are provided in Code Fragment 8.8. As a formality, the new\nPosition class is declared to inherit immediately from BinaryTree.Position . Tech-\nnically, the BinaryTree class de\ufb01nition (see Code Fragment 8.7) does not formally\ndeclare such a nested class; it trivially inherits it from Tree.Position . A minor ben-\ne\ufb01t from this design is that our position class inherits the\n ne\n special method\nso that syntax p! =q is derived appropriately relative to\n eq\n .\nOur class de\ufb01nition continues, in Code Fragment 8.9, with a constructor and\nwith concrete implementations for the methods that remain abstract in the Tree and\nBinaryTree classes. The constructor creates an empty tree by initializing\n root to\nNone and\nsizeto zero. These accessor methods are implemented with careful use\nof the\n validate and\nmake\n position utilities to safeguard against boundary cases.\nOperations for Updating a Linked Binary Tree\nThus far, we have provided functionality for examining an existing binary tree.\nHowever, the constructor for our LinkedBinaryTree class results in an empty tree\nand we have not provided any means for changing the structure or content of a tree.\nWe chose not to declare update methods as part of the Tree orBinaryTree ab-\nstract base classes for several reasons. First, although the principle of encapsula-\ntion suggests that the outward behaviors of a class need not depend on the internal\nrepresentation, the ef\ufb01ciency of the operations depends greatly upon the representa-\ntion. We prefer to have each concrete implementation of a tree class offer the mostsuitable options for updating a tree.\nThe second reason we do not provide update methods in the base class is that\nwe may not want such update methods to be part of a public interface. There aremany applications of trees, and some forms of update operations that are suitablefor one application may be unacceptable in another. However, if we place an update\nmethod in a base class, any class that inherits from that base will inherit the update\nmethod. Consider, for example, the possibility of a method T.replace(p, e) that\nreplaces the element stored at position pwith another element e. Such a general\nmethod may be unacceptable in the context of an arithmetic expression tree (see\nExample 8.7 on page 312, and a later case study in Section 8.5), because we may\nwant to enforce that internal nodes store only operators as elements.", "8.3. Implementing Trees 319\nFor linked binary trees, a reasonable set of update methods to support for gen-\neral usage are the following:\nT.add\n root(e) :Create a root for an empty tree, storing eas the element,\nand return the position of that root; an error occurs if the\ntree is not empty.\nT.add\n left(p, e) :Create a new node storing element e, link the node as the\nleft child of position p, and return the resulting position;\nan error occurs if palready has a left child.\nT.add\n right(p, e) :Create a new node storing element e, link the node as the\nright child of position p, and return the resulting position;\nan error occurs if palready has a right child.\nT.replace(p, e) :Replace the element stored at position pwith element e,\nand return the previously stored element.\nT.delete(p): Remove the node at position p, replacing it with its child,\nif any, and return the element that had been stored at p;\nan error occurs if phas two children.\nT.attach(p, T1, T2) :Attach the internal structure of trees T1andT2, respec-\ntively, as the left and right subtrees of leaf position pof\nT, and reset T1andT2to empty trees; an error condition\noccurs if pis not a leaf.\nWe have speci\ufb01cally chosen this collection of operations because each can be\nimplemented in O(1)worst-case time with our linked representation. The most\ncomplex of these are delete andattach , due to the case analyses involving the\nvarious parent-child relationships and boundary conditions, yet there remains only\na constant number of operations to perform. (The implementation of both methods\ncould be greatly simpli\ufb01ed if we used a tree representation with a sentinel node,akin to our treatment of positional lists; see Exercise C-8.40).\nTo avoid the problem of undesirable update methods being inherited by sub-\nclasses of LinkedBinaryTree , we have chosen an implementation in which none\nof the above methods are publicly supported. Instead, we provide nonpublic ver-\nsions of each, for example, providing the underscored\ndelete in lieu of a public\ndelete . Our implementations of these six update methods are provided in Code\nFragments 8.10 and 8.11.\nIn particular applications, subclasses of LinkedBinaryTree can invoke the non-\npublic methods internally, while preserving a public interface that is appropriatefor the application. A subclass may also choose to wrap one or more of the non-public update methods with a public method to expose it to the user. We leave asan exercise (R-8.15), the task of de\ufb01ning a MutableLinkedBinaryTree subclass that\nprovides public methods wrapping each of these six update methods.", "320 Chapter 8. Trees\n1classLinkedBinaryTree(BinaryTree):\n2\u201d\u201d\u201dLinked representation of a binary tree structure.\u201d\u201d\u201d\n3\n4class\n Node: # Lightweight, nonpublic class for storing a node.\n5\n slots\n =\n_element\n ,\n_parent\n ,\n_left\n ,\n_right\n6 def\n init\n(self,e l e m e n t ,p a r e n t = None,l e f t =None,r i g h t = None):\n7 self.\nelement = element\n8 self.\nparent = parent\n9 self.\nleft = left\n10 self.\nright = right\n11\n12classPosition(BinaryTree.Position):\n13 \u201d\u201d\u201dAn abstraction representing the location of a single element.\u201d\u201d\u201d\n1415 def\ninit\n(self,c o n t a i n e r ,n o d e ) :\n16 \u201d\u201d\u201dConstructor should not be invoked by user.\u201d\u201d\u201d\n17 self.\ncontainer = container\n18 self.\nnode = node\n1920 defelement( self):\n21 \u201d\u201d\u201dReturn the element stored at this Position.\u201d\u201d\u201d\n22 return self .\nnode.\n element\n2324 def\neq\n(self,o t h e r ) :\n25 \u201d\u201d\u201dReturn True if other is a Position representing the same location.\u201d\u201d\u201d\n26 return type(other) istype(self)andother.\n nodeis self .\nnode\n2728def\nvalidate( self,p ) :\n29 \u201d\u201d\u201dReturn associated node, if position is valid.\u201d\u201d\u201d\n30 if not isinstance(p, self.Position):\n31 raiseTypeError(\n p must be proper Position type\n )\n32 ifp.\ncontainer is not self :\n33 raiseValueError(\n p does not belong to this container\n )\n34 ifp.\nnode.\n parent isp.\nnode: # convention for deprecated nodes\n35 raiseValueError(\n p is no longer valid\n )\n36 return p.\nnode\n3738def\nmake\n position( self,n o d e ) :\n39 \u201d\u201d\u201dReturn Position instance for given node (or None if no node).\u201d\u201d\u201d\n40 return self .Position( self,n o d e ) ifnodeis not None else None\nCode Fragment 8.8: The beginning of our LinkedBinaryTree class (continued in\nCode Fragments 8.9 through 8.11).", "8.3. Implementing Trees 321\n41 #-------------------------- binary tree constructor --------------------------\n42def\n init\n(self):\n43 \u201d\u201d\u201dCreate an initially empty binary tree.\u201d\u201d\u201d\n44 self.\nroot = None\n45 self.\nsize = 0\n46\n47 #-------------------------- public accessors --------------------------\n48def\n len\n(self):\n49 \u201d\u201d\u201dReturn the total number of elements in the tree.\u201d\u201d\u201d\n50 return self .\nsize\n5152defroot(self):\n53 \u201d\u201d\u201dReturn the root Position of the tree (or None if tree is empty).\u201d\u201d\u201d\n54 return self .\nmake\n position( self.\nroot)\n55\n56defparent( self,p ) :\n57 \u201d\u201d\u201dReturn the Position of p\n sp a r e n t( o rN o n ei fpi sr o o t ) . \u201d \u201d \u201d\n58 node = self.\nvalidate(p)\n59 return self .\nmake\n position(node.\n parent)\n60\n61defleft(self,p ) :\n62 \u201d\u201d\u201dReturn the Position of p\n s left child (or None if no left child).\u201d\u201d\u201d\n63 node = self.\nvalidate(p)\n64 return self .\nmake\n position(node.\n left)\n6566defright(self,p ) :\n67 \u201d\u201d\u201dReturn the Position of p\ns right child (or None if no right child).\u201d\u201d\u201d\n68 node = self.\nvalidate(p)\n69 return self .\nmake\n position(node.\n right)\n70\n71defnum\nchildren( self,p ) :\n72 \u201d\u201d\u201dReturn the number of children of Position p.\u201d\u201d\u201d\n73 node = self.\nvalidate(p)\n74 count = 0\n75 ifnode.\n leftis not None : # left child exists\n76 count += 1\n77 ifnode.\n rightis not None : # right child exists\n78 count += 1\n79 return count\nCode Fragment 8.9: Public accessors for our LinkedBinaryTree class. The class\nbegins in Code Fragment 8.8 and continues in Code Fragments 8.10 and 8.11.", "322 Chapter 8. Trees\n80def\nadd\nroot(self,e ) :\n81 \u201d\u201d\u201dPlace element e at the root of an empty tree and return new Position.\n82\n83 Raise ValueError if tree nonempty.\n84 \u201d\u201d\u201d\n85 if self.\nrootis not None :raiseValueError(\n Root exists\n )\n86 self.\nsize = 1\n87 self.\nroot = self.\nNode(e)\n88 return self .\nmake\n position( self.\nroot)\n89\n90def\nadd\nleft(self,p ,e ) :\n91 \u201d\u201d\u201dCreate a new left child for Position p, storing element e.\n9293 Return the Position of new node.\n94 Raise ValueError if Position p is invalid or p already has a left child.\n95 \u201d\u201d\u201d\n96 node = self.\nvalidate(p)\n97 ifnode.\n leftis not None :raiseValueError(\n Left child exists\n )\n98 self.\nsize += 1\n99 node.\n left = self.\nNode(e, node) # node is its parent\n100 return self .\nmake\n position(node.\n left)\n101102 def\nadd\nright(self,p ,e ) :\n103 \u201d\u201d\u201dCreate a new right child for Position p, storing element e.\n104105 Return the Position of new node.\n106 Raise ValueError if Position p is invalid or p already has a right child.\n107 \u201d\u201d\u201d\n108 node = self.\nvalidate(p)\n109 ifnode.\n rightis not None :raiseValueError(\n Right child exists\n )\n110 self.\nsize += 1\n111 node.\n right = self.\nNode(e, node) # node is its parent\n112 return self .\nmake\n position(node.\n right)\n113114 def\nreplace( self,p ,e ) :\n115 \u201d\u201d\u201dReplace the element at position p with e, and return old element.\u201d\u201d\u201d\n116 node = self.\nvalidate(p)\n117 old = node.\n element\n118 node.\n element = e\n119 return old\nCode Fragment 8.10: Nonpublic update methods for the LinkedBinaryTree class\n(continued in Code Fragment 8.11).", "8.3. Implementing Trees 323\n120 def\ndelete( self,p ) :\n121 \u201d\u201d\u201dDelete the node at Position p, and replace it with its child, if any.\n122\n123 Return the element that had been stored at Position p.\n124 Raise ValueError if Position p is invalid or p has two children.\n125 \u201d\u201d\u201d\n126 node = self.\nvalidate(p)\n127 if self.num\n children(p) == 2: raiseValueError(\n p has two children\n )\n128 child = node.\n leftifnode.\n leftelsenode.\n right # might be None\n129 ifchildis not None :\n130 child.\n parent = node.\n parent # child\n s grandparent becomes parent\n131 ifnodeis self .\nroot:\n132 self.\nroot = child # child becomes root\n133 else:\n134 parent = node.\n parent\n135 ifnodeisparent.\n left:\n136 parent.\n left = child\n137 else:\n138 parent.\n right = child\n139 self.\nsize\u2212=1\n140 node.\n parent = node # convention for deprecated node\n141 return node.\n element\n142\n143 def\nattach( self,p ,t 1 ,t 2 ) :\n144 \u201d\u201d\u201dAttach trees t1 and t2 as left and right subtrees of external p.\u201d\u201d\u201d\n145 node = self.\nvalidate(p)\n146 if not self .is\nleaf(p): raiseValueError(\n position must be leaf\n )\n147 if not type(self)istype(t1) istype(t2): # all 3 trees must be same type\n148 raiseTypeError(\n Tree types must match\n )\n149 self.\nsize += len(t1) + len(t2)\n150 if not t1.is\nempty(): # attached t1 as left subtree of node\n151 t1.\nroot.\nparent = node\n152 node.\n left = t1.\n root\n153 t1.\nroot = None # set t1 instance to empty\n154 t1.\nsize = 0\n155 if not t2.is\nempty(): # attached t2 as right subtree of node\n156 t2.\nroot.\nparent = node\n157 node.\n right = t2.\n root\n158 t2.\nroot = None # set t2 instance to empty\n159 t2.\nsize = 0\nCode Fragment 8.11: Nonpublic update methods for the LinkedBinaryTree class\n(continued from Code Fragment 8.10).", "324 Chapter 8. Trees\nPerformance of the Linked Binary Tree Implementation\nTo summarize the ef\ufb01ciencies of the linked structure representation, we analyze the\nrunning times of the LinkedBinaryTree methods, including derived methods that\nare inherited from the Tree andBinaryTree classes:\n\u2022Thelenmethod, implemented in LinkedBinaryTree , uses an instance variable\nstoring the number of nodes of Tand takes O(1)time. Method is\nempty ,\ninherited from Tree, relies on a single call to lenand thus takes O(1)time.\n\u2022The accessor methods root,left,right ,parent ,a n dnum\nchildren are imple-\nmented directly in LinkedBinaryTree and take O(1)time. The sibling and\nchildren methods are derived in BinaryTree based on a constant number of\ncalls to these other accessors, so they run in O(1)time as well.\n\u2022Theis\nroot andis\nleafmethods, from the Tree class, both run in O(1)time,\nasis\nroot callsroot and then relies on equivalence testing of positions, while\nis\nleafcallsleftandright and veri\ufb01es that None is returned by both.\n\u2022Methods depth andheight were each analyzed in Section 8.1.3. The depth\nmethod at position pruns in O(dp+1)time where dpis its depth; the height\nmethod on the root of the tree runs in O(n)time.\n\u2022The various update methods add\nroot,add\nleft,add\nright ,replace ,delete ,\nandattach (that is, their nonpublic implementations) each run in O(1)time,\nas they involve relinking only a constant number of nodes per operation.\nTable 8.1 summarizes the performance of the linked structure implementation of abinary tree.\nOperation\n Running Time\nlen,is\nempty\n O(1)\nroot,parent ,left,right ,sibling ,children ,num\nchildren\n O(1)\nis\nroot,is\nleaf\n O(1)\ndepth(p)\n O(dp+1)\nheight\n O(n)\nadd\nroot,add\nleft,add\nright ,replace ,delete ,attach\n O(1)\nTable 8.1: Running times for the methods of an n-node binary tree implemented\nwith a linked structure. The space usage is O(n).", "8.3. Implementing Trees 325\n8.3.2 Array-Based Representation of a Binary Tree\nAn alternative representation of a binary tree Tis based on a way of numbering the\npositions of T. For every position pofT,l e tf(p)be the integer de\ufb01ned as follows.\n\u2022Ifpis the root of T,t h e n f(p)=0.\n\u2022Ifpis the left child of position q,t h e n f(p)=2f(q)+1.\n\u2022Ifpis the right child of position q,t h e n f(p)=2f(q)+2.\nThe numbering function fis known as a level numbering of the positions in a\nbinary tree T, for it numbers the positions on each level of Tin increasing order\nfrom left to right. (See Figure 8.12.) Note well that the level numbering is based\nonpotential positions within the tree, not actual positions of a given tree, so they\nare not necessarily consecutive. For example, in Figure 8.12(b), there are no nodeswith level numbering 13 or 14, because the node with level numbering 6 has nochildren.\n(a)\n... ...4\n10 11 12 13 14 8 70\n2\n6 51\n3\n9\n(b)\n15+\u2212\n+\u00d7\n3\n95+\n2 \u2212\u00d7\n3 \u22126\n31 74/0\n12\n5 4 36\n12 11 10\n25 26 209\n1978\n16\nFigure 8.12: Binary tree level numbering: (a) general scheme; (b) an example.", "326 Chapter 8. Trees\nThe level numbering function fsuggests a representation of a binary tree T\nby means of an array-based structure A(such as a Python list), with the element\nat position pofTstored at index f(p)of the array. We show an example of an\narray-based representation of a binary tree in Figure 8.13.\n/\n420\n2 1\n34 56\n12 11 8 7\n31+\u00d7\n95\u2212+\n06 1 2 12345 789 1 0 1 1 1 3 1 45 \u00d7 ++ 4\u2212231 9 /\nFigure 8.13: Representation of a binary tree by means of an array.\nOne advantage of an array-based representation of a binary tree is that a posi-\ntion pcan be represented by the single integer f(p), and that position-based meth-\nods such as root,parent ,left,a n dright can be implemented using simple arithmetic\noperations on the number f(p). Based on our formula for the level numbering, the\nleft child of phas index 2 f(p)+1, the right child of phas index 2 f(p)+2, and\nthe parent of phas index \u230a(f(p)\u22121)/2\u230b. We leave the details of a complete im-\nplementation as an exercise (R-8.18).\nThe space usage of an array-based representation depends greatly on the shape\nof the tree. Let nbe the number of nodes of T,a n dl e t fMbe the maximum value\noff(p)over all the nodes of T. The array Arequires length N=1+fM,s i n c e\nelements range from A[0]toA[fM]. Note that Amay have a number of empty cells\nthat do not refer to existing nodes of T. In fact, in the worst case, N=2n\u22121,\nthe justi\ufb01cation of which is left as an exercise (R-8.16). In Section 9.3, we will\nsee a class of binary trees, called \u201cheaps\u201d for which N=n. Thus, in spite of the\nworst-case space usage, there are applications for which the array representation\nof a binary tree is space ef\ufb01cient. Still, for general binary trees, the exponentialworst-case space requirement of this representation is prohibitive.\nAnother drawback of an array representation is that some update operations for\ntrees cannot be ef\ufb01ciently supported. For example, deleting a node and promotingits child takes O(n)time because it is not just the child that moves locations within\nthe array, but all descendants of that child.", "8.3. Implementing Trees 327\n8.3.3 Linked Structure for General Trees\nWhen representing a binary tree with a linked structure, each node explicitly main-\ntains \ufb01elds leftandright as references to individual children. For a general tree,\nthere is no a priori limit on the number of children that a node may have. A natural\nway to realize a general tree Tas a linked structure is to have each node store a\nsingle container of references to its children. For example, a children \ufb01eld of a\nnode can be a Python list of references to the children of the node (if any). Such a\nlinked representation is schematically illustrated in Figure 8.14.\nelementparent\nchildrenBaltimore ChicagoNew York\nProvidence Seattle\n(a) (b)\nFigure 8.14: The linked structure for a general tree: (a) the structure of a node; (b) a\nlarger portion of the data structure associated with a node and its children.\nTable 8.2 summarizes the performance of the implementation of a general tree\nusing a linked structure. The analysis is left as an exercise (R-8.14), but we note\nthat, by using a collection to store the children of each position p, we can implement\nchildren (p)by simply iterating that collection.\nOperation\n Running Time\nlen,is\nempty\n O(1)\nroot,parent ,is\nroot,is\nleaf\n O(1)\nchildren (p)\nO(cp+1)\ndepth(p)\n O(dp+1)\nheight\n O(n)\nTable 8.2: Running times of the accessor methods of an n-node general tree im-\nplemented with a linked structure. We let cpdenote the number of children of a\nposition p. The space usage is O(n).", "328 Chapter 8. Trees\n8.4 Tree Traversal Algorithms\nAtraversal of a tree Tis a systematic way of accessing, or \u201cvisiting,\u201d all the posi-\ntions of T. The speci\ufb01c action associated with the \u201cvisit\u201d of a position pdepends\non the application of this traversal, and could involve anything from increment-\ning a counter to performing some complex computation for p. In this section, we\ndescribe several common traversal schemes for trees, implement them in the con-text of our various tree classes, and discuss several common applications of tree\ntraversals.\n8.4.1 Preorder and Postorder Traversals of General Trees\nIn a preorder traversal of a tree T, the root of Tis visited \ufb01rst and then the sub-\ntrees rooted at its children are traversed recursively. If the tree is ordered, thenthe subtrees are traversed according to the order of the children. The pseudo-code\nfor the preorder traversal of the subtree rooted at a position pis shown in Code\nFragment 8.12.\nAlgorithm preorder(T, p) :\nperform the \u201cvisit\u201d action for position p\nforeach child cinT.children(p) do\npreorder(T, c) {recursively traverse the subtree rooted at c}\nCode Fragment 8.12: Algorithm preorder for performing the preorder traversal of a\nsubtree rooted at position pof a tree T.\nFigure 8.15 portrays the order in which positions of a sample tree are visited\nduring an application of the preorder traversal algorithm.\nPaper\nTitle Abstract \u00a7 1 References \u00a7 2 \u00a7 3\n\u00a7 1.1 \u00a7 1.2 \u00a7 2.1 \u00a7 2.2 \u00a7 2.3 \u00a7 3.1 \u00a7 3.2\nFigure 8.15: Preorder traversal of an ordered tree, where the children of each posi-\ntion are ordered from left to right.", "8.4. Tree Traversal Algorithms 329\nPostorder Traversal\nAnother important tree traversal algorithm is the postorder traversal .I n s o m e\nsense, this algorithm can be viewed as the opposite of the preorder traversal, be-\ncause it recursively traverses the subtrees rooted at the children of the root \ufb01rst, and\nthen visits the root (hence, the name \u201cpostorder\u201d). Pseudo-code for the postordertraversal is given in Code Fragment 8.13, and an example of a postorder traversalis portrayed in Figure 8.16.\nAlgorithm postorder(T, p) :\nforeach child cinT.children(p) do\npostorder(T, c) {recursively traverse the subtree rooted at c}\nperform the \u201cvisit\u201d action for position p\nCode Fragment 8.13: Algorithm postorder for performing the postorder traversal of\na subtree rooted at position pof a tree T.\nPaper\nTitle Abstract \u00a7 1 References \u00a7 2 \u00a7 3\n\u00a7 1.1 \u00a7 1.2 \u00a7 2.1 \u00a7 2.2 \u00a7 2.3 \u00a7 3.1 \u00a7 3.2\nFigure 8.16: Postorder traversal of the ordered tree of Figure 8.15.\nRunning-Time Analysis\nBoth preorder and postorder traversal algorithms are ef\ufb01cient ways to access all the\npositions of a tree. The analysis of either of these traversal algorithms is similar tothat of algorithm height2 , given in Code Fragment 8.5 of Section 8.1.3. At each\nposition p, the nonrecursive part of the traversal algorithm requires time O(c\np+1),\nwhere cpis the number of children of p, under the assumption that the \u201cvisit\u201d itself\ntakes O(1)time. By Proposition 8.5, the overall running time for the traversal of\ntreeTisO(n),w h e r e nis the number of positions in the tree. This running time is\nasymptotically optimal since the traversal must visit all the npositions of the tree.", "330 Chapter 8. Trees\n8.4.2 Breadth-First Tree Traversal\nAlthough the preorder and postorder traversals are common ways of visiting the\npositions of a tree, another common approach is to traverse a tree so that we visitall the positions at depth dbefore we visit the positions at depth d+1. Such an\nalgorithm is known as a breadth-\ufb01rst traversal .\nA breadth-\ufb01rst traversal is a common approach used in software for playing\ngames. A game tree represents the possible choices of moves that might be made\nby a player (or computer) during a game, with the root of the tree being the initialcon\ufb01guration for the game. For example, Figure 8.17 displays a partial game tree\nfor Tic-Tac-Toe.\nXX X\nO\nXXO XO X\nOX\nOX\nOOX X\nOX\nOX\nOO\nXX\nO\n163 241\n56 8 7 9 10 11 12 13 14 15\nFigure 8.17: Partial game tree for Tic-Tac-Toe, with annotations displaying the or-\nder in which positions are visited in a breadth-\ufb01rst traversal.\nA breadth-\ufb01rst traversal of such a game tree is often performed because a computer\nmay be unable to explore a complete game tree in a limited amount of time. So the\ncomputer will consider all moves, then responses to those moves, going as deep as\ncomputational time allows.\nPseudo-code for a breadth-\ufb01rst traversal is given in Code Fragment 8.14. The\nprocess is not recursive, since we are not traversing entire subtrees at once. We usea queue to produce a FIFO (i.e., \ufb01rst-in \ufb01rst-out) semantics for the order in which\nwe visit nodes. The overall running time is O(n), due to the ncalls to enqueue and\nncalls to dequeue .\nAlgorithm breadth\ufb01rst(T) :\nInitialize queue Qto contain T.root()\nwhile Qnot empty do\np=Q.dequeue() {p is the oldest entry in the queue }\nperform the \u201cvisit\u201d action for position p\nforeach child cinT.children(p) do\nQ.enqueue(c) {addp\u2019s children to the end of the queue for later visits }\nCode Fragment 8.14: Algorithm for performing a breadth-\ufb01rst traversal of a tree.", "8.4. Tree Traversal Algorithms 331\n8.4.3 Inorder Traversal of a Binary Tree\nThe standard preorder, postorder, and breadth-\ufb01rst traversals that were introduced\nfor general trees, can be directly applied to binary trees. In this section, we intro-duce another common traversal algorithm speci\ufb01cally for a binary tree.\nDuring an inorder traversal , we visit a position between the recursive traver-\nsals of its left and right subtrees. The inorder traversal of a binary tree Tcan be\ninformally viewed as visiting the nodes of T\u201cfrom left to right.\u201d Indeed, for every\nposition p, the inorder traversal visits pafter all the positions in the left subtree of\npand before all the positions in the right subtree of p. Pseudo-code for the inorder\ntraversal algorithm is given in Code Fragment 8.15, and an example of an inordertraversal is portrayed in Figure 8.18.\nAlgorithm inorder(p) :\nifphas a left child lcthen\ninorder(lc) {recursively traverse the left subtree of p}\nperform the \u201cvisit\u201d action for position p\nifphas a right child rcthen\ninorder(rc) {recursively traverse the right subtree of p}\nCode Fragment 8.15: Algorithm inorder for performing an inorder traversal of a\nsubtree rooted at position pof a binary tree.\n3 1 9 5 4 7+ 3 2 \u2212 3 \u2212\u00d7 + \u00d7 6/ +\u2212\nFigure 8.18: Inorder traversal of a binary tree.\nThe inorder traversal algorithm has several important applications. When using\na binary tree to represent an arithmetic expression, as in Figure 8.18, the inorder\ntraversal visits positions in a consistent order with the standard representation of\nthe expression, as in 3 +1\u00d73/9\u22125+2...(albeit without parentheses).", "332 Chapter 8. Trees\nBinary Search Trees\nAn important application of the inorder traversal algorithm arises when we store an\nordered sequence of elements in a binary tree, de\ufb01ning a structure we call a binary\nsearch tree .L e t Sbe a set whose unique elements have an order relation. For\nexample, Scould be a set of integers. A binary search tree for Sis a binary tree T\nsuch that, for each position pofT:\n\u2022Position pstores an element of S, denoted as e(p).\n\u2022Elements stored in the left subtree of p(if any) are less than e(p).\n\u2022Elements stored in the right subtree of p(if any) are greater than e(p).\nAn example of a binary search tree is shown in Figure 8.19. The above properties\nassure that an inorder traversal of a binary search tree Tvisits the elements in\nnondecreasing order.\n362531\n42\n1262\n7558\n90\nFigure 8.19: A binary search tree storing integers. The solid path is traversed when\nsearching (successfully) for 36. The dashed path is traversed when searching (un-\nsuccessfully) for 70.\nWe can use a binary search tree Tfor set Sto \ufb01nd whether a given search\nvalue vis in S, by traversing a path down the tree T, starting at the root. At each\ninternal position pencountered, we compare our search value vwith the element\ne(p)stored at p.I f v<e(p), then the search continues in the left subtree of p.\nIfv=e(p), then the search terminates successfully. If v>e(p), then the search\ncontinues in the right subtree of p. Finally, if we reach an empty subtree, the search\nterminates unsuccessfully. In other words, a binary search tree can be viewed as abinary decision tree (recall Example 8.6), where the question asked at each internalnode is whether the element at that node is less than, equal to, or larger than the\nelement being searched for. We illustrate several examples of the search operation\nin Figure 8.19.\nNote that the running time of searching in a binary search tree Tis proportional\nto the height of T. Recall from Proposition 8.8 that the height of a binary tree with\nnnodes can be as small as log (n+1)\u22121 or as large as n\u22121. Thus, binary search\ntrees are most ef\ufb01cient when they have small height. Chapter 11 is devoted to thestudy of search trees.", "8.4. Tree Traversal Algorithms 333\n8.4.4 Implementing Tree Traversals in Python\nWhen \ufb01rst de\ufb01ning the tree ADT in Section 8.1.2, we stated that tree Tshould\ninclude support for the following methods:\nT.positions() :Generate an iteration of all positions of tree T.\niter(T) :Generate an iteration of all elements stored within tree T.\nAt that time, we did not make any assumption about the order in which these\niterations report their results. In this section, we demonstrate how any of the tree\ntraversal algorithms we have introduced could be used to produce these iterations.\nTo begin, we note that it is easy to produce an iteration of all elements of a\ntree, if we rely on a presumed iteration of all positions. Therefore, support fortheiter(T) syntax can be formally provided by a concrete implementation of the\nspecial method\niter\n within the abstract base class Tree. We rely on Python\u2019s\ngenerator syntax as the mechanism for producing iterations. (See Section 1.8.) Ourimplementation of Tree.\niter\n is given in Code Fragment 8.16.\n75def\n iter\n(self):\n76 \u201d\u201d\u201dGenerate an iteration of the tree\n s elements.\u201d\u201d\u201d\n77 forpin self.positions(): # use same order as positions()\n78 yieldp.element( ) # but yield each element\nCode Fragment 8.16: Iterating all elements of a Tree instance, based upon an iter-\nation of the positions of the tree. This code should be included in the body of theTree class.\nTo implement the positions method, we have a choice of tree traversal algo-\nrithms. Given that there are advantages to each of those traversal orders, we willprovide independent implementations of each strategy that can be called directly\nby a user of our class. We can then trivially adapt one of those as a default order\nfor the positions method of the tree ADT.\nPreorder Traversal\nWe begin by considering the preorder traversal algorithm. We will support a public\nmethod with calling signature T.preorder() for tree T, which generates a preorder\niteration of all positions within the tree. However, the recursive algorithm for gen-\nerating a preorder traversal, as originally described in Code Fragment 8.12, must\nbe parameterized by a speci\ufb01c position within the tree that serves as the root of asubtree to traverse. A standard solution for such a circumstance is to de\ufb01ne a non-public utility method with the desired recursive parameterization, and then to havethe public method preorder invoke the nonpublic method upon the root of the tree.\nOur implementation of such a design is given in Code Fragment 8.17.", "334 Chapter 8. Trees\n79defpreorder( self):\n80 \u201d\u201d\u201dGenerate a preorder iteration of positions in the tree.\u201d\u201d\u201d\n81 if not self .is\nempty():\n82 forpin self.\n subtree\n preorder( self.root()): # start recursion\n83 yieldp\n84\n85def\nsubtree\n preorder( self,p ) :\n86 \u201d\u201d\u201dGenerate a preorder iteration of positions in subtree rooted at p.\u201d\u201d\u201d\n87 yieldp # visit p before its subtrees\n88 forcin self .children(p): # for each child c\n89 forotherin self .\nsubtree\n preorder(c): # do preorder of c\u2019s subtree\n90 yieldother # yielding each to our caller\nCode Fragment 8.17: Support for performing a preorder traversal of a tree. This\ncode should be included in the body of the Tree class.\nFormally, both preorder and the utility\n subtree\n preorder are generators. Rather\nthan perform a \u201cvisit\u201d action from within this code, we yield each position to thecaller and let the caller decide what action to perform at that position.\nThe\nsubtree\n preorder method is the recursive one. However, because we are\nrelying on generators rather than traditional functions, the recursion has a slightlydifferent form. In order to yield all positions within the subtree of child c, we loop\nover the positions yielded by the recursive call self.\nsubtree\n preorder(c) ,a n dr e -\nyield each position in the outer context. Note that if pis a leaf, the for loop over\nself.children(p) is trivial (this is the base case for our recursion).\nWe rely on a similar technique in the public preorder method to re-yield all\npositions that are generated by the recursive process starting at the root of the tree;if the tree is empty, nothing is yielded. At this point, we have provided full support\nfor the preorder generator. A user of the class can therefore write code such as\nforpinT.preorder():\n# \u201dvisit\u201d position p\nThe of\ufb01cial tree ADT requires that all trees support a positions method as well. To\nuse a preorder traversal as the default order of iteration, we include the de\ufb01nition\nshown in Code Fragment 8.18 within our Tree class. Rather than loop over the\nresults returned by the preorder call, we return the entire iteration as an object.\n91defpositions( self):\n92 \u201d\u201d\u201dGenerate an iteration of the tree\ns positions.\u201d\u201d\u201d\n93 return self .preorder( ) # return entire preorder iteration\nCode Fragment 8.18: An implementation of the positions method for the Tree class\nthat relies on a preorder traversal to generate the results.", "8.4. Tree Traversal Algorithms 335\nPostorder Traversal\nWe can implement a postorder traversal using very similar technique as with a\npreorder traversal. The only difference is that within the recursive utility for a post-order we wait to yield position puntil after we have recursively yield the positions\nin its subtrees. An implementation is given in Code Fragment 8.19.\n94defpostorder( self):\n95 \u201d\u201d\u201dGenerate a postorder iteration of positions in the tree.\u201d\u201d\u201d\n96 if not self .is\nempty():\n97 forpin self.\n subtree\n postorder( self.root()): # start recursion\n98 yieldp\n99\n100 def\nsubtree\n postorder( self,p ) :\n101 \u201d\u201d\u201dGenerate a postorder iteration of positions in subtree rooted at p.\u201d\u201d\u201d\n102 forcin self .children(p): # for each child c\n103 forotherin self .\nsubtree\n postorder(c): # do postorder of c\u2019s subtree\n104 yieldother # yielding each to our caller\n105 yieldp # visit p after its subtrees\nCode Fragment 8.19: Support for performing a postorder traversal of a tree. This\ncode should be included in the body of the Tree class.\nBreadth-First Traversal\nIn Code Fragment 8.20, we provide an implementation of the breadth-\ufb01rst traversalalgorithm in the context of our Tree class. Recall that the breadth-\ufb01rst traversal\nalgorithm is not recursive; it relies on a queue of positions to manage the traver-sal process. Our implementation uses the LinkedQueue class from Section 7.1.2,\nalthough any implementation of the queue ADT would suf\ufb01ce.\nInorder Traversal for Binary Trees\nThe preorder, postorder, and breadth-\ufb01rst traversal algorithms are applicable toall trees, and so we include their implementations within the Tree abstract base\nclass. Those methods are inherited by the abstract BinaryTree class, the concrete\nLinkedBinaryTree class, and any other dependent tree classes we might develop.\nThe inorder traversal algorithm, because it explicitly relies on the notion of a\nleft and right child of a node, only applies to binary trees. We therefore include itsde\ufb01nition within the body of the BinaryTree class. We use a similar technique to\nimplement an inorder traversal (Code Fragment 8.21) as we did with preorder and\npostorder traversals.", "336 Chapter 8. Trees\n106 defbreadth\ufb01rst(self ):\n107 \u201d\u201d\u201dGenerate a breadth-\ufb01rst iteration of the positions of the tree.\u201d\u201d\u201d\n108 if not self .is\nempty():\n109 fringe = LinkedQueue( ) # known positions not yet yielded\n110 fringe.enqueue( self.root()) # starting with the root\n111 while not fringe.is\n empty():\n112 p = fringe.dequeue( ) # remove from front of the queue\n113 yieldp # report this position\n114 forcin self.children(p):\n115 fringe.enqueue(c) # add children to back of queue\nCode Fragment 8.20: An implementation of a breadth-\ufb01rst traversal of a tree. This\ncode should be included in the body of the Tree class.\n37definorder( self):\n38 \u201d\u201d\u201dGenerate an inorder iteration of positions in the tree.\u201d\u201d\u201d\n39 if not self .is\nempty():\n40 forpin self.\n subtree\n inorder( self.root()):\n41 yieldp\n42\n43def\nsubtree\n inorder( self,p ) :\n44 \u201d\u201d\u201dGenerate an inorder iteration of positions in subtree rooted at p.\u201d\u201d\u201d\n45 if self.left(p) is not None : # if left child exists, traverse its subtree\n46 forotherin self .\nsubtree\n inorder( self.left(p)):\n47 yieldother\n48 yieldp # visit p between its subtrees\n49 if self.right(p) is not None :# if right child exists, traverse its subtree\n50 forotherin self .\nsubtree\n inorder( self.right(p)):\n51 yieldother\nCode Fragment 8.21: Support for performing an inorder traversal of a binary tree.\nThis code should be included in the BinaryTree class (given in Code Fragment 8.7).\nFor many applications of binary trees, an inorder traversal provides a natural\niteration. We could make it the default for the BinaryTree class by overriding the\npositions method that was inherited from the Tree class (see Code Fragment 8.22).\n52 # override inherited version to make inorder the default\n53defpositions( self):\n54 \u201d\u201d\u201dGenerate an iteration of the tree\n s positions.\u201d\u201d\u201d\n55 return self .inorder( ) # make inorder the default\nCode Fragment 8.22: De\ufb01ning the BinaryTree.position method so that positions are\nreported using inorder traversal.", "8.4. Tree Traversal Algorithms 337\n8.4.5 Applications of Tree Traversals\nIn this section, we demonstrate several representative applications of tree traversals,\nincluding some customizations of the standard traversal algorithms.\nTable of Contents\nWhen using a tree to represent the hierarchical structure of a document, a preordertraversal of the tree can naturally be used to produce a table of contents for the doc-\nument. For example, the table of contents associated with the tree from Figure 8.15\nis displayed in Figure 8.20. Part (a) of that \ufb01gure gives a simple presentation withone element per line; part (b) shows a more attractive presentation produced byindenting each element based on its depth within the tree. A similar presentationcould be used to display the contents of a computer\u2019s \ufb01le system, based on its tree\nrepresentation (as in Figure 8.3).\nPaper PaperTitle Title\nAbstract Abstract\n1\n 1\n1.1\n 1.1\n1.2\n 1.2\n2\n 2\n2.1\n 2.1\n... ...\n(a) (b)\nFigure 8.20: Table of contents for a document represented by the tree in Figure 8.15:\n(a) without indentation; (b) with indentation based on depth within the tree.\nThe unindented version of the table of contents, given a tree T, can be produced\nwith the following code:\nforpinT.preorder():\nprint(p.element())\nTo produce the presentation of Figure 8.20(b), we indent each element with a\nnumber of spaces equal to twice the element\u2019s depth in the tree (hence, the root ele-\nment was unindented). Although we could replace the body of the above loop with\nthe statement print(2\n T.depth(p)\n +s t r ( p . e l e m e n t ( ) ) ) , such an approach is\nunnecessarily inef\ufb01cient. Although the work to produce the preorder traversal runsinO(n)time, based on the analysis of Section 8.4.1, the calls to depth incur a hid-\nden cost. Making a call to depth from every position of the tree results in O(n\n2)\nworst-case time, as noted when analyzing the algorithm height1 in Section 8.1.3.", "338 Chapter 8. Trees\nA preferred approach to producing an indented table of contents is to redesign\na top-down recursion that includes the current depth as an additional parameter.\nSuch an implementation is provided in Code Fragment 8.23. This implementationruns in worst-case O(n)time (except, technically, the time it takes to print strings\nof increasing lengths).\n1defpreorder\nindent(T, p, d):\n2\u201d\u201d\u201dPrint preorder representation of subtree of T rooted at p at depth d.\u201d\u201d\u201d\n3print(2\n d\n +str(p.element())) # use depth for indentation\n4forcinT.children(p):\n5 preorder\n indent(T, c, d+1) # child depth is d+1\nCode Fragment 8.23: Ef\ufb01cient recursion for printing indented version of a pre-\norder traversal. On a complete tree T, the recursion should be started with form\npreorder\n indent(T, T.root(), 0).\nIn the example of Figure 8.20, we were fortunate in that the numbering was\nembedded within the elements of the tree. More generally, we might be interested\nin using a preorder traversal to display the structure of a tree, with indentation and\nalso explicit numbering that was not present in the tree. For example, we mightdisplay the tree from Figure 8.2 beginning as:\nElectronics R\u2019Us\n1 R&D2 Sales\n2.1 Domestic2.2 International\n2.2.1 Canada2.2.2 S. America\nThis is more challenging, because the numbers used as labels are implicit in\nthe structure of the tree. A label depends on the index of each position, relative toits siblings, along the path from the root to the current position. To accomplish thetask, we add a representation of that path as an additional parameter to the recursive\nsignature. Speci\ufb01cally, we use a list of zero-indexed numbers, one for each position\nalong the downward path, other than the root. (We convert those numbers to one-indexed form when printing.)\nAt the implementation level, we wish to avoid the inef\ufb01ciency of duplicating\nsuch lists when sending a new parameter from one level of the recursion to the next.A standard solution is to share the same list instance throughout the recursion. Atone level of the recursion, a new entry is temporarily added to the end of the listbefore making further recursive calls. In order to \u201cleave no trace,\u201d that same blockof code must remove the extraneous entry from the list before completing its task.\nAn implementation based on this approach is given in Code Fragment 8.24.", "8.4. Tree Traversal Algorithms 339\n1defpreorder\n label(T, p, d, path):\n2\u201d\u201d\u201dPrint labeled representation of subtree of T rooted at p at depth d.\u201d\u201d\u201d\n3label =\n .\n.join(str(j+1)forjinpath) # displayed labels are one-indexed\n4print(2\n d\n +l a b e l ,p . e l e m e n t ( ) )\n5path.append(0) # path entries are zero-indexed\n6forcinT.children(p):\n7 preorder\n label(T, c, d+1, path) # child depth is d+1\n8 path[\u22121] += 1\n9path.pop()\nCode Fragment 8.24: Ef\ufb01cient recursion for printing an indented and labeled pre-\nsentation of a preorder traversal.\nParenthetic Representations of a Tree\nIt is not possible to reconstruct a general tree, given only the preorder sequence\nof elements, as in Figure 8.20(a). Some additional context is necessary for the\nstructure of the tree to be well de\ufb01ned. The use of indentation or numbered labels\nprovides such context, with a very human-friendly presentation. However, thereare more concise string representations of trees that are computer-friendly.\nIn this section, we explore one such representation. The parenthetic string\nrepresentation P(T)of tree Tis recursively de\ufb01ned as follows. If Tconsists of a\nsingle position p,t h e n\nP(T)=str(p.element ()).\nOtherwise, it is de\ufb01ned recursively as,\nP(T)=str(p.element ()) +\n(\n+P(T1)+\n,\n +\u00b7\u00b7\u00b7 +\n,\n +P(Tk)+\n)\nwhere pis the root of TandT1,T2,..., Tkare the subtrees rooted at the children\nofp, which are given in order if Tis an ordered tree. We are using \u201c +\u201dh e r et o\ndenote string concatenation. As an example, the parenthetic representation of thetree of Figure 8.2 would appear as follows (line breaks are cosmetic):\nElectronics R\u2019Us (R&D, Sales (Domestic, International (Canada,\nS. America, Overseas (Africa, Europe, Asia, Australia))),\nPurchasing, Manufacturing (TV, CD, Tuner))\nAlthough the parenthetic representation is essentially a preorder traversal, we\ncannot easily produce the additional punctuation using the formal implementationofpreorder , as given in Code Fragment 8.17. The opening parenthesis must be\nproduced just before the loop over a position\u2019s children and the closing parenthesismust be produced just after that loop. Furthermore, the separating commas mustbe produced. The Python function parenthesize , shown in Code Fragment 8.25, is\na custom traversal that prints such a parenthetic string representation of a tree T.", "340 Chapter 8. Trees\n1defparenthesize(T, p):\n2\u201d\u201d\u201dPrint parenthesized representation of subtree of T rooted at p.\u201d\u201d\u201d\n3print(p.element(), end=\n ) # use of end avoids trailing newline\n4if not T.is\nleaf(p):\n5 \ufb01rst\ntime = True\n6 forcinT.children(p):\n7 sep =\n (\nif\ufb01rst\ntimeelse\n ,\n # determine proper separator\n8 print(sep, end=\n )\n9 \ufb01rst\ntime = False # any future passes will not be the \ufb01rst\n10 parenthesize(T, c) # recur on child\n11 print(\n )\n, end=\n ) # include closing parenthesis\nCode Fragment 8.25: Function that prints parenthetic string representation of a tree.\nComputing Disk Space\nIn Example 8.1, we considered the use of a tree as a model for a \ufb01le-system struc-\nture, with internal positions representing directories and leaves representing \ufb01les.In fact, when introducing the use of recursion back in Chapter 4, we speci\ufb01cally\nexamined the topic of \ufb01le systems (see Section 4.1.4). Although we did not explic-\nitly model it as a tree at that time, we gave an implementation of an algorithm forcomputing the disk usage (Code Fragment 4.5).\nThe recursive computation of disk space is emblematic of a postorder traversal,\nas we cannot effectively compute the total space used by a directory until after we\nknow the space that is used by its children directories. Unfortunately, the formal\nimplementation of postorder , as given in Code Fragment 8.19 does not suf\ufb01ce for\nthis purpose. As it visits the position of a directory, there is no easy way to discern\nwhich of the previous positions represent children of that directory, nor how muchrecursive disk space was allocated.\nWe would like to have a mechanism for children to return information to the\nparent as part of the traversal process. A custom solution to the disk space prob-lem, with each level of recursion providing a return value to the (parent) caller, isprovided in Code Fragment 8.26.\n1defdisk\nspace(T, p):\n2\u201d\u201d\u201dReturn total disk space for subtree of T rooted at p.\u201d\u201d\u201d\n3subtotal = p.element().space( ) # space used at position p\n4forcinT.children(p):\n5 subtotal += disk\n space(T, c) # add child\u2019s space to subtotal\n6return subtotal\nCode Fragment 8.26: Recursive computation of disk space for a tree. We assume\nthat a space() method of each tree element reports the local space used at that\nposition.", "8.4. Tree Traversal Algorithms 341\n8.4.6 Euler Tours and the Template Method Pattern \u22c6\nThe various applications described in Section 8.4.5 demonstrate the great power\nof recursive tree traversals. Unfortunately, they also show that the speci\ufb01c imple-mentations of the preorder andpostorder methods of our Tree class, or the inorder\nmethod of the BinaryTree class, are not general enough to capture the range of\ncomputations we desire. In some cases, we need more of a blending of the ap-proaches, with initial work performed before recurring on subtrees, additional workperformed after those recursions, and in the case of a binary tree, work performed\nbetween the two possible recursions. Furthermore, in some contexts it was impor-\ntant to know the depth of a position, or the complete path from the root to thatposition, or to return information from one level of the recursion to another. Foreach of the previous applications, we were able to develop a custom implementa-tion to properly adapt the recursive ideas, but the great principles of object-oriented\nprogramming introduced in Section 2.1.1 include adaptability andreusability .\nIn this section, we develop a more general framework for implementing tree\ntraversals based on a concept known as an Euler tour traversal . The Euler tour\ntraversal of a general tree Tcan be informally de\ufb01ned as a \u201cwalk\u201d around T,w h e r e\nwe start by going from the root toward its leftmost child, viewing the edges of Tas\nbeing \u201cwalls\u201d that we always keep to our left. (See Figure 8.21.)\n3 1 9 5 4 7+ 3 2 \u2212 3 \u2212\u00d7 + \u00d7 6/ +\u2212\nFigure 8.21: Euler tour traversal of a tree.\nThe complexity of the walk is O(n), because it progresses exactly two times\nalong each of the n\u22121 edges of the tree\u2014once going downward along the edge, and\nlater going upward along the edge. To unify the concept of preorder and postordertraversals, we can think of there being two notable \u201cvisits\u201d to each position p:\n\u2022A \u201cpre visit\u201d occurs when \ufb01rst reaching the position, that is, when the walk\npasses immediately left of the node in our visualization.\n\u2022A \u201cpost visit\u201d occurs when the walk later proceeds upward from that position,\nthat is, when the walk passes to the right of the node in our visualization.", "342 Chapter 8. Trees\nThe process of an Euler tour can easily be viewed recursively. In between the\n\u201cpre visit\u201d and \u201cpost visit\u201d of a given position will be a recursive tour of each of\nits subtrees. Looking at Figure 8.21 as an example, there is a contiguous portionof the entire tour that is itself an Euler tour of the subtree of the node with element\u201c/\u201d. That tour contains two contiguous subtours, one traversing that position\u2019s leftsubtree and another traversing the right subtree. The pseudo-code for an Euler tour\ntraversal of a subtree rooted at a position pis shown in Code Fragment 8.27.\nAlgorithm eulertour(T, p) :\nperform the \u201cpre visit\u201d action for position p\nforeach child cinT.children(p) do\neulertour(T, c) {recursively tour the subtree rooted at c}\nperform the \u201cpost visit\u201d action for position p\nCode Fragment 8.27: Algorithm eulertour for performing an Euler tour traversal of\na subtree rooted at position pof a tree.\nThe Template Method Pattern\nTo provide a framework that is reusable and adaptable, we rely on an interesting\nobject-oriented software design pattern, the template method pattern . The template\nmethod pattern describes a generic computation mechanism that can be specializedfor a particular application by rede\ufb01ning certain steps. To allow customization, the\nprimary algorithm calls auxiliary functions known as hooks at designated steps of\nthe process.\nIn the context of an Euler tour traversal, we de\ufb01ne two separate hooks, a pre-\nvisit hook that is called before the subtrees are traversed, and a postvisit hook that is\ncalled after the completion of the subtree traversals. Our implementation will takethe form of an EulerTour class that manages the process, and de\ufb01nes trivial de\ufb01-\nnitions for the hooks that do nothing. The traversal can be customized by de\ufb01ninga subclass of EulerTour and overriding one or both hooks to provide specialized\nbehavior.\nPython Implementation\nOur implementation of an EulerTour class is provided in Code Fragment 8.28. The\nprimary recursive process is de\ufb01ned in the nonpublic\n tour method. A tour instance\nis created by sending a reference to a speci\ufb01c tree to the constructor, and then bycalling the public execute method, which beings the tour and returns a \ufb01nal result\nof the computation.", "8.4. Tree Traversal Algorithms 343\n1classEulerTour:\n2\u201d\u201d\u201dAbstract base class for performing Euler tour of a tree.\n3\n4\n hook\nprevisit and\n hook\npostvisit may be overridden by subclasses.\n5\u201d\u201d\u201d\n6def\n init\n(self, tree):\n7 \u201d\u201d\u201dPrepare an Euler tour template for given tree.\u201d\u201d\u201d\n8 self.\ntree = tree\n9\n10deftree(self):\n11 \u201d\u201d\u201dReturn reference to the tree being traversed.\u201d\u201d\u201d\n12 return self .\ntree\n13\n14defexecute( self):\n15 \u201d\u201d\u201dPerform the tour and return any result from post visit of root.\u201d\u201d\u201d\n16 iflen(self.\ntree)>0:\n17 return self .\ntour(self.\ntree.root(), 0, [ ]) # start the recursion\n1819def\ntour(self,p ,d ,p a t h ) :\n20 \u201d\u201d\u201dPerform tour of subtree rooted at Position p.\n21\n22 p Position of current node being visited\n23 d d e p t ho fpi nt h et r e e\n24 path list of indices of children on path from root to p\n25 \u201d\u201d\u201d\n26 self.\nhook\nprevisit(p, d, path) # \u201dpre visit\u201d p\n27 results = [ ]\n28 path.append(0) # add new index to end of path before recursion\n29 forcin self .\ntree.children(p):\n30 results.append( self.\ntour(c, d+1, path)) # recur on child\n s subtree\n31 path[\u22121] += 1 # increment index\n32 path.pop( ) # remove extraneous index from end of path\n33 answer = self.\nhook\npostvisit(p, d, path, results) # \u201dpost visit\u201d p\n34 return answer\n3536def\nhook\nprevisit( self,p ,d ,p a t h ) : # can be overridden\n37 pass\n38\n39def\nhook\npostvisit( self,p ,d ,p a t h ,r e s u l t s ) : # can be overridden\n40 pass\nCode Fragment 8.28: AnEulerTour base class providing a framework for perform-\ning Euler tour traversals of a tree.", "344 Chapter 8. Trees\nBased on our experience of customizing traversals for sample applications Sec-\ntion 8.4.5, we build support into the primary EulerTour for maintaining the re-\ncursive depth and the representation of the recursive path through a tree, using the\napproach that we introduced in Code Fragment 8.24. We also provide a mechanismfor one recursive level to return a value to another when post-processing. Formally,our framework relies on the following two hooks that can be specialized:\n\u2022method\nhook\nprevisit(p, d, path)\nThis function is called once for each position, immediately before its subtrees(if any) are traversed. Parameter pis a position in the tree, dis the depth of\nthat position, and path is a list of indices, using the convention described in\nthe discussion of Code Fragment 8.24. No return value is expected from thisfunction.\n\u2022method\nhook\npostvisit(p, d, path, results)\nThis function is called once for each position, immediately after its subtrees(if any) are traversed. The \ufb01rst three parameters use the same convention asdid\nhook\nprevisit . The \ufb01nal parameter is a list of objects that were provided\nas return values from the post visits of the respective subtrees of p.A n yv a l u e\nreturned by this call will be available to the parent of pduring its postvisit.\nFor more complex tasks, subclasses of EulerTour may also choose to initialize\nand maintain additional state in the form of instance variables that can be accessedwithin the bodies of the hooks.\nUsing the Euler Tour Framework\nTo demonstrate the \ufb02exibility of our Euler tour framework, we revisit the sampleapplications from Section 8.4.5. As a simple example, an indented preorder traver-sal, akin to that originally produced by Code Fragment 8.23, can be generated withthe simple subclass given in Code Fragment 8.29.\n1classPreorderPrintIndentedTour(EulerTour):\n2def\nhook\nprevisit( self,p ,d ,p a t h ) :\n3 print(2\n d\n +str(p.element()))\nCode Fragment 8.29: A subclass of EulerTour that produces an indented preorder\nlist of a tree\u2019s elements.\nSuch a tour would be started by creating an instance of the subclass for a given\ntreeT, and invoking its execute method. This could be expressed as follows:\ntour = PreorderPrintIndentedTour(T)tour.execute()", "8.4. Tree Traversal Algorithms 345\nA labeled version of an indented, preorder presentation, akin to Code Frag-\nment 8.24, could be generated by the new subclass of EulerTour s h o w ni nC o d e\nFragment 8.30.\n1classPreorderPrintIndentedLabeledTour(EulerTour):\n2def\nhook\nprevisit( self,p ,d ,p a t h ) :\n3 label =\n .\n.join(str(j+1)forjinpath) # labels are one-indexed\n4 print(2\n d\n +l a b e l ,p . e l e m e n t ( ) )\nCode Fragment 8.30: A subclass of EulerTour that produces a labeled and indented,\npreorder list of a tree\u2019s elements.\nTo produce the parenthetic string representation, originally achieved with Code\nFragment 8.25, we de\ufb01ne a subclass that overrides both the previsit and postvisit\nhooks. Our new implementation is given in Code Fragment 8.31.\n1classParenthesizeTour(EulerTour):\n2def\nhook\nprevisit( self,p ,d ,p a t h ) :\n3 ifpathandpath[\u22121]>0: # p follows a sibling\n4 print(\n ,\n,e n d =\n ) # so preface with comma\n5 print(p.element(), end=\n ) # then print element\n6 if not self .tree().is\n leaf(p): # if p has children\n7 print(\n (\n,e n d =\n ) # print opening parenthesis\n8\n9def\nhook\npostvisit( self,p ,d ,p a t h ,r e s u l t s ) :\n10 if not self .tree().is\n leaf(p): # if p has children\n11 print(\n )\n,e n d =\n ) # print closing parenthesis\nCode Fragment 8.31: A subclass of EulerTour that prints a parenthetic string repre-\nsentation of a tree.\nNotice that in this implementation, we need to invoke a method on the tree instance\nthat is being traversed from within the hooks. The public tree() method of the\nEulerTour class serves as an accessor for that tree.\nFinally, the task of computing disk space, as originally implemented in Code\nFragment 8.26, can be performed quite easily with the EulerTour subclass shown\nin Code Fragment 8.32. The postvisit result of the root will be returned by the calltoexecute() .\n1classDiskSpaceTour(EulerTour):\n2def\nhook\npostvisit( self,p ,d ,p a t h ,r e s u l t s ) :\n3 # we simply add space associated with p to that of its subtrees\n4 return p.element().space( ) + sum(results)\nCode Fragment 8.32: A subclass of EulerTour that computes disk space for a tree.", "346 Chapter 8. Trees\nThe Euler Tour Traversal of a Binary Tree\nIn Section 8.4.6, we introduced the concept of an Euler tour traversal of a general\ngraph, using the template method pattern in designing the EulerTour class. That\nclass provided methods\n hook\nprevisit and\nhook\npostvisit that could be overrid-\nden to customize a tour. In Code Fragment 8.33 we provide a BinaryEulerTour\nspecialization that includes an additional\n hook\ninvisit that is called once for each\nposition\u2014after its left subtree is traversed, but before its right subtree is traversed.\nOur implementation of BinaryEulerTour replaces the original\n tour utility to\nspecialize to the case in which a node has at most two children. If a node has onlyone child, a tour differentiates between whether that is a left child or a right child,with the \u201cin visit\u201d taking place after the visit of a sole left child, but before the visitof a sole right child. In the case of a leaf, the three hooks are called in succession.\n1classBinaryEulerTour(EulerTour):\n2\u201d\u201d\u201dAbstract base class for performing Euler tour of a binary tree.\n34This version includes an additional\nhook\ninvisit that is called after the tour\n5of the left subtree (if any), yet before the tour of the right subtree (if any).\n67Note: Right child is always assigned index 1 in path, even if no left sibling.\n8\u201d\u201d\u201d\n9def\ntour(self,p ,d ,p a t h ) :\n10 results = [ None,None] # will update with results of recursions\n11 self.\nhook\nprevisit(p, d, path) # \u201dpre visit\u201d for p\n12 if self.\ntree.left(p) is not None : # consider left child\n13 path.append(0)\n14 results[0] = self.\ntour(self.\ntree.left(p), d+1, path)\n15 path.pop()\n16 self.\nhook\ninvisit(p, d, path) #\u201d i nv i s i t \u201df o rp\n17 if self.\ntree.right(p) is not None : # consider right child\n18 path.append(1)\n19 results[1] = self.\ntour(self.\ntree.right(p), d+1, path)\n20 path.pop()\n21 answer = self.\nhook\npostvisit(p, d, path, results) # \u201dpost visit\u201d p\n22 return answer\n23\n24def\nhook\ninvisit( self,p ,d ,p a t h ) : pass # can be overridden\nCode Fragment 8.33: ABinaryEulerTour base class providing a specialized tour for\nbinary trees. The original EulerTour base class was given in Code Fragment 8.28.", "8.4. Tree Traversal Algorithms 347\n3210\n0123456789 1 0 1 1 1 24Figure 8.22: An inorder drawing of a binary tree.\nTo demonstrate use of the BinaryEulerTour framework, we develop a subclass\nthat computes a graphical layout of a binary tree, as shown in Figure 8.22. The\ngeometry is determined by an algorithm that assigns x-a n d y-coordinates to each\nposition pof a binary tree Tusing the following two rules:\n\u2022x(p)is the number of positions visited before pin an inorder traversal of T.\n\u2022y(p)is the depth of pinT.\nIn this application, we take the convention common in computer graphics that x-\ncoordinates increase left to right and y-coordinates increase top to bottom. So the\norigin is in the upper left corner of the computer screen.\nCode Fragment 8.34 provides an implementation of a BinaryLayout subclass\nthat implements the above algorithm for assigning (x,y)coordinates to the element\nstored at each position of a binary tree. We adapt the BinaryEulerTour framework\nby introducing additional state in the form of a\n count instance variable that repre-\nsents the number of \u201cin visits\u201d that we have performed. The x-coordinate for each\nposition is set according to that counter.\n1classBinaryLayout(BinaryEulerTour):\n2\u201d\u201d\u201dClass for computing (x,y) coordinates for each node of a binary tree.\u201d\u201d\u201d\n3def\n init\n(self, tree):\n4 super().\ninit\n(tree) # must call the parent constructor\n5 self.\ncount = 0 # initialize count of processed nodes\n67def\nhook\ninvisit( self,p ,d ,p a t h ) :\n8 p.element().setX( self.\ncount) # x-coordinate serialized by count\n9 p.element().setY(d) # y-coordinate is depth\n10 self.\ncount += 1 # advance count of processed nodes\nCode Fragment 8.34: ABinaryLayout class that computes coordinates at which to\ndraw positions of a binary tree. We assume that the element type for the originaltree supports setX andsetY methods.", "348 Chapter 8. Trees\n8.5 Case Study: An Expression Tree\nIn Example 8.7, we introduced the use of a binary tree to represent the structure of\nan arithmetic expression. In this section, we de\ufb01ne a new ExpressionTree class that\nprovides support for constructing such trees, and for displaying and evaluating the\narithmetic expression that such a tree represents. Our ExpressionTree class is de-\n\ufb01ned as a subclass of LinkedBinaryTree , and we rely on the nonpublic mutators to\nconstruct such trees. Each internal node must store a string that de\ufb01nes a binary op-\nerator (e.g.,\n +\n), and each leaf must store a numeric value (or a string representing\na numeric value).\nOur eventual goal is to build arbitrarily complex expression trees for compound\narithmetic expressions such as (((3+1)\u00d74)/((9\u22125)+2)). However, it suf\ufb01ces\nfor the ExpressionTree class to support two basic forms of initialization:\nExpressionTree(value) :Create a tree storing the given value at the root.\nExpressionTree (op,E1,E2):Create a tree storing string opat the root (e.g., +),\nand with the structures of existing ExpressionTree\ninstances E1andE2as the left and right subtrees of\nthe root, respectively.\nSuch a constructor for the ExpressionTree class is given in Code Fragment 8.35.\nThe class formally inherits from LinkedBinaryTree , so it has access to all the non-\npublic update methods that were de\ufb01ned in Section 8.3.1. We use\n add\nroot to cre-\nate an initial root of the tree storing the token provided as the \ufb01rst parameter. Thenwe perform run-time checking of the parameters to determine whether the caller\ninvoked the one-parameter version of the constructor (in which case, we are done),\nor the three-parameter form. In that case, we use the inherited\nattach method to\nincorporate the structure of the existing trees as subtrees of the root.\nComposing a Parenthesized String Representation\nA string representation of an existing expression tree instance, for example, as\n(((3+1)x4)/((9-5)+2))\n , can be produced by displaying tree elements us-\ning an inorder traversal, but with opening and closing parentheses inserted witha preorder and postorder step, respectively. In the context of an ExpressionTree\nclass, we support a special\nstr\n method (see Section 2.3.2) that returns the\nappropriate string. Because it is more ef\ufb01cient to \ufb01rst build a sequence of individ-ual strings to be joined together (see discussion of \u201cComposing Strings\u201d in Sec-tion 5.4.2), the implementation of\nstr\n relies on a nonpublic, recursive method\nnamed\n parenthesize\n recur that appends a series of strings to a list. These methods\nare included in Code 8.35.", "8.5. Case Study: An Expression Tree 349\n1classExpressionTree(LinkedBinaryTree):\n2\u201d\u201d\u201dAn arithmetic expression tree.\u201d\u201d\u201d\n3\n4def\n init\n(self,t o k e n ,l e f t = None,r i g h t = None):\n5 \u201d\u201d\u201dCreate an expression tree.\n67 In a single parameter form, token should be a leaf value (e.g.,\n42\n),\n8 and the expression tree will have that value at an isolated node.\n9\n10 In a three-parameter version, token should be an operator,\n11 and left and right should be existing ExpressionTree instances\n12 that become the operands for the binary operator.\n13 \u201d\u201d\u201d\n14 super().\ninit\n() # LinkedBinaryTree initialization\n15 if not isinstance(token, str):\n16 raiseTypeError(\n Token must be a string\n )\n17 self.\nadd\nroot(token) # use inherited, nonpublic method\n18 ifleftis not None : # presumably three-parameter form\n19 iftokennot in\n +-*x/\n :\n20 raiseValueError(\n token must be valid operator\n )\n21 self.\nattach( self.root(), left, right) # use inherited, nonpublic method\n2223def\nstr\n(self):\n24 \u201d\u201d\u201dReturn string representation of the expression.\u201d\u201d\u201d\n25 pieces = [ ] # sequence of piecewise strings to compose\n26 self.\nparenthesize\n recur(self.root(), pieces)\n27 return\n .join(pieces)\n28\n29def\nparenthesize\n recur(self,p ,r e s u l t ) :\n30 \u201d\u201d\u201dAppend piecewise representation of p\n s subtree to resulting list.\u201d\u201d\u201d\n31 if self.is\nleaf(p):\n32 result.append( str(p.element())) # leaf value as a string\n33 else:\n34 result.append(\n (\n) # opening parenthesis\n35 self.\nparenthesize\n recur(self.left(p), result) # left subtree\n36 result.append(p.element()) #o p e r a t o r\n37 self.\nparenthesize\n recur(self.right(p), result) # right subtree\n38 result.append(\n )\n) # closing parenthesis\nCode Fragment 8.35: The beginning of an ExpressionTree class.", "350 Chapter 8. Trees\nExpression Tree Evaluation\nThe numeric evaluation of an expression tree can be accomplished with a simple\napplication of a postorder traversal. If we know the values represented by the twosubtrees of an internal position, we can calculate the result of the computation that\nposition designates. Pseudo-code for the recursive evaluation of the value repre-\nsented by a subtree rooted at position pis given in Code Fragment 8.36.\nAlgorithm evaluate\nrecur(p) :\nifpi sal e a f then\nreturn the value stored at p\nelse\nlet\u25e6be the operator stored at p\nx=evaluate\n recur(left(p))\ny=evaluate\n recur(right(p))\nreturn x\u25e6y\nCode Fragment 8.36: Algorithm evaluate\n recur for evaluating the expression rep-\nresented by a subtree of an arithmetic expression tree rooted at position p.\nTo implement this algorithm in the context of a Python ExpressionTree class,\nwe provide a public evaluate method that is invoked on instance TasT.evaluate() .\nCode Fragment 8.37 provides such an implementation, relying on a nonpublic\nevaluate\n recur method that computes the value of a designated subtree.\n39defevaluate( self):\n40 \u201d\u201d\u201dReturn the numeric result of the expression.\u201d\u201d\u201d\n41 return self .\nevaluate\n recur(self.root())\n42\n43def\nevaluate\n recur(self,p ) :\n44 \u201d\u201d\u201dReturn the numeric result of subtree rooted at p.\u201d\u201d\u201d\n45 if self.is\nleaf(p):\n46 return \ufb02oat(p.element()) # we assume element is numeric\n47 else:\n48 op = p.element()\n49 left\nval =self.\nevaluate\n recur(self.left(p))\n50 right\nval =self.\nevaluate\n recur(self.right(p))\n51 ifop ==\n +\n:return left\nval + right\n val\n52 elifop ==\n -\n:return left\nval\u2212right\nval\n53 elifop ==\n /\n:return left\nval / right\n val\n54 else:return left\nval\nright\nval #t r e a t\n x\nor\n as multiplication\nCode Fragment 8.37: Support for evaluating an ExpressionTree instance.", "8.5. Case Study: An Expression Tree 351\nBuilding an Expression Tree\nThe constructor for the ExpressionTree class, from Code Fragment 8.35, provides\nbasic functionality for combining existing trees to build larger expression trees.\nHowever, the question still remains how to construct a tree that represents an ex-\npression for a given string, such as\n (((3+1)x4)/((9-5)+2))\n .\nTo automate this process, we rely on a bottom-up construction algorithm, as-\nsuming that a string can \ufb01rst be tokenized so that multidigit numbers are treatedatomically (see Exercise R-8.30), and that the expression is fully parenthesized.\nThe algorithm uses a stack Swhile scanning tokens of the input expression Eto\n\ufb01nd values, operators, and right parentheses. (Left parentheses are ignored.)\n\u2022When we see an operator \u25e6, we push that string on the stack.\n\u2022When we see a literal value v, we create a single-node expression tree T\nstoring v, and push Ton the stack.\n\u2022When we see a right parenthesis,\n)\n, we pop the top three items from the\nstack S, which represent a subexpression (E1\u25e6E2). We then construct a\ntreeTusing trees for E1andE2as subtrees of the root storing \u25e6, and push\nthe resulting tree Tback on the stack.\nWe repeat this until the expression Ehas been processed, at which time the top\nelement on the stack is the expression tree for E. The total running time is O(n).\nAn implementation of this algorithm is given in Code Fragment 8.38 in the form\nof a stand-alone function named build\nexpression\n tree, which produces and returns\nan appropriate ExpressionTree instance, assuming the input has been tokenized.\n1defbuild\nexpression\n tree(tokens):\n2\u201d\u201d\u201dReturns an ExpressionTree based upon by a tokenized expression.\u201d\u201d\u201d\n3S=[] # we use Python list as stack\n4fortintokens:\n5 iftin\n+-x*/\n : # t is an operator symbol\n6 S.append(t) # push the operator symbol\n7 eliftnot in\n ()\n: # consider t to be a literal\n8 S.append(ExpressionTree(t)) # push trivial tree storing value\n9 elift= =\n )\n: # compose a new tree from three constituent parts\n10 right = S.pop( ) # right subtree as per LIFO\n11 op = S.pop( ) # operator symbol\n12 left = S.pop( ) # left subtree\n13 S.append(ExpressionTree(op, left, right)) # repush tree\n14 # we ignore a left parenthesis\n15return S.pop()\nCode Fragment 8.38: Implementation of a build\nexpression\n tree that produces an\nExpressionTree from a sequence of tokens representing an arithmetic expression.", "352 Chapter 8. Trees\n8.6 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-8.1 The following questions refer to the tree of Figure 8.3.\na. Which node is the root?\nb. What are the internal nodes?\nc. How many descendants does node cs016/ have?\nd. How many ancestors does node cs016/ have?\ne. What are the siblings of node homeworks/ ?\nf. Which nodes are in the subtree rooted at node projects/ ?\ng. What is the depth of node papers/ ?\nh. What is the height of the tree?\nR-8.2 Show a tree achieving the worst-case running time for algorithm depth .\nR-8.3 Give a justi\ufb01cation of Proposition 8.4.\nR-8.4 What is the running time of a call to T.\nheight2(p) when called on a\nposition pdistinct from the root of T? (See Code Fragment 8.5.)\nR-8.5 Describe an algorithm, relying only on the BinaryTree operations, that\ncounts the number of leaves in a binary tree that are the leftchild of their\nrespective parent.\nR-8.6 LetTbe an n-node binary tree that may be improper. Describe how to\nrepresent Tby means of a proper binary tree T/primewith O(n)nodes.\nR-8.7 What are the minimum and maximum number of internal and external\nnodes in an improper binary tree with nnodes?\nR-8.8 Answer the following questions so as to justify Proposition 8.8.\na. What is the minimum number of external nodes for a proper binary\ntree with height h? Justify your answer.\nb. What is the maximum number of external nodes for a proper binary\ntree with height h? Justify your answer.\nc. Let Tbe a proper binary tree with height handnnodes. Show that\nlog (n+1)\u22121\u2264h\u2264(n\u22121)/2.\nd. For which values of nandhcan the above lower and upper bounds\nonhbe attained with equality?\nR-8.9 Give a proof by induction of Proposition 8.9.\nR-8.10 Give a direct implementation of the num\nchildren method within the class\nBinaryTree .", "8.6. Exercises 353\nR-8.11 Find the value of the arithmetic expression associated with each subtree\nof the binary tree of Figure 8.8.\nR-8.12 Draw an arithmetic expression tree that has four external nodes, storingthe numbers 1, 5, 6, and 7 (with each number stored in a distinct external\nnode, but not necessarily in this order), and has three internal nodes, eachstoring an operator from the set {+,\u2212,\u00d7,/}, so that the value of the root\nis 21. The operators may return and act on fractions, and an operator maybe used more than once.\nR-8.13 Draw the binary tree representation of the following arithmetic expres-sion: \u201c (((5+2)\u2217(2\u22121))/((2+9)+( ( 7\u22122)\u22121))\u22178)\u201d.\nR-8.14 Justify Table 8.2, summarizing the running time of the methods of a treerepresented with a linked structure, by providing, for each method, a de-\nscription of its implementation, and an analysis of its running time.\nR-8.15 TheLinkedBinaryTree class provides only nonpublic versions of the up-\ndate methods discussed on page 319. Implement a simple subclass named\nMutableLinkedBinaryTree that provides public wrapper functions for each\nof the inherited nonpublic update methods.\nR-8.16 LetTbe a binary tree with nnodes, and let f()be the level numbering\nfunction of the positions of T, as given in Section 8.3.2.\na. Show that, for every position pofT,f(p)\u22642\nn\u22122.\nb. Show an example of a binary tree with seven nodes that attains the\nabove upper bound on f(p)for some position p.\nR-8.17 Show how to use the Euler tour traversal to compute the level number\nf(p), as de\ufb01ned in Section 8.3.2, of each position in a binary tree T.\nR-8.18 LetTbe a binary tree with npositions that is realized with an array rep-\nresentation A,a n dl e t f()be the level numbering function of the positions\nofT, as given in Section 8.3.2. Give pseudo-code descriptions of each of\nthe methods root,parent ,left,right ,is\nleaf,a n dis\nroot.\nR-8.19 Our de\ufb01nition of the level numbering function f(p), as given in Sec-\ntion 8.3.2, began with the root having number 0. Some authors preferto use a level numbering g(p)in which the root is assigned number 1, be-\ncause it simpli\ufb01es the arithmetic for \ufb01nding neighboring positions. RedoExercise R-8.18, but assuming that we use a level numbering g(p)in\nwhich the root is assigned number 1.\nR-8.20 Draw a binary tree Tthat simultaneously satis\ufb01es the following:\n\u2022Each internal node of Tstores a single character.\n\u2022Apreorder traversal of Tyields EXAMFUN .\n\u2022Aninorder traversal of Tyields MAFXUEN .\nR-8.21 In what order are positions visited during a preorder traversal of the tree\nof Figure 8.8?", "354 Chapter 8. Trees\nR-8.22 In what order are positions visited during a postorder traversal of the tree\nof Figure 8.8?\nR-8.23 LetTbe an ordered tree with more than one node. Is it possible that the\npreorder traversal of Tvisits the nodes in the same order as the postorder\ntraversal of T? If so, give an example; otherwise, explain why this cannot\noccur. Likewise, is it possible that the preorder traversal of Tvisits the\nnodes in the reverse order of the postorder traversal of T? If so, give an\nexample; otherwise, explain why this cannot occur.\nR-8.24 Answer the previous question for the case when Tis a proper binary tree\nwith more than one node.\nR-8.25 Consider the example of a breadth-\ufb01rst traversal given in Figure 8.17.\nUsing the annotated numbers from that \ufb01gure, describe the contents ofthe queue before each pass of the while loop in Code Fragment 8.14. Toget started, the queue has contents {1}before the \ufb01rst pass, and contents\n{2,3,4}before the second pass.\nR-8.26 Thecollections.deque class supports an extend method that adds a col-\nlection of elements to the end of the queue at once. Reimplement thebreadth\ufb01rst method of the Tree class to take advantage of this feature.\nR-8.27 Give the output of the function parenthesize(T, T.root()) , as described\nin Code Fragment 8.25, when Tis the tree of Figure 8.8.\nR-8.28 What is the running time of parenthesize(T, T.root()) , as given in Code\nFragment 8.25, for a tree Twith nnodes?\nR-8.29 Describe, in pseudo-code, an algorithm for computing the number of de-\nscendants of each node of a binary tree. The algorithm should be based\non the Euler tour traversal.\nR-8.30 Thebuild\nexpression\n tree method of the ExpressionTree class requires\ninput that is an iterable of string tokens. We used a convenient exam-\nple,\n (((3+1)x4)/((9-5)+2))\n , in which each character is its own to-\nken, so that the string itself suf\ufb01ced as input to build\nexpression\n tree.\nIn general, a string, such as\n (35 + 14)\n , must be explicitly tokenized\ninto list [\n(\n,\n35\n,\n+\n,\n14\n,\n)\n]so as to ignore whitespace and to\nrecognize multidigit numbers as a single token. Write a utility method,tokenize(raw) , that returns such a list of tokens for a rawstring.\nCreativity\nC-8.31 De\ufb01ne the internal path length ,I(T), of a tree Tto be the sum of the\ndepths of all the internal positions in T. Likewise, de\ufb01ne the external path\nlength ,E(T), of a tree Tto be the sum of the depths of all the external\npositions in T. Show that if Tis a proper binary tree with npositions, then\nE(T)=I(T)+n\u22121.", "8.6. Exercises 355\nC-8.32 LetTbe a (not necessarily proper) binary tree with nnodes, and let Dbe\nthe sum of the depths of all the external nodes of T. Show that if Thas the\nminimum number of external nodes possible, then DisO(n)and if Thas\nthe maximum number of external nodes possible, then DisO(nlogn).\nC-8.33 LetTbe a (possibly improper) binary tree with nnodes, and let Dbe the\nsum of the depths of all the external nodes of T. Describe a con\ufb01guration\nforTsuch that Dis\u03a9(n2). Such a tree would be the worst case for the\nasymptotic running time of method\n height1 (Code Fragment 8.4).\nC-8.34 For a tree T,l e t nIdenote the number of its internal nodes, and let nE\ndenote the number of its external nodes. Show that if every internal node\ninThas exactly 3 children, then nE=2nI+1.\nC-8.35 Two ordered trees T/primeandT/prime/primeare said to be isomorphic if one of the fol-\nlowing holds:\n\u2022Both T/primeandT/prime/primeare empty.\n\u2022The roots of T/primeandT/prime/primehave the same number k\u22650 of subtrees, and\ntheithsuch subtree of T/primeis isomorphic to the ithsuch subtree of T/prime/prime\nfori=1,..., k.\nDesign an algorithm that tests whether two given ordered trees are iso-morphic. What is the running time of your algorithm?\nC-8.36 Show that there are more than 2\nnimproper binary trees with ninternal\nnodes such that no pair are isomorphic (see Exercise C-8.35).\nC-8.37 If we exclude isomorphic trees (see Exercise C-8.35), exactly how manyproper binary trees exist with exactly 4 leaves?\nC-8.38 Add support in LinkedBinaryTree for a method,\ndelete\n subtree(p) ,t h a t\nremoves the entire subtree rooted at position p, making sure to maintain\nthe count on the size of the tree. What is the running time of your imple-mentation?\nC-8.39 Add support in LinkedBinaryTree for a method,\nswap(p,q) ,t h a th a st h e\neffect of restructuring the tree so that the node referenced by ptakes the\nplace of the node referenced by q, and vice versa. Make sure to properly\nhandle the case when the nodes are adjacent.\nC-8.40 We can simplify parts of our LinkedBinaryTree implementation if we\nmake use of of a single sentinel node, referenced as the\n sentinel member\nof the tree instance, such that the sentinel is the parent of the real root ofthe tree, and the root is referenced as the left child of the sentinel. Fur-thermore, the sentinel will take the place of None as the value of the\nleft\nor\nright member for a node without such a child. Give a new imple-\nmentation of the update methods\n delete and\nattach , assuming such a\nrepresentation.", "356 Chapter 8. Trees\nC-8.41 Describe how to clone a LinkedBinaryTree instance representing a proper\nbinary tree, with use of the\n attach method.\nC-8.42 Describe how to clone a LinkedBinaryTree instance representing a (not\nnecessarily proper) binary tree, with use of the\n add\nleftandadd\nright\nmethods.\nC-8.43 We can de\ufb01ne a binary tree representation T/primefor an ordered general tree\nTas follows (see Figure 8.23):\n\u2022For each position pofT, there is an associated position p/primeofT/prime.\n\u2022Ifpi sal e a fo f T,t h e n p/primeinT/primedoes not have a left child; otherwise\nthe left child of p/primeisq/prime,w h e r e qis the \ufb01rst child of pinT.\n\u2022Ifphas a sibling qordered immediately after it in T,t h e nq/primeis the\nright child of p/primeinT; otherwise p/primedoes not have a right child.\nGiven such a representation T/primeof a general ordered tree T, answer each\nof the following questions:\na. Is a preorder traversal of T/primeequivalent to a preorder traversal of T?\nb. Is a postorder traversal of T/primeequivalent to a postorder traversal of T?\nc. Is an inorder traversal of T/primeequivalent to one of the standard traver-\nsals of T? If so, which one?\nD\nFG ECA\nBA\nD F\nGECB\n(a) (b)\nFigure 8.23: Representation of a tree with a binary tree: (a) tree T; (b) binary tree\nT/primeforT. The dashed edges connect nodes of T/primethat are siblings in T.\nC-8.44 Give an ef\ufb01cient algorithm that computes and prints, for every position p\nof a tree T, the element of pfollowed by the height of p\u2019s subtree.\nC-8.45 Give an O(n)-time algorithm for computing the depths of all positions of\na tree T,w h e r e nis the number of nodes of T.\nC-8.46 Thepath length of a tree Tis the sum of the depths of all positions in T.\nDescribe a linear-time method for computing the path length of a tree T.\nC-8.47 Thebalance factor of an internal position pof a proper binary tree is the\ndifference between the heights of the right and left subtrees of p.S h o w\nhow to specialize the Euler tour traversal of Section 8.4.6 to print the\nbalance factors of all the internal nodes of a proper binary tree.", "8.6. Exercises 357\nC-8.48 Given a proper binary tree T,d e \ufb01 n et h e re\ufb02ection ofTto be the binary\ntreeT/primesuch that each node vinTis also in T/prime, but the left child of vinT\nisv\u2019s right child in T/primeand the right child of vinTisv\u2019s left child in T/prime.\nShow that a preorder traversal of a proper binary tree Tis the same as the\npostorder traversal of T\u2019s re\ufb02ection, but in reverse order.\nC-8.49 Let the rank of a position pduring a traversal be de\ufb01ned such that the \ufb01rst\nelement visited has rank 1, the second element visited has rank 2, and so\non. For each position pin a tree T,l e tpre(p)be the rank of pin a preorder\ntraversal of T,l e tpost (p)be the rank of pin a postorder traversal of T,l e t\ndepth (p)be the depth of p,a n dl e t desc (p)be the number of descendants\nofp, including pitself. Derive a formula de\ufb01ning post (p)in terms of\ndesc (p),depth (p),a n dpre(p), for each node pinT.\nC-8.50 Design algorithms for the following operations for a binary tree T:\n\u2022preorder\n next(p) : Return the position visited after pin a preorder\ntraversal of T(orNone ifpis the last node visited).\n\u2022inorder\n next(p) : Return the position visited after pin an inorder\ntraversal of T(orNone ifpis the last node visited).\n\u2022postorder\n next(p) : Return the position visited after pin a postorder\ntraversal of T(orNone ifpis the last node visited).\nWhat are the worst-case running times of your algorithms?\nC-8.51 To implement the preorder method of the LinkedBinaryTree class, we re-\nlied on the convenience of Python\u2019s generator syntax and the yield state-\nment. Give an alternative implementation of preorder that returns an ex-\nplicit instance of a nested iterator class. (See Section 2.3.4 for discussion\nof iterators.)\nC-8.52 Algorithm preorder\n draw draws a binary tree Tby assigning x-a n d y-\ncoordinates to each position psuch that x(p)is the number of nodes pre-\nceding pin the preorder traversal of Tandy(p)is the depth of pinT.\na. Show that the drawing of Tproduced by preorder\n draw has no pairs\nof crossing edges.\nb. Redraw the binary tree of Figure 8.22 using preorder\n draw .\nC-8.53 Redo the previous problem for the algorithm postorder\n draw that is simi-\nlar topreorder\n draw except that it assigns x(p)to be the number of nodes\npreceding position pin the postorder traversal.\nC-8.54 Design an algorithm for drawing general trees, using a style similar to the\ninorder traversal approach for drawing binary trees.\nC-8.55 Exercise P-4.27 described the walk function of the osmodule. This func-\ntion performs a traversal of the implicit tree represented by the \ufb01le system.Read the formal documentation for the function, and in particular its useof an optional Boolean parameter named topdown . Describe how its be-\nhavior relates to tree traversal algorithms described in this chapter.", "358 Chapter 8. Trees\nEurope Asia Africa AustraliaCanada Overseas S. AmericaDomestic InternationalSalesSales (\nDomestic\nInternational (\nCanada\nS. AmericaOverseas (\nAfrica\nEurope\nAsia\nAustralia\n)\n)\n)\n(a) (b)\nFigure 8.24: (a) Tree T; (b) indented parenthetic representation of T.\nC-8.56 Theindented parenthetic representation of a tree Tis a variation of the\nparenthetic representation of T(see Code Fragment 8.25) that uses inden-\ntation and line breaks as illustrated in Figure 8.24. Give an algorithm that\nprints this representation of a tree.\nC-8.57 LetTbe a binary tree with npositions. De\ufb01ne a Roman position to be\na position pinT, such that the number of descendants in p\u2019s left subtree\ndiffer from the number of descendants in p\u2019s right subtree by at most 5.\nDescribe a linear-time method for \ufb01nding each position pofT, such that\npis not a Roman position, but all of p\u2019s descendants are Roman.\nC-8.58 LetTbe a tree with npositions. De\ufb01ne the lowest common ancestor\n(LCA) between two positions pandqas the lowest position in Tthat has\nboth pandqas descendants (where we allow a position to be a descendant\nof itself). Given two positions pandq, describe an ef\ufb01cient algorithm for\n\ufb01nding the LCA of pandq. What is the running time of your algorithm?\nC-8.59 LetTbe a binary tree with npositions, and, for any position pinT,l e tdp\ndenote the depth of pinT.T h e distance between two positions pandq\ninTisdp+dq\u22122da,w h e r e ais the lowest common ancestor (LCA) of p\nandq.T h e diameter ofTis the maximum distance between two positions\ninT. Describe an ef\ufb01cient algorithm for \ufb01nding the diameter of T.W h a t\nis the running time of your algorithm?\nC-8.60 Suppose each position pof a binary tree Tis labeled with its value f(p)in\na level numbering of T. Design a fast method for determining f(a)for the\nlowest common ancestor (LCA), a, of two positions pandqinT,g i v e n\nf(p)andf(q). You do not need to \ufb01nd position a, just value f(a).\nC-8.61 Give an alternative implementation of the build\nexpression\n tree method\nof the ExpressionTree class that relies on recursion to perform an implicit\nEuler tour of the tree that is being built.", "8.6. Exercises 359\nC-8.62 Note that the build\nexpression\n tree function of the ExpressionTree class\nis written in such a way that a leaf token can be any string; for exam-\nple, it parses the expression\n (a*(b+c))\n . However, within the evaluate\nmethod, an error would occur when attempting to convert a leaf token toa number. Modify the evaluate method to accept an optional Python dic-\ntionary that can be used to map such string variables to numeric values,\nwith a syntax such as T.evaluate( {\na\n:3,\nb\n:1,\nc\n:5}).I n t h i s w a y ,\nthe same algebraic expression can be evaluated using different values.\nC-8.63 As mentioned in Exercise C-6.22, post\ufb01x notation is an unambiguous way\nof writing an arithmetic expression without parentheses. It is de\ufb01ned sothat if \u201c (exp\n1)op(exp2)\u201d is a normal (in\ufb01x) fully parenthesized expres-\nsion with operation op, then its post\ufb01x equivalent is \u201c pexp1pexp2op\u201d,\nwhere pexp1is the post\ufb01x version of exp1andpexp2is the post\ufb01x ver-\nsion of exp2. The post\ufb01x version of a single number or variable is just\nthat number or variable. So, for example, the post\ufb01x version of the in\ufb01x\nexpression \u201c ((5+2)\u2217(8\u22123))/4\u201d is \u201c5 2 +83\u2212\u2217 4/\u201d. Implement a\npost\ufb01x method of the ExpressionTree class of Section 8.5 that produces\nthe post\ufb01x notation for the given expression.\nProjects\nP-8.64 Implement the binary tree ADT using the array-based representation de-scribed in Section 8.3.2.\nP-8.65 Implement the tree ADT using a linked structure as described in Sec-tion 8.3.3. Provide a reasonable set of update methods for your tree.\nP-8.66 The memory usage for the LinkedBinaryTree class can be streamlined by\nremoving the parent reference from each node, and instead having each\nPosition instance keep a member,\npath , that is a list of nodes representing\nthe entire path from the root to that position. (This generally saves mem-\nory because there are typically relatively few stored position instances.)Reimplement the LinkedBinaryTree class using this strategy.\nP-8.67 Aslicing \ufb02oor plan divides a rectangle with horizontal and vertical sides\nusing horizontal and vertical cuts. (See Figure 8.25a.) A slicing \ufb02oor plan\ncan be represented by a proper binary tree, called a slicing tree , whose\ninternal nodes represent the cuts, and whose external nodes represent the\nbasic rectangles into which the \ufb02oor plan is decomposed by the cuts. (See\nFigure 8.25b.) The compaction problem for a slicing \ufb02oor plan is de\ufb01ned\nas follows. Assume that each basic rectangle of a slicing \ufb02oor plan is\nassigned a minimum width wand a minimum height h. The compaction\nproblem is to \ufb01nd the smallest possible height and width for each rectangle\nof the slicing \ufb02oor plan that is compatible with the minimum dimensions", "360 Chapter 8. Trees\nA\nBCDEF\nDE F\nB\nCA\n(a) (b)\nFigure 8.25: (a) Slicing \ufb02oor plan; (b) slicing tree associated with the \ufb02oor plan.\nof the basic rectangles. Namely, this problem requires the assignment of\nvalues h(p)andw(p)to each position pof the slicing tree such that:\nw(p)=\u23a7\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9wifpis a leaf whose basic rectangle has\nminimum width w\nmax (w(/lscript),w(r))ifpis an internal position, associated with\na horizontal cut, with left child /lscriptand right\nchild r\nw(/lscript)+w(r)ifpis an internal position, associated with\na vertical cut, with left child /lscriptand right\nchild r\nh(p)=\u23a7\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a8\n\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23aa\u23a9hifpis\na leaf node whose basic rectangle\nhas minimum height h\nh(/lscript)+h(r)ifpis an internal position, associated with\na horizontal cut, with left child /lscriptand right\nchild r\nmax (h(/lscript),h(r))ifpis an internal position, associated with\na vertical cut, with left child /lscriptand right\nchild r\nDesign a data structure for slicing \ufb02oor plans that supports the operations:\n\u2022Create a \ufb02oor plan consisting of a single basic rectangle.\n\u2022Decompose a basic rectangle by means of a horizontal cut.\n\u2022Decompose a basic rectangle by means of a vertical cut.\n\u2022Assign minimum height and width to a basic rectangle.\n\u2022Draw the slicing tree associated with the \ufb02oor plan.\n\u2022Compact and draw the \ufb02oor plan.", "Chapter Notes 361\nP-8.68 Write a program that can play Tic-Tac-Toe effectively. (See Section 5.6.)\nTo do this, you will need to create a game tree T, which is a tree where\neach position corresponds to a game con\ufb01guration , which, in this case,\nis a representation of the Tic-Tac-Toe board. (See Section 8.4.2.) The\nroot corresponds to the initial con\ufb01guration. For each internal position p\ninT, the children of pcorrespond to the game states we can reach from\np\u2019s game state in a single legal move for the appropriate player, A(the\n\ufb01rst player) or B(the second player). Positions at even depths correspond\nto moves for Aand positions at odd depths correspond to moves for B.\nLeaves are either \ufb01nal game states or are at a depth beyond which we do\nnot want to explore. We score each leaf with a value that indicates how\ngood this state is for player A. In large games, like chess, we have to use a\nheuristic scoring function, but for small games, like Tic-Tac-Toe, we canconstruct the entire game tree and score leaves as +1, 0,\u22121, indicating\nwhether player Ahas a win, draw, or lose in that con\ufb01guration. A good\nalgorithm for choosing moves is minimax . In this algorithm, we assign a\nscore to each internal position pinT, such that if prepresents A\u2019s turn, we\ncompute p\u2019s score as the maximum of the scores of p\u2019s children (which\ncorresponds to A\u2019s optimal play from p). If an internal node prepresents\nB\u2019s turn, then we compute p\u2019s score as the minimum of the scores of p\u2019s\nchildren (which corresponds to B\u2019s optimal play from p).\nP-8.69 Implement the tree ADT using the binary tree representation described in\nExercise C-8.43. You may adapt the LinkedBinaryTree implementation.\nP-8.70 Write a program that takes as input a general tree Tand a position pofT\nand converts Tto another tree with the same set of position adjacencies,\nbut now with pas its root.\nChapter Notes\nDiscussions of the classic preorder, inorder, and postorder tree traversal methods can be\nfound in Knuth\u2019s Fundamental Algorithms book [64]. The Euler tour traversal technique\ncomes from the parallel algorithms community; it is introduced by Tarjan and Vishkin [93]\nand is discussed by J\u00b4 aJ\u00b4a [54] and by Karp and Ramachandran [58]. The algorithm for\ndrawing a tree is generally considered to be a part of the \u201cfolklore\u201d of graph-drawing al-gorithms. The reader interested in graph drawing is referred to the book by Di Battista,\nEades, Tamassia, and Tollis [34] and the survey by Tamassia and Liotta [92]. The puzzle\nin Exercise R-8.12 was communicated by Micha Sharir.", "Chapter\n9Priority Queues\nContents\n9 . 1 T h eP r i o r i t yQ u e u eA b s t r a c tD a t aT y p e ........... 3 6 3\n9 . 1 . 1 P r i o r i t i e s...........................3 6 3\n9 . 1 . 2 T h e P r i o r i t y Q u e u e A D T ..................3 6 4\n9 . 2 I m p l e m e n t i n gaP r i o r i t yQ u e u e................ 3 6 5\n9 . 2 . 1 T h e C o m p o s i t i o n D e s i g n P a t t e r n..............3 6 59.2.2 Implementation with an Unsorted List . . . . . . . . . . . 366\n9.2.3 Implementation with a Sorted List . . . . . . . . . . . . . 368\n9 . 3 H e a p s.............................. 3 7 0\n9 . 3 . 1 T h e H e a p D a t a S t r u c t u r e..................3 7 0\n9.3.2 Implementing a Priority Queue with a Heap . . . . . . . . 372\n9.3.3 Array-Based Representation of a Complete Binary Tree . . 3769.3.4 Python Heap Implementation . . . . . . . . . . . . . . . . 376\n9 . 3 . 5 A n a l y s i s o f a H e a p - B a s e d P r i o r i t y Q u e u e ..........3 7 9\n9.3.6 Bottom-Up Heap Construction\n\u22c6.............3 8 0\n9 . 3 . 7 P y t h o n \u2019 s h e a p q M o d u l e...................3 8 4\n9 . 4 S o r t i n gw i t haP r i o r i t yQ u e u e ................. 3 8 5\n9 . 4 . 1 S e l e c t i o n - S o r t a n d I n s e r t i o n - S o r t..............3 8 69 . 4 . 2 H e a p - S o r t ..........................3 8 8\n9 . 5 A d a p t a b l eP r i o r i t yQ u e u e s .................. 3 9 0\n9 . 5 . 1 L o c a t o r s...........................3 9 09.5.2 Implementing an Adaptable Priority Queue . . . . . . . . 391\n9 . 6 E x e r c i s e s ............................ 3 9 5\n", "9.1. The Priority Queue Abstract Data Type 363\n9.1 The Priority Queue Abstract Data Type\n9.1.1 Priorities\nIn Chapter 6, we introduced the queue ADT as a collection of objects that are\nadded and removed according to the \ufb01rst-in, \ufb01rst-out (FIFO ) principle. A com-\npany\u2019s customer call center embodies such a model in which waiting customers are\ntold \u201ccalls will be answered in the order that they were received.\u201d In that setting, a\nnew call is added to the back of the queue, and each time a customer service rep-resentative becomes available, he or she is connected with the call that is removedfrom the front of the call queue.\nIn practice, there are many applications in which a queue-like structure is used\nto manage objects that must be processed in some way, but for which the \ufb01rst-in,\n\ufb01rst-out policy does not suf\ufb01ce. Consider, for example, an air-traf\ufb01c control center\nthat has to decide which \ufb02ight to clear for landing from among many approachingthe airport. This choice may be in\ufb02uenced by factors such as each plane\u2019s distancefrom the runway, time spent waiting in a holding pattern, or amount of remaining\nfuel. It is unlikely that the landing decisions are based purely on a FIFO policy.\nThere are other situations in which a \u201c\ufb01rst come, \ufb01rst serve\u201d policy might seem\nreasonable, yet for which other priorities come into play. To use another airline\nanalogy, suppose a certain \ufb02ight is fully booked an hour prior to departure. Be-cause of the possibility of cancellations, the airline maintains a queue of standbypassengers hoping to get a seat. Although the priority of a standby passenger is\nin\ufb02uenced by the check-in time of that passenger, other considerations include the\nfare paid and frequent-\ufb02yer status. So it may be that an available seat is given toa passenger who has arrived later than another, if such a passenger is assigned a\nbetter priority by the airline agent.\nIn this chapter, we introduce a new abstract data type known as a priority queue .\nThis is a collection of prioritized elements that allows arbitrary element insertion,\nand allows the removal of the element that has \ufb01rst priority. When an element is\nadded to a priority queue, the user designates its priority by providing an associatedkey. The element with the minimum key will be the next to be removed from the\nqueue (thus, an element with key 1 will be given priority over an element with\nkey 2). Although it is quite common for priorities to be expressed numerically, any\nPython object may be used as a key, as long as the object type supports a consistentmeaning for the test a<b, for any instances aandb, so as to de\ufb01ne a natural\norder of the keys. With such generality, applications may develop their own notionof priority for each element. For example, different \ufb01nancial analysts may assign\ndifferent ratings (i.e., priorities) to a particular asset, such as a share of stock.", "364 Chapter 9. Priority Queues\n9.1.2 The Priority Queue ADT\nFormally, we model an element and its priority as a key-value pair. We de\ufb01ne the\npriority queue ADT to support the following methods for a priority queue P:\nP.add(k, v) :Insert an item with key kand value vinto priority queue P.\nP.min() :Return a tuple, (k,v) , representing the key and value of an\nitem in priority queue Pwith minimum key (but do not re-\nmove the item); an error occurs if the priority queue is empty.\nP.remove\n min() :Remove an item with minimum key from priority queue P,\nand return a tuple, (k,v) , representing the key and value of the\nremoved item; an error occurs if the priority queue is empty.\nP.is\nempty() :Return True if priority queue Pdoes not contain any items.\nlen(P) :Return the number of items in priority queue P.\nA priority queue may have multiple entries with equivalent keys, in which case\nmethods minandremove\n minmay report an arbitrary choice of item having mini-\nmum key. Values may be any type of object.\nIn our initial model for a priority queue, we assume that an element\u2019s key re-\nmains \ufb01xed once it has been added to a priority queue. In Section 9.5, we consideran extension that allows a user to update an element\u2019s key within the priority queue.\nExample 9.1:\nThe following table shows a series of operations and their effects\non an initially empty priority queue P. The \u201cPriority Queue\u201d column is somewhat\ndeceiving since it shows the entries as tuples and sorted by key. Such an internal\nrepresentation is not required of a priority queue.\nOperation\n Return Value\n Priority Queue\nP.add(5,A)\n {(5,A)}\nP.add(9,C)\n {(5,A), (9,C) }\nP.add(3,B)\n {(3,B), (5,A), (9,C) }\nP.add(7,D)\n {(3,B), (5,A), (7,D), (9,C) }\nP.min()\n (3,B)\n {(3,B), (5,A), (7,D), (9,C) }\nP.remove\n min()\n (3,B)\n {(5,A), (7,D), (9,C) }\nP.remove\n min()\n (5,A)\n {(7,D), (9,C) }\nlen(P)\n 2\n {(7,D), (9,C) }\nP.remove\n min()\n (7,D)\n {(9,C)}\nP.remove\n min()\n (9,C)\n {}\nP.is\nempty()\n True\n {}\nP.remove\n min()\n \u201cerror\u201d\n {}\n", "9.2. Implementing a Priority Queue 365\n9.2 Implementing a Priority Queue\nIn this section, we show how to implement a priority queue by storing its entries in\na positional list L. (See Section 7.4.) We provide two realizations, depending on\nwhether or not we keep the entries in Lsorted by key.\n9.2.1 The Composition Design Pattern\nOne challenge in implementing a priority queue is that we must keep track of bothan element and its key, even as items are relocated within our data structure. Thisis reminiscent of a case study from Section 7.6 in which we maintain access counts\nwith each element. In that setting, we introduced the composition design pattern ,\nde\ufb01ning an\nItem class that assured that each element remained paired with its\nassociated count in our primary data structure.\nFor priority queues, we will use composition to store items internally as pairs\nconsisting of a key kand a value v. To implement this concept for all priority queue\nimplementations, we provide a PriorityQueueBase class (see Code Fragment 9.1)\nthat includes a de\ufb01nition for a nested class named\n Item . We de\ufb01ne the syntax\na<b, for item instances aandb, to be based upon the keys.\n1classPriorityQueueBase:\n2\u201d\u201d\u201dAbstract base class for a priority queue.\u201d\u201d\u201d\n34class\nItem:\n5 \u201d\u201d\u201dLightweight composite to store priority queue items.\u201d\u201d\u201d\n6\n slots\n =\n_key\n ,\n_value\n78 def\ninit\n(self,k ,v ) :\n9 self.\nkey = k\n10 self.\nvalue = v\n11\n12 def\n lt\n(self,o t h e r ) :\n13 return self .\nkey<other.\n key # compare items based on their keys\n1415defis\nempty( self): # concrete method assuming abstract len\n16 \u201d\u201d\u201dReturn True if the priority queue is empty.\u201d\u201d\u201d\n17 return len(self)= =0\nCode Fragment 9.1: APriorityQueueBase class with a nested\n Item class that com-\nposes a key and a value into a single object. For convenience, we provide a concreteimplementation of is\nempty that is based on a presumed\n len\n impelementation.", "366 Chapter 9. Priority Queues\n9.2.2 Implementation with an Unsorted List\nIn our \ufb01rst concrete implementation of a priority queue, we store entries within\nanunsorted list .O u rUnsortedPriorityQueue class is given in Code Fragment 9.2,\ninheriting from the PriorityQueueBase class introduced in Code Fragment 9.1. For\ninternal storage, key-value pairs are represented as composites, using instances of\nthe inherited\n Item class. These items are stored within a PositionalList , identi\ufb01ed\nas the\n data member of our class. We assume that the positional list is implemented\nwith a doubly-linked list, as in Section 7.4, so that all operations of that ADTexecute in O(1)time.\nWe begin with an empty list when a new priority queue is constructed. At all\ntimes, the size of the list equals the number of key-value pairs currently stored in thepriority queue. For this reason, our priority queue\nlen\n method simply returns\nthe length of the internal\n data list. By the design of our PriorityQueueBase class,\nwe inherit a concrete implementation of the is\nempty method that relies on a call to\nour\n len\n method.\nEach time a key-value pair is added to the priority queue, via the addmethod,\nwe create a new\n Item composite for the given key and value, and add that item to\nthe end of the list. Such an implementation takes O(1)time.\nThe remaining challenge is that when minorremove\n minis called, we must\nlocate the item with minimum key. Because the items are not sorted, we mustinspect all entries to \ufb01nd one with a minimum key. For convenience, we de\ufb01ne anonpublic\n\ufb01nd\nminutility that returns the position of an item with minimum key.\nKnowledge of the position allows the remove\n min method to invoke the delete\nmethod on the positional list. The minmethod simply uses the position to retrieve\nthe item when preparing a key-value tuple to return. Due to the loop for \ufb01nding theminimum key, both minandremove\nminmethods run in O(n)time, where nis the\nnumber of entries in the priority queue.\nA summary of the running times for the UnsortedPriorityQueue class is given\nin Table 9.1.\nOperation\n Running Time\nlen\n O(1)\nis\nempty\n O(1)\nadd\n O(1)\nmin\n O(n)\nremove\n min\n O(n)\nTable 9.1: Worst-case running times of the methods of a priority queue of size\nn, realized by means of an unsorted, doubly linked list. The space requirement\nisO(n).", "9.2. Implementing a Priority Queue 367\n1classUnsortedPriorityQueue(PriorityQueueBase): #b a s ec l a s sd e \ufb01 n e s\n Item\n2\u201d\u201d\u201dA min-oriented priority queue implemented with an unsorted list.\u201d\u201d\u201d\n3\n4def\n\ufb01nd\nmin(self): # nonpublic utility\n5 \u201d\u201d\u201dReturn Position of item with minimum key.\u201d\u201d\u201d\n6 if self.is\nempty(): #i s\nempty inherited from base class\n7 raiseEmpty(\n Priority queue is empty\n )\n8 small = self.\ndata.\ufb01rst()\n9 walk = self.\ndata.after(small)\n10 while walkis not None :\n11 ifwalk.element( ) <small.element():\n12 small = walk\n13 walk = self.\ndata.after(walk)\n14 return small\n1516def\ninit\n(self):\n17 \u201d\u201d\u201dCreate a new empty Priority Queue.\u201d\u201d\u201d\n18 self.\ndata = PositionalList()\n1920def\nlen\n(self):\n21 \u201d\u201d\u201dReturn the number of items in the priority queue.\u201d\u201d\u201d\n22 return len(self.\ndata)\n2324defadd(self,k e y ,v a l u e ) :\n25 \u201d\u201d\u201dAdd a key-value pair.\u201d\u201d\u201d\n26 self.\ndata.add\n last(self.\nItem(key, value))\n2728defmin(self):\n29 \u201d\u201d\u201dReturn but do not remove (k,v) tuple with minimum key.\u201d\u201d\u201d\n30 p=self.\n\ufb01nd\nmin()\n31 item = p.element()\n32 return (item.\n key, item.\n value)\n3334defremove\nmin(self):\n35 \u201d\u201d\u201dRemove and return (k,v) tuple with minimum key.\u201d\u201d\u201d\n36 p=self.\n\ufb01nd\nmin()\n37 item = self.\ndata.delete(p)\n38 return (item.\n key, item.\n value)\nCode Fragment 9.2: An implementation of a priority queue using an unsorted\nlist. The parent class PriorityQueueBase is given in Code Fragment 9.1, and the\nPositionalList class is from Section 7.4.", "368 Chapter 9. Priority Queues\n9.2.3 Implementation with a Sorted List\nAn alternative implementation of a priority queue uses a positional list, yet main-\ntaining entries sorted by nondecreasing keys. This ensures that the \ufb01rst element ofthe list is an entry with the smallest key.\nOurSortedPriorityQueue class is given in Code Fragment 9.3. The implemen-\ntation of minandremove\nminare rather straightforward given knowledge that the\n\ufb01rst element of a list has a minimum key. We rely on the \ufb01rst method of the posi-\ntional list to \ufb01nd the position of the \ufb01rst item, and the delete method to remove the\nentry from the list. Assuming that the list is implemented with a doubly linked list,operations minandremove\nmintake O(1)time.\nThis bene\ufb01t comes at a cost, however, for method addnow requires that we scan\nthe list to \ufb01nd the appropriate position to insert the new item. Our implementation\nstarts at the end of the list, walking backward until the new key is smaller than\nan existing item; in the worst case, it progresses until reaching the front of thelist. Therefore, the addmethod takes O(n)worst-case time, where nis the number\nof entries in the priority queue at the time the method is executed. In summary,\nwhen using a sorted list to implement a priority queue, insertion runs in linear time,\nwhereas \ufb01nding and removing the minimum can be done in constant time.\nComparing the Two List-Based Implementations\nTable 9.2 compares the running times of the methods of a priority queue realizedby means of a sorted and unsorted list, respectively. We see an interesting trade-\noff when we use a list to implement the priority queue ADT. An unsorted list\nsupports fast insertions but slow queries and deletions, whereas a sorted list allowsfast queries and deletions, but slow insertions.\nOperation\n Unsorted List\n Sorted List\nlen\n O(1)\n O(1)\nis\nempty\n O(1)\n O(1)\nadd\n O(1)\n O(n)\nmin\n O(n)\n O(1)\nremove\n min\n O(n)\n O(1)\nTable 9.2: Worst-case running times of the methods of a priority queue of size n,\nrealized by means of an unsorted or sorted list, respectively. We assume that the\nlist is implemented by a doubly linked list. The space requirement is O(n).", "9.2. Implementing a Priority Queue 369\n1classSortedPriorityQueue(PriorityQueueBase): #b a s ec l a s sd e \ufb01 n e s\n Item\n2\u201d\u201d\u201dA min-oriented priority queue implemented with a sorted list.\u201d\u201d\u201d\n3\n4def\n init\n(self):\n5 \u201d\u201d\u201dCreate a new empty Priority Queue.\u201d\u201d\u201d\n6 self.\ndata = PositionalList()\n78def\nlen\n(self):\n9 \u201d\u201d\u201dReturn the number of items in the priority queue.\u201d\u201d\u201d\n10 return len(self.\ndata)\n11\n12defadd(self,k e y ,v a l u e ) :\n13 \u201d\u201d\u201dAdd a key-value pair.\u201d\u201d\u201d\n14 newest = self.\nItem(key, value) # make new item instance\n15 walk = self.\ndata.last( ) # walk backward looking for smaller key\n16 while walkis not None and newest <walk.element():\n17 walk = self.\ndata.before(walk)\n18 ifwalkis None :\n19 self.\ndata.add\n \ufb01rst(newest) # new key is smallest\n20 else:\n21 self.\ndata.add\n after(walk, newest) # newest goes after walk\n22\n23defmin(self):\n24 \u201d\u201d\u201dReturn but do not remove (k,v) tuple with minimum key.\u201d\u201d\u201d\n25 if self.is\nempty():\n26 raiseEmpty(\n Priority queue is empty.\n )\n27 p=self.\ndata.\ufb01rst()\n28 item = p.element()\n29 return (item.\n key, item.\n value)\n3031defremove\nmin(self):\n32 \u201d\u201d\u201dRemove and return (k,v) tuple with minimum key.\u201d\u201d\u201d\n33 if self.is\nempty():\n34 raiseEmpty(\n Priority queue is empty.\n )\n35 item = self.\ndata.delete( self.\ndata.\ufb01rst())\n36 return (item.\n key, item.\n value)\nCode Fragment 9.3: An implementation of a priority queue using a sorted list.\nThe parent class PriorityQueueBase is given in Code Fragment 9.1, and the\nPositionalList class is from Section 7.4.", "370 Chapter 9. Priority Queues\n9.3 Heaps\nThe two strategies for implementing a priority queue ADT in the previous section\ndemonstrate an interesting trade-off. When using an unsorted list to store entries,\nwe can perform insertions in O(1)time, but \ufb01nding or removing an element with\nminimum key requires an O(n)-time loop through the entire collection. In contrast,\nif using a sorted list, we can trivially \ufb01nd or remove the minimum element in O(1)\ntime, but adding a new element to the queue may require O(n)time to restore the\nsorted order.\nIn this section, we provide a more ef\ufb01cient realization of a priority queue using\na data structure called a binary heap . This data structure allows us to perform both\ninsertions and removals in logarithmic time, which is a signi\ufb01cant improvementover the list-based implementations discussed in Section 9.2. The fundamental\nway the heap achieves this improvement is to use the structure of a binary tree to\ufb01nd a compromise between elements being entirely unsorted and perfectly sorted.\n9.3.1 The Heap Data Structure\nA heap (see Figure 9.1) is a binary tree Tthat stores a collection of items at its\npositions and that satis\ufb01es two additional properties: a relational property de\ufb01ned\nin terms of the way keys are stored in Tand a structural property de\ufb01ned in terms\nof the shape of Titself. The relational property is the following:\nHeap-Order Property :In a heap T, for every position pother than the root, the\nkey stored at pis greater than or equal to the key stored at p\u2019s parent.\nAs a consequence of the heap-order property, the keys encountered on a path from\nthe root to a leaf of Tare in nondecreasing order. Also, a minimum key is always\nstored at the root of T. This makes it easy to locate such an item when min or\nremove\n minis called, as it is informally said to be \u201cat the top of the heap\u201d (hence,\nthe name \u201cheap\u201d for the data structure). By the way, the heap data structure de\ufb01ned\nhere has nothing to do with the memory heap (Section 15.1.1) used in the run-time\nenvironment supporting a programming language like Python.\nFor the sake of ef\ufb01ciency, as will become clear later, we want the heap Tto have\nas small a height as possible. We enforce this requirement by insisting that the heap\nTsatisfy an additional structural property\u2014it must be what we term complete .\nComplete Binary Tree Property: A heap Twith height his acomplete binary tree\nif levels 0 ,1,2,..., h\u22121o f Thave the maximum number of nodes possible\n(namely, level ihas 2inodes, for 0 \u2264i\u2264h\u22121) and the remaining nodes at\nlevel hreside in the leftmost possible positions at that level.", "9.3. Heaps 371\n(14,E)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H)(4,C)\nFigure 9.1: Example of a heap storing 13 entries with integer keys. The last position\nis the one storing entry (13,W).\nThe tree in Figure 9.1 is complete because levels 0, 1, and 2 are full, and the six\nnodes in level 3 are in the six leftmost possible positions at that level. In formalizing\nwhat we mean by the leftmost possible positions, we refer to the discussion of level\nnumbering from Section 8.3.2, in the context of an array-based representation of a\nbinary tree. (In fact, in Section 9.3.3 we will discuss the use of an array to representa heap.) A complete binary tree with nelements is one that has positions with level\nnumbering 0 through n\u22121. For example, in an array-based representation of the\nabove tree, its 13 entries would be stored consecutively from A[0]toA[12].\nThe Height of a Heap\nLethdenote the height of T. Insisting that Tbe complete also has an important\nconsequence, as shown in Proposition 9.2.\nProposition 9.2: A heap Tstoring nentries has height h=\u230alogn\u230b.\nJusti\ufb01cation: From the fact that Tis complete, we know that the number of\nnodes in levels 0 through h\u22121o fTis precisely 1 +2+4+\u00b7\u00b7\u00b7+2h\u22121=2h\u22121, and\nthat the number of nodes in level his at least 1 and at most 2h. Therefore\nn\u22652h\u22121+1=2hand n\u22642h\u22121+2h=2h+1\u22121.\nBy taking the logarithm of both sides of inequality 2h\u2264n, we see that height\nh\u2264logn. By rearranging terms and taking the logarithm of both sides of inequality\nn\u22642h+1\u22121, we see that log (n+1)\u22121\u2264h.S i n c e his an integer, these two\ninequalities imply that h=\u230alogn\u230b.\n", "372 Chapter 9. Priority Queues\n9.3.2 Implementing a Priority Queue with a Heap\nProposition 9.2 has an important consequence, for it implies that if we can perform\nupdate operations on a heap in time proportional to its height, then those opera-tions will run in logarithmic time. Let us therefore turn to the problem of how to\nef\ufb01ciently perform various priority queue methods using a heap.\nWe will use the composition pattern from Section 9.2.1 to store key-value pairs\nas items in the heap. The lenandis\nempty methods can be implemented based\non examination of the tree, and the min operation is equally trivial because the\nheap property assures that the element at the root of the tree has a minimum key.\nThe interesting algorithms are those for implementing the add andremove\n min\nmethods.\nAdding an Item to the Heap\nLet us consider how to perform add(k,v) on a priority queue implemented with a\nheap T. We store the pair (k,v)as an item at a new node of the tree. To maintain\nthecomplete binary tree property , that new node should be placed at a position p\njust beyond the rightmost node at the bottom level of the tree, or as the leftmostposition of a new level, if the bottom level is already full (or if the heap is empty).\nUp-Heap Bubbling After an Insertion\nAfter this action, the tree Tis complete, but it may violate the heap-order property .\nHence, unless position pis the root of T(that is, the priority queue was empty\nbefore the insertion), we compare the key at position pto that of p\u2019s parent, which\nwe denote as q.I fk e y kp\u2265kq, the heap-order property is satis\ufb01ed and the algorithm\nterminates. If instead kp<kq, then we need to restore the heap-order property,\nwhich can be locally achieved by swapping the entries stored at positions pandq.\n(See Figure 9.2c and d.) This swap causes the new item to move up one level.Again, the heap-order property may be violated, so we repeat the process, going upinTuntil no violation of the heap-order property occurs. (See Figure 9.2e and h.)\nThe upward movement of the newly inserted entry by means of swaps is con-\nventionally called up-heap bubbling . A swap either resolves the violation of the\nheap-order property or propagates it one level up in the heap. In the worst case, up-heap bubbling causes the new entry to move all the way up to the root of heap T.\nThus, in the worst case, the number of swaps performed in the execution of method\naddis equal to the height of T. By Proposition 9.2, that bound is \u230alogn\u230b.", "9.3. Heaps 373\n(14,E)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H)(4,C)\n(2,T)(5,A) (6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E)(4,C)\n(a) (b)\n(20,B)(5,A) (6,Z)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E)(2,T)(4,C)\n(2,T)(5,A) (6,Z)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(4,C)\n(c) (d)\n(2,T)(5,A)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(6,Z)(4,C)\n(6,Z)(5,A)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(2,T)(4,C)\n(e) (f)\n(4,C)\n(7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(6,Z)(2,T)\n(5,A)\n(6,Z) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B)(2,T)\n(4,C) (5,A)\n(g) (h)\nFigure 9.2: Insertion of a new entry with key 2 into the heap of Figure 9.1: (a)\ninitial heap; (b) after performing operation add; (c and d) swap to locally restore\nthe partial order property; (e and f) another swap; (g and h) \ufb01nal swap.", "374 Chapter 9. Priority Queues\nRemoving the Item with Minimum Key\nLet us now turn to method remove\n minof the priority queue ADT. We know that\nan entry with the smallest key is stored at the root rofT(even if there is more than\none entry with smallest key). However, in general we cannot simply delete node r,\nbecause this would leave two disconnected subtrees.\nInstead, we ensure that the shape of the heap respects the complete binary tree\nproperty by deleting the leaf at the lastposition pofT, de\ufb01ned as the rightmost\nposition at the bottommost level of the tree. To preserve the item from the last\nposition p, we copy it to the root r(in place of the item with minimum key that is\nbeing removed by the operation). Figure 9.3a and b illustrates an example of thesesteps, with minimal item (4,C)being removed from the root and replaced by item\n(13,W)from the last position. The node at the last position is removed from the\ntree.\nDown-Heap Bubbling After a Removal\nWe are not yet done, however, for even though Tis now complete, it likely violates\nthe heap-order property. If Thas only one node (the root), then the heap-order\nproperty is trivially satis\ufb01ed and the algorithm terminates. Otherwise, we distin-guish two cases, where pinitially denotes the root of T:\n\u2022Ifphas no right child, let cbe the left child of p.\n\u2022Otherwise ( phas both children), let cbe a child of pwith minimal key.\nIf key k\np\u2264kc, the heap-order property is satis\ufb01ed and the algorithm terminates. If\ninstead kp>kc, then we need to restore the heap-order property. This can be locally\nachieved by swapping the entries stored at pandc. (See Figure 9.3c and d.) It is\nworth noting that when phas two children, we intentionally consider the smaller\nkey of the two children. Not only is the key of csmaller than that of p,i ti sa t\nleast as small as the key at c\u2019s sibling. This ensures that the heap-order property is\nlocally restored when that smaller key is promoted above the key that had been at\npand that at c\u2019s sibling.\nHaving restored the heap-order property for node prelative to its children, there\nmay be a violation of this property at c; hence, we may have to continue swapping\ndown Tuntil no violation of the heap-order property occurs. (See Figure 9.3e\u2013h.)\nThis downward swapping process is called down-heap bubbling . A swap either\nresolves the violation of the heap-order property or propagates it one level down inthe heap. In the worst case, an entry moves all the way down to the bottom level.(See Figure 9.3.) Thus, the number of swaps performed in the execution of methodremove\nminis, in the worst case, equal to the height of heap T, that is, it is \u230alogn\u230b\nby Proposition 9.2.", "9.3. Heaps 375\n(13,W)\n(6,Z)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(4,C)\n(5,A)(13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (9,F) (7,Q) (20,B)(6,Z) (5,A)\n(a) (b)\n(13,W)\n(20,B) (7,Q) (9,F) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(5,A) (6,Z) (13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (9,F) (7,Q) (20,B)(6,Z)(5,A)\n(c) (d)\n(9,F)\n(20,B) (7,Q) (15,K)\n(11,S) (16,X) (25,J) (12,H) (14,E)(5,A)\n(13,W)(6,Z)\n(13,W)\n(14,E) (12,H) (25,J) (16,X) (11,S)(15,K) (7,Q) (20,B)(6,Z)(5,A)\n(9,F)\n(e) (f)\n(13,W)(20,B) (7,Q) (15,K)(5,A)\n(9,F)\n(11,S) (14,E) (25,J) (16,X)(12,H)(6,Z)\n(13,W)(20,B) (7,Q) (15,K)(5,A)\n(9,F)\n(12,H)\n(11,S) (14,E) (25,J) (16,X)(6,Z)\n(g) (h)\nFigure 9.3: Removal of the entry with the smallest key from a heap: (a and b)\ndeletion of the last node, whose entry gets stored into the root; (c and d) swap to\nlocally restore the heap-order property; (e and f) another swap; (g and h) \ufb01nal swap.", "376 Chapter 9. Priority Queues\n9.3.3 Array-Based Representation of a Complete Binary Tree\nThe array-based representation of a binary tree (Section 8.3.2) is especially suitable\nfor a complete binary tree T. We recall that in this implementation, the elements\nofTare stored in an array-based list Asuch that the element at position pinTis\nstored in Awith index equal to the level number f(p)ofp, de\ufb01ned as follows:\n\u2022Ifpis the root of T,t h e n f(p)=0.\n\u2022Ifpis the left child of position q,t h e n f(p)=2f(q)+1.\n\u2022Ifpis the right child of position q,t h e n f(p)=2f(q)+2.\nWith this implementation, the elements of Thave contiguous indices in the range\n[0,n\u22121]and the last position of Tis always at index n\u22121, where nis the number\nof positions of T. For example, Figure 9.4 illustrates the array-based representation\nof the heap structure originally portrayed in Figure 9.1.\n0 123456789 1 0 1 1 1 2(4,C) (6,Z) (15,K) (9,F) (7,Q) (20,B) (16,X) (25,J) (14,E) (12,H) (11,S) (8,W) (5,A)\nFigure 9.4: An array-based representation of the heap from Figure 9.1.\nImplementing a priority queue using an array-based heap representation allows\nus to avoid some complexities of a node-based tree structure. In particular, the add\nandremove\n min operations of a priority queue both depend on locating the last\nindex of a heap of size n. With the array-based representation, the last position\nis at index n\u22121 of the array. Locating the last position of a complete binary tree\nimplemented with a linked structure requires more effort. (See Exercise C-9.34.)\nIf the size of a priority queue is not known in advance, use of an array-based\nrepresentation does introduce the need to dynamically resize the array on occasion,\nas is done with a Python list. The space usage of such an array-based representation\nof a complete binary tree with nnodes is O(n), and the time bounds of methods for\nadding or removing elements become amortized . (See Section 5.3.1.)\n9.3.4 Python Heap Implementation\nWe provide a Python implementation of a heap-based priority queue in Code Frag-ments 9.4 and 9.5. We use an array-based representation, maintaining a Python listof item composites. Although we do not formally use the binary tree ADT, CodeFragment 9.4 includes nonpublic utility functions that compute the level numberingof a parent or child of another. This allows us to describe the rest of our algorithms\nusing tree-like terminology of parent ,left,a n d right . However, the relevant vari-\nables are integer indexes (not \u201cposition\u201d objects). We use recursion to implement\nthe repetition in the\nupheap and\ndownheap utilities.", "9.3. Heaps 377\n1classHeapPriorityQueue(PriorityQueueBase): #b a s ec l a s sd e \ufb01 n e s\n Item\n2\u201d\u201d\u201dA min-oriented priority queue implemented with a binary heap.\u201d\u201d\u201d\n3#------------------------------ nonpublic behaviors ------------------------------\n4def\nparent( self,j ) :\n5 return (j\u22121) // 2\n6\n7def\nleft(self,j ) :\n8 return 2\nj+1\n9\n10def\nright(self,j ) :\n11 return 2\nj+2\n12\n13def\nhas\nleft(self,j ) :\n14 return self .\nleft(j) <len(self.\ndata) # index beyond end of list?\n15\n16def\nhas\nright(self,j ) :\n17 return self .\nright(j) <len(self.\ndata) # index beyond end of list?\n1819def\nswap(self,i ,j ) :\n20 \u201d\u201d\u201dSwap the elements at indices i and j of array.\u201d\u201d\u201d\n21 self.\ndata[i], self.\ndata[j] = self.\ndata[j], self.\ndata[i]\n2223def\nupheap( self,j ) :\n24 parent = self.\nparent(j)\n25 ifj>0and self .\ndata[j] <self.\ndata[parent]:\n26 self.\nswap(j, parent)\n27 self.\nupheap(parent) # recur at position of parent\n2829def\ndownheap( self,j ) :\n30 if self.\nhas\nleft(j):\n31 left = self.\nleft(j)\n32 small\n child = left # although right may be smaller\n33 if self.\nhas\nright(j):\n34 right = self.\nright(j)\n35 if self.\n data[right] <self.\ndata[left]:\n36 small\n child = right\n37 if self.\ndata[small\n child] <self.\ndata[j]:\n38 self.\nswap(j, small\n child)\n39 self.\ndownheap(small\n child) # recur at position of small child\nCode Fragment 9.4: An implementation of a priority queue using an array-based\nheap (continued in Code Fragment 9.5). The extends the PriorityQueueBase class\nfrom Code Fragment 9.1.", "378 Chapter 9. Priority Queues\n40 #------------------------------ public behaviors ------------------------------\n41def\n init\n(self):\n42 \u201d\u201d\u201dCreate a new empty Priority Queue.\u201d\u201d\u201d\n43 self.\ndata = [ ]\n44\n45def\n len\n(self):\n46 \u201d\u201d\u201dReturn the number of items in the priority queue.\u201d\u201d\u201d\n47 return len(self.\ndata)\n48\n49defadd(self,k e y ,v a l u e ) :\n50 \u201d\u201d\u201dAdd a key-value pair to the priority queue.\u201d\u201d\u201d\n51 self.\ndata.append( self.\nItem(key, value))\n52 self.\nupheap(len( self.\ndata) \u22121) # upheap newly added position\n5354defmin(self):\n55 \u201d\u201d\u201dReturn but do not remove (k,v) tuple with minimum key.\n56\n57 Raise Empty exception if empty.\n58 \u201d\u201d\u201d\n59 if self.is\nempty():\n60 raiseEmpty(\n Priority queue is empty.\n )\n61 item = self.\ndata[0]\n62 return (item.\n key, item.\n value)\n6364defremove\nmin(self):\n65 \u201d\u201d\u201dRemove and return (k,v) tuple with minimum key.\n66\n67 Raise Empty exception if empty.\n68 \u201d\u201d\u201d\n69 if self.is\nempty():\n70 raiseEmpty(\n Priority queue is empty.\n )\n71 self.\nswap(0, len( self.\ndata) \u22121) # put minimum item at the end\n72 item = self.\ndata.pop( ) # and remove it from the list;\n73 self.\ndownheap(0) # then \ufb01x new root\n74 return (item.\n key, item.\n value)\nCode Fragment 9.5: An implementation of a priority queue using an array-based\nheap (continued from Code Fragment 9.4).", "9.3. Heaps 379\n9.3.5 Analysis of a Heap-Based Priority Queue\nTable 9.3 shows the running time of the priority queue ADT methods for the heap\nimplementation of a priority queue, assuming that two keys can be compared inO(1)time and that the heap Tis implemented with an array-based or linked-based\ntree representation.\nIn short, each of the priority queue ADT methods can be performed in O(1)or\ninO(logn)time, where nis the number of entries at the time the method is exe-\ncuted. The analysis of the running time of the methods is based on the following:\n\u2022The heap Thasnnodes, each storing a reference to a key-value pair.\n\u2022The height of heap TisO(logn),s i n c eT is complete (Proposition 9.2).\n\u2022Theminoperation runs in O(1)because the root of the tree contains such an\nelement.\n\u2022Locating the last position of a heap, as required for addandremove\nmin,\ncan be performed in O(1)time for an array-based representation, or O(logn)\ntime for a linked-tree representation. (See Exercise C-9.34.)\n\u2022In the worst case, up-heap and down-heap bubbling perform a number ofswaps equal to the height of T.\nOperation\n Running Time\nlen(P) ,P.is\nempty()\n O(1)\nP.min()\n O(1)\nP.add()\n O(logn)\u2217\nP.remove\n min()\n O(logn)\u2217\n\u2217amortized, if array-based\nTable 9.3: Performance of a priority queue, P, realized by means of a heap. We\nletndenote the number of entries in the priority queue at the time an operation is\nexecuted. The space requirement is O(n). The running time of operations minand\nremove\n minare amortized for an array-based representation, due to occasional re-\nsizing of a dynamic array; those bounds are worst case with a linked tree structure.\nWe conclude that the heap data structure is a very ef\ufb01cient realization of the\npriority queue ADT, independent of whether the heap is implemented with a linkedstructure or an array. The heap-based implementation achieves fast running timesfor both insertion and removal, unlike the implementations that were based on usingan unsorted or sorted list.", "380 Chapter 9. Priority Queues\n9.3.6 Bottom-Up Heap Construction \u22c6\nIf we start with an initially empty heap, nsuccessive calls to the addoperation will\nrun in O(nlogn)time in the worst case. However, if all nkey-value pairs to be\nstored in the heap are given in advance, such as during the \ufb01rst phase of the heap-\nsort algorithm, there is an alternative bottom-up construction method that runs in\nO(n)time. (Heap-sort, however, still requires \u0398(nlogn)time because of the second\nphase in which we repeatedly remove the remaining element with smallest key.)\nIn this section, we describe the bottom-up heap construction, and provide an\nimplementation that can be used by the constructor of a heap-based priority queue.\nFor simplicity of exposition, we describe this bottom-up heap construction as-\nsuming the number of keys, n, is an integer such that n=2h+1\u22121. That is,\nthe heap is a complete binary tree with every level being full, so the heap has\nheight h=log (n+1)\u22121. Viewed nonrecursively, bottom-up heap construction\nconsists of the following h+1=log (n+1)steps:\n1. In the \ufb01rst step (see Figure 9.5b), we construct (n+1)/2 elementary heaps\nstoring one entry each.\n2. In the second step (see Figure 9.5c\u2013d), we form (n+1)/4 heaps, each storing\nthree entries, by joining pairs of elementary heaps and adding a new entry.\nThe new entry is placed at the root and may have to be swapped with theentry stored at a child to preserve the heap-order property.\n3. In the third step (see Figure 9.5e\u2013f), we form (n+1)/8 heaps, each storing\n7 entries, by joining pairs of 3-entry heaps (constructed in the previous step)\nand adding a new entry. The new entry is placed initially at the root, but may\nhave to move down with a down-heap bubbling to preserve the heap-orderproperty.\n...\ni. In the generic i\nthstep, 2 \u2264i\u2264h,w ef o r m (n+1)/2iheaps, each storing 2i\u22121\nentries, by joining pairs of heaps storing (2i\u22121\u22121)entries (constructed in the\nprevious step) and adding a new entry. The new entry is placed initially at\nthe root, but may have to move down with a down-heap bubbling to preserve\nthe heap-order property.\n...\nh+1. In the last step (see Figure 9.5g\u2013h), we form the \ufb01nal heap, storing all the\nnentries, by joining two heaps storing (n\u22121)/2 entries (constructed in the\nprevious step) and adding a new entry. The new entry is placed initially at\nthe root, but may have to move down with a down-heap bubbling to preserve\nthe heap-order property.\nWe illustrate bottom-up heap construction in Figure 9.5 for h=3.", "9.3. Heaps 381\n4 15 12 6 7 23 20 16\n(a) (b)\n4 16 159\n12 6 711\n2317\n2025\n20 16 25 94\n12 11 76\n2317 15\n(c) (d)\n25 12 11 23 2017 15\n168\n4\n95\n6\n7 25 12 11 23 2017 15\n16 85\n946\n7\n(e) (f)\n25 12 11 8 23 2017 7 156\n16514\n4\n9 25 12 11 8 23 2017 7 156\n16 144\n5\n9\n(g) (h)\nFigure 9.5: Bottom-up construction of a heap with 15 entries: (a and b) we begin by\nconstructing 1-entry heaps on the bottom level; (c and d) we combine these heaps\ninto 3-entry heaps, and then (e and f) 7-entry heaps, until (g and h) we create the\ufb01nal heap. The paths of the down-heap bubblings are highlighted in (d, f, and h).\nFor simplicity, we only show the key within each node instead of the entire entry.", "382 Chapter 9. Priority Queues\nPython Implementation of a Bottom-Up Heap Construction\nImplementing a bottom-up heap construction is quite easy, given the existence of\na \u201cdown-heap\u201d utility function. The \u201cmerging\u201d of two equally sized heaps that\nare subtrees of a common position p, as described in the opening of this section,\ncan be accomplished simply by down-heaping p\u2019s entry. For example, that is what\nhappened to the key 14 in going from Figure 9.5(f) to (g).\nWith our array-based representation of a heap, if we initially store all nitems in\narbitrary order within the array, we can implement the bottom-up heap construction\nprocess with a single loop that makes a call to\n downheap from each position of\nthe tree, as long as those calls are ordered starting with the deepest level and ending\nwith the root of the tree. In fact, that loop can start with the deepest nonleaf, sincethere is no effect when down-heap is called at a leaf position.\nIn Code Fragment 9.6, we augment the original HeapPriorityQueue class from\nSection 9.3.4 to provide support for the bottom-up construction of an initial col-lection. We introduce a nonpublic utility method,\nheapify , that calls\n downheap\non each nonleaf position, beginning with the deepest and concluding with a call atthe root of the tree. We have redesigned the constructor of the class to accept an\noptional parameter that can be any sequence of (k,v) tuples. Rather than initializing\nself.\ndata to an empty list, we use a list comprehension syntax (see Section 1.9.2)\nto create an initial list of item composites based on the given contents. We de-\nclare an empty sequence as the default parameter value so that the default syntaxHeapPriorityQueue() continues to result in an empty priority queue.\ndef\ninit\n(self,c o n t e n t s = ( ) ) :\n\u201d\u201d\u201dCreate a new priority queue.\nBy default, queue will be empty. If contents is given, it should be as an\niterable sequence of (k,v) tuples specifying the initial contents.\n\u201d\u201d\u201d\nself.\ndata = [ self.\nItem(k,v) fork,vincontents ] # empty by default\niflen(self.\ndata) >1:\nself.\nheapify()\ndef\nheapify( self):\nstart = self.\nparent(len( self)\u22121) # start at PARENT of last leaf\nforjinrange(start, \u22121,\u22121): # going to and including the root\nself.\ndownheap(j)\nCode Fragment 9.6: Revision to the HeapPriorityQueue class of Code Frag-\nments 9.4 and 9.5 to support a linear-time construction given an initial sequence\nof entries.", "9.3. Heaps 383\nAsymptotic Analysis of Bottom-Up Heap Construction\nBottom-up heap construction is asymptotically faster than incrementally inserting\nnkeys into an initially empty heap. Intuitively, we are performing a single down-\nheap operation at each position in the tree, rather than a single up-heap operationfrom each. Since more nodes are closer to the bottom of a tree than the top, thesum of the downward paths is linear, as shown in the following proposition.\nProposition 9.3:\nBottom-up construction of a heap with nentries takes O(n)\ntime, assuming two keys can be compared in O(1)time.\nJusti\ufb01cation: The primary cost of the construction is due to the down-heap\nsteps performed at each nonleaf position. Let \u03c0vdenote the path of Tfrom nonleaf\nnode vto its \u201cinorder successor\u201d leaf, that is, the path that starts at v, goes to the\nright child of v, and then goes down leftward until it reaches a leaf. Although,\n\u03c0vis not necessarily the path followed by the down-heap bubbling step from v,\nthe length /bardbl\u03c0v/bardbl(its number of edges) is proportional to the height of the subtree\nrooted at v, and thus a bound on the complexity of the down-heap operation at v.\nWe can bound the total running time of the bottom-up heap construction algorithm\nbased on the sum of the sizes of paths, \u2211v/bardbl\u03c0v/bardbl. For intuition, Figure 9.6 illustrates\nthe justi\ufb01cation \u201cvisually,\u201d marking each edge with the label of the nonleaf node v\nwhose path \u03c0vcontains that edge.\nWe claim that the paths \u03c0vfor all nonleaf vare edge-disjoint, and thus the sum\nof the path lengths is bounded by the number of total edges in the tree, hence O(n).\nTo show this, we consider what we term \u201cright-leaning\u201d and \u201cleft-leaning\u201d edges\n(i.e., those going from a parent to a right, respectively left, child). A particular right-\nleaning edge ecan only be part of the path \u03c0vfor node vthat is the parent in the\nrelationship represented by e. Left-leaning edges can be partitioned by considering\nthe leaf that is reached if continuing down leftward until reaching a leaf. Eachnonleaf node only uses left-leaning edges in the group leading to that nonleaf node\u2019s\ninorder successor. Since each nonleaf node must have a different inorder successor,\nno two such paths can contain the same left-leaning edge. We conclude that thebottom-up construction of heap Ttakes O(n)time.\n1564\n165\n25 14 12 11 8 23 2017 7 9\n15 7 1754 6\n5464\n9\nFigure 9.6: Visual justi\ufb01cation of the linear running time of bottom-up heap con-\nstruction. Each edge eis labeled with a node vfor which \u03c0vcontains e(if any).", "384 Chapter 9. Priority Queues\n9.3.7 Python\u2019s heapq Module\nPython\u2019s standard distribution includes a heapq module that provides support for\nheap-based priority queues. That module does not provide any priority queue class;\ninstead it provides functions that allow a standard Python list to be managed as a\nheap. Its model is essentially the same as our own, with nelements stored in list\ncells L[0]through L[n\u22121], based on the level-numbering indices with the small-\nest element at the root in L[0]. We note that heapq does not separately manage\nassociated values; elements serve as their own key.\nTheheapq module supports the following functions, all of which presume that\nexisting list Lsatis\ufb01es the heap-order property prior to the call:\nheappush(L, e) :Push element eonto list Land restore the heap-order\nproperty. The function executes in O(logn)time.\nheappop(L) :Pop and return the element with smallest value from list L,\nand reestablish the heap-order property. The operation\nexecutes in O(logn)time.\nheappushpop(L, e) :Push element eon list Land then pop and return the\nsmallest item. The time is O(logn), but it is slightly more\nef\ufb01cient than separate calls to push andpopbecause the\nsize of the list never changes. If the newly pushed el-ement becomes the smallest, it is immediately returned.\nOtherwise, the new element takes the place of the popped\nelement at the root and a down-heap is performed.\nheapreplace(L, e) :Similar to heappushpop , but equivalent to the pop be-\ning performed before the push (in other words, the newelement cannot be returned as the smallest). Again, thetime is O(logn), but it is more ef\ufb01cient that two separate\noperations.\nThe module supports additional functions that operate on sequences that do not\npreviously satisfy the heap-order property.\nheapify(L) :T\nransform unordered list to satisfy the heap-order prop-\nerty. This executes in O(n)time by using the bottom-up\nconstruction algorithm.\nnlargest(k, iterable) :Produce a list of the klargest values from a given iterable.\nThis can be implemented to run in O(n+klogn)time,\nwhere we use nto denote the length of the iterable (see\nExercise C-9.42).\nnsmallest(k, iterable) :Produce a list of the ksmallest values from a given it-\nerable. This can be implemented to run in O(n+klogn)\ntime, using similar technique as with nlargest .", "9.4. Sorting with a Priority Queue 385\n9.4 Sorting with a Priority Queue\nIn de\ufb01ning the priority queue ADT, we noted that any type of object can be used as\na key, but that any pair of keys must be comparable to each other, and that the set\nof keys be naturally ordered. In Python, it is common to rely on the <operator to\nde\ufb01ne such an order, in which case the following properties must be satis\ufb01ed:\n\u2022Irre\ufb02exive property :k/negationslash<k.\n\u2022Transitive property :i fk1<k2andk2<k3,t h e n k1<k3.\nFormally, such a relationship de\ufb01nes what is known as a strict weak order ,a si t\nallows for keys to be considered equal to each other, but the broader equivalence\nclasses are totally ordered , as they can be uniquely arranged from smallest to largest\ndue to the transitive property.\nAs our \ufb01rst application of priority queues, we demonstrate how they can be\nused to sort a collection Cof comparable elements. That is, we can produce a\nsequence of elements of Cin increasing order (or at least in nondecreasing order if\nthere are duplicates). The algorithm is quite simple\u2014we insert all elements into an\ninitially empty priority queue, and then we repeatedly call remove\n minto retrieve\nthe elements in nondecreasing order.\nAn implementation of this algorithm is given in Code Fragment 9.7, assuming\nthatCis a positional list. (See Chapter 7.4.) We use an original element of the\ncollection as both a key and value when calling P.add(element, element) .\n1defpq\nsort(C):\n2\u201d\u201d\u201dSort a collection of elements stored in a positional list.\u201d\u201d\u201d\n3n=l e n ( C )\n4P = PriorityQueue()\n5forjinrange(n):\n6 element = C.delete(C.\ufb01rst())\n7 P.add(element, element) # use element as key and value\n8forjinrange(n):\n9 (k,v) = P.remove\n min()\n10 C.add\n last(v) # store smallest remaining element in C\nCode Fragment 9.7: An implementation of the pq\nsortfunction, assuming an ap-\npropriate implementation of a PriorityQueue class. Note that each element of the\ninput list Cserves as its own key in the priority queue P.\nWith a minor modi\ufb01cation to this code, we can provide more general sup-\nport, sorting elements according to an ordering other than the default. For exam-\nple, when working with strings, the <operator de\ufb01nes a lexicographic ordering ,\nwhich is an extension of the alphabetic ordering to Unicode. For example, we have\nthat\n 12\n<\n4\nbecause of the order of the \ufb01rst character of each string, just as", "386 Chapter 9. Priority Queues\napple\n <\nbanana\n . Suppose that we have an application in which we have a\nlist of strings that are all known to represent integral values (e.g.,\n 12\n), and our\ngoal is to sort the strings according to those integral values.\nIn Python, the standard approach for customizing the order for a sorting algo-\nrithm is to provide, as an optional parameter to the sorting function, an object that\nis itself a one-parameter function that computes a key for a given element. (See\nSections 1.5 and 1.10 for a discussion of this approach in the context of the built-\ninmax function.) For example, with a list of (numeric) strings, we might wish\nto use the value of int(s) as a key for a string sof the list. In this case, the con-\nstructor for the intclass can serve as the one-parameter function for computing a\nkey. In that way, the string\n 4\nwill be ordered before string\n 12\nbecause its key\nint(\n 4\n)<int(\n 12\n). We leave it as an exercise to support such an optional key\nparameter for the pq\nsortfunction. (See Exercise C-9.46.)\n9.4.1 Selection-Sort and Insertion-Sort\nOurpq\nsort function works correctly given any valid implementation of the pri-\nority queue class. However, the running time of the sorting algorithm depends\non the running times of the operations addandremove\n minfor the given priority\nqueue class. We next discuss a choice of priority queue implementations that in\neffect cause the pq\nsort computation to behave as one of several classic sorting\nalgorithms.\nSelection-Sort\nIf we implement Pwith an unsorted list, then Phase 1 of pq\nsorttakes O(n)time,\nfor we can add each element in O(1)time. In Phase 2, the running time of each\nremove\n minoperation is proportional to the size of P. Thus, the bottleneck com-\nputation is the repeated \u201cselection\u201d of the minimum element in Phase 2. For thisreason, this algorithm is better known as selection-sort .( S e eF i g u r e9 . 7 . )\nAs noted above, the bottleneck is in Phase 2 where we repeatedly remove an\nentry with smallest key from the priority queue P. The size of Pstarts at nand\nincrementally decreases with each remove\nminuntil it becomes 0. Thus, the \ufb01rst\noperation takes time O(n), the second one takes time O(n\u22121), and so on. There-\nfore, the total time needed for the second phase is\nO(n+(n\u22121)+\u00b7\u00b7\u00b7+2+1)=O(\u2211n\ni=1i).\nBy Proposition 3.3, we have \u2211ni=1i=n(n+1)/2. Thus, Phase 2 takes time O(n2),\nas does the entire selection-sort algorithm.", "9.4. Sorting with a Priority Queue 387\nCollection C\n Priority Queue P\nInput\n (7,4,8,2,5,3)\n ()\nPhase 1 (a)\n (4,8,2,5,3)\n (7)\n(b)\n (8,2,5,3)\n (7,4)\n...\n...\n ...\n(f)\n ()\n (7,4,8,2,5,3)\nPhase 2 (a)\n (2)\n (7,4,8,5,3)\n(b)\n (2,3)\n (7,4,8,5)\n(c)\n (2,3,4)\n (7,8,5)\n(d)\n (2,3,4,5)\n (7,8)\n(e)\n (2,3,4,5,7)\n (8)\n(f)\n (2,3,4,5,7,8)\n ()\nFigure 9.7: Execution of selection-sort on collection C=(7,4,8,2,5,3).\nInsertion-Sort\nIf we implement the priority queue Pusing a sorted list, then we improve the run-\nning time of Phase 2 to O(n), for each remove\n minoperation on Pnow takes O(1)\ntime. Unfortunately, Phase 1 becomes the bottleneck for the running time, since,\nin the worst case, each addoperation takes time proportional to the current size of\nP. This sorting algorithm is better known as insertion-sort (see Figure 9.8); in fact,\nour implementation for adding an element to a priority queue is almost identical to\na step of insertion-sort as presented in Section 7.5.\nThe worst-case running time of Phase 1 of insertion-sort is\nO(1+2+...+(n\u22121)+n)=O(\u2211n\ni=1i).\nAgain, by Proposition 3.3, this implies a worst-case O(n2)time for Phase 1, and\nthus, the entire insertion-sort algorithm. However, unlike selection-sort, insertion-\nsort has a best-case running time of O(n).\nCollection C\n Priority Queue P\nInput\n (7,4,8,2,5,3)\n ()\nPhase 1 (a)\n (4,8,2,5,3)\n (7)\n(b)\n (8,2,5,3)\n (4,7)\n(c)\n (2,5,3)\n (4,7,8)\n(d)\n (5,3)\n (2,4,7,8)\n(e)\n (3)\n (2,4,5,7,8)\n(f)\n ()\n (2,3,4,5,7,8)\nPhase 2 (a)\n (2)\n (3,4,5,7,8)\n(b)\n (2,3)\n (4,5,7,8)\n...\n...\n...\n(f)\n (2,3,4,5,7,8)\n ()\nFigure 9.8: Execution of insertion-sort on collection C=(7,4,8,2,5,3).", "388 Chapter 9. Priority Queues\n9.4.2 Heap-Sort\nAs we have previously observed, realizing a priority queue with a heap has the\nadvantage that all the methods in the priority queue ADT run in logarithmic time orbetter. Hence, this realization is suitable for applications where fast running timesare sought for all the priority queue methods. Therefore, let us again consider thepq\nsortscheme, this time using a heap-based implementation of the priority queue.\nDuring Phase 1, the ithaddoperation takes O(logi)time, since the heap has i\nentries after the operation is performed. Therefore this phase takes O(nlogn)time.\n(It could be improved to O(n)with the bottom-up heap construction described in\nSection 9.3.6.)\nDuring the second phase of pq\nsort,t h e jthremove\n min operation runs in\nO(log (n\u2212j+1)), since the heap has n\u2212j+1 entries at the time the operation\nis performed. Summing over all j, this phase takes O(nlogn)time, so the entire\npriority-queue sorting algorithm runs in O(nlogn)time when we use a heap to im-\nplement the priority queue. This sorting algorithm is better known as heap-sort ,\nand its performance is summarized in the following proposition.\nProposition 9.4: The heap-sort algorithm sorts a collection Cofnelements in\nO(nlogn)time, assuming two elements of Ccan be compared in O(1)time.\nLet us stress that the O(nlogn)running time of heap-sort is considerably better\nthan the O(n2)running time of selection-sort and insertion-sort (Section 9.4.1).\nImplementing Heap-Sort In-Place\nIf the collection Cto be sorted is implemented by means of an array-based se-\nquence, most notably as a Python list, we can speed up heap-sort and reduce its\nspace requirement by a constant factor using a portion of the list itself to store the\nheap, thus avoiding the use of an auxiliary heap data structure. This is accomplished\nby modifying the algorithm as follows:\n1. We rede\ufb01ne the heap operations to be a maximum-oriented heap, with each\nposition\u2019s key being at least as large as its children. This can be done by\nrecoding the algorithm, or by adjusting the notion of keys to be negativelyoriented. At any time during the execution of the algorithm, we use the left\nportion of C, up to a certain index i\u22121, to store the entries of the heap, and\nthe right portion of C, from index iton\u22121, to store the elements of the\nsequence. Thus, the \ufb01rst ielements of C(at indices 0 ,..., i\u22121) provide the\narray-list representation of the heap.\n2. In the \ufb01rst phase of the algorithm, we start with an empty heap and move the\nboundary between the heap and the sequence from left to right, one step at a\ntime. In step i,f o r i=1,..., n, we expand the heap by adding the element at\nindex i\u22121.", "9.4. Sorting with a Priority Queue 389\n3. In the second phase of the algorithm, we start with an empty sequence and\nmove the boundary between the heap and the sequence from right to left, one\nstep at a time. At step i,f o r i=1,..., n, we remove a maximum element\nfrom the heap and store it at index n\u2212i.\nIn general, we say that a sorting algorithm is in-place if it uses only a small\namount of memory in addition to the sequence storing the objects to be sorted. The\nvariation of heap-sort above quali\ufb01es as in-place; instead of transferring elements\nout of the sequence and then back in, we simply rearrange them. We illustrate thesecond phase of in-place heap-sort in Figure 9.9.\n(e) 45 79\n6 24\n257 6 2 4( c ) 645 79\n6795 4\n456796\n2 (f) 25( b ) 765249\n2\n2\n45 67\n4 2\n429( a ) 975264\n25 (d)\nFigure 9.9: Phase 2 of an in-place heap-sort. The heap portion of each sequence\nrepresentation is highlighted. The binary tree that each sequence (implicitly) repre-\nsents is diagrammed with the most recent path of down-heap bubbling highlighted.", "390 Chapter 9. Priority Queues\n9.5 Adaptable Priority Queues\nThe methods of the priority queue ADT given in Section 9.1.2 are suf\ufb01cient for\nmost basic applications of priority queues, such as sorting. However, there aresituations in which additional methods would be useful, as shown by the scenariosbelow involving the standby airline passenger application.\n\u2022A standby passenger with a pessimistic attitude may become tired of waitingand decide to leave ahead of the boarding time, requesting to be removedfrom the waiting list. Thus, we would like to remove from the priority queuethe entry associated with this passenger. Operation remove\nmin does not\nsuf\ufb01ce since the passenger leaving does not necessarily have \ufb01rst priority.\nInstead, we want a new operation, remove , that removes an arbitrary entry.\n\u2022Another standby passenger \ufb01nds her gold frequent-\ufb02yer card and shows it tothe agent. Thus, her priority has to be modi\ufb01ed accordingly. To achieve this\nchange of priority, we would like to have a new operation update allowing\nus to replace the key of an existing entry with a new key.\nWe will see another application of adaptable priority queues when implementing\ncertain graph algorithms in Sections 14.6.2 and 14.7.1.\nIn this section, we develop an adaptable priority queue ADT and demonstrate\nhow to implement this abstraction as an extension to our heap-based priority queue.\n9.5.1 Locators\nIn order to implement methods update andremove ef\ufb01ciently, we need a mecha-\nnism for \ufb01nding a user\u2019s element within a priority queue that avoids performing a\nlinear search through the entire collection. To support our goal, when a new ele-\nment is added to the priority queue, we return a special object known as a locator to\nthe caller. We then require the user to provide an appropriate locator as a parameter\nwhen invoking the update orremove method, as follows, for a priority queue P:\nP.update(loc, k, v) :Replace key and value for the item identi\ufb01ed by locator loc.\nP.remove(loc) :Remove the item identi\ufb01ed by locator locfrom the priority\nqueue and return its (key,value) pair.\nThe locator abstraction is somewhat akin to the Position abstraction used in our\npositional list ADT from Section 7.4, and our tree ADT from Chapter 8. However,we differentiate between a locator and a position because a locator for a priorityqueue does not represent a tangible placement of an element within the structure.In our priority queue, an element may be relocated within our data structure duringan operation that does not seem directly relevant to that element. A locator for an\nitem will remain valid, as long as that item remains somewhere in the queue.", "9.5. Adaptable Priority Queues 391\n9.5.2 Implementing an Adaptable Priority Queue\nIn this section, we provide a Python implementation of an adaptable priority queue\nas an extension of our HeapPriorityQueue class from Section 9.3.4. To implement a\nLocator class, we will extend the existing\n Item composite to add an additional \ufb01eld\ndesignating the current index of the element within the array-based representation\nof our heap, as shown in Figure 9.10.\n01234567token\n(15,K,3) (16,X,7) (9,F,4) (5,A,1) (20,B,6) (6,Z,2) (7,Q,5) (4,C,0)\nFigure 9.10: Representing a heap using a sequence of locators. The third element\nof each locator instance corresponds to the index of the item within the array. Iden-ti\ufb01ertoken is presumed to be a locator reference in the user\u2019s scope.\nThe list is a sequence of references to locator instances, each of which stores a\nkey, value, and the current index of the item within the list. The user will be givena reference to the Locator instance for each inserted element, as portrayed by the\ntoken identi\ufb01er in Figure 9.10.\nWhen we perform priority queue operations on our heap, and items are relo-\ncated within our structure, we reposition the locator instances within the list and we\nupdate the third \ufb01eld of each locator to re\ufb02ect its new index within the list. As an ex-\nample, Figure 9.11 shows the state of the above heap after a call to remove\nmin() .\nThe heap operation caused the minimum entry, (4,C) , to be removed, and the en-\ntry,(16,X) , to be temporarily moved from the last position to the root, followed by\na down-heap bubble phase. During the down-heap, element (16,X) was swapped\n1234567 0token\n(9,F,1) (16,X,4) (7,Q,5) (15,K,3) (20,B,6) (6,Z,2) (5,A,0)\nFigure 9.11: The result of a call to remove\n min() on the heap originally portrayed\nin Figure 9.10. Identi\ufb01er token continues to reference the same locator instance\nas in the original con\ufb01guration, but the placement of that locator in the list haschanged, as has the third \ufb01eld of the locator.", "392 Chapter 9. Priority Queues\nwith its left child, (5,A) , at index 1 of the list, then swapped with its right child,\n(9,F) , at index 4 of the list. In the \ufb01nal con\ufb01guration, the locator instances for all\naffected elements have been modi\ufb01ed to re\ufb02ect their new location.\nIt is important to emphasize that the locator instances have not changed iden-\ntity. The user\u2019s token reference, portrayed in Figures 9.10 and 9.11, continues to\nreference the same instance; we have simply changed the third \ufb01eld of that instance,\nand we have changed where that instance is referenced within the list sequence.\nWith this new representation, providing the additional support for the adaptable\npriority queue ADT is rather straightforward. When a locator instance is sent as a\nparameter to update orremove , we may rely on the third \ufb01eld of that structure to\ndesignate where the element resides in the heap. With that knowledge, the updateof a key may simply require an up-heap or down-heap bubbling step to reestablish\nthe heap-order property. (The complete binary tree property remains intact.) To\nimplement the removal of an arbitrary element, we move the element at the lastposition to the vacated location, and again perform an appropriate bubbling step tosatisfy the heap-order property.\nPython Implementation\nCode Fragments 9.8 and 9.9 present a Python implementation of an adaptable pri-ority queue, as a subclass of the HeapPriorityQueue class from Section 9.3.4. Our\nmodi\ufb01cations to the original class are relatively minor. We de\ufb01ne a public Locator\nclass that inherits from the nonpublic\nItem class and augments it with an addi-\ntional\n index \ufb01eld. We make it a public class because we will be using locators\nas return values and parameters; however, the public interface for the locator class\ndoes not include any other functionality for the user.\nTo update locators during the \ufb02ow of our heap operations, we rely on an inten-\ntional design decision that our original class uses a nonpublic\n swap method for all\ndata movement. We override that utility to execute the additional step of updating\nthe stored indices within the two swapped locator instances.\nWe provide a new\n bubble utility that manages the reinstatement of the heap-\norder property when a key has changed at an arbitrary position within the heap,either due to a key update, or the blind replacement of a removed element with theitem from the last position of the tree. The\nbubble utility determines whether to\napply up-heap or down-heap bubbling, depending on whether the given locationhas a parent with a smaller key. (If an updated key coincidentally remains valid for\nits current location, we technically call\ndownheap but no swaps result.)\nThe public methods are provided in Code Fragment 9.9. The existing add\nmethod is overridden, both to make use of a Locator instance rather than an\n Item\ninstance for storage of the new element, and to return the locator to the caller. The\nremainder of that method is similar to the original, with the management of locator\nindices enacted by the use of the new version of\n swap . There is no reason to over-", "9.5. Adaptable Priority Queues 393\nride the remove\n minmethod because the only change in behavior for the adaptable\npriority queue is again provided by the overridden\n swap method.\nTheupdate andremove methods provide the core new functionality for the\nadaptable priority queue. We perform robust checking of the validity of a locator\nthat is sent by a caller (although in the interest of space, our displayed code doesnot do preliminary type-checking to ensure that the parameter is indeed a Locator\ninstance). To ensure that a locator is associated with a current element of the givenpriority queue, we examine the index that is encapsulated within the locator object,and then verify that the entry of the list at that index is the very same locator.\nIn conclusion, the adaptable priority queue provides the same asymptotic ef\ufb01-\nciency and space usage as the nonadaptive version, and provides logarithmic per-formance for the new locator-based update andremove methods. A summary of\nthe performance is given in Table 9.4.\n1classAdaptableHeapPriorityQueue(HeapPriorityQueue):\n2\u201d\u201d\u201dA locator-based priority queue implemented with a binary heap.\u201d\u201d\u201d\n34#------------------------------ nested Locator class ------------------------------\n5classLocator(HeapPriorityQueue.\nItem):\n6 \u201d\u201d\u201dToken for locating an entry of the priority queue.\u201d\u201d\u201d\n7\n slots\n =\n_index\n # add index as additional \ufb01eld\n8\n9 def\n init\n(self,k ,v ,j ) :\n10 super().\ninit\n(k,v)\n11 self.\nindex = j\n12\n13 #------------------------------ nonpublic behaviors ------------------------------\n14 # override swap to record new indices\n15def\nswap(self,i ,j ) :\n16 super().\nswap(i,j) #p e r f o r mt h es w a p\n17 self.\ndata[i].\n index = i # reset locator index (post-swap)\n18 self.\ndata[j].\n index = j # reset locator index (post-swap)\n1920def\nbubble( self,j ) :\n21 ifj>0and self .\ndata[j] <self.\ndata[self.\nparent(j)]:\n22 self.\nupheap(j)\n23 else:\n24 self.\ndownheap(j)\nCode Fragment 9.8: An implementation of an adaptable priority queue (continued\nin Code Fragment 9.9). This extends the HeapPriorityQueue class of Code Frag-\nments 9.4 and 9.5", "394 Chapter 9. Priority Queues\n25defadd(self,k e y ,v a l u e ) :\n26 \u201d\u201d\u201dAdd a key-value pair.\u201d\u201d\u201d\n27 token = self.Locator(key, value, len( self.\ndata)) # initiaize locator index\n28 self.\ndata.append(token)\n29 self.\nupheap(len( self.\ndata) \u22121)\n30 return token\n31\n32defupdate( self,l o c ,n e w k e y ,n e w v a l ) :\n33 \u201d\u201d\u201dUpdate the key and value for the entry identi\ufb01ed by Locator loc.\u201d\u201d\u201d\n34 j=l o c .\n index\n35 if not (0<=j<len(self)and self .\ndata[j] isloc):\n36 raiseValueError(\n Invalid locator\n )\n37 loc.\nkey = newkey\n38 loc.\nvalue = newval\n39 self.\nbubble(j)\n4041defremove( self,l o c ) :\n42 \u201d\u201d\u201dRemove and return the (k,v) pair identi\ufb01ed by Locator loc.\u201d\u201d\u201d\n43 j=l o c .\nindex\n44 if not (0<=j<len(self)and self .\ndata[j] isloc):\n45 raiseValueError(\n Invalid locator\n )\n46 ifj= =l e n ( self)\u22121: # item at last position\n47 self.\ndata.pop( ) #j u s tr e m o v ei t\n48 else:\n49 self.\nswap(j, len( self)\u22121) # swap item to the last position\n50 self.\ndata.pop( ) # remove it from the list\n51 self.\nbubble(j) # \ufb01x item displaced by the swap\n52 return (loc.\nkey, loc.\n value)\nCode Fragment 9.9: An implementation of an adaptable priority queue (continued\nfrom Code Fragment 9.8).\nOperation\n Running Time\nlen(P) ,P.is\nempty() ,P.min()\n O(1)\nP.add(k,v)\n O(logn)\u2217\nP.update(loc, k, v)\n O(logn)\nP.remove(loc)\n O(logn)\u2217\nP.remove\n min()\n O(logn)\u2217\n\u2217amortized with dynamic array\nTable 9.4: Running times of the methods of an adaptable priority queue, P,o fs i z e n,\nrealized by means of our array-based heap representation. The space requirementisO(n).", "9.6. Exercises 395\n9.6 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-9.1 How long would it take to remove the \u2308logn\u2309smallest elements from a\nheap that contains nentries, using the remove\n minoperation?\nR-9.2 Suppose you label each position pof a binary tree Twith a key equal to\nits preorder rank. Under what circumstances is Ta heap?\nR-9.3 What does each remove\n mincall return within the following sequence of\npriority queue ADT methods: add(5,A) ,add(4,B) ,add(7,F) ,add(1,D) ,\nremove\n min() ,add(3,J), add(6,L) ,remove\n min() ,remove\n min() ,\nadd(8,G) ,remove\n min() ,add(2,H) ,remove\n min() ,remove\n min() ?\nR-9.4 An airport is developing a computer simulation of air-traf\ufb01c control that\nhandles events such as landings and takeoffs. Each event has a time stamp\nthat denotes the time when the event will occur. The simulation programneeds to ef\ufb01ciently perform the following two fundamental operations:\n\u2022Insert an event with a given time stamp (that is, add a future event).\n\u2022Extract the event with smallest time stamp (that is, determine thenext event to process).\nWhich data structure should be used for the above operations? Why?\nR-9.5 Themin method for the UnsortedPriorityQueue class executes in O(n)\ntime, as analyzed in Table 9.2. Give a simple modi\ufb01cation to the class so\nthatminruns in O(1)time. Explain any necessary modi\ufb01cations to other\nmethods of the class.\nR-9.6 Can you adapt your solution to the previous problem to make remove\nmin\nrun in O(1)time for the UnsortedPriorityQueue class? Explain your an-\nswer.\nR-9.7 Illustrate the execution of the selection-sort algorithm on the following\ninput sequence: (22,15,36,44,10,3,9,13,29,25).\nR-9.8 Illustrate the execution of the insertion-sort algorithm on the input se-\nquence of the previous problem.\nR-9.9 Give an example of a worst-case sequence with nelements for insertion-\nsort, and show that insertion-sort runs in \u03a9(n2)time on such a sequence.\nR-9.10 At which positions of a heap might the third smallest key be stored?\nR-9.11 At which positions of a heap might the largest key be stored?", "396 Chapter 9. Priority Queues\nR-9.12 Consider a situation in which a user has numeric keys and wishes to have\na priority queue that is maximum-oriented . How could a standard (min-\noriented) priority queue be used for such a purpose?\nR-9.13 Illustrate the execution of the in-place heap-sort algorithm on the follow-\ning input sequence: (2,5,16,4,10,23,39,18,26,15).\nR-9.14 LetTbe a complete binary tree such that position pstores an element\nwith key f(p),w h e r e f(p)is the level number of p(see Section 8.3.2). Is\ntreeTa heap? Why or why not?\nR-9.15 Explain why the description of down-heap bubbling does not consider thecase in which position phas a right child but not a left child.\nR-9.16 Is there a heap Hstoring seven entries with distinct keys such that a pre-\norder traversal of Hyields the entries of Hin increasing or decreasing\norder by key? How about an inorder traversal? How about a postorder\ntraversal? If so, give an example; if not, say why.\nR-9.17 LetHbe a heap storing 15 entries using the array-based representation of\na complete binary tree. What is the sequence of indices of the array that\nare visited in a preorder traversal of H? What about an inorder traversal\nofH? What about a postorder traversal of H?\nR-9.18 Show that the sum\nn\n\u2211\ni=1logi,\nwhich appears in the analysis of heap-sort, is \u03a9(nlogn).\nR-9.19 Bill claims that a preorder traversal of a heap will list its keys in nonde-creasing order. Draw an example of a heap that proves him wrong.\nR-9.20 Hillary claims that a postorder traversal of a heap will list its keys in non-increasing order. Draw an example of a heap that proves her wrong.\nR-9.21 Show all the steps of the algorithm for removing the entry (16,X)from the\nheap of Figure 9.1, assuming the entry had been identi\ufb01ed with a locator.\nR-9.22 Show all the steps of the algorithm for replacing key of entry (5,A)with\n18 in the heap of Figure 9.1, assuming the entry had been identi\ufb01ed witha locator.\nR-9.23 Draw an example of a heap whose keys are all the odd numbers from 1 to59 (with no repeats), such that the insertion of an entry with key 32 wouldcause up-heap bubbling to proceed all the way up to a child of the root\n(replacing that child\u2019s key with 32).\nR-9.24 Describe a sequence of ninsertions in a heap that requires \u03a9(nlogn)time\nto process.\nR-9.25 Complete Figure 9.9 by showing all the steps of the in-place heap-sort\nalgorithm. Show both the array and the associated heap at the end of eachstep.", "9.6. Exercises 397\nCreativity\nC-9.26 Show how to implement the stack ADT using only a priority queue and\none additional integer instance variable.\nC-9.27 Show how to implement the FIFO queue ADT using only a priority queueand one additional integer instance variable.\nC-9.28 Professor Idle suggests the following solution to the previous problem.Whenever an item is inserted into the queue, it is assigned a key that isequal to the current size of the queue. Does such a strategy result in FIFOsemantics? Prove that it is so or provide a counterexample.\nC-9.29 Reimplement the SortedPriorityQueue using a Python list. Make sure to\nmaintain remove\nmin\u2019sO(1)performance.\nC-9.30 Give a nonrecursive implementation of the\n upheap method for the class\nHeapPriorityQueue .\nC-9.31 Give a nonrecursive implementation of the\n downheap method for the\nclassHeapPriorityQueue .\nC-9.32 Assume that we are using a linked representation of a complete binary\ntreeT, and an extra reference to the last node of that tree. Show how to\nupdate the reference to the last node after operations addorremove\n min\ninO(logn)time, where nis the current number of nodes of T.B e s u r e\nand handle all possible cases, as illustrated in Figure 9.12.\nC-9.33 When using a linked-tree representation for a heap, an alternative method\nfor \ufb01nding the last node during an insertion in a heap Tis to store, in the\nlast node and each leaf node of T, a reference to the leaf node immedi-\nately to its right (wrapping to the \ufb01rst node in the next lower level for therightmost leaf node). Show how to maintain such references in O(1)time\nper operation of the priority queue ADT assuming that Tis implemented\nwith a linked structure.\n(11,S)(2,B)\n(5,A) (4,C)\n(6,Z) (9,F) (15,K)\n(25,J) (12,H) (14,E) (16,X)(7,Q)\n(8,W) (10,L) (20,B)\nzw(5,A) (6,Z)\n(20,B) (9,F) (15,K)\n(25,J) (14,E) (16,X)(7,Q)\n(12,H)(4,C)\nwz\n(a) (b)\nFigure 9.12: Updating the last node in a complete binary tree after operation addor\nremove . Node wis the last node before operation addor after operation remove .\nNode zis the last node after operation addor before operation remove .", "398 Chapter 9. Priority Queues\nC-9.34 We can represent a path from the root to a given node of a binary tree\nby means of a binary string, where 0 means \u201cgo to the left child\u201d and 1\nmeans \u201cgo to the right child.\u201d For example, the path from the root to thenode storing (8,W)in the heap of Figure 9.12a is represented by \u201c101.\u201d\nDesign an O(logn)-time algorithm for \ufb01nding the last node of a complete\nbinary tree with nnodes, based on the above representation. Show how\nthis algorithm can be used in the implementation of a complete binary treeby means of a linked structure that does not keep a reference to the lastnode.\nC-9.35 Given a heap Tand a key k, give an algorithm to compute all the entries\ninThaving a key less than or equal to k. For example, given the heap of\nFigure 9.12a and query k=7, the algorithm should report the entries with\nkeys 2, 4, 5, 6, and 7 (but not necessarily in this order). Your algorithmshould run in time proportional to the number of entries returned, andshould notmodify the heap\nC-9.36 Provide a justi\ufb01cation of the time bounds in Table 9.4.\nC-9.37 Give an alternative analysis of bottom-up heap construction by showing\nthe following summation is O(1), for any positive integer h:\nh\n\u2211\ni=1/parenleftbig\ni/2i/parenrightbig\n.\nC-9.38 Suppose two binary trees, T1andT2, hold entries satisfying the heap-order\nproperty (but not necessarily the complete binary tree property). Describe\na method for combining T1andT2into a binary tree T, whose nodes hold\nthe union of the entries in T1andT2and also satisfy the heap-order prop-\nerty. Your algorithm should run in time O(h1+h2)where h1andh2are\nthe respective heights of T1andT2.\nC-9.39 Implement a heappushpop method for the HeapPriorityQueue class, with\nsemantics akin to that described for the heapq module in Section 9.3.7.\nC-9.40 Implement a heapreplace method for the HeapPriorityQueue class, with\nsemantics akin to that described for the heapq module in Section 9.3.7.\nC-9.41 Tamarindo Airlines wants to give a \ufb01rst-class upgrade coupon to their toplognfrequent \ufb02yers, based on the number of miles accumulated, where\nnis the total number of the airlines\u2019 frequent \ufb02yers. The algorithm they\ncurrently use, which runs in O(nlogn)time, sorts the \ufb02yers by the number\nof miles \ufb02own and then scans the sorted list to pick the top log n\ufb02yers.\nDescribe an algorithm that identi\ufb01es the top log n\ufb02yers in O(n)time.\nC-9.42 Explain how the klargest elements from an unordered collection of size n\ncan be found in time O(n+klogn)using a maximum-oriented heap.\nC-9.43 Explain how the klargest elements from an unordered collection of size n\ncan be found in time O(nlogk)using O(k)auxiliary space.", "9.6. Exercises 399\nC-9.44 Given a class, PriorityQueue , that implements the minimum-oriented pri-\nority queue ADT, provide an implementation of a MaxPriorityQueue class\nthat adapts to provide a maximum-oriented abstraction with methods add,\nmax,a n dremove\n max. Your implementation should not make any as-\nsumption about the internal workings of the original PriorityQueue class,\nnor the type of keys that might be used.\nC-9.45 Write a key function for nonnegative integers that determines order based\non the number of 1\u2019s in each integer\u2019s binary expansion.\nC-9.46 Give an alternative implementation of the pq\nsort function, from Code\nFragment 9.7, that accepts a key function as an optional parameter.\nC-9.47 Describe an in-place version of the selection-sort algorithm for an arraythat uses only O(1)space for instance variables in addition to the array.\nC-9.48 Assuming the input to the sorting problem is given in an array A, describe\nhow to implement the insertion-sort algorithm using only the array Aand\nat most a constant number of additional variables.\nC-9.49 Give an alternate description of the in-place heap-sort algorithm usingthe standard minimum-oriented priority queue (instead of a maximum-oriented one).\nC-9.50 An online computer system for trading stocks needs to process orders ofthe form \u201cbuy 100 shares at $ xeach\u201d or \u201csell 100 shares at $ yeach.\u201d A\nbuy order for $ xcan only be processed if there is an existing sell order\nwith price $ ysuch that y\u2264x. Likewise, a sell order for $ ycan only be\nprocessed if there is an existing buy order with price $ xsuch that y\u2264x.\nIf a buy or sell order is entered but cannot be processed, it must wait for afuture order that allows it to be processed. Describe a scheme that allowsbuy and sell orders to be entered in O(logn)time, independent of whether\nor not they can be immediately processed.\nC-9.51 Extend a solution to the previous problem so that users are allowed to\nupdate the prices for their buy or sell orders that have yet to be processed.\nC-9.52 A group of children want to play a game, called Unmonopoly , where in\neach turn the player with the most money must give half of his/her moneyto the player with the least amount of money. What data structure(s)\nshould be used to play this game ef\ufb01ciently? Why?\nProjects\nP-9.53 Implement the in-place heap-sort algorithm. Experimentally compare its\nrunning time with that of the standard heap-sort that is not in-place.\nP-9.54 Use the approach of either Exercise C-9.42 or C-9.43 to reimplement the\ntopmethod of the FavoritesListMTF class from Section 7.6.2. Make sure\nthat results are generated from largest to smallest.", "400 Chapter 9. Priority Queues\nP-9.55 Write a program that can process a sequence of stock buy and sell orders\nas described in Exercise C-9.50.\nP-9.56 LetSbe a set of npoints in the plane with distinct integer x-a n d y-\ncoordinates. Let Tbe a complete binary tree storing the points from S\nat its external nodes, such that the points are ordered left to right by in-\ncreasing x-coordinates. For each node vinT,l e tS(v)denote the subset of\nSconsisting of points stored in the subtree rooted at v. For the root rof\nT,d e \ufb01 n e top (r)to be the point in S=S(r)with maximum y-coordinate.\nFor every other node v,d e \ufb01 n e top (r)to be the point in Swith highest y-\ncoordinate in S(v)that is not also the highest y-coordinate in S(u),w h e r e\nuis the parent of vinT(if such a point exists). Such labeling turns Tinto\napriority search tree . Describe a linear-time algorithm for turning Tinto\na priority search tree. Implement this approach.\nP-9.57 One of the main applications of priority queues is in operating systems\u2014forscheduling jobs on a CPU. In this project you are to build a program\nthat schedules simulated CPU jobs. Your program should run in a loop,\neach iteration of which corresponds to a time slice for the CPU. Each job\nis assigned a priority, which is an integer between \u221220\n(highest priority)\nand 19 (lowest priority), inclusive. From among all jobs waiting to be pro-\ncessed in a time slice, the CPU must work on a job with highest priority.In this simulation, each job will also come with a length value, which is an\ninteger between 1 and 100, inclusive, indicating the number of time slicesthat are needed to process this job. For simplicity, you may assume jobscannot be interrupted\u2014once it is scheduled on the CPU, a job runs for anumber of time slices equal to its length. Your simulator must output the\nname of the job running on the CPU in each time slice and must process\na sequence of commands, one per time slice, each of which is of the form\u201cadd job name with length nand priority p\u201d or \u201cno new job this slice\u201d.\nP-9.58 Develop a Python implementation of an adaptable priority queue that isbased on an unsorted list and supports location-aware entries.\nChapter Notes\nKnuth\u2019s book on sorting and searching [65] describes the motivation and history for the\nselection-sort, insertion-sort, and heap-sort algorithms. The heap-sort algorithm is due\nto Williams [103], and the linear-time heap cons truction algorithm is due to Floyd [39].\nAdditional algorithms and analyses for heaps and heap-sort variations can be found in\npapers by Bentley [15], Carlsson [24], Gonnet and Munro [45], McDiarmid and Reed [74],\nand Schaffer and Sedgewick [88].", "Chapter\n10Maps, Hash Tables, and Skip Lists\nContents\n1 0 . 1M a p sa n dD i c t i o n a r i e s..................... 4 0 2\n1 0 . 1 . 1T h e M a p A D T .......................4 0 3\n10.1.2 Application: Counting Word Frequencies . . . . . . . . . . 405\n10.1.3 Python\u2019s MutableMapping Abstract Base Class . . . . . . 406\n1 0 . 1 . 4O u r M a p B a s e C l a s s.....................4 0 7\n10.1.5 Simple Unsorted Map Implementation . . . . . . . . . . . 408\n1 0 . 2H a s hT a b l e s .......................... 4 1 0\n1 0 . 2 . 1H a s h F u n c t i o n s .......................4 1 110.2.2 Collision-Handling Schemes . . . . . . . . . . . . . . . . . 417\n1 0 . 2 . 3L o a d F a c t o r s , R e h a s h i n g , a n d E \ufb03 c i e n c y..........4 2 0\n10.2.4 Python Hash Table Implementation . . . . . . . . . . . . 422\n1 0 . 3S o r t e dM a p s .......................... 4 2 7\n1 0 . 3 . 1S o r t e d S e a r c h T a b l e s ....................4 2 8\n1 0 . 3 . 2T w o A p p l i c a t i o n s o f S o r t e d M a p s .............4 3 4\n1 0 . 4S k i pL i s t s............................ 4 3 7\n10.4.1 Search and Update Operations in a Skip List . . . . . . . 439\n10.4.2 Probabilistic Analysis of Skip Lists\u22c6............4 4 3\n1 0 . 5S e t s ,M u l t i s e t s ,a n dM u l t i m a p s ............... 4 4 6\n1 0 . 5 . 1T h e S e t A D T ........................4 4 6\n10.5.2 Python\u2019s MutableSet Abstract Base Class . . . . . . . . . 448\n10.5.3 Implementing Sets, Multisets, and Multimaps . . . . . . . 450\n1 0 . 6E x e r c i s e s ............................ 4 5 2\n", "402 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.1 Maps and Dictionaries\nPython\u2019s dict class is arguably the most signi\ufb01cant data structure in the language. It\nrepresents an abstraction known as a dictionary in which unique keys are mapped\nto associated values . Because of the relationship they express between keys and\nvalues, dictionaries are commonly known as associative arrays ormaps .I n t h i s\nbook, we use the term dictionary when speci\ufb01cally discussing Python\u2019s dictclass,\nand the term map when discussing the more general notion of the abstract data type.\nAs a simple example, Figure 10.1 illustrates a map from the names of countries\nto their associated units of currency.\nRupeeTurkey Spain China United States India Greece\nLira Euro Yuan Dollar\nFigure 10.1: A map from countries (the keys) to their units of currency (the values).\nWe note that the keys (the country names) are assumed to be unique, but the values\n(the currency units) are not necessarily unique. For example, we note that Spain\nand Greece both use the euro for currency. Maps use an array-like syntax for in-dexing, such as currency[\nGreece\n ]to access a value associated with a given key\norcurrency[\n Greece\n ]=\n Drachma\n to remap it to a new value. Unlike a stan-\ndard array, indices for a map need not be consecutive nor even numeric. Common\napplications of maps include the following.\n\u2022A university\u2019s information system relies on some form of a student ID as a\nkey that is mapped to that student\u2019s associated record (such as the student\u2019s\nname, address, and course grades) serving as the value.\n\u2022The domain-name system (DNS) maps a host name, such as www.wiley.com,\nto an Internet-Protocol (IP) address, such as 208.215.179.146.\n\u2022A social media site typically relies on a (nonnumeric) username as a key thatcan be ef\ufb01ciently mapped to a particular user\u2019s associated information.\n\u2022A computer graphics system may map a color name, such as\nturquoise\n ,\nto the triple of numbers that describes the color\u2019s RGB (red-green-blue) rep-resentation, such as (64,224,208) .\n\u2022Python uses a dictionary to represent each namespace, mapping an identifyingstring, such as\npi\n, to an associated object, such as 3.14159 .\nIn this chapter and the next we demonstrate that a map may be implemented so\nthat a search for a key, and its associated value, can be performed very ef\ufb01ciently,thereby supporting fast lookup in such applications.", "10.1. Maps and Dictionaries 403\n10.1.1 The Map ADT\nIn this section, we introduce the map ADT , and de\ufb01ne its behaviors to be consistent\nwith those of Python\u2019s built-in dictclass. We begin by listing what we consider the\nmost signi\ufb01cant \ufb01ve behaviors of a map Mas follows:\nM[k] :Return the value vassociated with key kin map M,i f\none exists; otherwise raise a KeyError . In Python, this is\nimplemented with the special method\n getitem\n .\nM[k] = v :Associate value vwith key kin map M, replacing the ex-\nisting value if the map already contains an item with key\nequal to k. In Python, this is implemented with the special\nmethod\n setitem\n .\ndel M[k] :Remove from map Mthe item with key equal to k;i fM\nhas no such item, then raise a KeyError . In Python, this is\nimplemented with the special method\n delitem\n .\nlen(M) :Return the number of items in map M. In Python, this is\nimplemented with the special method\n len\n .\niter(M) :The default iteration for a map generates a sequence of\nkeys in the map. In Python, this is implemented with the\nspecial method\n iter\n , and it allows loops of the form,\nforkinM.\nWe have highlighted the above \ufb01ve behaviors because they demonstrate the corefunctionality of a map\u2014namely, the ability to query, add, modify, or delete a key-value pair, and the ability to report all such pairs. For additional convenience,map Mshould also support the following behaviors:\nki nM :Return True if the map contains an item with key k.I n\nPython, this is implemented with the special\ncontains\nmethod.\nM.get(k, d=None) :Return M[k] if key kexists in the map; otherwise return\ndefault value d. This provides a form to query M[k] with-\nout risk of a KeyError .\nM.setdefault(k, d) :If key kexists in the map, simply return M[k] ;i fk e y k\ndoes not exist, set M[k] = d and return that value.\nM.pop(k, d=None) :Remove the item associated with key kfrom the map and\nreturn its associated value v.I f k e y kis not in the map,\nreturn default value d(or raise KeyError if parameter dis\nNone ).", "404 Chapter 10. Maps, Hash Tables, and Skip Lists\nM.popitem() :Remove an arbitrary key-value pair from the map, and re-\nturn a (k,v) tuple representing the removed pair. If map is\nempty, raise a KeyError .\nM.clear() :Remove all key-value pairs from the map.\nM.keys() :Return a set-like view of all keys of M.\nM.values() :Return a set-like view of all values of M.\nM.items() :Return a set-like view of (k,v) tuples for all entries of M.\nM.update(M2) :Assign M[k] = v for every (k,v) pair in map M2.\nM= =M 2 :Return True if maps MandM2have identical key-value\nassociations.\nM! =M 2 :Return True if maps MandM2do not have identical key-\nvalue associations.\nExample 10.1: In the following, we show the effect of a series of operations on\nan initially empty map storing items with integer keys and single-character values.\nWe use the literal syntax for Python\u2019s dictclass to describe the map contents.\nOperation\n Return Value\n Map\nlen(M)\n 0\n {}\nM[\nK\n]=2\n \u2013\n {\nK\n:2}\nM[\nB\n]=4\n \u2013\n {\nK\n:2 ,\n B\n:4}\nM[\nU\n]=2\n \u2013\n {\nK\n:2 ,\n B\n:4 ,\n U\n:2}\nM[\nV\n]=8\n \u2013\n {\nK\n:2 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM[\nK\n]=9\n \u2013\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM[\nB\n]\n 4\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM[\nX\n]\n KeyError\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM.get(\n F\n)\n None\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM.get(\n F\n,5 )\n 5\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nM.get(\n K\n,5 )\n 9\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\nlen(M)\n 4\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2 ,\n V\n:8}\ndel M[\n V\n]\n \u2013\n {\nK\n:9 ,\n B\n:4 ,\n U\n:2}\nM.pop(\n K\n)\n 9\n {\nB\n:4 ,\n U\n:2}\nM.keys()\n B\n,\nU\n {\nB\n:4 ,\n U\n:2}\nM.values()\n 4, 2\n {\nB\n:4 ,\n U\n:2}\nM.items()\n (\nB\n,4 ) ,(\n U\n,2 )\n {\nB\n:4 ,\n U\n:2}\nM.setdefault(\n B\n,1 )\n 4\n {\nB\n:4 ,\n U\n:2}\nM.setdefault(\n A\n,1 )\n 1\n {\nA\n:1 ,\n B\n:4 ,\n U\n:2}\nM.popitem()\n (\nB\n,4 )\n {\nA\n:1 ,\n U\n:2}\n", "10.1. Maps and Dictionaries 405\n10.1.2 Application: Counting Word Frequencies\nAs a case study for using a map, consider the problem of counting the number\nof occurrences of words in a document. This is a standard task when performing a\nstatistical analysis of a document, for example, when categorizing an email or news\narticle. A map is an ideal data structure to use here, for we can use words as keysand word counts as values. We show such an application in Code Fragment 10.1.\nWe break apart the original document using a combination of \ufb01le and string\nmethods that results in a loop over a lowercased version of all whitespace separatedpieces of the document. We omit all nonalphabetic characters so that parentheses,\napostrophes, and other such punctuation are not considered part of a word.\nIn terms of map operations, we begin with an empty Python dictionary named\nfreq. During the \ufb01rst phase of the algorithm, we execute the command\nf r e q [ w o r d ]=1+f r e q . g e t ( w o r d ,0 )\nfor each word occurrence. We use the getmethod on the right-hand side because the\ncurrent word might not exist in the dictionary; the default value of 0 is appropriatein that case.\nDuring the second phase of the algorithm, after the full document has been pro-\ncessed, we examine the contents of the frequency map, looping over freq.items()\nto determine which word has the most occurrences.\n1freq = {}\n2forpieceinopen(\ufb01lename).read().lower().split():\n3# only consider alphabetic characters within this piece\n4word =\n.join(c forcinpieceifc.isalpha())\n5ifword: # require at least one alphabetic character\n6 f r e q [ w o r d ]=1+f r e q . g e t ( w o r d ,0 )\n7\n8max\nword =\n9max\ncount = 0\n10for(w,c)infreq.items(): # (key, value) tuples represent (word, count)\n11ifc>max\ncount:\n12 max\nword = w\n13 max\ncount = c\n14print(\n The most frequent word is\n ,m a x\n word)\n15print(\n Its number of occurrences is\n ,m a x\n count)\nCode Fragment 10.1: A program for counting word frequencies in a document, and\nreporting the most frequent word. We use Python\u2019s dict class for the map. We\nconvert the input to lowercase and ignore any nonalphabetic characters.", "406 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.1.3 Python\u2019s MutableMapping Abstract Base Class\nSection 2.4.3 provides an introduction to the concept of an abstract base class\nand the role of such classes in Python\u2019s collections module. Methods that are de-\nclared to be abstract in such a base class must be implemented by concrete sub-\nclasses. However, an abstract base class may provide concrete implementation of\nother methods that depend upon use of the presumed abstract methods. (This is an\nexample of the template method design pattern .)\nThecollections module provides two abstract base classes that are relevant to\nour current discussion: the Mapping andMutableMapping classes. The Mapping\nclass includes all nonmutating methods supported by Python\u2019s dictclass, while the\nMutableMapping class extends that to include the mutating methods. What we\nde\ufb01ne as the map ADT in Section 10.1.1 is akin to the MutableMapping abstract\nbase class in Python\u2019s collections module.\nThe signi\ufb01cance of these abstract base classes is that they provide a framework\nto assist in creating a user-de\ufb01ned map class. In particular, the MutableMapping\nclass provides concrete implementations for all behaviors other than the \ufb01rst \ufb01ve\noutlined in Section 10.1.1:\n getitem\n ,\nsetitem\n ,\ndelitem\n ,\nlen\n ,a n d\niter\n . As we implement the map abstraction with various data structures, as\nlong as we provide the \ufb01ve core behaviors, we can inherit all other derived behav-iors by simply declaring MutableMapping as a parent class.\nTo better understand the MutableMapping class, we provide a few examples of\nhow concrete behaviors can be derived from the \ufb01ve core abstractions. For example,the\ncontains\n method, supporting the syntax ki nM , could be implemented by\nmaking a guarded attempt to retrieve self[k] to determine if the key exists.\ndef\n contains\n (self,k ) :\ntry:\nself[k] # access via\n getitem\n (ignore result)\nreturn True\nexcept KeyError:\nreturn False # attempt failed\nA similar approach might be used to provide the logic of the setdefault method.\ndefsetdefault( self,k ,d ) :\ntry:\nreturn self [k] #i f\n getitem\n succeeds, return value\nexcept KeyError: #o t h e r w i s e :\nself[k] = d # set default value with\n setitem\nreturn d # and return that newly assigned value\nWe leave as exercises the implementations of the remaining concrete methods of\ntheMutableMapping class.", "10.1. Maps and Dictionaries 407\n10.1.4 Our MapBase Class\nWe will be providing many different implementations of the map ADT, in the re-\nmainder of this chapter and next, using a variety of data structures demonstrating atrade-off of advantages and disadvantages. Figure 10.2 provides a preview of those\nclasses.\nTheMutableMapping abstract base class, from Python\u2019s collections module\nand discussed in the preceding pages, is a valuable tool when implementing a map.\nHowever, in the interest of greater code reuse, we de\ufb01ne our own MapBase class,\nwhich is itself a subclass of the MutableMapping class. Our MapBase class pro-\nvides additional support for the composition design pattern. This is a technique\nwe introduced when implementing a priority queue (see Section 9.2.1) in order to\ngroup a key-value pair as a single instance for internal use.\nMore formally, our MapBase class is de\ufb01ned in Code Fragment 10.2, extend-\ning the existing MutableMapping abstract base class so that we inherit the many\nuseful concrete methods that class provides. We then de\ufb01ne a nonpublic nested\nItem class, whose instances are able to store both a key and value. This nested\nclass is reasonably similar in design to the\n Item class that was de\ufb01ned within our\nPriorityQueueBase class in Section 9.2.1, except that for a map we provide sup-\nport for both equality tests and comparisons, both of which rely on the item\u2019s key.The notion of equality is necessary for all of our map implementations, as a way to\ndetermine whether a key given as a parameter is equivalent to one that is already\nstored in the map. The notion of comparisons between keys, using the <operator,\nwill become relevant when we later introduce a sorted map ADT (Section 10.3).\nProbeHashMapMutableMapping\n(Section 10.1.4)MapBase\n(Section 10.3.1)SortedTableMap\n(Chapter 11)TreeMap\n(Section 10.2.4)HashMapBase\n(Section 10.2.4)ChainHashMap\n(Section 10.2.4)(collections module)\n(Section 10.1.5)UnsortedTableMap\n(additional subclasses)\nFigure 10.2: Our hierarchy of map types (with references to where they are de\ufb01ned).", "408 Chapter 10. Maps, Hash Tables, and Skip Lists\n1classMapBase(MutableMapping):\n2\u201d\u201d\u201dOur own abstract base class that includes a nonpublic\n Item class.\u201d\u201d\u201d\n3\n4#------------------------------- nested\n Item class -------------------------------\n5class\n Item:\n6 \u201d\u201d\u201dLightweight composite to store key-value pairs as map items.\u201d\u201d\u201d\n7\n slots\n =\n_key\n ,\n_value\n89 def\ninit\n(self,k ,v ) :\n10 self.\nkey = k\n11 self.\nvalue = v\n1213 def\neq\n(self,o t h e r ) :\n14 return self .\nkey == other.\n key # compare items based on their keys\n1516 def\nne\n(self,o t h e r ) :\n17 return not (self== other) #o p p o s i t eo f\n eq\n18\n19 def\n lt\n(self,o t h e r ) :\n20 return self .\nkey<other.\n key # compare items based on their keys\nCode Fragment 10.2: Extending the MutableMapping abstract base class to provide\na nonpublic\n Item class for use in our various map implementations.\n10.1.5 Simple Unsorted Map Implementation\nWe demonstrate the use of the MapBase class with a very simple concrete imple-\nmentation of the map ADT. Code Fragment 10.3 presents an UnsortedTableMap\nclass that relies on storing key-value pairs in arbitrary order within a Python list.\nAn empty table is initialized as self.\ntable within the constructor for our map.\nWhen a new key is entered into the map, via line 22 of the\n setitem\n method,\nwe create a new instance of the nested\n Item class, which is inherited from our\nMapBase class.\nThis list-based map implementation is simple, but it is not particularly ef\ufb01cient.\nEach of the fundamental methods,\n getitem\n ,\nsetitem\n ,a n d\n delitem\n ,\nrelies on a for loop to scan the underlying list of items in search of a matching key.\nIn a best-case scenario, such a match may be found near the beginning of the list, inwhich case the loop terminates; in the worst case, the entire list will be examined.\nTherefore, each of these methods runs in O(n)time on a map with nitems.", "10.1. Maps and Dictionaries 409\n1classUnsortedTableMap(MapBase):\n2\u201d\u201d\u201dMap implementation using an unordered list.\u201d\u201d\u201d\n3\n4def\n init\n(self):\n5 \u201d\u201d\u201dCreate an empty map.\u201d\u201d\u201d\n6 self.\ntable = [ ] # list of\n Item\u2019s\n78def\ngetitem\n (self,k ) :\n9 \u201d\u201d\u201dReturn value associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n10 foritemin self .\ntable:\n11 ifk= =i t e m .\n key:\n12 return item.\n value\n13 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n14\n15def\n setitem\n (self,k ,v ) :\n16 \u201d\u201d\u201dAssign value v to key k, overwriti ng existing value if present.\u201d\u201d\u201d\n17 foritemin self .\ntable:\n18 ifk= =i t e m .\n key: # Found a match:\n19 item.\n value = v # reassign value\n20 return # and quit\n21 # did not \ufb01nd match for key\n22 self.\ntable.append( self.\nItem(k,v))\n2324def\ndelitem\n (self,k ) :\n25 \u201d\u201d\u201dRemove item associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n26 forjinrange(len( self.\ntable)):\n27 ifk= =self.\ntable[j].\n key: # Found a match:\n28 self.\ntable.pop(j) #r e m o v ei t e m\n29 return # and quit\n30 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n3132def\nlen\n(self):\n33 \u201d\u201d\u201dReturn number of items in the map.\u201d\u201d\u201d\n34 return len(self.\ntable)\n35\n36def\n iter\n(self):\n37 \u201d\u201d\u201dGenerate iteration of the map\n s keys.\u201d\u201d\u201d\n38 foritemin self .\ntable:\n39 yielditem.\n key # yield the KEY\nCode Fragment 10.3: An implementation of a map using a Python list as an unsorted\ntable. Parent class MapBase is given in Code Fragment 10.2.", "410 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2 Hash Tables\nIn this section, we introduce one of the most practical data structures for imple-\nmenting a map, and the one that is used by Python\u2019s own implementation of the\ndictclass. This structure is known as a hash table .\nIntuitively, a map Msupports the abstraction of using keys as indices with a\nsyntax such as M[k] . As a mental warm-up, consider a restricted setting in which\na map with nitems uses keys that are known to be integers in a range from 0 to\nN\u22121f o rs o m e N\u2265n. In this case, we can represent the map using a lookup table\nof length N, as diagrammed in Figure 10.3.\n0 123456789 1 0\nDZ C Q\nFigure 10.3: A lookup table with length 11 for a map containing items (1,D), (3,Z),\n(6,C), and (7,Q).\nIn this representation, we store the value associated with key kat index kof the\ntable (presuming that we have a distinct way to represent an empty slot). Basic mapoperations of\ngetitem\n ,\nsetitem\n ,a n d\n delitem\n can be implemented in\nO(1)worst-case time.\nThere are two challenges in extending this framework to the more general set-\nting of a map. First, we may not wish to devote an array of length Nif it is the case\nthatN/greatermuchn. Second, we do not in general require that a map\u2019s keys be integers.\nThe novel concept for a hash table is the use of a hash function to map general\nkeys to corresponding indices in a table. Ideally, keys will be well distributed in therange from 0 to N\u22121 by a hash function, but in practice there may be two or more\ndistinct keys that get mapped to the same index. As a result, we will conceptualize\nour table as a bucket array , as shown in Figure 10.4, in which each bucket may\nmanage a collection of items that are sent to a speci\ufb01c index by the hash function.\n(To save space, an empty bucket may be replaced by None .)\n0 123456789 1 0\n(1,D) (25,C)\n(3,F)\n(14,Z)(39,C)(6,A) (7,Q)\nFigure 10.4: A bucket array of capacity 11 with items (1,D), (25,C), (3,F), (14,Z),\n(6,A), (39,C), and (7,Q), using a simple hash function.", "10.2. Hash Tables 411\n10.2.1 Hash Functions\nThe goal of a hash function ,h, is to map each key kto an integer in the range\n[0,N\u22121],w h e r e Nis the capacity of the bucket array for a hash table. Equipped\nwith such a hash function, h, the main idea of this approach is to use the hash\nfunction value, h(k), as an index into our bucket array, A, instead of the key k\n(which may not be appropriate for direct use as an index). That is, we store the\nitem (k,v)in the bucket A[h(k)].\nIf there are two or more keys with the same hash value, then two different items\nwill be mapped to the same bucket in A. In this case, we say that a collision has\noccurred. To be sure, there are ways of dealing with collisions, which we willdiscuss later, but the best strategy is to try to avoid them in the \ufb01rst place. We saythat a hash function is \u201cgood\u201d if it maps the keys in our map so as to suf\ufb01cientlyminimize collisions. For practical reasons, we also would like a hash function to\nbe fast and easy to compute.\nIt is common to view the evaluation of a hash function, h(k), as consisting of\ntwo portions\u2014a hash code that maps a key kto an integer, and a compression\nfunction that maps the hash code to an integer within a range of indices, [0,N\u22121],\nfor a bucket array. (See Figure 10.5.)\n-1hash code\n120 -2... ...\ncompression function\n120N - 1 ...Arbitrary Objects\nFigure 10.5: Two parts of a hash function: a hash code and a compression function.\nThe advantage of separating the hash function into two such components is that\nthe hash code portion of that computation is independent of a speci\ufb01c hash table\nsize. This allows the development of a general hash code for each object that canbe used for a hash table of any size; only the compression function depends uponthe table size. This is particularly convenient, because the underlying bucket arrayfor a hash table may be dynamically resized, depending on the number of items\ncurrently stored in the map. (See Section 10.2.3.)", "412 Chapter 10. Maps, Hash Tables, and Skip Lists\nHash Codes\nThe \ufb01rst action that a hash function performs is to take an arbitrary key kin our\nmap and compute an integer that is called the hash code fork; this integer need not\nbe in the range [0,N\u22121], and may even be negative. We desire that the set of hash\ncodes assigned to our keys should avoid collisions as much as possible. For if the\nhash codes of our keys cause collisions, then there is no hope for our compression\nfunction to avoid them. In this subsection, we begin by discussing the theory ofhash codes. Following that, we discuss practical implementations of hash codes inPython.\nTreating the Bit Representation as an Integer\nTo begin, we note that, for any data type Xthat is represented using at most as many\nbits as our integer hash codes, we can simply take as a hash code for Xan integer\ninterpretation of its bits. For example, the hash code for key 314 could simply be\n314. The hash code for a \ufb02oating-point number such as 3 .14 could be based upon\nan interpretation of the bits of the \ufb02oating-point representation as an integer.\nFor a type whose bit representation is longer than a desired hash code, the above\nscheme is not immediately applicable. For example, Python relies on 32-bit hash\ncodes. If a \ufb02oating-point number uses a 64-bit representation, its bits cannot be\nviewed directly as a hash code. One possibility is to use only the high-order 32 bits\n(or the low-order 32 bits). This hash code, of course, ignores half of the informationpresent in the original key, and if many of the keys in our map only differ in thesebits, then they will collide using this simple hash code.\nA better approach is to combine in some way the high-order and low-order por-\ntions of a 64-bit key to form a 32-bit hash code, which takes all the original bitsinto consideration. A simple implementation is to add the two components as 32-bit numbers (ignoring over\ufb02ow), or to take the exclusive-or of the two components.These approaches of combining components can be extended to any object xwhose\nbinary representation can be viewed as an n-tuple (x\n0,x1,..., xn\u22121)of 32-bit inte-\ngers, for example, by forming a hash code for xas\u2211n\u22121\ni=0xi,o ra s x0\u2295x1\u2295\u00b7\u00b7\u00b7\u2295 xn\u22121,\nwhere the \u2295symbol represents the bitwise exclusive-or operation (which is \u02c6in\nPython).\nPolynomial Hash Codes\nThe summation and exclusive-or hash codes, described above, are not good choicesfor character strings or other variable-length objects that can be viewed as tuples ofthe form (x\n0,x1,..., xn\u22121), where the order of the xi\u2019s is signi\ufb01cant. For example,\nconsider a 16-bit hash code for a character string sthat sums the Unicode values\nof the characters in s. This hash code unfortunately produces lots of unwanted", "10.2. Hash Tables 413\ncollisions for common groups of strings. In particular, \"temp01\" and\"temp10\"\ncollide using this function, as do \"stop\", \"tops\", \"pots\",a n d \"spot\". A better\nhash code should somehow take into consideration the positions of the xi\u2019s. An\nalternative hash code, which does exactly this, is to choose a nonzero constant,\na/negationslash=1, and use as a hash code the value\nx0an\u22121+x1an\u22122+\u00b7\u00b7\u00b7+xn\u22122a+xn\u22121.\nMathematically speaking, this is simply a polynomial in athat takes the compo-\nnents (x0,x1,..., xn\u22121)of an object xas its coef\ufb01cients. This hash code is therefore\ncalled a polynomial hash code . By Horner\u2019s rule (see Exercise C-3.50), this poly-\nnomial can be computed as\nxn\u22121+a(xn\u22122+a(xn\u22123+\u00b7\u00b7\u00b7+a(x2+a(x1+ax0))\u00b7\u00b7\u00b7)).\nIntuitively, a polynomial hash code uses multiplication by different powers as a\nway to spread out the in\ufb02uence of each component across the resulting hash code.\nOf course, on a typical computer, evaluating a polynomial will be done using\nthe \ufb01nite bit representation for a hash code; hence, the value will periodically over-\ufb02ow the bits used for an integer. Since we are more interested in a good spread ofthe object xwith respect to other keys, we simply ignore such over\ufb02ows. Still, we\nshould be mindful that such over\ufb02ows are occurring and choose the constant aso\nthat it has some nonzero, low-order bits, which will serve to preserve some of theinformation content even as we are in an over\ufb02ow situation.\nWe have done some experimental studies that suggest that 33, 37, 39, and 41\nare particularly good choices for awhen working with character strings that are\nEnglish words. In fact, in a list of over 50,000 English words formed as the unionof the word lists provided in two variants of Unix, we found that taking ato be 33,\n37, 39, or 41 produced less than 7 collisions in each case!\nCyclic-Shift Hash Codes\nA variant of the polynomial hash code replaces multiplication by awith a cyclic\nshift of a partial sum by a certain number of bits. For example, a 5-bit cyclic shiftof the 32-bit value 00111\n101100101101010100010101000 is achieved by taking\nthe leftmost \ufb01ve bits and placing those on the rightmost side of the representation,resulting in 10110010110101010001010100000111\n. While this operation has little\nnatural meaning in terms of arithmetic, it accomplishes the goal of varying the bitsof the calculation. In Python, a cyclic shift of bits can be accomplished throughcareful use of the bitwise operators <<and>>, taking care to truncate results to\n32-bit integers.", "414 Chapter 10. Maps, Hash Tables, and Skip Lists\nAn implementation of a cyclic-shift hash code computation for a character\nstring in Python appears as follows:\ndefhash\ncode(s):\nmask = (1 <<32)\u22121 # limit to 32-bit integers\nh=0\nforcharacter ins:\nh=( h <<5&m a s k ) |(h>>27) # 5-bit cyclic shift of running sum\nh += ord(character) # add in value of next character\nreturn h\nAs with the traditional polynomial hash code, \ufb01ne-tuning is required when using a\ncyclic-shift hash code, as we must wisely choose the amount to shift by for eachnew character. Our choice of a 5-bit shift is justi\ufb01ed by experiments run on a list ofjust over 230,000 English words, comparing the number of collisions for various\nshift amounts (see Table 10.1).\nCollisions\nShift\n Total\n Max\n0\n234735\n 623\n1\n165076\n 43\n2\n 38471\n 13\n3\n 7174\n 5\n4\n 1379\n 3\n5\n 190\n 3\n6\n 502\n 2\n7\n 560\n 2\n8\n 5546\n 4\n9\n 393\n 3\n10\n 5194\n 5\n11\n 11559\n 5\n12\n 822\n 2\n13\n 900\n 4\n14\n 2001\n 4\n15\n 19251\n 8\n16\n 211781\n 37\nTable 10.1: Comparison of collision behavior for the cyclic-shift hash code as ap-\nplied to a list of 230,000 English words. The \u201cTotal\u201d column records the total num-\nber of words that collide with at least one other, and the \u201cMax\u201d column records themaximum number of words colliding at any one hash code. Note that with a cyclicshift of 0, this hash code reverts to the one that simply sums all the characters.", "10.2. Hash Tables 415\nHash Codes in Python\nThe standard mechanism for computing hash codes in Python is a built-in function\nwith signature hash(x) that returns an integer value that serves as the hash code for\nobject x. However, only immutable data types are deemed hashable in Python. This\nrestriction is meant to ensure that a particular object\u2019s hash code remains constant\nduring that object\u2019s lifespan. This is an important property for an object\u2019s use as\na key in a hash table. A problem could occur if a key were inserted into the hashtable, yet a later search were performed for that key based on a different hash codethan that which it had when inserted; the wrong bucket would be searched.\nAmong Python\u2019s built-in data types, the immutable int,\ufb02oat ,str,tuple ,a n d\nfrozenset classes produce robust hash codes, via the hash function, using tech-\nniques similar to those discussed earlier in this section. Hash codes for characterstrings are well crafted based on a technique similar to polynomial hash codes,except using exclusive-or computations rather than additions. If we repeat the ex-periment described in Table 10.1 using Python\u2019s built-in hash codes, we \ufb01nd thatonly 8 strings out of the set of more than 230,000 collide with another. Hash codes\nfor tuples are computed with a similar technique based upon a combination of the\nhash codes of the individual elements of the tuple. When hashing a frozenset ,t h e\norder of the elements should be irrelevant, and so a natural option is to compute theexclusive-or of the individual hash codes without any shifting. If hash(x) is called\nfor an instance xof a mutable type, such as a list,aTypeError is raised.\nInstances of user-de\ufb01ned classes are treated as unhashable by default, with a\nTypeError raised by the hash function. However, a function that computes hash\ncodes can be implemented in the form of a special method named\nhash\n within\na class. The returned hash code should re\ufb02ect the immutable attributes of an in-stance. It is common to return a hash code that is itself based on the computed hashof the combination of such attributes. For example, a Color class that maintains\nthree numeric red, green, and blue components might implement the method as:\ndef\nhash\n (self):\nreturn hash( ( self.\nred,self.\ngreen, self.\nblue) ) # hash combined tuple\nAn important rule to obey is that if a class de\ufb01nes equivalence through\n eq\n ,\nthen any implementation of\n hash\n must be consistent, in that if x= =y ,t h e n\nhash(x) == hash(y) . This is important because if two instances are considered\nto be equivalent and one is used as a key in a hash table, a search for the secondinstance should result in the discovery of the \ufb01rst. It is therefore important that thehash code for the second match the hash code for the \ufb01rst, so that the proper bucketis examined. This rule extends to any well-de\ufb01ned comparisons between objectsof different classes. For example, since Python treats the expression 5= =5 . 0 as\ntrue, it ensures that hash(5) andhash(5.0) are the same.", "416 Chapter 10. Maps, Hash Tables, and Skip Lists\nCompression Functions\nThe hash code for a key kwill typically not be suitable for immediate use with a\nbucket array, because the integer hash code may be negative or may exceed the ca-\npacity of the bucket array. Thus, once we have determined an integer hash code for\na key object k, there is still the issue of mapping that integer into the range [0,N\u22121].\nThis computation, known as a compression function , is the second action per-\nformed as part of an overall hash function. A good compression function is one\nthat minimizes the number of collisions for a given set of distinct hash codes.\nThe Division Method\nA simple compression function is the division method , which maps an integer ito\nimod N,\nwhere N, the size of the bucket array, is a \ufb01xed positive integer. Additionally, if we\ntake Nto be a prime number, then this compression function helps \u201cspread out\u201d the\ndistribution of hashed values. Indeed, if Nis not prime, then there is greater risk\nthat patterns in the distribution of hash codes will be repeated in the distribution ofhash values, thereby causing collisions. For example, if we insert keys with hash\ncodes {200,205,210,215,220,..., 600}into a bucket array of size 100, then each\nhash code will collide with three others. But if we use a bucket array of size 101,\nthen there will be no collisions. If a hash function is chosen well, it should ensurethat the probability of two different keys getting hashed to the same bucket is 1 /N.\nChoosing Nto be a prime number is not always enough, however, for if there is\na repeated pattern of hash codes of the form pN +qfor several different p\u2019s, then\nthere will still be collisions.\nThe MAD Method\nA more sophisticated compression function, which helps eliminate repeated pat-terns in a set of integer keys, is the Multiply-Add-and-Divide (or \u201cMAD\u201d) method.\nThis method maps an integer ito\n[(ai+b)mod p]mod N,\nwhere Nis the size of the bucket array, pis a prime number larger than N,a n d a\nandbare integers chosen at random from the interval [0,p\u22121], with a>0. This\ncompression function is chosen in order to eliminate repeated patterns in the set ofhash codes and get us closer to having a \u201cgood\u201d hash function, that is, one such thatthe probability any two different keys collide is 1 /N. This good behavior would be\nthe same as we would have if these keys were \u201cthrown\u201d into Auniformly at random.", "10.2. Hash Tables 417\n10.2.2 Collision-Handling Schemes\nThe main idea of a hash table is to take a bucket array, A, and a hash function, h,a n d\nuse them to implement a map by storing each item (k,v)in the \u201cbucket\u201d A[h(k)].\nThis simple idea is challenged, however, when we have two distinct keys, k1andk2,\nsuch that h(k1)=h(k2). The existence of such collisions prevents us from simply\ninserting a new item (k,v)directly into the bucket A[h(k)]. It also complicates our\nprocedure for performing insertion, search, and deletion operations.\nSeparate Chaining\nA simple and ef\ufb01cient way for dealing with collisions is to have each bucket A[j]\nstore its own secondary container, holding items (k,v)such that h(k)=j. A natural\nchoice for the secondary container is a small map instance implemented using a list,\nas described in Section 10.1.5. This collision resolution rule is known as separate\nchaining , and is illustrated in Figure 10.6.\nA123456789 1 0 01 112\n123825\n9054\n28413618 10\nFigure 10.6: A hash table of size 13, storing 10 items with integer keys, with colli-\nsions resolved by separate chaining. The compression function is h(k)=kmod 13.\nFor simplicity, we do not show the values associated with the keys.\nIn the worst case, operations on an individual bucket take time proportional to\nthe size of the bucket. Assuming we use a good hash function to index the nitems\nof our map in a bucket array of capacity N, the expected size of a bucket is n/N.\nTherefore, if given a good hash function, the core map operations run in O(\u2308n/N\u2309).\nThe ratio \u03bb=n/N, called the load factor of the hash table, should be bounded by\na small constant, preferably below 1. As long as \u03bbisO(1), the core operations on\nthe hash table run in O(1)expected time.", "418 Chapter 10. Maps, Hash Tables, and Skip Lists\nOpen Addressing\nThe separate chaining rule has many nice properties, such as affording simple im-\nplementations of map operations, but it nevertheless has one slight disadvantage:\nIt requires the use of an auxiliary data structure\u2014a list\u2014to hold items with collid-ing keys. If space is at a premium (for example, if we are writing a program for asmall handheld device), then we can use the alternative approach of always storingeach item directly in a table slot. This approach saves space because no auxiliary\nstructures are employed, but it requires a bit more complexity to deal with colli-\nsions. There are several variants of this approach, collectively referred to as open\naddressing schemes, which we discuss next. Open addressing requires that the load\nfactor is always at most 1 and that items are stored directly in the cells of the bucket\narray itself.\nLinear Probing and Its Variants\nA simple method for collision handling with open addressing is linear probing .\nWith this approach, if we try to insert an item (k,v)into a bucket A[j]that is already\noccupied, where j=h(k), then we next try A[(j+1)mod N].I fA[(j+1)mod N]\nis also occupied, then we try A[(j+2)mod N], and so on, until we \ufb01nd an empty\nbucket that can accept the new item. Once this bucket is located, we simply in-\nsert the item there. Of course, this collision resolution strategy requires that wechange the implementation when searching for an existing key\u2014the \ufb01rst step of all\ngetitem\n ,\nsetitem\n ,o r\n delitem\n operations. In particular, to attempt\nto locate an item with key equal to k, we must examine consecutive slots, starting\nfrom A[h(k)], until we either \ufb01nd an item with that key or we \ufb01nd an empty bucket.\n(See Figure 10.7.) The name \u201clinear probing\u201d comes from the fact that accessing acell of the bucket array can be viewed as a \u201cprobe.\u201d\n26123456789 1 0 0New element with\nkey = 15 to be insertedMust probe 4 times\nbefore \ufb01nding empty slot\n53 7 1 6 2 1 13\nFigure 10.7: Insertion into a hash table with integer keys using linear probing. The\nhash function is h(k)=kmod 11. Values associated with keys are not shown.", "10.2. Hash Tables 419\nTo implement a deletion, we cannot simply remove a found item from its slot\nin the array. For example, after the insertion of key 15 portrayed in Figure 10.7,\nif the item with key 37 were trivially deleted, a subsequent search for 15 wouldfail because that search would start by probing at index 4, then index 5, and thenindex 6, at which an empty cell is found. A typical way to get around this dif\ufb01-culty is to replace a deleted item with a special \u201cavailable\u201d marker object. With\nthis special marker possibly occupying spaces in our hash table, we modify our\nsearch algorithm so that the search for a key kwill skip over cells containing the\navailable marker and continue probing until reaching the desired item or an emptybucket (or returning back to where we started from). Additionally, our algorithm\nfor\nsetitem\n should remember an available cell encountered during the search\nfork, since this is a valid place to put a new item (k,v), if no existing item is found.\nAlthough use of an open addressing scheme can save space, linear probing\nsuffers from an additional disadvantage. It tends to cluster the items of a map intocontiguous runs, which may even overlap (particularly if more than half of the cells\nin the hash table are occupied). Such contiguous runs of occupied hash cells causesearches to slow down considerably.\nAnother open addressing strategy, known as quadratic probing , iteratively tries\nthe buckets A[(h(k)+f(i))mod N],f o ri=0,1,2,...,w h e r e f(i)=i\n2, until \ufb01nding\nan empty bucket. As with linear probing, the quadratic probing strategy compli-cates the removal operation, but it does avoid the kinds of clustering patterns thatoccur with linear probing. Nevertheless, it creates its own kind of clustering, calledsecondary clustering , where the set of \ufb01lled array cells still has a non-uniform\npattern, even if we assume that the original hash codes are distributed uniformly.When Nis prime and the bucket array is less than half full, the quadratic probing\nstrategy is guaranteed to \ufb01nd an empty slot. However, this guarantee is not validonce the table becomes at least half full, or if Nis not chosen as a prime number;\nwe explore the cause of this type of clustering in an exercise (C-10.36).\nAn open addressing strategy that does not cause clustering of the kind produced\nby linear probing or the kind produced by quadratic probing is the double hashing\nstrategy. In this approach, we choose a secondary hash function, h\n/prime,a n di f hmaps\nsome key kto a bucket A[h(k)]that is already occupied, then we iteratively try\nthe buckets A[(h(k)+f(i))mod N]next, for i=1,2,3,...,w h e r e f(i)=i\u00b7h/prime(k).\nIn this scheme, the secondary hash function is not allowed to evaluate to zero; acommon choice is h\n/prime(k)=q\u2212(kmod q), for some prime number q<N.A l s o , N\nshould be a prime.\nAnother approach to avoid clustering with open addressing is to iteratively try\nbuckets A[(h(k)+f(i))mod N]where f(i)is based on a pseudo-random number\ngenerator, providing a repeatable, but somewhat arbitrary, sequence of subsequentprobes that depends upon bits of the original hash code. This is the approach cur-\nrently used by Python\u2019s dictionary class.", "420 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2.3 Load Factors, Rehashing, and E\ufb03ciency\nIn the hash table schemes described thus far, it is important that the load factor,\n\u03bb=n/N, be kept below 1. With separate chaining, as \u03bbgets very close to 1, the\nprobability of a collision greatly increases, which adds overhead to our operations,\nsince we must revert to linear-time list-based methods in buckets that have col-\nlisions. Experiments and average-case analyses suggest that we should maintain\u03bb<0.9 for hash tables with separate chaining.\nWith open addressing, on the other hand, as the load factor \u03bbgrows beyond 0 .5\nand starts approaching 1, clusters of entries in the bucket array start to grow as well.These clusters cause the probing strategies to \u201cbounce around\u201d the bucket array fora considerable amount of time before they \ufb01nd an empty slot. In Exercise C-10.36,we explore the degradation of quadratic probing when \u03bb\u22650.5. Experiments sug-\ngest that we should maintain \u03bb<0.5 for an open addressing scheme with linear\nprobing, and perhaps only a bit higher for other open addressing schemes (for ex-ample, Python\u2019s implementation of open addressing enforces that \u03bb<2/3).\nIf an insertion causes the load factor of a hash table to go above the speci\ufb01ed\nthreshold, then it is common to resize the table (to regain the speci\ufb01ed load factor)\nand to reinsert all objects into this new table. Although we need not de\ufb01ne a new\nhash code for each object, we do need to reapply a new compression function thattakes into consideration the size of the new table. Each rehashing will generally\nscatter the items throughout the new bucket array. When rehashing to a new table, itis a good requirement for the new array\u2019s size to be at least double the previous size.\nIndeed, if we always double the size of the table with each rehashing operation, then\nwe can amortize the cost of rehashing all the entries in the table against the timeused to insert them in the \ufb01rst place (as with dynamic arrays; see Section 5.3).\nE\ufb03ciency of Hash Tables\nAlthough the details of the average-case analysis of hashing are beyond the scopeof this book, its probabilistic basis is quite intuitive. If our hash function is good,then we expect the entries to be uniformly distributed in the Ncells of the bucket\narray. Thus, to store nentries, the expected number of keys in a bucket would\nbe\u2308n/N\u2309,w h i c hi s O(1)ifnisO(N).\nThe costs associated with a periodic rehashing, to resize a table after occasional\ninsertions or deletions can be accounted for separately, leading to an additional\nO(1)amortized cost for\nsetitem\n and\n getitem\n .\nIn the worst case, a poor hash function could map every item to the same bucket.\nThis would result in linear-time performance for the core map operations with sepa-\nrate chaining, or with any open addressing model in which the secondary sequenceof probes depends only on the hash code. A summary of these costs is given in\nTable 10.2.", "10.2. Hash Tables 421\nOperation\n List\n Hash Table\nexpected\n worst case\ngetitem\n O(n)\n O(1)\n O(n)\nsetitem\n O(n)\n O(1)\n O(n)\ndelitem\n O(n)\n O(1)\n O(n)\nlen\n O(1)\n O(1)\n O(1)\niter\n O(n)\n O(n)\n O(n)\nTable 10.2: Comparison of the running times of the methods of a map realized by\nmeans of an unsorted list (as in Section 10.1.5) or a hash table. We let ndenote\nthe number of items in the map, and we assume that the bucket array supporting\nthe hash table is maintained such that its capacity is proportional to the number ofitems in the map.\nIn practice, hash tables are among the most ef\ufb01cient means for implementing\na map, and it is essentially taken for granted by programmers that their core oper-\nations run in constant time. Python\u2019s dictclass is implemented with hashing, and\nthe Python interpreter relies on dictionaries to retrieve an object that is referenced\nby an identi\ufb01er in a given namespace. (See Sections 1.10 and 2.5.) The basic com-mand c=a+b involves two calls to\ngetitem\n in the dictionary for the local\nnamespace to retrieve the values identi\ufb01ed as aandb, and a call to\n setitem\nto store the result associated with name cin that namespace. In our own algorithm\nanalysis, we simply presume that such dictionary operations run in constant time,independent of the number of entries in the namespace. (Admittedly, the numberof entries in a typical namespace can almost surely be bounded by a constant.)\nIn a 2003 academic paper [31], researchers discuss the possibility of exploiting\na hash table\u2019s worst-case performance to cause a denial-of-service (DoS) attackof Internet technologies. For many published algorithms that compute hash codes,\nthey note that an attacker could precompute a very large number of moderate-length\nstrings that all hash to the identical 32-bit hash code. (Recall that by any of thehashing schemes we describe, other than double hashing, if two keys are mappedto the same hash code, they will be inseparable in the collision resolution.)\nIn late 2011, another team of researchers demonstrated an implementation of\njust such an attack [61]. Web servers allow a series of key-value parameters to beembedded in a URL using a syntax such as ?key1=val1&key2=val2&key3=val3 .\nTypically, those key-value pairs are immediately stored in a map by the server,\nand a limit is placed on the length and number of such parameters presuming that\nstorage time in the map will be linear in the number of entries. If all keys wereto collide, that storage requires quadratic time (causing the server to perform aninordinate amount of work). In spring of 2012, Python developers distributed asecurity patch that introduces randomization into the computation of hash codes\nfor strings, making it less tractable to reverse engineer a set of colliding strings.", "422 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.2.4 Python Hash Table Implementation\nIn this section, we develop two implementations of a hash table, one using sepa-\nrate chaining and the other using open addressing with linear probing. While theseapproaches to collision resolution are quite different, there are a great many com-monalities to the hashing algorithms. For that reason, we extend the MapBase\nclass (from Code Fragment 10.2), to de\ufb01ne a new HashMapBase class (see Code\nFragment 10.4), providing much of the common functionality to our two hash tableimplementations. The main design elements of the HashMapBase class are:\n\u2022The bucket array is represented as a Python list, named self.\ntable , with all\nentries initialized to None .\n\u2022We maintain an instance variable self.\nnthat represents the number of dis-\ntinct items that are currently stored in the hash table.\n\u2022If the load factor of the table increases beyond 0 .5, we double the size of the\ntable and rehash all items into the new table.\n\u2022We de\ufb01ne a\n hash\nfunction utility method that relies on Python\u2019s built-in\nhash function to produce hash codes for keys, and a randomized Multiply-\nAdd-and-Divide (MAD) formula for the compression function.\nWhat is not implemented in the base class is any notion of how a \u201cbucket\u201d\nshould be represented. With separate chaining, each bucket will be an independentstructure. With open addressing, however, there is no tangible container for eachbucket; the \u201cbuckets\u201d are effectively interleaved due to the probing sequences.\nIn our design, the HashMapBase class presumes the following to be abstract\nmethods, which must be implemented by each concrete subclass:\n\u2022\nbucket\n getitem(j, k)\nThis method should search bucket jfor an item having key k, returning the\nassociated value, if found, or else raising a KeyError .\n\u2022\nbucket\n setitem(j, k, v)\nThis method should modify bucket jso that key kbecomes associated with\nvalue v. If the key already exists, the new value overwrites the existing value.\nOtherwise, a new item is inserted and this method is responsible for incre-\nmenting self.\nn.\n\u2022\nbucket\n delitem(j, k)\nThis method should remove the item from bucket jhaving key k, or raise a\nKeyError if no such item exists. ( self.\nnis decremented after this method.)\n\u2022\niter\nThis is the standard map method to iterate through all keys of the map. Ourbase class does not delegate this on a per-bucket basis because \u201cbuckets\u201d inopen addressing are not inherently disjoint.", "10.2. Hash Tables 423\n1classHashMapBase(MapBase):\n2\u201d\u201d\u201dAbstract base class for map using hash-table with MAD compression.\u201d\u201d\u201d\n3\n4def\n init\n(self, cap=11, p=109345121):\n5 \u201d\u201d\u201dCreate an empty hash-table map.\u201d\u201d\u201d\n6 self.\ntable = cap\n [None ]\n7 self.\nn=0 # number of entries in the map\n8 self.\nprime = p # prime for MAD compression\n9 self.\nscale = 1 + randrange(p \u22121) #s c a l ef r o m1t op - 1f o rM A D\n10 self.\nshift = randrange(p) # shift from 0 to p-1 for MAD\n11\n12def\nhash\nfunction( self,k ) :\n13 return (hash(k)\n self.\nscale + self.\nshift) % self.\nprime % len( self.\ntable)\n14\n15def\n len\n(self):\n16 return self .\nn\n1718def\ngetitem\n (self,k ) :\n19 j=self.\nhash\nfunction(k)\n20 return self .\nbucket\n getitem(j, k) # may raise KeyError\n21\n22def\n setitem\n (self,k ,v ) :\n23 j=self.\nhash\nfunction(k)\n24 self.\nbucket\n setitem(j, k, v) # subroutine maintains self.\n n\n25 if self.\nn>len(self.\ntable) // 2: # keep load factor <=0 . 5\n26 self.\nresize(2\n len(self.\ntable) \u22121) # number 2\u02c6x - 1 is often prime\n27\n28def\n delitem\n (self,k ) :\n29 j=self.\nhash\nfunction(k)\n30 self.\nbucket\n delitem(j, k) # may raise KeyError\n31 self.\nn\u2212=1\n3233def\nresize(self,c ) : # resize bucket array to capacity c\n34 old =list(self.items()) # use iteration to record existing items\n35 self.\ntable = c\n [None] # then reset table to desired capacity\n36 self.\nn=0 # n recomputed during subsequent adds\n37 for(k,v)inold:\n38 self[k] = v # reinsert old key-value pair\nCode Fragment 10.4: A base class for our hash table implementations, extending\nourMapBase class from Code Fragment 10.2.", "424 Chapter 10. Maps, Hash Tables, and Skip Lists\nSeparate Chaining\nCode Fragment 10.5 provides a concrete implementation of a hash table with sepa-\nrate chaining, in the form of the ChainHashMap class. To represent a single bucket,\nit relies on an instance of the UnsortedTableMap class from Code Fragment 10.3.\nThe \ufb01rst three methods in the class use index jto access the potential bucket in\nthe bucket array, and a check for the special case in which that table entry is None .\nThe only time we need a new bucket structure is when\n bucket\n setitem is called on\nan otherwise empty slot. The remaining functionality relies on map behaviors thatare already supported by the individual UnsortedTableMap instances. We need a\nbit of forethought to determine whether the application of\nsetitem\n on the chain\ncauses a net increase in the size of the map (that is, whether the given key is new).\n1classChainHashMap(HashMapBase):\n2\u201d\u201d\u201dHash map implemented with separate chaining for collision resolution.\u201d\u201d\u201d\n34def\nbucket\n getitem( self,j ,k ) :\n5 bucket = self.\ntable[j]\n6 ifbucket is None :\n7 raiseKeyError(\n Key Error:\n +r e p r ( k ) ) # no match found\n8 return bucket[k] # may raise KeyError\n9\n10def\nbucket\n setitem( self,j ,k ,v ) :\n11 if self.\ntable[j] is None :\n12 self.\ntable[j] = UnsortedTableMap( ) # bucket is new to the table\n13 oldsize = len( self.\ntable[j])\n14 self.\ntable[j][k] = v\n15 iflen(self.\ntable[j]) >oldsize: # key was new to the table\n16 self.\nn+ =1 # increase overall map size\n1718def\nbucket\n delitem( self,j ,k ) :\n19 bucket = self.\ntable[j]\n20 ifbucket is None :\n21 raiseKeyError(\n Key Error:\n +r e p r ( k ) ) # no match found\n22 delbucket[k] # may raise KeyError\n23\n24def\n iter\n(self):\n25 forbucket in self .\ntable:\n26 ifbucket is not None : # a nonempty slot\n27 forkeyinbucket:\n28 yieldkey\nCode Fragment 10.5: Concrete hash map class with separate chaining.", "10.2. Hash Tables 425\nLinear Probing\nOur implementation of a ProbeHashMap class, using open addressing with linear\nprobing, is given in Code Fragments 10.6 and 10.7. In order to support deletions,\nwe use a technique described in Section 10.2.2 in which we place a special marker\nin a table location at which an item has been deleted, so that we can distinguishbetween it and a location that has always been empty. In our implementation, wedeclare a class-level attribute,\nAVAIL , as a sentinel. (We use an instance of the\nbuilt-in object class because we do not care about any behaviors of the sentinel,\njust our ability to differentiate it from other objects.)\nThe most challenging aspect of open addressing is to properly trace the series\nof probes when collisions occur during an insertion or search for an item. To thisend, we de\ufb01ne a nonpublic utility,\n\ufb01nd\nslot, that searches for an item with key k\nin \u201cbucket\u201d j(that is, where jis the index returned by the hash function for key k).\n1classProbeHashMap(HashMapBase):\n2\u201d\u201d\u201dHash map implemented with linear probing for collision resolution.\u201d\u201d\u201d\n3\n AVAIL = object( ) # sentinal marks locations of previous deletions\n45def\nis\navailable( self,j ) :\n6 \u201d\u201d\u201dReturn True if index j is available in table.\u201d\u201d\u201d\n7 return self .\ntable[j] is None or self .\ntable[j] isProbeHashMap.\n AVAIL\n89def\n\ufb01nd\nslot(self,j ,k ) :\n10 \u201d\u201d\u201dSearch for key k in bucket at index j.\n1112 Return (success, index) tuple, described as follows:\n13 If match was found, success is True and index denotes its location.\n14 If no match found, success is False and index denotes \ufb01rst available slot.\n15 \u201d\u201d\u201d\n16 \ufb01rstAvail = None\n17 while True :\n18 if self.\nis\navailable(j):\n19 if\ufb01rstAvail is None :\n20 \ufb01rstAvail = j # mark this as \ufb01rst avail\n21 if self.\n table[j] is None :\n22 return (False,\ufb01 r s t A v a i l ) # search has failed\n23 elifk= =self.\ntable[j].\n key:\n24 return (True,j ) # found a match\n25 j=( j+1 )%l e n ( self.\ntable) # keep looking (cyclically)\nCode Fragment 10.6: Concrete ProbeHashMap class that uses linear probing for\ncollision resolution (continued in Code Fragment 10.7).", "426 Chapter 10. Maps, Hash Tables, and Skip Lists\n26def\nbucket\n getitem( self,j ,k ) :\n27 found, s = self.\n\ufb01nd\nslot(j, k)\n28 if not found:\n29 raiseKeyError(\n Key Error:\n +r e p r ( k ) ) # no match found\n30 return self .\ntable[s].\n value\n31\n32def\nbucket\n setitem( self,j ,k ,v ) :\n33 found, s = self.\n\ufb01nd\nslot(j, k)\n34 if not found:\n35 self.\ntable[s] = self.\nItem(k,v) # insert new item\n36 self.\nn+ =1 # size has increased\n37 else:\n38 self.\ntable[s].\n value = v # overwrite existing\n3940def\nbucket\n delitem( self,j ,k ) :\n41 found, s = self.\n\ufb01nd\nslot(j, k)\n42 if not found:\n43 raiseKeyError(\n Key Error:\n +r e p r ( k ) ) # no match found\n44 self.\ntable[s] = ProbeHashMap.\n AVAIL # mark as vacated\n4546def\niter\n(self):\n47 forjinrange(len( self.\ntable)): # scan entire table\n48 if not self .\nis\navailable(j):\n49 yield self .\ntable[j].\n key\nCode Fragment 10.7: Concrete ProbeHashMap class that uses linear probing for\ncollision resolution (continued from Code Fragment 10.6).\nThe three primary map operations each rely on the\n \ufb01nd\nslotutility. When at-\ntempting to retrieve the value associated with a given key, we must continue probinguntil we \ufb01nd the key, or until we reach a table slot with the None value. We cannot\nstop the search upon reaching an\nAVAIL sentinel, because it represents a location\nthat may have been \ufb01lled when the desired item was once inserted.\nWhen a key-value pair is being assigned in the map, we must attempt to \ufb01nd\nan existing item with the given key, so that we might overwrite its value, before\nadding a new item to the map. Therefore, we must search beyond any occurrences\nof the\n AVAIL sentinel when inserting. However, if no match is found, we prefer to\nrepurpose the \ufb01rst slot marked with\n AVAIL , if any, when placing the new element\nin the table. The\n \ufb01nd\nslotmethod enacts this logic, continuing the search until a\ntruly empty slot, but returning the index of the \ufb01rst available slot for an insertion.\nWhen deleting an existing item within\n bucket\n delitem , we intentionally set\nthe table entry to the\n AVAIL sentinel in accordance with our strategy.", "10.3. Sorted Maps 427\n10.3 Sorted Maps\nThe traditional map ADT allows a user to look up the value associated with a given\nkey, but the search for that key is a form known as an exact search .\nFor example, computer systems often maintain information about events that\nhave occurred (such as \ufb01nancial transactions), organizing such events based upon\nwhat are known as time stamps . If we can assume that time stamps are unique\nfor a particular system, then we might organize a map with a time stamp servingas the key, and a record about the event that occurred at that time as the value. Aparticular time stamp could serve as a reference ID for an event, in which case wecan quickly retrieve information about that event from the map. However, the mapADT does not provide any way to get a list of all events ordered by the time at\nwhich they occur, or to search for which event occurred closest to a particular time.\nIn fact, the fast performance of hash-based implementations of the map ADT relieson the intentionally scattering of keys that may seem very \u201cnear\u201d to each other inthe original domain, so that they are more uniformly distributed in a hash table.\nIn this section, we introduce an extension known as the sorted map ADT that\nincludes all behaviors of the standard map, plus the following:\nM.\ufb01nd\nmin() :Return the (key,value) pair with minimum key(orNone , if map is empty).\nM.\ufb01nd\nmax() :Return the (key,value) pair with maximum key\n(orNone , if map is empty).\nM.\ufb01nd\n lt(k) :Return the (key,value) pair with the greatest key that\nis strictly less than k(orNone ,i fn os u c hi t e me x i s t s ) .\nM.\ufb01nd\n le(k) :Return the (key,value) pair with the greatest key thatis less than or equal to k(orNone ,i fn os u c hi t e m\nexists).\nM.\ufb01nd\ngt(k) :Return the (key,value) pair with the least key that isstrictly greater than k(orNone ,i fn os u c hi t e me x i s t s ) .\nM.\ufb01nd\nge(k) :Return the (key,value) pair with the least key that isgreater than or equal to k(orNone ,i fn os u c hi t e m ) .\nM.\ufb01nd\nrange(start, stop) :Iterate all (key,value) pairs with start<=k e y <stop.\nIfstart isNone , iteration begins with minimum key; if\nstop isNone , iteration concludes with maximum key.\niter(M) :Iterate all keys of the map according to their natural\norder, from smallest to largest.\nreversed(M) :Iterate all keys of the map in reverse order; in Python,\nthis is implemented with the\n reversed\n method.", "428 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.3.1 Sorted Search Tables\nSeveral data structures can ef\ufb01ciently support the sorted map ADT, and we will\nexamine some advanced techniques in Section 10.4 and Chapter 11. In this section,we begin by exploring a simple implementation of a sorted map. We store themap\u2019s items in an array-based sequence Aso that they are in increasing order of\ntheir keys, assuming the keys have a naturally de\ufb01ned order. (See Figure 10.8.) We\nrefer to this implementation of a map as a sorted search table .\n9 2 4 5 7 8 12 14 17 19 22 25 27 28 335\n3701234 6789 1 0 1 1 1 2 1 3 1 4 1 5\nFigure 10.8: Realization of a map by means of a sorted search table. We show only\nthe keys for this map, so as to highlight their ordering.\nAs was the case with the unsorted table map of Section 10.1.5, the sorted search\ntable has a space requirement that is O(n), assuming we grow and shrink the array\nto keep its size proportional to the number of items in the map. The primary advan-\ntage of this representation, and our reason for insisting that Abe array-based, is that\nit allows us to use the binary search algorithm for a variety of ef\ufb01cient operations.\nBinary Search and Inexact Searches\nWe originally presented the binary search algorithm in Section 4.1.3, as a means for\ndetecting whether a given target is stored within a sorted sequence. In our original\npresentation (Code Fragment 4.3 on page 156), a binary\n search function returned\nTrue ofFalse to designate whether the desired target was found. While such an\napproach could be used to implement the\n contains\n method of the map ADT,\nwe can adapt the binary search algorithm to provide far more useful information\nwhen performing forms of inexact search in support of the sorted map ADT.\nThe important realization is that while performing a binary search, we can de-\ntermine the index at or near where a target might be found. During a successful\nsearch, the standard implementation determines the precise index at which the tar-\nget is found. During an unsuccessful search, although the target is not found, the\nalgorithm will effectively determine a pair of indices designating elements of thecollection that are just less than or just greater than the missing target.\nAs a motivating example, our original simulation from Figure 4.5 on page 156\nshows a successful binary search for a target of 22, using the same data we portrayin Figure 10.8. Had we instead been searching for 21, the \ufb01rst four steps of thealgorithm would be the same. The subsequent difference is that we would make anadditional call with inverted parameters high=9 andlow=10 , effectively conclud-\ning that the missing target lies in the gap between values 19 and 22 in that example.", "10.3. Sorted Maps 429\nImplementation\nIn Code Fragments 10.8 through 10.10, we present a complete implementation of a\nclass,SortedTableMap , that supports the sorted map ADT. The most notable fea-\nture of our design is the inclusion of a\n \ufb01nd\nindex utility function. This method\nusing the binary search algorithm, but by convention returns the index of the left-\nmost item in the search interval having key greater than or equal to k. Therefore, if\nthe key is present, it will return the index of the item having that key. (Recall thatkeys are unique in a map.) When the key is missing, the function returns the index\nof the item in the search interval that is just beyond where the key would have beenlocated. As a technicality, the method returns index high +1 to indicate that no\nitems of the interval had a key greater than k.\nWe rely on this utility method when implementing the traditional map opera-\ntions and the new sorted map operations. The body of each of the\ngetitem\n ,\nsetitem\n ,a n d\n delitem\n methods begins with a call to\n \ufb01nd\nindex to deter-\nmine a candidate index at which a matching key might be found. For\n getitem\n ,\nwe simply check whether that is a valid index containing the target to determine theresult. For\nsetitem\n , recall that the goal is to replace the value of an existing\nitem, if one with key kis found, but otherwise to insert a new item into the map. The\nindex returned by\n \ufb01nd\nindex will be the index of the match, if one exists, or oth-\nerwise the exact index at which the new item should be inserted. For\n delitem\n ,\nwe again rely on the convenience of\n \ufb01nd\nindex to determine the location of the\nitem to be popped, if any.\nOur\n\ufb01nd\nindex utility is equally valuable when implementing the various in-\nexact search methods given in Code Fragment 10.10. For each of the methods\ufb01nd\nlt,\ufb01nd\nle,\ufb01nd\ngt,a n d\ufb01nd\nge, we begin with a call to\n \ufb01nd\nindex utility,\nwhich locates the \ufb01rst index at which there is an element with key \u2265k,i fa n y .T h i s\nis precisely what we want for \ufb01nd\nge, if valid, and just beyond the index we want\nfor\ufb01nd\nlt.F o r\ufb01nd\ngtand\ufb01nd\nlewe need some extra case analysis to distinguish\nwhether the indicated index has a key equal to k. For example, if the indicated\nitem has a matching key, our \ufb01nd\ngtimplementation increments the index before\ncontinuing with the process. (We omit the implementation of \ufb01nd\nle, for brevity.)\nIn all cases, we must properly handle boundary cases, reporting None when unable\nto \ufb01nd a key with the desired property.\nOur strategy for implementing \ufb01nd\nrange is to use the\n \ufb01nd\nindex utility to\nlocate the \ufb01rst item with key \u2265start (assuming start is notNone ). With that knowl-\nedge, we use a while loop to sequentially report items until reaching one that hasa key greater than or equal to the stopping value (or until reaching the end of thetable). It is worth noting that the while loop may trivially iterate zero items if the\ufb01rst key that is greater than or equal to start also happens to be greater than or equal\ntostop. This represents an empty range in the map.", "430 Chapter 10. Maps, Hash Tables, and Skip Lists\n1classSortedTableMap(MapBase):\n2\u201d\u201d\u201dMap implementation using a sorted table.\u201d\u201d\u201d\n3\n4#----------------------------- nonpublic behaviors -----------------------------\n5def\n\ufb01nd\nindex(self,k ,l o w ,h i g h ) :\n6 \u201d\u201d\u201dReturn index of the leftmost item with key greater than or equal to k.\n78 Return high + 1 if no such item quali\ufb01es.\n9\n10 That is, j will be returned such that:\n11 all items of slice table[low:j] have key <k\n12 all items of slice table[j:high+1] have key >=k\n13 \u201d\u201d\u201d\n14 ifhigh<low:\n15 return high + 1 # no element quali\ufb01es\n16 else:\n17 mid = (low + high) // 2\n18 ifk= =self.\ntable[mid].\n key:\n19 return mid # found exact match\n20 elifk<self.\ntable[mid].\n key:\n21 return self .\n\ufb01nd\nindex(k, low, mid \u22121) # Note: may return mid\n22 else:\n23 return self .\n\ufb01nd\nindex(k, mid + 1, high) # answer is right of mid\n2425 #----------------------------- public behaviors -----------------------------\n26def\ninit\n(self):\n27 \u201d\u201d\u201dCreate an empty map.\u201d\u201d\u201d\n28 self.\ntable = [ ]\n2930def\nlen\n(self):\n31 \u201d\u201d\u201dReturn number of items in the map.\u201d\u201d\u201d\n32 return len(self.\ntable)\n3334def\ngetitem\n (self,k ) :\n35 \u201d\u201d\u201dReturn value associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n36 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121)\n37 ifj= =l e n ( self.\ntable) or self .\ntable[j].\n key != k:\n38 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n39 return self .\ntable[j].\n value\nCode Fragment 10.8: An implementation of a SortedTableMap class (continued in\nCode Fragments 10.9 and 10.10).", "10.3. Sorted Maps 431\n40def\n setitem\n (self,k ,v ) :\n41 \u201d\u201d\u201dAssign value v to key k, overwriti ng existing value if present.\u201d\u201d\u201d\n42 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121)\n43 ifj<len(self.\ntable) and self .\ntable[j].\n key == k:\n44 self.\ntable[j].\n value = v # reassign value\n45 else:\n46 self.\ntable.insert(j, self.\nItem(k,v)) # adds new item\n47\n48def\n delitem\n (self,k ) :\n49 \u201d\u201d\u201dRemove item associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n50 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121)\n51 ifj= =l e n ( self.\ntable) or self .\ntable[j].\n key != k:\n52 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n53 self.\ntable.pop(j) # delete item\n5455def\niter\n(self):\n56 \u201d\u201d\u201dGenerate keys of the map ordered from minimum to maximum.\u201d\u201d\u201d\n57 foritemin self .\ntable:\n58 yielditem.\n key\n5960def\nreversed\n (self):\n61 \u201d\u201d\u201dGenerate keys of the map ordered from maximum to minimum.\u201d\u201d\u201d\n62 foriteminreversed( self.\ntable):\n63 yielditem.\n key\n64\n65def\ufb01nd\nmin(self):\n66 \u201d\u201d\u201dReturn (key,value) pair with minimum key (or None if empty).\u201d\u201d\u201d\n67 iflen(self.\ntable) >0:\n68 return (self.\ntable[0].\n key,self.\ntable[0].\n value)\n69 else:\n70 return None\n71\n72def\ufb01nd\nmax(self):\n73 \u201d\u201d\u201dReturn (key,value) pair with maximum key (or None if empty).\u201d\u201d\u201d\n74 iflen(self.\ntable) >0:\n75 return (self.\ntable[\u22121].\nkey,self.\ntable[\u22121].\nvalue)\n76 else:\n77 return None\nCode Fragment 10.9: An implementation of a SortedTableMap class (together with\nCode Fragments 10.8 and 10.10).", "432 Chapter 10. Maps, Hash Tables, and Skip Lists\n78def\ufb01nd\nge(self,k ) :\n79 \u201d\u201d\u201dReturn (key,value) pair with least key greater than or equal to k.\u201d\u201d\u201d\n80 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121) #j\nsk e y>=k\n81 ifj<len(self.\ntable):\n82 return (self.\ntable[j].\n key,self.\ntable[j].\n value)\n83 else:\n84 return None\n85\n86def\ufb01nd\nlt(self,k ) :\n87 \u201d\u201d\u201dReturn (key,value) pair with greatest key strictly less than k.\u201d\u201d\u201d\n88 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121) #j\nsk e y>=k\n89 ifj>0:\n90 return (self.\ntable[j \u22121].\nkey,self.\ntable[j \u22121].\nvalue) # Note use of j-1\n91 else:\n92 return None\n9394def\ufb01nd\ngt(self,k ) :\n95 \u201d\u201d\u201dReturn (key,value) pair with least key strictly greater than k.\u201d\u201d\u201d\n96 j=self.\n\ufb01nd\nindex(k, 0, len( self.\ntable) \u22121) #j\nsk e y>=k\n97 ifj<len(self.\ntable) and self .\ntable[j].\n key == k:\n98 j+ =1 # advanced past match\n99 ifj<len(self.\ntable):\n100 return (self.\ntable[j].\n key,self.\ntable[j].\n value)\n101 else:\n102 return None\n103104 def\ufb01nd\nrange(self, start, stop):\n105 \u201d\u201d\u201dIterate all (key,value) pairs such that start <=k e y <stop.\n106107 If start is None, iteration begins with minimum key of map.\n108 If stop is None, iteration continues through the maximum key of map.\n109 \u201d\u201d\u201d\n110 ifstartis None :\n111 j=0\n112 else:\n113 j=self.\n\ufb01nd\nindex(start, 0, len( self.\ntable)\u22121) # \ufb01nd \ufb01rst result\n114 while j<len(self.\ntable) and(stopis None or self .\ntable[j].\n key<stop):\n115 yield(self.\ntable[j].\n key,self.\ntable[j].\n value)\n116 j+ =1\nCode Fragment 10.10: An implementation of a SortedTableMap class (continued\nfrom Code Fragments 10.9 and 10.10). We omit the \ufb01nd\nlemethod due to space.", "10.3. Sorted Maps 433\nAnalysis\nWe conclude by analyzing the performance of our SortedTableMap implementa-\ntion. A summary of the running times for all methods of the sorted map ADT\n(including the traditional map operations) is given in Table 10.3. It should be clear\nthat the\n len\n ,\ufb01nd\nmin,a n d\ufb01nd\nmax methods run in O(1)time, and that iter-\nating the keys of the table in either direction can be peformed in O(n)time.\nThe analysis for the various forms of search all depend on the fact that a binary\nsearch on a table with nentries runs in O(logn)time. This claim was originally\nshown as Proposition 4.2 in Section 4.2, and that analysis clearly applies to our\n\ufb01nd\nindex method as well. We therefore claim an O(logn)worst-case running\ntime for methods\n getitem\n ,\ufb01nd\nlt,\ufb01nd\ngt,\ufb01nd\nle,a n d\ufb01nd\nge. Each of these\nmakes a single call to\n \ufb01nd\nindex , followed by a constant number of additional\nsteps to determine the appropriate answer based on the index. The analysis of\ufb01nd\nrange is a bit more interesting. It begins with a binary search to \ufb01nd the \ufb01rst\nitem within the range (if any). After that, it executes a loop that takes O(1)time per\niteration to report subsequent values until reaching the end of the range. If there are\nsitems reported in the range, the total running time is O(s+logn).\nIn contrast to the ef\ufb01cient search operations, update operations for a sorted table\nmay take considerable time. Although binary search can help identify the index at\nwhich an update occurs, both insertions and deletions require, in the worst case, thatlinearly many existing elements be shifted in order to maintain the sorted order of\nthe table. Speci\ufb01cally, the potential call to\ntable.insert from within\n setitem\nand\ntable.pop from within\n delitem\n lead to O(n)worst-case time. (See the\ndiscussion of corresponding operations of the listclass in Section 5.4.1.)\nIn conclusion, sorted tables are primarily used in situations where we expect\nmany searches but relatively few updates.\nOperation\n Running Time\nlen(M)\n O(1)\nki nM\n O(logn)\nM[k] = v\n O(n)worst case; O(logn)if existing k\ndel M[k]\n O(n)worst case\nM.\ufb01nd\n min() ,M.\ufb01nd\n max()\n O(1)\nM.\ufb01nd\n lt(k),M.\ufb01nd\n gt(k)\nO(logn)\nM.\ufb01nd\n le(k) ,M.\ufb01nd\n ge(k)\nM.\ufb01nd\n range(start, stop)\n O(s+logn)where sitems are reported\niter(M) ,reversed(M)\n O(n)\nTable 10.3: Performance of a sorted map, as implemented with SortedTableMap .\nWe use nto denote the number of items in the map at the time the operation is\nperformed. The space requirement is O(n).", "434 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.3.2 Two Applications of Sorted Maps\nIn this section, we explore applications in which there is particular advantage to\nusing a sorted map rather than a traditional (unsorted) map. To apply a sorted\nmap, keys must come from a domain that is totally ordered. Furthermore, to take\nadvantage of the inexact or range searches afforded by a sorted map, there should\nbe some reason why nearby keys have relevance to a search.\nFlight Databases\nThere are several Web sites on the Internet that allow users to perform queries on\ufb02ight databases to \ufb01nd \ufb02ights between various cities, typically with the intent to\nbuy a ticket. To make a query, a user speci\ufb01es origin and destination cities, a depar-\nture date, and a departure time. To support such queries, we can model the \ufb02ightdatabase as a map, where keys are Flight objects that contain \ufb01elds corresponding\nto these four parameters. That is, a key is a tuple\nk=(origin,destination ,date,time ).\nAdditional information about a \ufb02ight, such as the \ufb02ight number, the number of seatsstill available in \ufb01rst (F) and coach (Y) class, the \ufb02ight duration, and the fare, can\nbe stored in the value object.\nFinding a requested \ufb02ight is not simply a matter of \ufb01nding an exact match\nfor a requested query. Although a user typically wants to exactly match the ori-\ngin and destination cities, he or she may have \ufb02exibility for the departure date,and certainly will have some \ufb02exibility for the departure time on a speci\ufb01c day.\nWe can handle such a query by ordering our keys lexicographically. Then, an ef-\n\ufb01cient implementation for a sorted map would be a good way to satisfy users\u2019queries. For instance, given a user query key k, we could call \ufb01nd\nge(k) to return\nthe \ufb01rst \ufb02ight between the desired cities, having a departure date and time match-ing the desired query or later. Better yet, with well-constructed keys, we could\nuse\ufb01nd\nrange(k1, k2) to \ufb01nd all \ufb02ights within a given range of times. For exam-\nple, if k1=(ORD, PVD, 05May, 09:30 ),a n d k2=(ORD, PVD, 05May, 20:00 ),\na respective call to \ufb01nd\nrange(k1, k2) might result in the following sequence of\nkey-value pairs:\n(ORD, PVD, 05May, 09:53 ):( AA 1840, F5, Y15, 02:05,\n 251),\n(ORD, PVD, 05May, 13:29 ):( AA 600, F2, Y0, 02:16,\n 713),\n(ORD, PVD, 05May, 17:39 ):( AA 416, F3, Y9, 02:09,\n 365),\n(ORD, PVD, 05May, 19:50 ):( AA 1828, F9, Y25, 02:13,\n 186)", "10.3. Sorted Maps 435\nMaxima Sets\nLife is full of trade-offs. We often have to trade off a desired performance measure\nagainst a corresponding cost. Suppose, for the sake of an example, we are interested\nin maintaining a database rating automobiles by their maximum speeds and their\ncost. We would like to allow someone with a certain amount of money to query ourdatabase to \ufb01nd the fastest car they can possibly afford.\nWe can model such a trade-off problem as this by using a key-value pair to\nmodel the two parameters that we are trading off, which in this case would be thepair (cost,speed )for each car. Notice that some cars are strictly better than other\ncars using this measure. For example, a car with cost-speed pair (20000,100 )is\nstrictly better than a car with cost-speed pair (30000,90). At the same time, there\nare some cars that are not strictly dominated by another car. For example, a car withcost-speed pair (20000,100 )may be better or worse than a car with cost-speed pair\n(30000,120 ), depending on how much money we have to spend. (See Figure 10.9.)\nFigure 10.9: Illustrating the cost-performance trade-off with pairs represented by\npoints in the plane. Notice that point pis strictly better than points c,d,a n d e,b u t\nmay be better or worse than points a,b,f,g,a n d h, depending on the price we are\nwilling to pay. Thus, if we were to add pto our set, we could remove the points c,\nd,a n d e, but not the others.\nFormally, we say a cost-performance pair (a,b)dominates pair (c,d)/negationslash=(a,b)\nifa\u2264candb\u2265d, that is, if the \ufb01rst pair has no greater cost and at least as good\nperformance. A pair (a,b)is called a maximum pair if it is not dominated by any\nother pair. We are interested in maintaining the set of maxima of a collection ofcost-performance pairs. That is, we would like to add new pairs to this collection(for example, when a new car is introduced), and to query this collection for a given\ndollar amount, d, to \ufb01nd the fastest car that costs no more than ddollars.Performance\nCostdfh\napg\nbe\nc", "436 Chapter 10. Maps, Hash Tables, and Skip Lists\nMaintaining a Maxima Set with a Sorted Map\nWe can store the set of maxima pairs in a sorted map, M, so that the cost is the\nkey \ufb01eld and performance (speed) is the value \ufb01eld. We can then implement opera-\ntionsadd (c,p), which adds a new cost-performance pair (c,p),a n dbest (c),w h i c h\nreturns the best pair with cost at most c, as shown in Code Fragment 10.11.\n1classCostPerformanceDatabase:\n2\u201d\u201d\u201dMaintain a database of maximal (cost,performance) pairs.\u201d\u201d\u201d\n3\n4def\n init\n(self):\n5 \u201d\u201d\u201dCreate an empty database.\u201d\u201d\u201d\n6 self.\nM = SortedTableMap( ) # or a more e\ufb03cient sorted map\n78defbest(self,c ) :\n9 \u201d\u201d\u201dReturn (cost,performance) pair with largest cost not exceeding c.\n1011 Return None if there is no such pair.\n12 \u201d\u201d\u201d\n13 return self .\nM.\ufb01nd\n le(c)\n1415defadd(self,c ,p ) :\n16 \u201d\u201d\u201dAdd new entry with cost c and performance p.\u201d\u201d\u201d\n17 # determine if (c,p) is dominated by an existing pair\n18 other = self.\nM.\ufb01nd\n le(c) # other is at least as cheap as c\n19 ifotheris not None and other[1] >=p :# if its performance is as good,\n20 return # (c,p) is dominated, so ignore\n21 self.\nM[c] = p # else, add (c,p) to database\n22 # and now remove any pairs that are dominated by (c,p)\n23 other = self.\nM.\ufb01nd\n gt(c) # other more expensive than c\n24 while otheris not None and other[1] <=p :\n25 del self .\nM[other[0]]\n26 other = self.\nM.\ufb01nd\n gt(c)\nCode Fragment 10.11: An implementation of a class maintaining a set of maxima\ncost-performance pairs using a sorted map.\nUnfortunately, if we implement Musing the SortedTableMap ,t h eaddbehavior\nhas O(n)worst-case running time. If, on the other hand, we implement Musing\na skip list, which we next describe, we can perform best (c)queries in O(logn)\nexpected time and add (c,p)updates in O((1+r)logn)expected time, where ris\nthe number of points removed.", "10.4. Skip Lists 437\n10.4 Skip Lists\nAn interesting data structure for realizing the sorted map ADT is the skip list .I n\nSection 10.3.1, we saw that a sorted array will allow O(logn)-time searches via the\nbinary search algorithm. Unfortunately, update operations on a sorted array have\nO(n)worst-case running time because of the need to shift elements. In Chapter 7\nwe demonstrated that linked lists support very ef\ufb01cient update operations, as long\nas the position within the list is identi\ufb01ed. Unfortunately, we cannot perform fastsearches on a standard linked list; for example, the binary search algorithm requires\nan ef\ufb01cient means for direct accessing an element of a sequence by index.\nSkip lists provide a clever compromise to ef\ufb01ciently support search and update\noperations. A skip list Sfor a map Mconsists of a series of lists {S\n0,S1,..., Sh}.\nEach list Sistores a subset of the items of Msorted by increasing keys, plus items\nwith two sentinel keys denoted \u2212\u221e and +\u221e,w h e r e \u2212\u221e is smaller than every\npossible key that can be inserted in Mand +\u221eis larger than every possible key\nthat can be inserted in M. In addition, the lists in Ssatisfy the following:\n\u2022List S0contains every item of the map M(plus sentinels \u2212\u221e and +\u221e).\n\u2022For i=1,..., h\u22121, list Sicontains (in addition to \u2212\u221e and +\u221e) a randomly\ngenerated subset of the items in list Si\u22121.\n\u2022List Shcontains only \u2212\u221e and +\u221e.\nAn example of a skip list is shown in Figure 10.10. It is customary to visualize a\nskip list Swith list S0at the bottom and lists S1,..., Shabove it. Also, we refer to h\nas the height of skip list S.\nIntuitively, the lists are set up so that Si+1contains more or less alternate items\nofSi. As we shall see in the details of the insertion method, the items in Si+1are\nchosen at random from the items in Siby picking each item from Sito also be in\nSi+1with probability 1 /2. That is, in essence, we \u201c\ufb02ip a coin\u201d for each item in Si\n3125\n25\n-\u221e-\u221e-\u221e-\u221e\n-\u221e\n-\u221e1717\n17\n17 12S5\nS4\nS3\nS2\nS1\nS055\n55\n55\n55 12 17 20 25 31 38 39 44 50 +\u221e+\u221e+\u221e+\u221e+\u221e+\u221e\n44 383125\nFigure 10.10: Example of a skip list storing 10 items. For simplicity, we show only\nthe items\u2019 keys, not their associated values.", "438 Chapter 10. Maps, Hash Tables, and Skip Lists\nand place that item in Si+1if the coin comes up \u201cheads.\u201d Thus, we expect S1to have\nabout n/2 items, S2to have about n/4 items, and, in general, Sito have about n/2i\nitems. In other words, we expect the height hofSto be about log n. The halving of\nthe number of items from one list to the next is not enforced as an explicit property\nof skip lists, however. Instead, randomization is used.\nFunctions that generate numbers that can be viewed as random numbers are\nbuilt into most modern computers, because they are used extensively in computergames, cryptography, and computer simulations, Some functions, called pseudo-\nrandom number generators , generate random-like numbers, starting with an initial\nseed. (See discusion of random module in Section 1.11.1.) Other methods use\nhardware devices to extract \u201ctrue\u201d random numbers from nature. In any case, wewill assume that our computer has access to numbers that are suf\ufb01ciently randomfor our analysis.\nThe main advantage of using randomization in data structure and algorithm\ndesign is that the structures and functions that result are usually simple and ef\ufb01cient.\nThe skip list has the same logarithmic time bounds for searching as is achieved by\nthe binary search algorithm, yet it extends that performance to update methodswhen inserting or deleting items. Nevertheless, the bounds are expected for the\nskip list, while binary search has a worst-case bound with a sorted table.\nA skip list makes random choices in arranging its structure in such a way that\nsearch and update times are O(logn)on average ,w h e r e nis the number of items\nin the map. Interestingly, the notion of average time complexity used here does notdepend on the probability distribution of the keys in the input. Instead, it dependson the use of a random-number generator in the implementation of the insertions\nto help decide where to place the new item. The running time is averaged over all\npossible outcomes of the random numbers used when inserting entries.\nUsing the position abstraction used for lists and trees, we view a skip list as a\ntwo-dimensional collection of positions arranged horizontally into levels and ver-\ntically into towers . Each level is a list S\niand each tower contains positions storing\nthe same item across consecutive lists. The positions in a skip list can be traversedusing the following operations:\nnext(p) :Return the position following pon the same level.\nprev(p) :Return the position preceding pon the same level.\nbelow(p) :Return the position below pin the same tower.\nabove(p) :Return the position above pin the same tower.\nWe conventionally assume that the above operations return None if the position\nrequested does not exist. Without going into the details, we note that we can eas-ily implement a skip list by means of a linked structure such that the individualtraversal methods each take O(1)time, given a skip-list position p. Such a linked\nstructure is essentially a collection of hdoubly linked lists aligned at towers, which\nare also doubly linked lists.", "10.4. Skip Lists 439\n10.4.1 Search and Update Operations in a Skip List\nThe skip-list structure affords simple map search and update algorithms. In fact,\nall of the skip-list search and update algorithms are based on an elegant SkipSearch\nmethod that takes a key kand \ufb01nds the position pof the item in list S0that has the\nlargest key less than or equal to k(which is possibly \u2212\u221e).\nSearching in a Skip List\nSuppose we are given a search key k. We begin the SkipSearch method by setting\na position variable pto the topmost, left position in the skip list S, called the start\nposition ofS. That is, the start position is the position of Shstoring the special\nentry with key \u2212\u221e. We then perform the following steps (see Figure 10.11), where\nkey (p)denotes the key of the item at position p:\n1. If S.below (p)isNone , then the search terminates\u2014we are at the bottom and\nhave located the item in Swith the largest key less than or equal to the search\nkey k. Otherwise, we drop down to the next lower level in the present tower\nby setting p=S.below (p).\n2. Starting at position p, we move pforward until it is at the rightmost position\non the present level such that key (p)\u2264k. We call this the scan forward step.\nNote that such a position always exists, since each level contains the keys+\u221eand\u2212\u221e. It may be that premains where it started after we perform\nsuch a forward scan for this level.\n3. Return to step 1.\n55S1S2S3S4S5\n+\u221e\n+\u221e+\u221e\n+\u221e\n+\u221e+\u221e\n-\u221e-\u221e\n-\u221e 1212 -\u221e17\n17 25\n25 20 17 31 38 39-\u221e\n-\u221e 1717 25\n25 31\n31 38 44\n44 50555555\nS0\nFigure 10.11: Example of a search in a skip list. The positions examined when\nsearching for key 50 are highlighted.\nWe give a pseudo-code description of the skip-list search algorithm, SkipSearch ,\nin Code Fragment 10.12. Given this method, the map operation M[k]is performed\nby computing p=SkipSearch (k)and testing whether or not key (p)= k. If these\ntwo keys are equal, we return the associated value; otherwise, we raise a KeyError .", "440 Chapter 10. Maps, Hash Tables, and Skip Lists\nAlgorithm SkipSearch(k) :\nInput: A search key k\nOutput: Position pin the bottom list S0with the largest key such that key (p)\u2264k\np=start {begin at start position }\nwhile below (p)/negationslash=None do\np=below (p) {drop down }\nwhile k\u2265key (next (p))do\np=next(p) {scan forward }\nreturn p.\nCode Fragment 10.12: Algorthm to search a skip list Sfor key k.\nAs it turns out, the expected running time of algorithm SkipSearch on a skip list\nwith nentries is O(logn). We postpone the justi\ufb01cation of this fact, however, until\nafter we discuss the implementation of the update methods for skip lists. Navigation\nstarting at the position identi\ufb01ed by SkipSearch(k) can be easily used to provide the\nadditional forms of searches in the sorted map ADT (e.g., \ufb01nd\ngt,\ufb01nd\nrange ).\nI n s e r t i o ni naS k i pL i s t\nThe execution of the map operation M[k] = v begins with a call to SkipSearch (k).\nThis gives us the position pof the bottom-level item with the largest key less than or\nequal to k(note that pmay hold the special item with key \u2212\u221e). Ifkey (p)= k,t h e\nassociated value is overwritten with v. Otherwise, we need to create a new tower for\nitem (k,v). We insert (k,v)immediately after position pwithin S0. After inserting\nthe new item at the bottom level, we use randomization to decide the height of the\ntower for the new item. We \u201c\ufb02ip\u201d a coin, and if the \ufb02ip comes up tails, then we stop\nhere. Else (the \ufb02ip comes up heads), we backtrack to the previous (next higher)\nlevel and insert (k,v)in this level at the appropriate position. We again \ufb02ip a coin;\nif it comes up heads, we go to the next higher level and repeat. Thus, we continueto insert the new item (k,v)in lists until we \ufb01nally get a \ufb02ip that comes up tails.\nWe link together all the references to the new item (k,v)created in this process to\ncreate its tower. A coin \ufb02ip can be simulated with Python\u2019s built-in pseudo-randomnumber generator from the random module by calling randrange(2), which returns\n0 or 1, each with probability 1 /2.\nWe give the insertion algorithm for a skip list Sin Code Fragment 10.13 and\nwe illustrate it in Figure 10.12. The algorithm uses an insertAfterAbove (p,q,(k,v))\nmethod that inserts a position storing the item (k,v)after position p(on the same\nlevel as p) and above position q, returning the new position r(and setting internal\nreferences so that next,prev,above ,a n dbelow methods will work correctly for p,\nq,a n d r). The expected running time of the insertion algorithm on a skip list with\nnentries is O(logn), which we show in Section 10.4.2.", "10.4. Skip Lists 441\nAlgorithm SkipInsert(k,v) :\nInput: Keykand value v\nOutput: Topmost position of the item inserted in the skip list\np=SkipSearch (k)\nq=None {qwill represent top node in new item\u2019s tower }\ni=\u22121\nrepeat\ni=i+1\nifi\u2265hthen\nh=h+1 {add a new level to the skip list }\nt=next(s)\ns=insertAfterAbove (None,s,(\u2212\u221e,None )) {grow leftmost tower }\ninsertAfterAbove (s,t,(+\u221e,None )) {grow rightmost tower }\nwhile above (p)isNone do\np=prev (p) {scan backward }\np=ab ove (p) {jump up to higher level }\nq=insertAfterAbove (p,q,(k,v)){increase height of new item\u2019s tower }\nuntilcoinFlip ()==tails\nn=n+1\nreturn q\nCode Fragment 10.13: Insertion in a skip list. Method coinFlip ()returns \u201cheads\u201d or\n\u201ctails\u201d, each with probability 1 /2. Instance variables n,h,a n d shold the number\nof entries, the height, and the start node of the skip list.\n55 S1S2S3S4S5\n+\u221e\n+\u221e+\u221e\n+\u221e\n+\u221e+\u221e\n-\u221e-\u221e\n-\u221e 1212 -\u221e17\n17 25\n25 20 17 31-\u221e\n-\u221e 1717 25\n25 31\n31 38 44\n44424242\n5555\n55 38 39 42 50 S0\nFigure 10.12: Insertion of an entry with key 42 into the skip list of Figure 10.10. We\nassume that the random \u201ccoin \ufb02ips\u201d for the new entry came up heads three times in a\nrow, followed by tails. The positions visited are highlighted. The positions insertedto hold the new entry are drawn with thick lines, and the positions preceding themare \ufb02agged.", "442 Chapter 10. Maps, Hash Tables, and Skip Lists\nRemoval in a Skip List\nLike the search and insertion algorithms, the removal algorithm for a skip list is\nquite simple. In fact, it is even easier than the insertion algorithm. That is, to per-\nform the map operation del M[k] we begin by executing method SkipSearch (k).\nIf the position pstores an entry with key different from k, we raise a KeyError .\nOtherwise, we remove pand all the positions above p, which are easily accessed\nby using above operations to climb up the tower of this entry in Sstarting at posi-\ntion p. While removing levels of the tower, we reestablish links between the hor-\nizontal neighbors of each removed position. The removal algorithm is illustratedin Figure 10.13 and a detailed description of it is left as an exercise (R-10.24). Aswe show in the next subsection, deletion operation in a skip list with nentries has\nO(logn)expected running time.\nBefore we give this analysis, however, there are some minor improvements to\nthe skip-list data structure we would like to discuss. First, we do not actually needto store references to values at the levels of the skip list above the bottom level,because all that is needed at these levels are references to keys. In fact, we can\nmore ef\ufb01ciently represent a tower as a single object, storing the key-value pair,\nand maintaining jprevious references and jnext references if the tower reaches\nlevel S\nj. Second, for the horizontal axes, it is possible to keep the list singly linked,\nstoring only the next references. We can perform insertions and removals in strictlya top-down, scan-forward fashion. We explore the details of this optimization in\nExercise C-10.44. Neither of these optimizations improve the asymptotic perfor-\nmance of skip lists by more than a constant factor, but these improvements can,nevertheless, be meaningful in practice. In fact, experimental evidence suggeststhat optimized skip lists are faster in practice than AVL trees and other balanced\nsearch trees, which are discussed in Chapter 11.\n31S5\nS4\nS3\nS2\nS1-\u221e-\u221e\n-\u221e 1212 -\u221e\n1717 25\n25 31\n3142\n55 5055+\u221e\n+\u221e+\u221e\n+\u221e\n+\u221e-\u221e-\u221e\n17\n38\n38 39 424242\n44\n445555+\u221e\n17\n17\n20 2525\nS0\nFigure 10.13: Removal of the entry with key 25 from the skip list of Figure 10.12.\nThe positions visited after the search for the position of S0holding the entry are\nhighlighted. The positions removed are drawn with dashed lines.", "10.4. Skip Lists 443\nMaintaining the Topmost Level\nA skip list Smust maintain a reference to the start position (the topmost, left po-\nsition in S) as an instance variable, and must have a policy for any insertion that\nwishes to continue inserting a new entry past the top level of S.T h e r e a r e t w o\npossible courses of action we can take, both of which have their merits.\nOne possibility is to restrict the top level, h, to be kept at some \ufb01xed value that\nis a function of n, the number of entries currently in the map (from the analysis we\nwill see that h=max{10,2\u2308logn\u2309}is a reasonable choice, and picking h=3\u2308logn\u2309\nis even safer). Implementing this choice means that we must modify the insertion\nalgorithm to stop inserting a new position once we reach the topmost level (unless\n\u2308logn\u2309<\u2308log (n+1)\u2309, in which case we can now go at least one more level, since\nthe bound on the height is increasing).\nThe other possibility is to let an insertion continue inserting a new position as\nlong as heads keeps getting returned from the random number generator. This isthe approach taken by algorithm SkipInsert of Code Fragment 10.13. As we show\nin the analysis of skip lists, the probability that an insertion will go to a level that ismore than O(logn)is very low, so this design choice should also work.\nEither choice will still result in the expected O(logn)time to perform search,\ninsertion, and removal, however, which we show in the next section.\n10.4.2 Probabilistic Analysis of Skip Lists \u22c6\nAs we have shown above, skip lists provide a simple implementation of a sortedmap. In terms of worst-case performance, however, skip lists are not a superior datastructure. In fact, if we do not of\ufb01cially prevent an insertion from continuing signif-icantly past the current highest level, then the insertion algorithm can go into whatis almost an in\ufb01nite loop (it is not actually an in\ufb01nite loop, however, since the prob-\nability of having a fair coin repeatedly come up heads forever is 0). Moreover, we\ncannot in\ufb01nitely add positions to a list without eventually running out of memory.In any case, if we terminate position insertion at the highest level h, then the worst-\ncase running time for performing the\ngetitem\n ,\nsetitem\n ,a n d\n delitem\nmap operations in a skip list Swith nentries and height hisO(n+h). This worst-\ncase performance occurs when the tower of every entry reaches level h\u22121, where\nhis the height of S. However, this event has very low probability. Judging from\nthis worst case, we might conclude that the skip-list structure is strictly inferior tothe other map implementations discussed earlier in this chapter. But this would not\nbe a fair analysis, for this worst-case behavior is a gross overestimate.", "444 Chapter 10. Maps, Hash Tables, and Skip Lists\nBounding the Height of a Skip List\nBecause the insertion step involves randomization, a more accurate analysis of skip\nlists involves a bit of probability. At \ufb01rst, this might seem like a major undertaking,\nfor a complete and thorough probabilistic analysis could require deep mathemat-ics (and, indeed, there are several such deep analyses that have appeared in datastructures research literature). Fortunately, such an analysis is not necessary to un-\nderstand the expected asymptotic behavior of skip lists. The informal and intuitive\nprobabilistic analysis we give below uses only basic concepts of probability theory.\nLet us begin by determining the expected value of the height hof a skip list S\nwith nentries (assuming that we do not terminate insertions early). The probability\nthat a given entry has a tower of height i\u22651 is equal to the probability of getting i\nconsecutive heads when \ufb02ipping a coin, that is, this probability is 1 /2\ni. Hence, the\nprobability Pithat level ihas at least one position is at most\nPi\u2264n\n2i,\nfor the probability that any one of ndifferent events occurs is at most the sum of\nthe probabilities that each occurs.\nThe probability that the height hofSis larger than iis equal to the probability\nthat level ihas at least one position, that is, it is no more than Pi. This means that h\nis larger than, say, 3log nwith probability at most\nP3log n\u2264n\n23log n\n=n\nn3=1\nn2.\nFor example, if n=1000, this probability is a one-in-a-million long shot. More\ngenerally, given a constant c>1,his larger than clognwith probability at most\n1/nc\u22121. That is, the probability that his smaller than clognis at least 1 \u22121/nc\u22121.\nThus, with high probability, the height hofSisO(logn).\nAnalyzing Search Time in a Skip List\nNext, consider the running time of a search in skip list S, and recall that such a\nsearch involves two nested while loops. The inner loop performs a scan forward on\nal e v e lo f Sas long as the next key is no greater than the search key k, and the outer\nloop drops down to the next level and repeats the scan forward iteration. Since theheight hofSisO(logn)with high probability, the number of drop-down steps is\nO(logn)with high probability.", "10.4. Skip Lists 445\nSo we have yet to bound the number of scan-forward steps we make. Let nibe\nthe number of keys examined while scanning forward at level i. Observe that, after\nthe key at the starting position, each additional key examined in a scan-forward at\nlevel icannot also belong to level i+1. If any of these keys were on the previous\nlevel, we would have encountered them in the previous scan-forward step. Thus,the probability that any key is counted in n\niis 1/2. Therefore, the expected value of\nniis exactly equal to the expected number of times we must \ufb02ip a fair coin before\nit comes up heads. This expected value is 2. Hence, the expected amount of timespent scanning forward at any level iisO(1).S i n c e Shas O(logn)levels with high\nprobability, a search in Stakes expected time O(logn). By a similar analysis, we\ncan show that the expected running time of an insertion or a removal is O(logn).\nSpace Usage in a Skip List\nFinally, let us turn to the space requirement of a skip list Swith nentries. As we\nobserved above, the expected number of positions at level iisn/2i, which means\nthat the expected total number of positions in Sis\nh\n\u2211\ni=0n\n2i=nh\n\u2211\ni=01\n2i.\nUsing Proposition 3.5 on geometric summations, we have\nh\n\u2211\ni=01\n2i=/parenleftbig1\n2/parenrightbigh+1\u22121\n1\n2\u22121=2\u00b7/parenleftbigg\n1\u22121\n2h+1/parenrightbigg\n<2f o r a l l h\u22650.\nHence, the expected space requirement of SisO(n).\nTable 10.4 summarizes the performance of a sorted map realized by a skip list.\nOperation\n Running Time\nlen(M)\n O(1)\nkinM\n O(logn)expected\nM[k] = v\n O(logn)expected\ndel M[k]\n O(logn)expected\nM.\ufb01nd\n min() ,M.\ufb01nd\n max()\n O(1)\nM.\ufb01nd\n lt(k),M.\ufb01nd\n gt(k)\nO(logn)expected\nM.\ufb01nd\n le(k),M.\ufb01nd\n ge(k)\nM.\ufb01nd\n range(start, stop)\n O(s+logn)expected, with sitems reported\niter(M) ,reversed(M)\n O(n)\nTable 10.4: Performance of a sorted map implemented with a skip list. We use nto\ndenote the number of entries in the dictionary at the time the operation is performed.The expected space requirement is O(n).", "446 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.5 Sets, Multisets, and Multimaps\nWe conclude this chapter by examining several additional abstractions that are\nclosely related to the map ADT, and that can be implemented using data structuressimilar to those for a map.\n\u2022Asetis an unordered collection of elements, without duplicates, that typi-\ncally supports ef\ufb01cient membership tests. In essence, elements of a set arelike keys of a map, but without any auxiliary values.\n\u2022Amultiset (also known as a bag) is a set-like container that allows duplicates.\n\u2022Amultimap is similar to a traditional map, in that it associates values with\nkeys; however, in a multimap the same key can be mapped to multiple val-\nues. For example, the index of this book maps a given term to one or more\nlocations at which the term occurs elsewhere in the book.\n10.5.1 The Set ADT\nPython provides support for representing the mathematical notion of a set throughthe built-in classes frozenset andset, as originally discussed in Chapter 1, with\nfrozenset being an immutable form. Both of those classes are implemented using\nhash tables in Python.\nPython\u2019s collections module de\ufb01nes abstract base classes that essentially mirror\nthese built-in classes. Although the choice of names is counterintuitive, the abstractbase class collections.Set matches the concrete frozenset class, while the abstract\nbase class collections.MutableSet is akin to the concrete setclass.\nIn our own discussion, we equate the \u201cset ADT\u201d with the behavior of the built-\ninsetclass (and thus, the collections.MutableSet base class). We begin by listing\nwhat we consider to be the \ufb01ve most fundamental behaviors for a set S:\nS.add(e) :Add element eto the set. This has no effect if the set\nalready contains e.\nS.discard(e) :Remove element efrom the set, if present. This has no\neffect if the set does not contain e.\nei nS :Return True if the set contains element e. In Python, this\nis implemented with the special\ncontains\n method.\nlen(S) :Return the number of elements in set S. In Python, this\nis implemented with the special method\n len\n .\niter(S) :Generate an iteration of all elements of the set. In Python,this is implemented with the special method\niter\n .", "10.5. Sets, Multisets, and Multimaps 447\nIn the next section, we will see that the above \ufb01ve methods suf\ufb01ce for deriving\nall other behaviors of a set. Those remaining behaviors can be naturally grouped as\nfollows. We begin by describing the following additional operations for removingone or more elements from a set:\nS.remove(e) :Remove element efrom the set. If the set does not contain e,\nraise a KeyError .\nS.pop() :Remove and return an arbitrary element from the set. If theset is empty, raise a KeyError .\nS.clear() :Remove all elements from the set.\nThe next group of behaviors perform Boolean comparisons between two sets.\nS= =T :Return True if sets SandThave identical contents.\nS! =T :Return True if sets SandTare not equivalent.\nS<=T :Return True if setSis a subset of set T.\nS<T:Return True if setSis a proper subset of set T.\nS>=T :Return True if setSis a superset of set T.\nS>T:Return True if setSis a proper superset of set T.\nS.isdisjoint(T) :Return True if sets SandThave no common elements.\nFinally, there exists a variety of behaviors that either update an existing set, orcompute a new set instance, based on classical set theory operations.\nS|T:R\neturn a new set representing the union of sets SandT.\nS|=T :Update set Sto be the union of Sand set T.\nS&T :Return a new set representing the intersection of sets SandT.\nS& =T :Update set Sto be the intersection of Sand set T.\nS\u02c6T :Return a new set representing the symmetric difference of setsSandT, that is, a set of elements that are in precisely one of\nSorT.\nS\u02c6 =T :Update set Sto become the symmetric difference of itself and\nsetT.\nS\u2212T:Return a new set containing elements in Sbut not T.\nS\u2212=T :Update set Sto remove all common elements with set T.", "448 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.5.2 Python\u2019s MutableSet Abstract Base Class\nTo aid in the creation of user-de\ufb01ned set classes, Python\u2019s collections module pro-\nvides a MutableSet abstract base class (just as it provides the MutableMapping ab-\nstract base class discussed in Section 10.1.3). The MutableSet base class provides\nconcrete implementations for all methods described in Section 10.5.1, except for\n\ufb01ve core behaviors ( add,discard ,\ncontains\n ,\nlen\n ,a n d\n iter\n ) that must\nbe implemented by any concrete subclass. This design is an example of what is\nknown as the template method pattern , as the concrete methods of the MutableSet\nclass rely on the presumed abstract methods that will subsequently be provided bya subclass.\nFor the purpose of illustration, we examine algorithms for implementing several\nof the derived methods of the MutableSet base class. For example, to determine if\none set is a proper subset of another, we must verify two conditions: a proper subset\nmust have size strictly smaller than that of its superset, and each element of a subset\nmust be contained in the superset. An implementation of the corresponding\nlt\nmethod based on this logic is given in Code Fragment 10.14.\ndef\n lt\n(self,o t h e r ) : # supports syntax S <T\n\u201d\u201d\u201dReturn true if this set is a proper subset of other.\u201d\u201d\u201diflen(self)>= len(other):\nreturn False # proper subset must have strictly smaller size\nforein self :\nifenot in other:\nreturn False # not a subset since element missing from other\nreturn True # success; all conditions are met\nCode Fragment 10.14: A possible implementation of the MutableSet.\n lt\nmethod, which tests if one set is a proper subset of another.\nAs another example, we consider the computation of the union of two sets.\nThe set ADT includes two forms for computing a union. The syntax S|Tshould\nproduce a new set that has contents equal to the union of existing sets Sand T.T h i s\noperation is implemented through the special method\n or\n in Python. Another\nsyntax, S|=T is used to update existing set Sto become the union of itself and\nset T. Therefore, all elements of Tthat are not already contained in Sshould\nbe added to S. We note that this \u201cin-place\u201d operation may be implemented more\nef\ufb01ciently than if we were to rely on the \ufb01rst form, using the syntax S=S |T,i n\nwhich identi\ufb01er Sis reassigned to a new set instance that represents the union. For\nconvenience, Python\u2019s built-in set class supports named version of these behaviors,withS.union(T) equivalent to S|T,a n dS.update(T) equivalent to S|=T (yet,\nthose named versions are not formally provided by the MutableSet abstract base\nclass).", "10.5. Sets, Multisets, and Multimaps 449\ndef\n or\n(self,o t h e r ) : # supports syntax S |T\n\u201d\u201d\u201dReturn a new set that is the union of two existing sets.\u201d\u201d\u201d\nresult = type( self)( ) # create new instance of concrete class\nforein self :\nresult.add(e)\nforeinother:\nresult.add(e)\nreturn result\nCode Fragment 10.15: An implementation of the MutableSet.\n or\n method,\nwhich computes the union of two existing sets.\nAn implementation of the behavior that computes a new set as a union of two\nothers is given in the form of the\n or\n special method, in Code Fragment 10.15.\nAn important subtlety in this implementation is the instantiation of the resultingset. Since the MutableSet class is designed as an abstract base class, instances\nmust belong to a concrete subclass. When computing the union of two such con-\ncrete instances, the result should presumably be an instance of the same class as the\noperands. The function type(self) returns a reference to the actual class of the in-\nstance identi\ufb01ed as self, and the subsequent parentheses in expression type(self)()\ncall the default constructor for that class.\nIn terms of ef\ufb01ciency, we analyze such set operations while letting ndenote\nthe size of Sand mdenote the size of set Tfor an operation such as S|T.I f\nthe concrete sets are implemented with hashing, the expected running time of the\nimplementation in Code Fragment 10.15 is O(m+n), because it loops over both\nsets, performing constant-time operations in the form of a containment check and\na possible insertion into the result.\nOur implementation of the in-place version of a union is given in Code Frag-\nment 10.16, in the form of the\nior\n special method that supports syntax S|=T.\nNotice that in this case, we do not create a new set instance, instead we modify andreturn the existing set, after updating its contents to re\ufb02ect the union operation. The\nin-place version of the union has expected running time O(m)where mis the size\nof the second set, because we only have to loop through that second set.\ndef\nior\n(self,o t h e r ) : # supports syntax S |=T\n\u201d\u201d\u201dModify this set to be the union of itself an another set.\u201d\u201d\u201d\nforeinother:\nself.add(e)\nreturn self # technical requirement of in-place operator\nCode Fragment 10.16: An implementation of the MutableSet.\n ior\n method,\nwhich performs an in-place union of one set with another.", "450 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.5.3 Implementing Sets, Multisets, and Multimaps\nSets\nAlthough sets and maps have very different public interfaces, they are really quite\nsimilar. A set is simply a map in which keys do not have associated values. Anydata structure used to implement a map can be modi\ufb01ed to implement the set ADTwith similar performance guarantees. We could trivially adapt any map class by\nstoring set elements as keys, and using None as an irrelevant value, but such an\nimplementation is unnecessarily wasteful. An ef\ufb01cient set implementation should\nabandon the\nItem composite that we use in our MapBase class and instead store\nset elements directly in a data structure.\nMultisets\nThe same element may occur several times in a multiset. All of the data structureswe have seen can be reimplemented to allow for duplicates to appear as separateelements. However, another way to implement a multiset is by using a map in\nwhich the map key is a (distinct) element of the multiset, and the associated valueis a count of the number of occurrences of that element within the multiset. In fact,that is essentially what we did in Section 10.1.2 when computing the frequency ofwords within a document.\nPython\u2019s standard collections module includes a de\ufb01nition for a class named\nCounter that is in essence a multiset. Formally, the Counter class is a subclass of\ndict, with the expectation that values are integers, and with additional functionality\nlike amost\ncommon(n) method that returns a list of the nmost common elements.\nThe standard\n iter\n reports each element only once (since those are formally the\nkeys of the dictionary). There is another method named elements() that iterates\nthrough the multiset with each element being repeated according to its count.\nMultimaps\nAlthough there is no multimap in Python\u2019s standard libraries, a common imple-mentation approach is to use a standard map in which the value associated with akey is itself a container class storing any number of associated values. We give anexample of such a MultiMap class in Code Fragment 10.17. Our implementation\nuses the standard dictclass as the map, and a list of values as a composite value in\nthe dictionary. We have designed the class so that a different map implementationcan easily be substituted by overriding the class-level\nMapType attribute at line 3.", "10.5. Sets, Multisets, and Multimaps 451\n1classMultiMap:\n2\u201d\u201d\u201dA multimap class built upon use of an underlying map for storage.\u201d\u201d\u201d\n3\n MapType = dict # Map type; can be rede\ufb01ned by subclass\n4\n5def\n init\n(self):\n6 \u201d\u201d\u201dCreate a new empty multimap instance.\u201d\u201d\u201d\n7 self.\nmap = self.\nMapType( ) # create map instance for storage\n8 self.\nn=0\n9\n10def\n iter\n(self):\n11 \u201d\u201d\u201dIterate through all (k,v) pairs in multimap.\u201d\u201d\u201d\n12 fork,secondary in self.\n map.items():\n13 forvinsecondary:\n14 yield(k,v)\n1516defadd(self,k ,v ) :\n17 \u201d\u201d\u201dAdd pair (k,v) to multimap.\u201d\u201d\u201d\n18 container = self.\nmap.setdefault(k, [ ]) # create empty list, if needed\n19 container.append(v)\n20 self.\nn+ =1\n21\n22defpop(self,k ) :\n23 \u201d\u201d\u201dRemove and return arbitrary (k,v) with key k (or raise KeyError).\u201d\u201d\u201d\n24 secondary = self.\nmap[k] # may raise KeyError\n25 v = secondary.pop()\n26 iflen(secondary) == 0:\n27 del self .\nmap[k] #n op a i r sl e f t\n28 self.\nn\u2212=1\n29 return (k, v)\n3031def\ufb01nd(self,k ) :\n32 \u201d\u201d\u201dReturn arbitrary (k,v) pair with given key (or raise KeyError).\u201d\u201d\u201d\n33 secondary = self.\nmap[k] # may raise KeyError\n34 return (k, secondary[0])\n3536def\ufb01nd\nall(self,k ) :\n37 \u201d\u201d\u201dGenerate iteration of all (k,v) pairs with given key.\u201d\u201d\u201d\n38 secondary = self.\nmap.get(k, [ ]) # empty list, by default\n39 forvinsecondary:\n40 yield(k,v)\nCode Fragment 10.17: An implementation of a MultiMap using a dictfor storage.\nThe\n len\n method, which returns self.\nn, is omitted from this listing.", "452 Chapter 10. Maps, Hash Tables, and Skip Lists\n10.6 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-10.1 Give a concrete implementation of the popmethod in the context of the\nMutableMapping class, relying only on the \ufb01ve primary abstract methods\nof that class.\nR-10.2 Give a concrete implementation of the items() method in the context of\ntheMutableMapping class, relying only on the \ufb01ve primary abstract meth-\nods of that class. What would its running time be if directly applied to the\nUnsortedTableMap subclass?\nR-10.3 Give a concrete implementation of the items() method directly within the\nUnsortedTableMap class, ensuring that the entire iteration runs in O(n)\ntime.\nR-10.4 What is the worst-case running time for inserting nkey-value pairs into an\ninitially empty map Mthat is implemented with the UnsortedTableMap\nclass?\nR-10.5 Reimplement the UnsortedTableMap class from Section 10.1.5, using the\nPositionalList class from Section 7.4 rather than a Python list.\nR-10.6 Which of the hash table collision-handling schemes could tolerate a load\nfactor above 1 and which could not?\nR-10.7 OurPosition classes for lists and trees support the\n eq\n method so that\ntwo distinct position instances are considered equivalent if they refer to thesame underlying node in a structure. For positions to be allowed as keysin a hash table, there must be a de\ufb01nition for the\nhash\n method that\nis consistent with this notion of equivalence. Provide such a\n hash\nmethod.\nR-10.8 What would be a good hash code for a vehicle identi\ufb01cation number thatis a string of numbers and letters of the form \u201c9X9XX99X9XX999999,\u201dwhere a \u201c9\u201d represents a digit and an \u201cX\u201d represents a letter?\nR-10.9 Draw the 11-entry hash table that results from using the hash function,h(i)=( 3i+5)mod 11, to hash the keys 12, 44, 13, 88, 23, 94, 11, 39, 20,\n16, and 5, assuming collisions are handled by chaining.\nR-10.10 What is the result of the previous exercise, assuming collisions are han-dled by linear probing?\nR-10.11 Show the result of Exercise R-10.9, assuming collisions are handled by\nquadratic probing, up to the point where the method fails.", "10.6. Exercises 453\nR-10.12 What is the result of Exercise R-10.9 when collisions are handled by dou-\nble hashing using the secondary hash function h/prime(k)=7\u2212(kmod 7 )?\nR-10.13 What is the worst-case time for putting nentries in an initially empty hash\ntable, with collisions resolved by chaining? What is the best case?\nR-10.14 Show the result of rehashing the hash table shown in Figure 10.6 into a\ntable of size 19 using the new hash function h(k)=3kmod 17.\nR-10.15 OurHashMapBase class maintains a load factor \u03bb\u22640.5. Reimplement\nthat class to allow the user to specify the maximum load, and adjust the\nconcrete subclasses accordingly.\nR-10.16 Give a pseudo-code description of an insertion into a hash table that uses\nquadratic probing to resolve collisions, assuming we also use the trick ofreplacing deleted entries with a special \u201cdeactivated entry\u201d object.\nR-10.17 Modify our ProbeHashMap to use quadratic probing.\nR-10.18 Explain why a hash table is not suited to implement a sorted map.\nR-10.19 Describe how a sorted list implemented as a doubly linked list could beused to implement the sorted map ADT.\nR-10.20 What is the worst-case asymptotic running time for performing ndeletions\nfrom a SortedTableMap instance that initially contains 2 nentries?\nR-10.21 Consider the following variant of the\n\ufb01nd\nindex method from Code Frag-\nment 10.8, in the context of the SortedTableMap class:\ndef\n\ufb01nd\nindex(self,k ,l o w ,h i g h ) :\nifhigh<low:\nreturn high + 1\nelse:\nmid = (low + high) // 2\nif self.\ntable[mid].\n key<k:\nreturn self .\n\ufb01nd\nindex(k, mid + 1, high)\nelse:\nreturn self .\n\ufb01nd\nindex(k, low, mid \u22121)\nDoes this always produce the same result as the original version? Justifyyour answer.\nR-10.22 What is the expected running time of the methods for maintaining a max-\nima set if we insert npairs such that each pair has lower cost and perfor-\nmance than one before it? What is contained in the sorted map at the end\nof this series of operations? What if each pair had a lower cost and higher\nperformance than the one before it?\nR-10.23 Draw an example skip list Sthat results from performing the following\nseries of operations on the skip list shown in Figure 10.13: del S[38] ,\nS[48] =\nx\n,S[24] =\n y\n,del S[55] . Record your coin \ufb02ips, as well.", "454 Chapter 10. Maps, Hash Tables, and Skip Lists\nR-10.24 Give a pseudo-code description of the\n delitem\n map operation when\nusing a skip list.\nR-10.25 Give a concrete implementation of the pop method, in the context of a\nMutableSet abstract base class, that relies only on the \ufb01ve core set behav-\niors described in Section 10.5.2.\nR-10.26 Give a concrete implementation of the isdisjoint method in the context\nof the MutableSet abstract base class, relying only on the \ufb01ve primary\nabstract methods of that class. Your algorithm should run in O(min (n,m))\nwhere nand mdenote the respective cardinalities of the two sets.\nR-10.27 What abstraction would you use to manage a database of friends\u2019 birth-\ndays in order to support ef\ufb01cient queries such as \u201c\ufb01nd all friends whosebirthday is today\u201d and \u201c\ufb01nd the friend who will be the next to celebrate abirthday\u201d?\nCreativity\nC-10.28 On page 406 of Section 10.1.3, we give an implementation of the methodsetdefault as it might appear in the MutableMapping abstract base class.\nWhile that method accomplishes the goal in a general fashion, its ef\ufb01-ciency is less than ideal. In particular, when the key is new, there will bea failed search due to the initial use of\ngetitem\n , and then a subse-\nquent insertion via\n setitem\n . For a concrete implementation, such as\ntheUnsortedTableMap , this is twice the work because a complete scan\nof the table will take place during the failed\n getitem\n , and then an-\nother complete scan of the table takes place due to the implementation of\nsetitem\n . A better solution is for the UnsortedTableMap class to over-\nridesetdefault to provide a direct solution that performs a single search.\nGive such an implementation of UnsortedTableMap.setdefault .\nC-10.29 Repeat Exercise C-10.28 for the ProbeHashMap class.\nC-10.30 Repeat Exercise C-10.28 for the ChainHashMap class.\nC-10.31 For an ideal compression function, the capacity of the bucket array for ahash table should be a prime number. Therefore, we consider the problemof locating a prime number in a range [M,2M]. Implement a method for\n\ufb01nding such a prime by using the sieve algorithm . In this algorithm, we\nallocate a 2 Mcell Boolean array A, such that cell iis associated with the\ninteger i. We then initialize the array cells to all be \u201ctrue\u201d and we \u201cmark\noff\u201d all the cells that are multiples of 2, 3, 5, 7, and so on. This processcan stop after it reaches a number larger than\u221a\n2M. (Hint: Consider a\nbootstrapping method for \ufb01nding the primes up to\u221a\n2M.)", "10.6. Exercises 455\nC-10.32 Perform experiments on our ChainHashMap andProbeHashMap classes\nto measure its ef\ufb01ciency using random key sets and varying limits on the\nload factor (see Exercise R-10.15).\nC-10.33 Our implementation of separate chaining in ChainHashMap conserves\nmemory by representing empty buckets in the table as None , rather than\nas empty instances of a secondary structure. Because many of these buck-ets will hold a single item, a better optimization is to have those slots of\nthe table directly reference the\nItem instance, and to reserve use of sec-\nondary containers for buckets that have two or more items. Modify our\nimplementation to provide this additional optimization.\nC-10.34 Computing a hash code can be expensive, especially for lengthy keys. Inour hash table implementations, we compute the hash code when \ufb01rst in-\nserting an item, and recompute each item\u2019s hash code each time we resize\nour table. Python\u2019s dict class makes an interesting trade-off. The hash\ncode is computed once, when an item is inserted, and the hash code isstored as an extra \ufb01eld of the item composite, so that it need not be recom-puted. Reimplement our HashTableBase class to use such an approach.\nC-10.35 Describe how to perform a removal from a hash table that uses linear\nprobing to resolve collisions where we do not use a special marker to\nrepresent deleted elements. That is, we must rearrange the contents so thatit appears that the removed entry was never inserted in the \ufb01rst place.\nC-10.36 The quadratic probing strategy has a clustering problem related to the wayit looks for open slots. Namely, when a collision occurs at bucket h(k),i t\nchecks buckets A[(h(k)+ i\n2)mod N],f o r i=1,2,..., N\u22121.\na. Show that i2mod Nwill assume at most (N+1)/2 distinct values,\nfor Nprime, as iranges from 1 to N\u22121. As a part of this justi\ufb01ca-\ntion, note that i2mod N=(N\u2212i)2mod Nfor all i.\nb. A better strategy is to choose a prime Nsuch that Nmod 4 =3a n d\nthen to check the buckets A[(h(k)\u00b1i2)mod N]asiranges from 1\nto(N\u22121)/2, alternating between plus and minus. Show that this\nalternate version is guaranteed to check every bucket in A.\nC-10.37 Refactor our ProbeHashMap design so that the sequence of secondary\nprobes for collision resolution can be more easily customized. Demon-strate your new framework by providing separate concrete subclasses forlinear probing and quadratic probing.\nC-10.38 Design a variation of binary search for performing the multimap opera-tion\ufb01nd\nall(k) implemented with a sorted search table that includes du-\nplicates, and show that it runs in time O(s+logn),w h e r e nis the number\nof elements in the dictionary and sis the number of items with given key k.", "456 Chapter 10. Maps, Hash Tables, and Skip Lists\nC-10.39 Although keys in a map are distinct, the binary search algorithm can be\napplied in a more general setting in which an array stores possibly duplica-\ntive elements in nondecreasing order. Consider the goal of identifying theindex of the leftmost element with key greater than or equal to given k.\nDoes the\n\ufb01nd\nindex method as given in Code Fragment 10.8 guarantee\nsuch a result? Does the\n \ufb01nd\nindex method as given in Exercise R-10.21\nguarantee such a result? Justify your answers.\nC-10.40 Suppose we are given two sorted search tables Sand T, each with nentries\n(with Sand Tbeing implemented with arrays). Describe an O(log2n)-\ntime algorithm for \ufb01nding the kthsmallest key in the union of the keys\nfrom Sand T(assuming no duplicates).\nC-10.41 Give an O(logn)-time solution for the previous problem.\nC-10.42 Suppose that each row of an n\u00d7narray Aconsists of 1\u2019s and 0\u2019s such that,\nin any row of A, all the 1\u2019s come before any 0\u2019s in that row. Assuming A\nis already in memory, describe a method running in O(nlogn)time (not\nO(n2)time!) for counting the number of 1\u2019s in A.\nC-10.43 Given a collection Cofncost-performance pairs (c,p), describe an algo-\nrithm for \ufb01nding the maxima pairs of CinO(nlogn)time.\nC-10.44 Show that the methods above (p)andprev (p)are not actually needed to\nef\ufb01ciently implement a map using a skip list. That is, we can imple-ment insertions and deletions in a skip list using a strictly top-down, scan-forward approach, without ever using the above orprev methods. (Hint:\nIn the insertion algorithm, \ufb01rst repeatedly \ufb02ip the coin to determine thelevel where you should start inserting the new entry.)\nC-10.45 Describe how to modify a skip-list representation so that index-based\noperations, such as retrieving the item at index j, can be performed in\nO(logn)expected time.\nC-10.46 For sets Sand T, the syntax S\u02c6T returns a new set that is the symmet-\nric difference, that is, a set of elements that are in precisely one of Sor\nT. This syntax is supported by the special\nxor\n method. Provide an\nimplementation of that method in the context of the MutableSet abstract\nbase class, relying only on the \ufb01ve primary abstract methods of that class.\nC-10.47 In the context of the MutableSet abstract base class, describe a concrete\nimplementation of the\n and\n method, which supports the syntax S&T\nfor computing the intersection of two existing sets.\nC-10.48 Aninverted \ufb01le is a critical data structure for implementing a search en-\ngine or the index of a book. Given a document D, which can be viewed\nas an unordered, numbered list of words, an inverted \ufb01le is an ordered list\nof words, L, such that, for each word winL, we store the indices of the\nplaces in Dwhere wappears. Design an ef\ufb01cient algorithm for construct-\ning Lfrom D.", "10.6. Exercises 457\nC-10.49 Python\u2019s collections module provides an OrderedDict class that is unre-\nlated to our sorted map abstraction. An OrderedDict is a subclass of the\nstandard hash-based dictclass that retains the expected O(1)performance\nfor the primary map operations, but that also guarantees that the\n iter\nmethod reports items of the map according to \ufb01rst-in, \ufb01rst-out (FIFO)\norder. That is, the key that has been in the dictionary the longest is re-\nported \ufb01rst. (The order is unaffected when the value for an existing key\nis overwritten.) Describe an algorithmic approach for achieving such per-formance.\nProjects\nP-10.50 Perform a comparative analysis that studies the collision rates for various\nhash codes for character strings, such as various polynomial hash codes\nfor different values of the parameter a. Use a hash table to determine\ncollisions, but only count collisions where different strings map to thesame hash code (not if they map to the same location in this hash table).\nTest these hash codes on text \ufb01les found on the Internet.\nP-10.51 Perform a comparative analysis as in the previous exercise, but for 10-digit\ntelephone numbers instead of character strings.\nP-10.52 Implement an OrderedDict class, as described in Exercise C-10.49, en-\nsuring that the primary map operations run in O(1)expected time.\nP-10.53 Design a Python class that implements the skip-list data structure. Use\nthis class to create a complete implementation of the sorted map ADT.\nP-10.54 Extend the previous project by providing a graphical animation of the\nskip-list operations. Visualize how entries move up the skip list duringinsertions and are linked out of the skip list during removals. Also, in asearch operation, visualize the scan-forward and drop-down actions.\nP-10.55 Write a spell-checker class that stores a lexicon of words, W, in a Python\nset, and implements a method, check (s), which performs a spell check\non the string swith respect to the set of words, W.I f sis in W,t h e n\nthe call to check (s) returns a list containing only s, as it is assumed to\nbe spelled correctly in this case. If sis not in W, then the call to check (s)\nreturns a list of every word in Wthat might be a correct spelling of s. Your\nprogram should be able to handle all the common ways that smight be a\nmisspelling of a word in W, including swapping adjacent characters in a\nword, inserting a single character in between two adjacent characters in aword, deleting a single character from a word, and replacing a character in\na word with another character. For an extra challenge, consider phonetic\nsubstitutions as well.", "458 Chapter 10. Maps, Hash Tables, and Skip Lists\nChapter Notes\nHashing is a well-studied technique. The reader interested in further study is encouraged\nto explore the book by Knuth [65], as well as the book by Vitter and Chen [100]. Skip\nlists were introduced by Pugh [86]. Our analy sis of skip lists is a simpli\ufb01cation of a pre-\nsentation given by Motwani and Raghavan [80 ]. For a more in-depth analysis of skip lists,\nplease see the various research papers on skip lists that have appeared in the data structures\nliterature [59, 81, 84]. Exercise C-10.36 was contributed by James Lee.", "Chapter\n11Search Trees\nContents\n1 1 . 1B i n a r yS e a r c hT r e e s...................... 4 6 0\n1 1 . 1 . 1N a v i g a t i n g a B i n a r y S e a r c hT r e e ..............4 6 1\n1 1 . 1 . 2S e a r c h e s...........................4 6 3\n1 1 . 1 . 3I n s e r t i o n s a n d D e l e t i o n s...................4 6 5\n11.1.4 Python Implementation . . . . . . . . . . . . . . . . . . . 468\n11.1.5 Performance of a Binary Search Tree . . . . . . . . . . . . 473\n1 1 . 2B a l a n c e dS e a r c hT r e e s .................... 4 7 5\n11.2.1 Python Framework for Balancing Search Trees . . . . . . . 478\n1 1 . 3A V LT r e e s ........................... 4 8 1\n1 1 . 3 . 1U p d a t e O p e r a t i o n s .....................4 8 3\n11.3.2 Python Implementation . . . . . . . . . . . . . . . . . . . 488\n1 1 . 4S p l a yT r e e s........................... 4 9 0\n1 1 . 4 . 1S p l a y i n g...........................4 9 0\n1 1 . 4 . 2W h e nt o S p l a y ........................4 9 4\n11.4.3 Python Implementation . . . . . . . . . . . . . . . . . . . 496\n11.4.4 Amortized Analysis of Splaying \u22c6.............4 9 7\n1 1 . 5( 2 , 4 )T r e e s........................... 5 0 2\n1 1 . 5 . 1M u l t i w a y S e a r c h T r e e s ...................5 0 2\n1 1 . 5 . 2( 2 , 4 ) - T r e e O p e r a t i o n s....................5 0 5\n1 1 . 6R e d - B l a c kT r e e s........................ 5 1 2\n1 1 . 6 . 1R e d - B l a c k T r e e O p e r a t i o n s.................5 1 4\n11.6.2 Python Implementation . . . . . . . . . . . . . . . . . . . 525\n1 1 . 7E x e r c i s e s ............................ 5 2 8\n", "460 Chapter 11. Search Trees\n11.1 Binary Search Trees\nIn Chapter 8 we introduced the tree data structure and demonstrated a variety of\napplications. One important use is as a search tree (as described on page 332). In\nthis chapter, we use a search tree structure to ef\ufb01ciently implement a sorted map .\nThe three most fundamental methods of a map M(see Section 10.1.1) are:\nM[k] :Return the value vassociated with key kin map M, if one exists;\notherwise raise a KeyError ; implemented with\n getitem\n method.\nM[k] = v :Associate value vwith key kin map M, replacing the existing value\nif the map already contains an item with key equal to k; implemented\nwith\n setitem\n method.\ndel M[k] :Remove from map Mthe item with key equal to k;i fMhas no such\nitem, then raise a KeyError ; implemented with\n delitem\n method.\nThe sorted map ADT includes additional functionality (see Section 10.3), guar-\nanteeing that an iteration reports keys in sorted order, and supporting additional\nsearches such as \ufb01nd\ngt(k) and\ufb01nd\nrange(start, stop) .\nBinary trees are an excellent data structure for storing items of a map, assuming\nwe have an order relation de\ufb01ned on the keys. In this context, a binary search tree\nis a binary tree Twith each position pstoring a key-value pair (k,v)such that:\n\u2022Keys stored in the left subtree of pare less than k.\n\u2022Keys stored in the right subtree of pare greater than k.\nAn example of such a binary search tree is given in Figure 11.1. As a matter ofconvenience, we will not diagram the values associated with keys in this chapter,since those values do not affect the placement of items within a search tree.\n8082 54 28 9365 32 9744\n88 17\n8\n76 29\nFigure 11.1: A binary search tree with integer keys. We omit the display of associ-\nated values in this chapter, since they are not relevant to the order of items within a\nsearch tree.", "11.1. Binary Search Trees 461\n11.1.1 Navigating a Binary Search Tree\nWe begin by demonstrating that a binary search tree hierarchically represents the\nsorted order of its keys. In particular, the structural property regarding the place-ment of keys within a binary search tree assures the following important conse-quence regarding an inorder traversal (Section 8.4.3) of the tree.\nProposition 11.1:\nAn inorder traversal of a binary search tree visits positions in\nincreasing order of their keys.\nJusti\ufb01cation: We prove this by induction on the size of a subtree. If a subtree\nhas at most one item, its keys are trivially visited in order. More generally, an\ninorder traversal of a (sub)tree consists of a recursive traversal of the (possibly\nempty) left subtree, followed by a visit of the root, and then a recursive traversal ofthe (possibly empty) right subtree. By induction, a recursive inorder traversal of theleft subtree will produce an iteration of the keys in that subtree in increasing order.\nFurthermore, by the binary search tree property, all keys in the left subtree have\nkeys strictly smaller than that of the root. Therefore, visiting the root just after thatsubtree extends the increasing order of keys. Finally, by the search tree property,all keys in the right subtree are strictly greater than the root, and by induction, aninorder traversal of that subtree will visit those keys in increasing order.\nSince an inorder traversal can be executed in linear time, a consequence of this\nproposition is that we can produce a sorted iteration of the keys of a map in lineartime, when represented as a binary search tree.\nAlthough an inorder traversal is typically expressed using a top-down recur-\nsion, we can provide nonrecursive descriptions of operations that allow more \ufb01ne-grained navigation among the positions of a binary search relative to the order of\ntheir keys. Our generic binary tree ADT from Chapter 8 is de\ufb01ned as a positional\nstructure, allowing direct navigation using methods such as parent(p) ,left(p),a n d\nright(p) . With a binary search tree, we can provide additional navigation based on\nthe natural order of the keys stored in the tree. In particular, we can support the\nfollowing methods, akin to those provided by a PositionalList (Section 7.4.1).\n\ufb01rst(): Return the position containing the least key, or None if the tree is empty.\nlast() :Return the position containing the greatest key, or None if empty tree.\nbefore(p) :Return the position containing the greatest key that is less than that of\nposition p(i.e., the position that would be visited immediately before p\nin an inorder traversal), or None ifpis the \ufb01rst position.\nafter(p) :Return the position containing the least key that is greater than that ofposition p(i.e., the position that would be visited immediately after p\nin an inorder traversal), or None ifpis the last position.", "462 Chapter 11. Search Trees\nThe \u201c\ufb01rst\u201d position of a binary search tree can be located by starting a walk at\nthe root and continuing to the left child, as long as a left child exists. By symmetry,\nthe last position is reached by repeated steps rightward starting at the root.\nThe successor of a position, after(p) , is determined by the following algorithm.\nAlgorithm after(p) :\nifright(p) is notNone then{successor is leftmost position in p\u2019s right subtree }\nwalk =right(p)\nwhile left(walk) is notNone do\nwalk =left(walk)\nreturn walk\nelse{successor is nearest ancestor having pin its left subtree }\nwalk =p\nancestor =parent(walk)\nwhile ancestor is notNone andwalk ==right(ancestor) do\nwalk =ancestor\nancestor =parent(walk)\nreturn ancestor\nCode Fragment 11.1: Computing the successor of a position in a binary search tree.\nThe rationale for this process is based purely on the workings of an inorder\ntraversal, given the correspondence of Proposition 11.1. If phas a right subtree,\nthat right subtree is recursively traversed immediately after pis visited, and so the\n\ufb01rst position to be visited after pis the leftmost position within the right subtree.\nIfpdoes not have a right subtree, then the \ufb02ow of control of an inorder traversal\nreturns to p\u2019s parent. If pwere in the right subtree of that parent, then the parent\u2019s\nsubtree traversal is complete and the \ufb02ow of control progresses to its parent andso on. Once an ancestor is reached in which the recursion is returning from itsleftsubtree, then that ancestor becomes the next position visited by the inorder\ntraversal, and thus is the successor of p. Notice that the only case in which no such\nancestor is found is when pwas the rightmost (last) position of the full tree, in\nwhich case there is no successor.\nA symmetric algorithm can be de\ufb01ned to determine the predecessor of a po-\nsition, before(p) . At this point, we note that the running time of single call to\nafter(p) orbefore(p) is bounded by the height hof the full tree, because it is found\nafter either a single downward walk or a single upward walk. While the worst-caserunning time is O(h), we note that either of these methods run in O(1)amortized\ntime, in that series of ncalls to after(p) starting at the \ufb01rst position will execute in a\ntotal of O(n)time. We leave a formal justi\ufb01cation of this fact to Exercise C-11.34,\nbut intuitively the upward and downward paths mimic steps of the inorder traversal\n(a related argument was made in the justi\ufb01cation of Proposition 9.3).", "11.1. Binary Search Trees 463\n11.1.2 Searches\nThe most important consequence of the structural property of a binary search tree\nis its namesake search algorithm. We can attempt to locate a particular key in abinary search tree by viewing it as a decision tree (recall Figure 8.7). In this case,the question asked at each position pis whether the desired key kis less than, equal\nto, or greater than the key stored at position p, which we denote as p.key() .I ft h e\nanswer is \u201cless than,\u201d then the search continues in the left subtree. If the answeris \u201cequal,\u201d then the search terminates successfully. If the answer is \u201cgreater than,\u201dthen the search continues in the right subtree. Finally, if we reach an empty subtree,\nthen the search terminates unsuccessfully. (See Figure 11.2.)\n8082 54 28 9365 32 9744\n88 17\n8\n76 2982 54 28 9365 32 9744\n88 17\n8\n8076 29\n(a) (b)\nFigure 11.2: (a) A successful search for key 65 in a binary search tree; (b) an\nunsuccessful search for key 68 that terminates because there is no subtree to the\nleft of the key 76.\nWe describe this approach in Code Fragment 11.2. If key koccurs in a subtree\nrooted at p, a call to TreeSearch(T, p, k) results in the position at which the key\nis found; in this case, the\n getitem\n map operation would return the associated\nvalue at that position. In the event of an unsuccessful search, the TreeSearch al-\ngorithm returns the \ufb01nal position explored on the search path (which we will latermake use of when determining where to insert a new item in a search tree).\nAlgorithm TreeSearch(T, p, k) :\nifk\n==p.key ()then\nreturn p {successful search }\nelse if k<p.key ()andT.left(p) is notNone then\nreturn TreeSearch(T, T.left(p), k) {recur on left subtree }\nelse if k>p.key ()andT.right(p) is notNone then\nreturn TreeSearch(T, T.right(p), k) {recur on right subtree }\nreturn p {unsuccessful search }\nCode Fragment 11.2: Recursive search in a binary search tree.", "464 Chapter 11. Search Trees\nAnalysis of Binary Tree Searching\nThe analysis of the worst-case running time of searching in a binary search tree\nTis simple. Algorithm TreeSearch is recursive and executes a constant number\nof primitive operations for each recursive call. Each recursive call of TreeSearch\nis made on a child of the previous position. That is, TreeSearch is called on the\npositions of a path of Tthat starts at the root and goes down one level at a time.\nThus, the number of such positions is bounded by h+1, where his the height of T.\nIn other words, since we spend O(1)time per position encountered in the search,\nthe overall search runs in O(h)time, where his the height of the binary search\ntree T. (See Figure 11.3.)\nTree T:Time per level\nTotal time:Height\nh\nO(h)O(1)\nO(1)\nO(1)\nFigure 11.3: Illustrating the running time of searching in a binary search tree. The\n\ufb01gure uses standard caricature of a binary search tree as a big triangle and a path\nfrom the root as a zig-zag line.\nIn the context of the sorted map ADT, the search will be used as a subroutine\nfor implementing the\n getitem\n method, as well as for the\n setitem\n and\ndelitem\n methods, since each of these begins by trying to locate an existing\nitem with a given key. To implement sorted map operations such as \ufb01nd\nltand\n\ufb01nd\ngt, we will combine this search with traversal methods before andafter .A l l\nof these operations will run in worst-case O(h)time for a tree with height h.W e\ncan use a variation of this technique to implement the \ufb01nd\nrange method in time\nO(s+h),w h e r e sis the number of items reported (see Exercise C-11.34).\nAdmittedly, the height hofTcan be as large as the number of entries, n,b u tw e\nexpect that it is usually much smaller. Indeed, later in this chapter we show various\nstrategies to maintain an upper bound of O(logn)on the height of a search tree T.", "11.1. Binary Search Trees 465\n11.1.3 Insertions and Deletions\nAlgorithms for inserting or deleting entries of a binary search tree are fairly straight-\nforward, although not trivial.\nInsertion\nThe map command M[k] = v , as supported by the\n setitem\n method, begins\nwith a search for key k(assuming the map is nonempty). If found, that item\u2019s\nexisting value is reassigned. Otherwise, a node for the new item can be insertedinto the underlying tree Tin place of the empty subtree that was reached at the end\nof the failed search. The binary search tree property is sustained by that placement(note that it is placed exactly where a search would expect it). Pseudo-code forsuch a TreeInsert algorithm is given in in Code Fragment 11.3.\nAlgorithm TreeInsert(T, k, v) :\nInput: A search key kto be associated with value v\np=TreeSearch (T,T.root (),k)\nifk\n==p.key ()then\nSetp\u2019s value to v\nelse if k<p.key ()then\nadd node with item (k,v) as left child of p\nelse\nadd node with item (k,v) as right child of p\nCode Fragment 11.3: Algorithm for inserting a key-value pair into a map that is\nrepresented as a binary search tree.\nAn example of insertion into a binary search tree is shown in Figure 11.4.\n82 54 28 9365 32 9744\n88 17\n8\n8076 29\n6854 28 9365 32 9744\n88 17\n8\n807682\n29\n(a) (b)\nFigure 11.4: Insertion of an item with key 68 into the search tree of Figure 11.2.\nFinding the position to insert is shown in (a), and the resulting tree is shown in (b).", "466 Chapter 11. Search Trees\nDeletion\nDeleting an item from a binary search tree Tis a bit more complex than inserting\na new item because the location of the deletion might be anywhere in the tree. (In\ncontrast, insertions are always enacted at the bottom of a path.) To delete an item\nwith key k, we begin by calling TreeSearch(T, T.root(), k) to \ufb01nd the position p\nofTstoring an item with key equal to k. If the search is successful, we distinguish\nbetween two cases (of increasing dif\ufb01culty):\n\u2022Ifphas at most one child, the deletion of the node at position pis easily\nimplemented. When introducing update methods for the LinkedBinaryTree\nclass in Section 8.3.1, we declared a nonpublic utility,\n delete(p) , that deletes\na node at position pand replaces it with its child (if any), presuming that phas\nat most one child. That is precisely the desired behavior. It removes the item\nwith key kfrom the map while maintaining all other ancestor-descendant\nrelationships in the tree, thereby assuring the upkeep of the binary searchtree property. (See Figure 11.5.)\n\u2022If position phas two children, we cannot simply remove the node from T\nsince this would create a \u201chole\u201d and two orphaned children. Instead, weproceed as follows (see Figure 11.6):\n\u25e6We locate position rcontaining the item having the greatest key that is\nstrictly less than that of position p,t h a ti s , r=b e f o r e ( p ) by the notation\nof Section 11.1.1. Because phas two children, its predecessor is the\nrightmost position of the left subtree of p.\n\u25e6We use r\u2019s item as a replacement for the one being deleted at position p.\nBecause rhas the immediately preceding key in the map, any items in\np\u2019s right subtree will have keys greater than rand any other items in p\u2019s\nleft subtree will have keys less than r. Therefore, the binary search tree\nproperty is satis\ufb01ed after the replacement.\n\u25e6Having used r\u2019s as a replacement for p, we instead delete the node at\nposition rfrom the tree. Fortunately, since rwas located as the right-\nmost position in a subtree, rdoes not have a right child. Therefore, its\ndeletion can be performed using the \ufb01rst (and simpler) approach.\nAs with searching and insertion, this algorithm for a deletion involves the\ntraversal of a single path downward from the root, possibly moving an item betweentwo positions of this path, and removing a node from that path and promoting its\nchild. Therefore, it executes in time O(h)where his the height of the tree.", "11.1. Binary Search Trees 467\nr32 8\n28\n2954 93\n6865\n7682p44\n17 88\n8097r\n28 8\n54 93\n6865\n768244\n17 88\n802997\n(a) (b)\nFigure 11.5: Deletion from the binary search tree of Figure 11.4b, where the item\nto delete (with key 32) is stored at a position pwith one child r: (a) before the\ndeletion; (b) after the deletion.\nr28 8\n54 93\n68768244\n17 88\n802965p\n97\nr\n6897 28 8\n54 93 7644\n17 82\n2965p\n80\n(a) (b)\nFigure 11.6: Deletion from the binary search tree of Figure 11.5b, where the item\nto delete (with key 88) is stored at a position pwith two children, and replaced by\nits predecessor r: (a) before the deletion; (b) after the deletion.", "468 Chapter 11. Search Trees\n11.1.4 Python Implementation\nIn Code Fragments 11.4 through 11.8 we de\ufb01ne a TreeMap class that implements\nthe sorted map ADT using a binary search tree. In fact, our implementation is more\ngeneral. We support all of the standard map operations (Section 10.1.1), all ad-\nditional sorted map operations (Section 10.3), and positional operations including\n\ufb01rst() ,last() ,\ufb01nd\nposition(k) ,before(p) ,after(p) ,a n ddelete(p) .\nOurTreeMap class takes advantage of multiple inheritance for code reuse,\ninheriting from the LinkedBinaryTree class of Section 8.3.1 for our representation\nas a positional binary tree, and from the MapBase class from Code Fragment 10.2\nof Section 10.1.4 to provide us with the key-value composite item and the concretebehaviors from the collections.MutableMapping abstract base class. We subclass\nthe nested Position class to support more speci\ufb01c p.key() andp.value() accessors\nfor our map, rather than the p.element() syntax inherited from the tree ADT.\nWe de\ufb01ne several nonpublic utilities, most notably a\nsubtree\n search(p, k)\nmethod that corresponds to the TreeSearch algorithm of Code Fragment 11.2. That\nreturns a position, ideally one that contains the key k, or otherwise the last position\nthat is visited on the search path. We rely on the fact that the \ufb01nal position dur-ing an unsuccessful search is either the nearest key less than kor the nearest key\ngreater than k. This search utility becomes the basis for the public \ufb01nd\nposition(k)\nmethod, and also for internal use when searching, inserting, or deleting items froma map, as well as for the robust searches of the sorted map ADT.\nWhen making structural modi\ufb01cations to the tree, we rely on nonpublic update\nmethods, such as\nadd\nright , that are inherited from the LinkedBinaryTree class\n(see Section 8.3.1). It is important that these inherited methods remain nonpublic,as the search tree property could be violated through misuse of such operations.\nFinally, we note that our code is peppered with calls to presumed methods\nnamed\nrebalance\n insert ,\nrebalance\n delete ,a n d\n rebalance\n access . These meth-\nods serve as hooks for future use when balancing search trees; we discuss them in\nSection 11.2. We conclude with a brief guide to the organization of our code.\nCode Fragment 11.4: Beginning of TreeMap class including rede\ufb01ned Position\nclass and nonpublic search utilities.\nCode Fragment 11.5: Positional methods \ufb01rst() ,last() ,before(p) ,after(p) ,\nand\ufb01nd\nposition(p) accessor.\nCode Fragment 11.6: Selected methods of the sorted map ADT: \ufb01nd\nmin() ,\n\ufb01nd\nge(k) ,a n d\ufb01nd\nrange(start, stop) ; related methods\nare omitted for the sake of brevity.\nCode Fragment 11.7:\n getitem\n (k),\nsetitem\n (k, v) ,a n d\n iter\n().\nCode Fragment 11.8: Deletion either by position, as delete(p) ,o rb yk e y ,a s\ndelitem\n (k).", "11.1. Binary Search Trees 469\n1classTreeMap(LinkedBinaryTree, MapBase):\n2\u201d\u201d\u201dSorted map implementation using a binary search tree.\u201d\u201d\u201d\n3\n4#---------------------------- override Position class ----------------------------\n5classPosition(LinkedBinaryTree.Position):\n6 defkey(self):\n7 \u201d\u201d\u201dReturn key of map\n s key-value pair.\u201d\u201d\u201d\n8 return self .element().\n key\n9\n10 defvalue(self):\n11 \u201d\u201d\u201dReturn value of map\n s key-value pair.\u201d\u201d\u201d\n12 return self .element().\n value\n1314 #------------------------------- nonpublic utilities -------------------------------\n15def\nsubtree\n search( self,p ,k ) :\n16 \u201d\u201d\u201dReturn Position of p\n s subtree having key k, or last node searched.\u201d\u201d\u201d\n17 ifk= =p . k e y ( ) : # found match\n18 return p\n19 elifk<p.key(): # search left subtree\n20 if self.left(p) is not None :\n21 return self .\nsubtree\n search( self.left(p), k)\n22 else: # search right subtree\n23 if self.right(p) is not None :\n24 return self .\nsubtree\n search( self.right(p), k)\n25 return p # unsucessful search\n2627def\nsubtree\n \ufb01rst\nposition( self,p ) :\n28 \u201d\u201d\u201dReturn Position of \ufb01rst item in subtree rooted at p.\u201d\u201d\u201d\n29 walk = p\n30 while self .left(walk) is not None : # keep walking left\n31 walk = self.left(walk)\n32 return walk\n3334def\nsubtree\n last\nposition( self,p ) :\n35 \u201d\u201d\u201dReturn Position of last item in subtree rooted at p.\u201d\u201d\u201d\n36 walk = p\n37 while self .right(walk) is not None : # keep walking right\n38 walk = self.right(walk)\n39 return walk\nCode Fragment 11.4: Beginning of a TreeMap class based on a binary search tree.", "470 Chapter 11. Search Trees\n40def\ufb01rst(self):\n41 \u201d\u201d\u201dReturn the \ufb01rst Position in the tree (or None if empty).\u201d\u201d\u201d\n42 return self .\nsubtree\n \ufb01rst\nposition( self.root()) iflen(self)>0else None\n43\n44deflast(self):\n45 \u201d\u201d\u201dReturn the last Position in the tree (or None if empty).\u201d\u201d\u201d\n46 return self .\nsubtree\n last\nposition( self.root()) iflen(self)>0else None\n4748defbefore( self,p ) :\n49 \u201d\u201d\u201dReturn the Position just before p in the natural order.\n5051 Return None if p is the \ufb01rst position.\n52 \u201d\u201d\u201d\n53 self.\nvalidate(p) # inherited from LinkedBinaryTree\n54 if self.left(p):\n55 return self .\nsubtree\n last\nposition( self.left(p))\n56 else:\n57 #w a l ku p w a r d\n58 walk = p\n59 above = self.parent(walk)\n60 while above is not None and walk == self.left(above):\n61 walk = above\n62 above = self.parent(walk)\n63 return above\n6465defafter(self,p ) :\n66 \u201d\u201d\u201dReturn the Position just after p in the natural order.\n67\n68 Return None if p is the last position.\n69 \u201d\u201d\u201d\n70 # symmetric to before(p)\n7172def\ufb01nd\nposition( self,k ) :\n73 \u201d\u201d\u201dReturn position with key k, or else neighbor (or None if empty).\u201d\u201d\u201d\n74 if self.is\nempty():\n75 return None\n76 else:\n77 p=self.\nsubtree\n search(self .root(), k)\n78 self.\nrebalance\n access(p) # hook for balanced tree subclasses\n79 return p\nCode Fragment 11.5: Navigational methods of the TreeMap class.", "11.1. Binary Search Trees 471\n80def\ufb01nd\nmin(self):\n81 \u201d\u201d\u201dReturn (key,value) pair with minimum key (or None if empty).\u201d\u201d\u201d\n82 if self.is\nempty():\n83 return None\n84 else:\n85 p=self.\ufb01rst()\n86 return (p.key(), p.value())\n87\n88def\ufb01nd\nge(self,k ) :\n89 \u201d\u201d\u201dReturn (key,value) pair with least key greater than or equal to k.\n90\n91 Return None if there does not exist such a key.\n92 \u201d\u201d\u201d\n93 if self.is\nempty():\n94 return None\n95 else:\n96 p=self.\ufb01nd\nposition(k) # may not \ufb01nd exact match\n97 ifp.key( ) <k: #p \u2019 sk e yi st o os m a l l\n98 p=self.after(p)\n99 return (p.key(), p.value()) ifpis not None else None\n100\n101 def\ufb01nd\nrange(self, start, stop):\n102 \u201d\u201d\u201dIterate all (key,value) pairs such that start <=k e y <stop.\n103104 If start is None, iteration begins with minimum key of map.\n105 If stop is None, iteration continues through the maximum key of map.\n106 \u201d\u201d\u201d\n107 if not self .is\nempty():\n108 ifstartis None :\n109 p=self.\ufb01rst()\n110 else:\n111 # we initialize p with logic similar to \ufb01nd\n ge\n112 p=self.\ufb01nd\nposition(start)\n113 ifp.key( ) <start:\n114 p=self.after(p)\n115 while pis not None and (stopis None or p.key( ) <stop):\n116 yield(p.key(), p.value())\n117 p=self.after(p)\nCode Fragment 11.6: Some of the sorted map operations for the TreeMap class.", "472 Chapter 11. Search Trees\n118 def\n getitem\n (self,k ) :\n119 \u201d\u201d\u201dReturn value associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n120 if self.is\nempty():\n121 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n122 else:\n123 p=self.\nsubtree\n search(self .root(), k)\n124 self.\nrebalance\n access(p) # hook for balanced tree subclasses\n125 ifk! =p . k e y ( ) :\n126 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\n127 return p.value()\n128\n129 def\n setitem\n (self,k ,v ) :\n130 \u201d\u201d\u201dAssign value v to key k, overwriti ng existing value if present.\u201d\u201d\u201d\n131 if self.is\nempty():\n132 leaf = self.\nadd\nroot(self.\nItem(k,v)) # from LinkedBinaryTree\n133 else:\n134 p=self.\nsubtree\n search(self .root(), k)\n135 ifp . k e y ()= =k :\n136 p.element().\n value = v # replace existing item\u2019s value\n137 self.\nrebalance\n access(p) # hook for balanced tree subclasses\n138 return\n139 else:\n140 item = self.\nItem(k,v)\n141 ifp.key( ) <k:\n142 leaf = self.\nadd\nright(p, item) # inherited from LinkedBinaryTree\n143 else:\n144 leaf = self.\nadd\nleft(p, item) # inherited from LinkedBinaryTree\n145 self.\nrebalance\n insert(leaf) # hook for balanced tree subclasses\n146147 def\niter\n(self):\n148 \u201d\u201d\u201dGenerate an iteration of all keys in the map in order.\u201d\u201d\u201d\n149 p=self.\ufb01rst()\n150 while pis not None :\n151 yieldp.key()\n152 p=self.after(p)\nCode Fragment 11.7: Map operations for accessing and inserting items in the\nTreeMap class. Reverse iteration can be implemented with\n reverse\n ,u s i n g\nsymmetric approach to\n iter\n .", "11.1. Binary Search Trees 473\n153 defdelete( self,p ) :\n154 \u201d\u201d\u201dRemove the item at given Position.\u201d\u201d\u201d\n155 self.\nvalidate(p) # inherited from LinkedBinaryTree\n156 if self.left(p) and self .right(p): # p has two children\n157 replacement = self.\nsubtree\n last\nposition( self.left(p))\n158 self.\nreplace(p, replacement.element()) # from LinkedBinaryTree\n159 p = replacement\n160 # now p has at most one child\n161 parent = self.parent(p)\n162 self.\ndelete(p) # inherited from LinkedBinaryTree\n163 self.\nrebalance\n delete(parent) # if root deleted, parent is None\n164\n165 def\n delitem\n (self,k ) :\n166 \u201d\u201d\u201dRemove item associated with key k (raise KeyError if not found).\u201d\u201d\u201d\n167 if not self .is\nempty():\n168 p=self.\nsubtree\n search(self .root(), k)\n169 ifk= =p . k e y ( ) :\n170 self.delete(p) # rely on positional version\n171 return # successful deletion complete\n172 self.\nrebalance\n access(p) # hook for balanced tree subclasses\n173 raiseKeyError(\n Key Error:\n +r e p r ( k ) )\nCode Fragment 11.8: Support for deleting an item from a TreeMap , located either\nby position or by key.\n11.1.5 Performance of a Binary Search Tree\nAn analysis of the operations of our TreeMap class is given in Table 11.1. Almost\nall operations have a worst-case running time that depends on h,w h e r e his the\nheight of the current tree. This is because most operations rely on a constant amountof work for each node along a particular path of the tree, and the maximum pathlength within a tree is proportional to the height of the tree. Most notably, our\nimplementations of map operations\ngetitem\n ,\nsetitem\n ,a n d\n delitem\neach begin with a call to the\n subtree\n search utility which traces a path downward\nfrom the root of the tree, using O(1)time at each node to determine how to con-\ntinue the search. Similar paths are traced when looking for a replacement during adeletion, or when computing a position\u2019s inorder predecessor or successor. We note\nthat although a single call to the after method has worst-case running time of O(h),\nthe nsuccessive calls made during a call to\niter\n require a total of O(n)time,\nsince each edge is traced at most twice; in a sense, those calls have O(1)amortized\ntime bounds. A similar argument can be used to prove the O(s+h)worst-case\nbound for a call to \ufb01nd\nrange that reports sresults (see Exercise C-11.34).", "474 Chapter 11. Search Trees\nOperation\n Running Time\nki nT\n O(h)\nT[k],T[k] = v\n O(h)\nT.delete(p) ,del T[k]\n O(h)\nT.\ufb01nd\n position(k)\n O(h)\nT.\ufb01rst() ,T.last() ,T.\ufb01nd\n min() ,T.\ufb01nd\n max()\n O(h)\nT.before(p) ,T.after(p)\n O(h)\nT.\ufb01nd\n lt(k),T.\ufb01nd\n le(k) ,T.\ufb01nd\n gt(k) ,T.\ufb01nd\n ge(k)\n O(h)\nT.\ufb01nd\n range(start, stop)\n O(s+h)\niter(T) ,reversed(T)\n O(n)\nTable 11.1: Worst-case running times of the operations for a TreeMap T . We denote\nthe current height of the tree with h, and the number of items reported by \ufb01nd\nrange\nass. The space usage is O(n),w h e r e nis the number of items stored in the map.\nA binary search tree Tis therefore an ef\ufb01cient implementation of a map with n\nentries only if its height is small. In the best case, Thas height h=\u2308log (n+1)\u2309\u22121,\nwhich yields logarithmic-time performance for all the map operations. In the worst\ncase, however, Thas height n, in which case it would look and feel like an ordered\nlist implementation of a map. Such a worst-case con\ufb01guration arises, for example,\nif we insert items with keys in increasing or decreasing order. (See Figure 11.7.)\n30\n4010\n20\nFigure 11.7: Example of a binary search tree with linear height, obtained by insert-\ning entries with keys in increasing order.\nWe can nevertheless take comfort that, on average, a binary search tree with\nnkeys generated from a random series of insertions and removals of keys has ex-\npected height O(logn); the justi\ufb01cation of this statement is beyond the scope of the\nbook, requiring careful mathematical language to precisely de\ufb01ne what we mean\nby a random series of insertions and removals, and sophisticated probability theory.\nIn applications where one cannot guarantee the random nature of updates, it\nis better to rely on variations of search trees, presented in the remainder of thischapter, that guarantee a worst-case height of O(logn), and thus O(logn)worst-\ncase time for searches, insertions, and deletions.", "11.2. Balanced Search Trees 475\n11.2 Balanced Search Trees\nIn the closing of the previous section, we noted that if we could assume a random\nseries of insertions and removals, the standard binary search tree supports O(logn)\nexpected running times for the basic map operations. However, we may only claimO(n)worst-case time, because some sequences of operations may lead to an unbal-\nanced tree with height proportional to n.\nIn the remainder of this chapter, we explore four search tree algorithms that\nprovide stronger performance guarantees. Three of the four data structures (AVLtrees, splay trees, and red-black trees) are based on augmenting a standard binary\nsearch tree with occasional operations to reshape the tree and reduce its height.\nThe primary operation to rebalance a binary search tree is known as a rotation .\nDuring a rotation, we \u201crotate\u201d a child to be above its parent, as diagrammed in\nFigure 11.8.\nyx\ny\nT1\nT2 T3 T1 T2T3x\nFigure 11.8: A rotation operation in a binary search tree. A rotation can be per-\nformed to transform the left formation into the right, or the right formation into theleft. Note that all keys in subtree T\n1have keys less than that of position x,a l lk e y s\nin subtree T2have keys that are between those of positions xand y,a n da l lk e y si n\nsubtree T3have keys that are greater than that of position y.\nTo maintain the binary search tree property through a rotation, we note that\nif position xwas a left child of position yprior to a rotation (and therefore the\nkey of xis less than the key of y), then ybecomes the right child of xafter the\nrotation, and vice versa. Furthermore, we must relink the subtree of items withkeys that lie between the keys of the two positions that are being rotated. Forexample, in Figure 11.8 the subtree labeled T\n2represents items with keys that are\nknown to be greater than that of position xand less than that of position y.I n t h e\n\ufb01rst con\ufb01guration of that \ufb01gure, T2is the right subtree of position x; in the second\ncon\ufb01guration, it is the left subtree of position y.\nBecause a single rotation modi\ufb01es a constant number of parent-child relation-\nships, it can be implemented in O(1)time with a linked binary tree representation.", "476 Chapter 11. Search Trees\nIn the context of a tree-balancing algorithm, a rotation allows the shape of a\ntree to be modi\ufb01ed while maintaining the search tree property. If used wisely, this\noperation can be performed to avoid highly unbalanced tree con\ufb01gurations. Forexample, a rightward rotation from the \ufb01rst formation of Figure 11.8 to the secondreduces the depth of each node in subtree T\n1by one, while increasing the depth\nof each node in subtree T3by one. (Note that the depth of nodes in subtree T2are\nunaffected by the rotation.)\nOne or more rotations can be combined to provide broader rebalancing within a\ntree. One such compound operation we consider is a trinode restructuring . For this\nmanipulation, we consider a position x, its parent y, and its grandparent z. The goal\nis to restructure the subtree rooted at zin order to reduce the overall path length\ntoxand its subtrees. Pseudo-code for a restructure (x)method is given in Code\nFragment 11.9 and illustrated in Figure 11.9. In describing a trinode restructuring,we temporarily rename the positions x,y,a n d zasa,b,a n d c,s ot h a t aprecedes b\nand bprecedes cin an inorder traversal of T. There are four possible orientations\nmapping x,y,a n d ztoa,b,a n d c, as shown in Figure 11.9, which are uni\ufb01ed\ninto one case by our relabeling. The trinode restructuring replaces zwith the node\nidenti\ufb01ed as b, makes the children of this node be aand c, and makes the children\nofaand cbe the four previous children of x,y,a n d z(other than xand y), while\nmaintaining the inorder relationships of all the nodes in T.\nAlgorithm restructure(x) :\nInput: A position xof a binary search tree Tthat has both a parent yand a\ngrandparent z\nOutput: Tr\nee Tafter a trinode restructuring (which corresponds to a single or\ndouble rotation) involving positions x,y,a n dz\n1:Let (a,b,c) be a left-to-right (inorder) listing of the positions x,y,a n dz,a n d\nlet (T1,T2,T3,T4)be a left-to-right (inorder) listing of the four subtrees of x,\ny,a n dznot rooted at x,y,o rz.\n2:Replace the subtree rooted at zwith a new subtree rooted at b.\n3:Letabe the left child of band let T1andT2be the left and right subtrees of a,\nrespectively.\n4:Letcbe the right child of band let T3andT4be the left and right subtrees of\nc, respectively.\nCode Fragment 11.9: The trinode restructuring operation in a binary search tree.\nIn practice, the modi\ufb01cation of a tree Tcaused by a trinode restructuring op-\neration can be implemented through case analysis either as a single rotation (as inFigure 11.9a and b) or as a double rotation (as in Figure 11.9c and d). The doublerotation arises when position xhas the middle of the three relevant keys and is \ufb01rst\nrotated above its parent, and then above what was originally its grandparent. In any\nof the cases, the trinode restructuring is completed with O(1)running time.", "11.2. Balanced Search Trees 477\nsingle rotation\nT1a=z\nb=y\nT2c=x\nT3 T4a=z\nT1 T2b=y\nc=x\nT3 T4\n(a)\nT1a=x\nT1 T2b=y\nc=z\nT3 T4single rotation\nT4c=z\nb=y\nT3a=x\nT2\n(b)\nT3T1a=z\nT1 T2b=x\nc=y\nT3 T4double rotation\na=z\nT4c=y\nb=x\nT2\n(c)\nT2T1 T2b=x\nc=z\nT3 T4double rotation\na=y\nT4c=z\nT1a=y\nb=x\nT3\n(d)\nFigure 11.9: Schematic illustration of a trinode restructuring operation: (a and b)\nrequire a single rotation; (c and d) require a double rotation.", "478 Chapter 11. Search Trees\n11.2.1 Python Framework for Balancing Search Trees\nOurTreeMap class, introduced in Section 11.1.4, is a concrete map implementation\nthat does not perform any explicit balancing operations. However, we designed\nthat class to also serve as a base class for other subclasses that implement more\nadvanced tree-balancing algorithms. A summary of our inheritance hierarchy is\nshown in Figure 11.10.\n(Section 11.3.2)MapBase\n(Section 8.3.1)\n(Section 11.4.3)TreeMap\nSplayTreeMap(Section 11.1.4)\n(Section 11.6.2)RedBlackTreeMap(Section 10.1.4)LinkedBinaryTree\nAVLTreeMap\nFigure 11.10: Our hierarchy of balanced search trees (with references to where they\nare de\ufb01ned). Recall that TreeMap inherits multiply from LinkedBinaryTree and\nMapBase .\nHooks for Rebalancing Operations\nOur implementation of the basic map operations in Section 11.1.4 includes strategiccalls to three nonpublic methods that serve as hooks for rebalancing algorithms:\n\u2022A call to\nrebalance\n insert(p) is made from within the\n setitem\n method\nimmediately after a new node is added to the tree at position p.\n\u2022A call to\n rebalance\n delete(p) is made each time a node has been deleted\nfrom the tree, with position pidentifying the parent of the node that has just\nbeen removed. Formally, this hook is called from within the public delete(p)\nmethod, which is indirectly invoked by the public\n delitem\n (k)behavior.\n\u2022We also provide a hook,\n rebalance\n access(p) , that is called when an item at\nposition pof a tree is accessed through a public method such as\n getitem\n .\nThis hook is used by the splay tree structure (see Section 11.4) to restructure\na tree so that more frequently accessed items are brought closer to the root.\nWe provide trivial declarations of these three methods, in Code Fragment 11.10,\nhaving bodies that do nothing (using the pass statement). A subclass of TreeMap\nmay override any of these methods to implement a nontrivial action to rebalancea tree. This is another example of the template method design pattern , as seen in\nSection 8.4.6.", "11.2. Balanced Search Trees 479\n174 def\nrebalance\n insert(self,p ) :pass\n175 def\nrebalance\n delete( self,p ) :pass\n176 def\nrebalance\n access( self,p ) :pass\nCode Fragment 11.10: Additional code for the TreeMap class (continued from Code\nFragment 11.8), providing stubs for the rebalancing hooks.\nNonpublic Methods for Rotating and Restructuring\nA second form of support for balanced search trees is our inclusion of nonpub-\nlic utility methods\n rotate and\nrestructure that, respectively, implement a single\nrotation and a trinode restructuring (described at the beginning of Section 11.2).Although these methods are not invoked by the public TreeMap operations, we\npromote code reuse by providing these implementation in this class so that they areinherited by all balanced-tree subclasses.\nOur implementations are provided in Code Fragment 11.11. To simplify the\ncode, we de\ufb01ne an additional\nrelink utility that properly links parent and child\nnodes to each other, including the special case in which a \u201cchild\u201d is a None ref-\nerence. The focus of the\n rotate method then becomes rede\ufb01ning the relationship\nbetween the parent and child, relinking a rotated node directly to its original grand-\nparent, and shifting the \u201cmiddle\u201d subtree (that labeled as T2in Figure 11.8) between\nthe rotated nodes. For the trinode restructuring, we determine whether to perform\na single or double rotation, as originally described in Figure 11.9.\nFactory for Creating Tree Nodes\nWe draw attention to an important subtlety in the design of both our TreeMap class\nand the original LinkedBinaryTree subclass. The low-level de\ufb01nition of a node is\nprovided by the nested\n Node class within LinkedBinaryTree . Yet, several of our\ntree-balancing strategies require that auxiliary information be stored at each nodeto guide the balancing process. Those classes will override the nested\nNode class\nto provide storage for an additional \ufb01eld.\nWhenever we add a new node to the tree, as within the\n add\nright method of\ntheLinkedBinaryTree (originally given in Code Fragment 8.10), we intentionally\ninstantiate the node using the syntax self.\nNode , rather than the quali\ufb01ed name\nLinkedBinaryTree.\n Node . This is vital to our framework! When the expression\nself.\nNode is applied to an instance of a tree (sub)class, Python\u2019s name resolution\nfollows the inheritance structure (as described in Section 2.5.2). If a subclass hasoverridden the de\ufb01nition for the\nNode class, instantiation of self.\nNode relies on\nthe newly de\ufb01ned node class. This technique is an example of the factory method\ndesign pattern , as we provide a subclass the means to control the type of node that\nis created within methods of the parent class.", "480 Chapter 11. Search Trees\n177 def\nrelink(self, parent, child, make\n left\nchild):\n178 \u201d\u201d\u201dRelink parent node with child node (we allow child to be None).\u201d\u201d\u201d\n179 ifmake\n left\nchild: # make it a left child\n180 parent.\n left = child\n181 else: # make it a right child\n182 parent.\n right = child\n183 ifchildis not None : # make child point to parent\n184 child.\n parent = parent\n185\n186 def\nrotate( self,p ) :\n187 \u201d\u201d\u201dRotate Position p above its parent.\u201d\u201d\u201d\n188 x=p .\n node\n189 y=x .\n parent # we assume this exists\n190 z=y .\n parent # grandparent (possibly None)\n191 ifzis None :\n192 self.\nroot = x # x becomes root\n193 x.\nparent = None\n194 else:\n195 self.\nrelink(z, x, y == z.\n left) # x becomes a direct child of z\n196 # now rotate x and y, including transfer of middle subtree\n197 ifx= =y .\n left:\n198 self.\nrelink(y, x.\n right,True) #x .\nright becomes left child of y\n199 self.\nrelink(x, y, False) # y becomes right child of x\n200 else:\n201 self.\nrelink(y, x.\n left,False) #x .\nleft becomes right child of y\n202 self.\nrelink(x, y, True) # y becomes left child of x\n203204 def\nrestructure( self,x ) :\n205 \u201d\u201d\u201dPerform trinode restructure of Position x with parent/grandparent.\u201d\u201d\u201d\n206 y=self.parent(x)\n207 z=self.parent(y)\n208 if(x == self.right(y)) == (y == self.right(z)): # matching alignments\n209 self.\nrotate(y) # single rotation (of y)\n210 return y # y is new subtree root\n211 else: # opposite alignments\n212 self.\nrotate(x) # double rotation (of x)\n213 self.\nrotate(x)\n214 return x # x is new subtree root\nCode Fragment 11.11: Additional code for the TreeMap class (continued from Code\nFragment 11.10), to provide nonpublic utilities for balanced search tree subclasses.", "11.3. A VL Trees 481\n11.3 AVL Trees\nTheTreeMap class, which uses a standard binary search tree as its data structure,\nshould be an ef\ufb01cient map data structure, but its worst-case performance for the\nvarious operations is linear time, because it is possible that a series of operations\nresults in a tree with linear height. In this section, we describe a simple balancing\nstrategy that guarantees worst-case logarithmic running time for all the fundamentalmap operations.\nDe\ufb01nition of an AVL Tree\nThe simple correction is to add a rule to the binary search tree de\ufb01nition that willmaintain a logarithmic height for the tree. Although we originally de\ufb01ned the\nheight of a subtree rooted at position pof a tree to be the number of edges on\nthe longest path from pto a leaf (see Section 8.1.3), it is easier for explanation in\nthis section to consider the height to be the number of nodes on such a longest path.\nBy this de\ufb01nition, a leaf position has height 1, while we trivially de\ufb01ne the height\nof a \u201cnull\u201d child to be 0.\nIn this section, we consider the following height-balance property , which char-\nacterizes the structure of a binary search tree Tin terms of the heights of its nodes.\nHeight-Balance Property :For every position pofT, the heights of the children\nofpd i f f e rb ya tm o s t1 .\nAny binary search tree Tthat satis\ufb01es the height-balance property is said to be an\nAVL tree , named after the initials of its inventors: Adel\u2019son-Vel\u2019skii and Landis.\nAn example of an AVL tree is shown in Figure 11.11.\n78\n8844\n0\n00 00\n00004\n23\n1 12\n1 117\n32\n48 6250\nFigure 11.11: An example of an AVL tree. The keys of the items are shown inside\nthe nodes, and the heights of the nodes are shown above the nodes (with empty\nsubtrees having height 0).", "482 Chapter 11. Search Trees\nAn immediate consequence of the height-balance property is that a subtree of an\nAVL tree is itself an AVL tree. The height-balance property has also the important\nconsequence of keeping the height small, as shown in the following proposition.\nProposition 11.2: The height of an AVL tree storing nentries is O(logn).\nJusti\ufb01cation: Instead of trying to \ufb01nd an upper bound on the height of an AVL\ntree directly, it turns out to be easier to work on the \u201cinverse problem\u201d of \ufb01nding a\nlower bound on the minimum number of nodes n(h)of an AVL tree with height h.\nWe will show that n(h)grows at least exponentially. From this, it will be an easy\nstep to derive that the height of an AVL tree storing nentries is O(logn).\nWe begin by noting that n(1)=1a n d n(2)=2, because an AVL tree of height\n1 must have exactly one node and an AVL tree of height 2 must have at least twonodes. Now, an AVL tree with the minimum number of nodes having height hfor\nh\u22653, is such that both its subtrees are AVL trees with the minimum number of\nnodes: one with height h\u22121 and the other with height h\u22122. Taking the root into\naccount, we obtain the following formula that relates n(h)ton(h\u22121)and n(h\u22122),\nforh\u22653:\nn(h)=1+n(h\u22121)+ n(h\u22122). (11.1)\nAt this point, the reader familiar with the properties of Fibonacci progressions (Sec-\ntion 1.8 and Exercise C-3.49) will already see that n(h)is a function exponential\ninh.\nTo formalize that observation, we proceed as follows.\nFormula 11.1 implies that n(h)is a strictly increasing function of h. Thus, we\nknow that n(h\u22121)>n(h\u22122). Replacing n(h\u22121)with n(h\u22122)in Formula 11.1\nand dropping the 1, we get, for h\u22653,\nn(h)>2\u00b7n(h\u22122). (11.2)\nFormula 11.2 indicates that n(h)at least doubles each time hincreases by 2, which\nintuitively means that n(h)grows exponentially. To show this fact in a formal way,\nwe apply Formula 11.2 repeatedly, yielding the following series of inequalities:\nn(h)>2\u00b7n(h\u22122)\n>4\u00b7n(h\u22124)\n>8\u00b7n(h\u22126)\n...\n>2i\u00b7n(h\u22122i). (11.3)\nThat is, n(h)>2i\u00b7n(h\u22122i), for any integer i, such that h\u22122i\u22651. Since we already\nknow the values of n(1)and n(2),w ep i c ki so that h\u22122iis equal to either 1 or 2.\nThat is, we pick\ni=/ceilingleftbiggh\n2/ceilingrightbigg\n\u22121.", "11.3. A VL Trees 483\nBy substituting the above value of iin Formula 11.3, we obtain, for h\u22653,\nn(h)>2\u2308h\n2\u2309\u22121\u00b7n/parenleftbigg\nh\u22122/ceilingleftbiggh\n2/ceilingrightbigg\n+2/parenrightbigg\n\u22652\u2308h\n2\u2309\u22121n(1)\n\u22652h\n2\u22121. (11.4)\nBy taking logarithms of both sides of Formula 11.4, we obtain\nlog (n(h))>h\n2\u22121,\nfrom which we get\nh<2log (n(h)) +2, (11.5)\nwhich implies that an A VL tree storing nentries has height at most 2log n+2.\nBy Proposition 11.2 and the analysis of binary search trees given in Section 11.1,\nthe operation\n getitem\n , in a map implemented with an A VL tree, runs in time\nO(logn),w h e r e nis the number of items in the map. Of course, we still have to\nshow how to maintain the height-balance property after an insertion or deletion.\n11.3.1 Update Operations\nGiven a binary search tree T, we say that a position is balanced if the absolute\nvalue of the difference between the heights of its children is at most 1, and we say\nthat it is unbalanced otherwise. Thus, the height-balance property characterizing\nAVL trees is equivalent to saying that every position is balanced.\nThe insertion and deletion operations for AVL trees begin similarly to the corre-\nsponding operations for (standard) binary search trees, but with post-processing foreach operation to restore the balance of any portions of the tree that are adverselyaffected by the change.\nInsertion\nSuppose that tree Tsatis\ufb01es the height-balance property, and hence is an AVL tree,\nprior to the insertion of a new item. An insertion of a new item in a binary searchtree, as described in Section 11.1.3, results in a new node at a leaf position p.T h i s\naction may violate the height-balance property (see, for example, Figure 11.12a),yet the only positions that may become unbalanced are ancestors of p, because\nthose are the only positions whose subtrees have changed. Therefore, let us de-\nscribe how to restructure Tto \ufb01x any unbalance that may have occurred.", "484 Chapter 11. Search Trees\nT1\nT2T4\n5462\n00481 3\n0088\n245\n44\n0\n017\n32\n0112\n1z\ny\n0\n00x\nT35078\nT4 T1 T262\nT30234\n44\n0\n017\n012\nx\ny78z2\n50\n001\n005411\n0048 8832\n(a) (b)\nFigure 11.12: An example insertion of an item with key 54 in the A VL tree of\nFigure 11.11: (a) after adding a new node for key 54, the nodes storing keys 78\nand 44 become unbalanced; (b) a trinode restructuring restores the height-balanceproperty. We show the heights of nodes above them, and we identify the nodes x,\ny,a n d zand subtrees T\n1,T2,T3,a n d T4participating in the trinode restructuring.\nWe restore the balance of the nodes in the binary search tree Tby a simple\n\u201csearch-and-repair\u201d strategy. In particular, let zbe the \ufb01rst position we encounter in\ngoing up from ptoward the root of Tsuch that zis unbalanced (see Figure 11.12a.)\nAlso, let ydenote the child of zwith higher height (and note that ymust be an\nancestor of p). Finally, let xbe the child of ywith higher height (there cannot be a\ntie and position xmust also be an ancestor of p, possibly pitself). We rebalance\nthe subtree rooted at zby calling the trinode restructuring method, restructure (x),\noriginally described in Section 11.2. An example of such a restructuring in thecontext of an AVL insertion is portrayed in Figure 11.12.\nTo formally argue the correctness of this process in reestablishing the AVL\nheight-balance property, we consider the implication of zbeing the nearest ancestor\nofpthat became unbalanced after the insertion of p. It must be that the height\nofyincreased by one due to the insertion and that it is now 2 greater than its\nsibling. Since yremains balanced, it must be that it formerly had subtrees with\nequal heights, and that the subtree containing xhas increased its height by one.\nThat subtree increased either because x=p, and thus its height changed from 0\nto 1, or because xpreviously had equal-height subtrees and the height of the one\ncontaining phas increased by 1. Letting h\u22650 denote the height of the tallest child\nofx, this scenario might be portrayed as in Figure 11.13.\nAfter the trinode restructuring, we see that each of x,y,a n d zhas become\nbalanced. Furthermore, the node that becomes the root of the subtree after therestructuring has height h+2, which is precisely the height that zhad before the\ninsertion of the new item. Therefore, any ancestor of zthat became temporarily\nunbalanced becomes balanced again, and this one restructuring restores the height-\nbalance property globally .", "11.3. A VL Trees 485\nT3 T2x h\u22121 h\u22121\nT4y\nT1z hh+2\nhhh+1\n(a)\nh+2\nT3T2x h\u22121 h\nT4y\nT1z hh+3\nhh+1\n(b)\nhh+2\nT2T1h z h\u22121h+1\nT4h\nT3yh+1\nx\n(c)\nFigure 11.13: Rebalancing of a subtree during a typical insertion into an A VL tree:\n(a) before the insertion; (b) after an insertion in subtree T3causes imbalance at z;\n(c) after restoring balance with trinode restructuring. Notice that the overall height\nof the subtree after the insertion is the same as before the insertion.", "486 Chapter 11. Search Trees\nDeletion\nRecall that a deletion from a regular binary search tree results in the structural\nremoval of a node having either zero or one children. Such a change may violate\nthe height-balance property in an AVL tree. In particular, if position prepresents\nthe parent of the removed node in tree T, there may be an unbalanced node on the\npath from pto the root of T. (See Figure 11.14a.) In fact, there can be at most one\nsuch unbalanced node. (The justi\ufb01cation of this fact is left as Exercise C-11.49.)\nT4 T2T11\n02\n00481\n00541\n8850z44\n62\n0 01\n0017\nT334\ny\n78x2\n32\nT1 T4\nT2z\n0 01\n0\nT3 01\n1724\n62\nxy\n443\n78\n2\n050\n0048 54\n001188\n(a) (b)\nFigure 11.14: Deletion of the item with key 32 from the AVL tree of Figure 11.12b:\n(a) after removing the node storing key 32, the root becomes unbalanced; (b) a\n(single) rotation restores the height-balance property.\nAs with insertion, we use trinode restructuring to restore balance in the tree T.\nIn particular, let zbe the \ufb01rst unbalanced position encountered going up from p\ntoward the root of T. Also, let ybe the child of zwith larger height (note that\nposition yis the child of zthat is not an ancestor of p), and let xbe the child of y\nde\ufb01ned as follows: If one of the children of yis taller than the other, let xbe the\ntaller child of y; else (both children of yhave the same height), let xbe the child of\nyon the same side as y(that is, if yis the left child of z,l e t xbe the left child of\ny,e l s el e t xbe the right child of y). In any case, we then perform a restructure (x)\noperation. (See Figure 11.14b.)\nThe restructured subtree is rooted at the middle position denoted as bin the\ndescription of the trinode restructuring operation. The height-balance property isguaranteed to be locally restored within the subtree of b. (See Exercises R-11.11\nand R-11.12). Unfortunately, this trinode restructuring may reduce the height of the\nsubtree rooted at bby 1, which may cause an ancestor of bto become unbalanced.\nSo, after rebalancing z, we continue walking up Tlooking for unbalanced positions.\nIf we \ufb01nd another, we perform a restructure operation to restore its balance, and\ncontinue marching up Tlooking for more, all the way to the root. Still, since the\nheight of TisO(logn),w h e r e nis the number of entries, by Proposition 11.2,\nO(logn)trinode restructurings are suf\ufb01cient to restore the height-balance property.", "11.3. A VL Trees 487\nPerformance of AVL Trees\nBy Proposition 11.2, the height of an AVL tree with nitems is guaranteed to be\nO(logn). Because the standard binary search tree operation had running times\nbounded by the height (see Table 11.1), and because the additional work in main-\ntaining balance factors and restructuring an AVL tree can be bounded by the lengthof a path in the tree, the traditional map operations run in worst-case logarithmictime with an A VL tree. We summarize these results in Table 11.2, and illustratethis performance in Figure 11.15.\nOperation\n Running Time\nki nT\n O(logn)\nT[k] = v\n O(logn)\nT.delete(p) ,del T[k]\n O(logn)\nT.\ufb01nd\n position(k)\n O(logn)\nT.\ufb01rst() ,T.last() ,T.\ufb01nd\n min() ,T.\ufb01nd\n max()\n O(logn)\nT.before(p) ,T.after(p)\n O(logn)\nT.\ufb01nd\n lt(k),T.\ufb01nd\n le(k) ,T.\ufb01nd\n gt(k) ,T.\ufb01nd\n ge(k)\n O(logn)\nT.\ufb01nd\n range(start, stop)\n O(s+logn)\niter(T) ,reversed(T)\n O(n)\nTable 11.2: Worst-case running times of operations for an n-item sorted map real-\nized as an AVL tree T, with sdenoting the number of items reported by \ufb01nd\nrange .\nO(logn)O(1)Height Time per level\nWorst-case time:AV L Tr e e T :\ndown phase\nup phaseO(logn)O(1)\nO(1)\nFigure 11.15: Illustrating the running time of searches and updates in an A VL tree.\nThe time performance is O(1)per level, broken into a down phase, which typi-\ncally involves searching, and an up phase, which typically involves updating heightvalues and performing local trinode restructurings (rotations).", "488 Chapter 11. Search Trees\n11.3.2 Python Implementation\nA complete implementation of an AVLTreeMap class is provided in Code Frag-\nments 11.12 and 11.13. It inherits from the standard TreeMap class and relies on\nthe balancing framework described in Section 11.2.1. We highlight two important\naspects of our implementation. First, the AVLTreeMap overrides the de\ufb01nition of\nthe nested\n Node class, as shown in Code Fragment 11.12, in order to provide sup-\nport for storing the height of the subtree stored at a node. We also provide several\nutilities involving heights of nodes, and the corresponding positions.\nTo implement the core logic of the AVL balancing strategy, we de\ufb01ne a utility,\nnamed\n rebalance , that suf\ufb01ces as a hook for restoring the height-balance property\nafter an insertion or a deletion. Although the inherited behaviors for insertion anddeletion are quite different, the necessary post-processing for an AVL tree can be\nuni\ufb01ed. In both cases, we trace an upward path from the position pat which the\nchange took place, recalculating the height of each position based on the (updated)\nheights of its children, and using a trinode restructuring operation if an imbalancedposition is reached. If we reach an ancestor with height that is unchanged by the\noverall map operation, or if we perform a trinode restructuring that results in the\nsubtree having the same height it had before the map operation, we stop the pro-cess; no further ancestor\u2019s height will change. To detect the stopping condition, werecord the \u201cold\u201d height of each node and compare it to the newly calculated height.\n1classAVLTreeMap(TreeMap):\n2\u201d\u201d\u201dSorted map implementation using an AVL tree.\u201d\u201d\u201d\n34#-------------------------- nested\nNode class --------------------------\n5class\n Node(TreeMap.\n Node):\n6 \u201d\u201d\u201dNode class for AVL maintains height value for balancing.\u201d\u201d\u201d\n7\n slots\n =\n_height\n # additional data member to store height\n89 def\ninit\n(self,e l e m e n t ,p a r e n t = None,l e f t =None,r i g h t = None):\n10 super().\ninit\n(element, parent, left, right)\n11 self.\nheight = 0 # will be recomputed during balancing\n1213 defleft\nheight( self):\n14 return self .\nleft.\nheight if self.\n leftis not None else 0\n15\n16 defright\nheight( self):\n17 return self .\nright.\n height if self.\nrightis not None else 0\nCode Fragment 11.12: AVLTreeMap class (continued in Code Fragment 11.13).", "11.3. A VL Trees 489\n18 #------------------------- positional-based utility methods -------------------------\n19def\nrecompute\n height( self,p ) :\n20 p.\nnode.\n height = 1 + max(p.\n node.left\n height(), p.\n node.right\n height())\n21\n22def\nisbalanced( self,p ) :\n23 return abs(p.\n node.left\n height( ) \u2212p.\nnode.right\n height()) <=1\n24\n25def\ntall\nchild(self,p ,f a v o r l e f t = False):# parameter controls tiebreaker\n26 ifp.\nnode.left\n height( ) + (1 iffavorleft else0)>p.\nnode.right\n height():\n27 return self .left(p)\n28 else:\n29 return self .right(p)\n30\n31def\ntall\ngrandchild( self,p ) :\n32 child = self.\ntall\nchild(p)\n33 # if child is on left, favor left grandchild; else favor right grandchild\n34 alignment = (child == self.left(p))\n35 return self .\ntall\nchild(child, alignment)\n3637def\nrebalance( self,p ) :\n38 while pis not None :\n39 old\nheight = p.\n node.\n height # trivially 0 if new node\n40 if not self .\nisbalanced(p): # imbalance detected!\n41 # perform trinode restructuring, setting p to resulting root,\n42 # and recompute new local heights after the restructuring\n43 p=self.\nrestructure( self.\ntall\ngrandchild(p))\n44 self.\nrecompute\n height( self.left(p))\n45 self.\nrecompute\n height( self.right(p))\n46 self.\nrecompute\n height(p) # adjust for recent changes\n47 ifp.\nnode.\n height == old\n height: # has height changed?\n48 p=None # no further changes needed\n49 else:\n50 p=self.parent(p) # repeat with parent\n5152 #---------------------------- override balancing hooks ----------------------------\n53def\nrebalance\n insert(self,p ) :\n54 self.\nrebalance(p)\n5556def\nrebalance\n delete( self,p ) :\n57 self.\nrebalance(p)\nCode Fragment 11.13: AVLTreeMap class (continued from Code Fragment 11.12).", "490 Chapter 11. Search Trees\n11.4 Splay Trees\nThe next search tree structure we study is known as a a splay tree . This structure is\nconceptually quite different from the other balanced search trees we discuss in this\nchapter, for a splay tree does not strictly enforce a logarithmic upper bound on theheight of the tree. In fact, there are no additional height, balance, or other auxiliarydata associated with the nodes of this tree.\nThe ef\ufb01ciency of splay trees is due to a certain move-to-root operation, called\nsplaying , that is performed at the bottommost position preached during every in-\nsertion, deletion, or even a search. (In essence, this is a tree variant of the move-to-front heuristic that we explored for lists in Section 7.6.2.) Intuitively, a splay\noperation causes more frequently accessed elements to remain nearer to the root,\nthereby reducing the typical search times. The surprising thing about splaying isthat it allows us to guarantee a logarithmic amortized running time, for insertions,deletions, and searches.\n11.4.1 Splaying\nGiven a node xof a binary search tree T,w e splay xby moving xto the root of T\nthrough a sequence of restructurings. The particular restructurings we perform areimportant, for it is not suf\ufb01cient to move xto the root of Tby just any sequence\nof restructurings. The speci\ufb01c operation we perform to move xup depends upon\nthe relative positions of x, its parent y, and (if it exists) x\u2019s grandparent z.T h e r ea r e\nthree cases that we consider.\nzig-zig :The node xand its parent yare both left children or both right children.\n(See Figure 11.16.) We promote x,m a k i n g ya child of xand za child of y,\nwhile maintaining the inorder relationships of the nodes in T.\nT1y\nT2\nT3 T4z10\nx20\n30\nT4\nT3\nT2 T120\ny1030\nzx\n(a) (b)\nFigure 11.16: Zig-zig: (a) before; (b) after. There is another symmetric con\ufb01gura-\ntion where xand yare left children.", "11.4. Splay Trees 491\nzig-zag :One of xand yis a left child and the other is a right child. (See Fig-\nure 11.17.) In this case, we promote xby making xhave yand zas its chil-\ndren, while maintaining the inorder relationships of the nodes in T.\nxz\nT4y\nT2 T33010\n20\nT110\nT2y\nT3 T420\nzx30\nT1\n(a) (b)\nFigure 11.17: Zig-zag: (a) before; (b) after. There is another symmetric con\ufb01gura-\ntion where xis a right child and yis a left child.\nzig:xdoes not have a grandparent. (See Figure 11.18.) In this case, we perform a\nsingle rotation to promote xover y,m a k i n g ya child of x, while maintaining\nthe relative inorder relationships of the nodes in T.\nT1\nT2 T310\ny\n20\nx\nT1 T2T310\ny20\nx\n(a) (b)\nFigure 11.18: Zig: (a) before; (b) after. There is another symmetric con\ufb01guration\nwhere xis originally a left child of y.\nWe perform a zig-zig or a zig-zag when xhas a grandparent, and we perform a\nzig when xhas a parent but not a grandparent. A splaying step consists of repeating\nthese restructurings at xuntil xbecomes the root of T. An example of the splaying\nof a node is shown in Figures 11.19 and 11.20.", "492 Chapter 11. Search Trees\n1411\n12\n57\n13 17168\n4\n631 0\n(a)\n166\n5\n178\n31 0\n4\n12\n71 4\n1311\n(b)\n1711 4\n12 6\n57\n13 16148\n31 0\n(c)\nFigure 11.19: Example of splaying a node: (a) splaying the node storing 14 starts\nwith a zig-zag; (b) after the zig-zag; (c) the next step will be a zig-zig. (Continues\nin Figure 11.20.)", "11.4. Splay Trees 493\n1768\n31 0\n12 1614\n7 51 1 1 34\n(d)\n8\n43\n6\n5712 1614\n17 11 1310\n(e)\n7417 810 1614\n31 3 11\n6\n512\n(f)\nFigure 11.20: Example of splaying a node:(d) after the zig-zig; (e) the next step is\nagain a zig-zig; (f) after the zig-zig. (Continued from Figure 11.19.)", "494 Chapter 11. Search Trees\n11.4.2 When to Splay\nThe rules that dictate when splaying is performed are as follows:\n\u2022When searching for key k,i f kis found at position p,w es p l a y p,e l s ew e\nsplay the leaf position at which the search terminates unsuccessfully. For\nexample, the splaying in Figures 11.19 and 11.20 would be performed aftersearching successfully for key 14 or unsuccessfully for key 15.\n\u2022When inserting key k, we splay the newly created internal node where k\ngets inserted. For example, the splaying in Figures 11.19 and 11.20 wouldbe performed if 14 were the newly inserted key. We show a sequence of\ninsertions in a splay tree in Figure 11.21.\n1 1\n3 13\n(a) (b) (c)\n3\n1\n2132 2\n13\n4\n(d) (e) (f)\n14\n3\n2\n(g)\nFigure 11.21: A sequence of insertions in a splay tree: (a) initial tree; (b) after\ninserting 3, but before a zig step; (c) after splaying; (d) after inserting 2, but beforea zig-zag step; (e) after splaying; (f) after inserting 4, but before a zig-zig step;\n(g) after splaying.", "11.4. Splay Trees 495\n\u2022When deleting a key k, we splay the position pthat is the parent of the re-\nmoved node; recall that by the removal algorithm for binary search trees, the\nremoved node may be that originally containing k, or a descendant node with\na replacement key. An example of splaying following a deletion is shown inFigure 11.22.\n8\n10 3\n4\n567\nwp11 410 3\n11\n67\n5\n(a) (b)\n510 6\n11 4\n37\n510 6\n11 4\n37\n(c) (d)\n3546\n7\n10\n11\n(e)\nFigure 11.22: Deletion from a splay tree: (a) the deletion of 8 from the root node\nis performed by moving to the root the key of its inorder predecessor w, deleting\nw, and splaying the parent pofw; (b) splaying pstarts with a zig-zig; (c) after the\nzig-zig; (d) the next step is a zig; (e) after the zig.", "496 Chapter 11. Search Trees\n11.4.3 Python Implementation\nAlthough the mathematical analysis of a splay tree\u2019s performance is complex (see\nSection 11.4.4), the implementation of splay trees is a rather simple adaptation to\na standard binary search tree. Code Fragment 11.14 provides a complete imple-\nmentation of a SplayTreeMap class, based upon the underlying TreeMap class and\nuse of the balancing framework described in Section 11.2.1. It is important to note\nthat our original TreeMap class makes calls to the\n rebalance\n access method, not\njust from within the\n getitem\n method, but also during\n setitem\n when mod-\nifying the value associated with an existing key, and after any map operations thatresult in a failed search.\n1classSplayTreeMap(TreeMap):\n2\u201d\u201d\u201dSorted map implementation using a splay tree.\u201d\u201d\u201d\n3#--------------------------------- splay operation --------------------------------\n4def\nsplay(self,p ) :\n5 while p! =self.root():\n6 parent = self.parent(p)\n7 grand = self.parent(parent)\n8 ifgrandis None :\n9 #z i gc a s e\n10 self.\nrotate(p)\n11 elif(parent == self.left(grand)) == (p == self.left(parent)):\n12 # zig-zig case\n13 self.\nrotate(parent) #m o v eP A R E N Tu p\n14 self.\nrotate(p) # then move p up\n15 else:\n16 #z i g - z a gc a s e\n17 self.\nrotate(p) #m o v epu p\n18 self.\nrotate(p) #m o v epu pa g a i n\n19\n20 #---------------------------- override balancing hooks ----------------------------\n21def\nrebalance\n insert(self,p ) :\n22 self.\nsplay(p)\n23\n24def\nrebalance\n delete( self,p ) :\n25 ifpis not None :\n26 self.\nsplay(p)\n2728def\nrebalance\n access( self,p ) :\n29 self.\nsplay(p)\nCode Fragment 11.14: A complete implementation of the SplayTreeMap class.", "11.4. Splay Trees 497\n11.4.4 Amortized Analysis of Splaying \u22c6\nAfter a zig-zig or zig-zag, the depth of position pdecreases by two, and after a zig\nthe depth of pdecreases by one. Thus, if phas depth d,s p l a y i n g pconsists of a\nsequence of \u230ad/2\u230bzig-zigs and/or zig-zags, plus one \ufb01nal zig if dis odd. Since a\nsingle zig-zig, zig-zag, or zig affects a constant number of nodes, it can be done in\nO(1)time. Thus, splaying a position pin a binary search tree Ttakes time O(d),\nwhere dis the depth of pinT. In other words, the time for performing a splaying\nstep for a position pis asymptotically the same as the time needed just to reach that\nposition in a top-down search from the root of T.\nWorst-Case Time\nIn the worst case, the overall running time of a search, insertion, or deletion in asplay tree of height hisO(h), since the position we splay might be the deepest\nposition in the tree. Moreover, it is possible for hto be as large as n,a ss h o w ni n\nFigure 11.21. Thus, from a worst-case point of view, a splay tree is not an attractivedata structure.\nIn spite of its poor worst-case performance, a splay tree performs well in an\namortized sense. That is, in a sequence of intermixed searches, insertions, anddeletions, each operation takes on average logarithmic time. We perform the amor-tized analysis of splay trees using the accounting method.\nAmortized Performance of Splay Trees\nFor our analysis, we note that the time for performing a search, insertion, or deletionis proportional to the time for the associated splaying. So let us consider only\nsplaying time.\nLet Tbe a splay tree with nkeys, and let wbe a node of T.W e d e \ufb01 n e t h e\nsize n(w)ofwas the number of nodes in the subtree rooted at w. Note that this\nde\ufb01nition implies that the size of a nonleaf node is one more than the sum of the\nsizes of its children. We de\ufb01ne the rank r(w)of a node was the logarithm in base 2\nof the size of w,t h a ti s , r(w)=log (n(w)). Clearly, the root of Thas the maximum\nsize, n, and the maximum rank, log n, while each leaf has size 1 and rank 0.\nWe use cyber-dollars to pay for the work we perform in splaying a position p\ninT, and we assume that one cyber-dollar pays for a zig, while two cyber-dollars\npay for a zig-zig or a zig-zag. Hence, the cost of splaying a position at depth dis\ndcyber-dollars. We keep a virtual account storing cyber-dollars at each position of\nT. Note that this account exists only for the purpose of our amortized analysis, and\ndoes not need to be included in a data structure implementing the splay tree T.", "498 Chapter 11. Search Trees\nAn Accounting Analysis of Splaying\nWhen we perform a splaying, we pay a certain number of cyber-dollars (the exact\nvalue of the payment will be determined at the end of our analysis). We distinguishthree cases:\n\u2022If the payment is equal to the splaying work, then we use it all to pay for thesplaying.\n\u2022If the payment is greater than the splaying work, we deposit the excess in theaccounts of several nodes.\n\u2022If the payment is less than the splaying work, we make withdrawals from theaccounts of several nodes to cover the de\ufb01ciency.\nWe show below that a payment of O(logn)cyber-dollars per operation is suf\ufb01cient\nto keep the system working, that is, to ensure that each node keeps a nonnegativeaccount balance.\nAn Accounting Invariant for Splaying\nWe use a scheme in which transfers are made between the accounts of the nodesto ensure that there will always be enough cyber-dollars to withdraw for paying forsplaying work when needed.\nIn order to use the accounting method to perform our analysis of splaying, we\nmaintain the following invariant:\nBefore and after a splaying, each node wofThas r(w)cyber-dollars\nin its account.\nNote that the invariant is \u201c\ufb01nancially sound,\u201d since it does not require us to make apreliminary deposit to endow a tree with zero keys.\nLet r(T)be the sum of the ranks of all the nodes of T. To preserve the invariant\nafter a splaying, we must make a payment equal to the splaying work plus the total\nchange in r(T). We refer to a single zig, zig-zig, or zig-zag operation in a splaying\nas a splaying substep . Also, we denote the rank of a node wofTbefore and after\na splaying substep with r(w)and r\n/prime(w), respectively. The following proposition\ngives an upper bound on the change of r(T)caused by a single splaying substep.\nWe will repeatedly use this lemma in our analysis of a full splaying of a node to theroot.", "11.4. Splay Trees 499\nProposition 11.3: Let\u03b4be the variation of r(T)caused by a single splaying sub-\nstep (a zig, zig-zig, or zig-zag) for a node xinT. We have the following:\n\u2022\u03b4\u22643(r/prime(x)\u2212r(x))\u22122if the substep is a zig-zig or zig-zag.\n\u2022\u03b4\u22643(r/prime(x)\u2212r(x))if the substep is a zig.\nJusti\ufb01cation: We use the fact (see Proposition B.1, Appendix A) that, if a>0,\nb>0, and c>a+b,\nloga+logb<2log c\u22122. (11.6)\nLet us consider the change in r(T)caused by each type of splaying substep.\nzig-zig :(Recall Figure 11.16.) Since the size of each node is one more than the\nsize of its two children, note that only the ranks of x,y,a n d zchange in a\nzig-zig operation, where yis the parent of xand zis the parent of y.A l s o ,\nr/prime(x)= r(z),r/prime(y)\u2264r/prime(x),a n d r(x)\u2264r(y). Thus,\n\u03b4 = r/prime(x)+ r/prime(y)+ r/prime(z)\u2212r(x)\u2212r(y)\u2212r(z)\n= r/prime(y)+ r/prime(z)\u2212r(x)\u2212r(y)\n\u2264 r/prime(x)+ r/prime(z)\u22122r(x). (11.7)\nNote that n(x)+ n/prime(z)<n/prime(x). Thus, r(x)+ r/prime(z)<2r/prime(x)\u22122, as per For-\nmula 11.6; that is,\nr/prime(z)<2r/prime(x)\u2212r(x)\u22122.\nThis inequality and Formula 11.7 imply\n\u03b4\u2264 r/prime(x)+(2r/prime(x)\u2212r(x)\u22122)\u22122r(x)\n\u22643(r/prime(x)\u2212r(x))\u22122.\nzig-zag :(Recall Figure 11.17.) Again, by the de\ufb01nition of size and rank, only the\nranks of x,y,a n d zchange, where ydenotes the parent of xand zdenotes the\nparent of y.A l s o , r(x)<r(y)<r(z)= r/prime(x). Thus,\n\u03b4 = r/prime(x)+ r/prime(y)+ r/prime(z)\u2212r(x)\u2212r(y)\u2212r(z)\n= r/prime(y)+ r/prime(z)\u2212r(x)\u2212r(y)\n\u2264 r/prime(y)+ r/prime(z)\u22122r(x). (11.8)\nNote that n/prime(y)+ n/prime(z)<n/prime(x); hence, r/prime(y)+ r/prime(z)<2r/prime(x)\u22122, as per For-\nmula 11.6. Thus,\n\u03b4\u22642r/prime(x)\u22122\u22122r(x)\n=2(r/prime(x)\u2212r(x))\u22122\u22643(r/prime(x)\u2212r(x))\u22122.\nzig:(Recall Figure 11.18.) In this case, only the ranks of xand ychange, where y\ndenotes the parent of x.A l s o , r/prime(y)\u2264r(y)and r/prime(x)\u2265r(x). Thus,\n\u03b4 = r/prime(y)+ r/prime(x)\u2212r(y)\u2212r(x)\n\u2264 r/prime(x)\u2212r(x)\n\u22643(r/prime(x)\u2212r(x)).\n", "500 Chapter 11. Search Trees\nProposition 11.4: Let Tbe a splay tree with root t,a n dl e t \u0394be the total variation\nofr(T)caused by splaying a node xat depth d.W eh a v e\n\u0394\u22643(r(t)\u2212r(x))\u2212d+2.\nJusti\ufb01cation: Splaying node xconsists of c=\u2308d/2\u2309splaying substeps, each\nof which is a zig-zig or a zig-zag, except possibly the last one, which is a zig if\ndis odd. Let r0(x)= r(x)be the initial rank of x,a n df o r i=1,..., c,l e t ri(x)be\nthe rank of xafter the ithsubstep and \u03b4ibe the variation of r(T)caused by the ith\nsubstep. By Proposition 11.3, the total variation \u0394ofr(T)caused by splaying xis\n\u0394 =c\n\u2211\ni=1\u03b4i\n\u22642+c\n\u2211\ni=13(ri(x)\u2212ri\u22121(x))\u22122\n=3(rc(x)\u2212r0(x))\u22122c+2\n\u22643(r(t)\u2212r(x))\u2212d+2.\nBy Proposition 11.4, if we make a payment of 3 (r(t)\u2212r(x)) +2 cyber-dollars\ntowards the splaying of node x, we have enough cyber-dollars to maintain the in-\nvariant, keeping r(w)cyber-dollars at each node winT, and pay for the entire\nsplaying work, which costs dcyber-dollars. Since the size of the root tisn, its\nrank r(t)=logn. Given that r(x)\u22650, the payment to be made for splaying is\nO(logn)cyber-dollars. To complete our analysis, we have to compute the cost for\nmaintaining the invariant when a node is inserted or deleted.\nWhen inserting a new node winto a splay tree with nkeys, the ranks of all\nthe ancestors of ware increased. Namely, let w0,wi,..., wdbe the ancestors of w,\nwhere w0=w,wiis the parent of wi\u22121,a n d wdis the root. For i=1,..., d,l e t\nn/prime(wi)and n(wi)be the size of wibefore and after the insertion, respectively, and\nletr/prime(wi)and r(wi)be the rank of wibefore and after the insertion. We have\nn/prime(wi)= n(wi)+1.\nAlso, since n(wi)+1\u2264n(wi+1),f o r i=0,1,..., d\u22121, we have the following for\neach iin this range:\nr/prime(wi)=log (n/prime(wi)) = log (n(wi)+1)\u2264log (n(wi+1)) = r(wi+1).\nThus, the total variation of r(T)caused by the insertion is\nd\n\u2211\ni=1/parenleftbig\nr/prime(wi)\u2212r(wi)/parenrightbig\n\u2264 r/prime(wd)+d\u22121\n\u2211\ni=1(r(wi+1)\u2212r(wi))\n= r/prime(wd)\u2212r(w0)\n\u2264logn.\nTherefore, a payment of O(logn)cyber-dollars is suf\ufb01cient to maintain the invariant\nwhen a new node is inserted.", "11.4. Splay Trees 501\nWhen deleting a node wfrom a splay tree with nkeys, the ranks of all the an-\ncestors of ware decreased. Thus, the total variation of r(T)caused by the deletion\nis negative, and we do not need to make any payment to maintain the invariant\nwhen a node is deleted. Therefore, we may summarize our amortized analysis inthe following proposition (which is sometimes called the \u201cbalance proposition\u201d forsplay trees):\nProposition 11.5:\nConsider a sequence of moperations on a splay tree, each one\na search, insertion, or deletion, starting from a splay tree with zero keys. Also, let\nnibe the number of keys in the tree after operation i,a n d nbe the total number of\ninsertions. The total running time for performing the sequence of operations is\nO/parenleftBigg\nm+m\n\u2211\ni=1logni/parenrightBigg\n,\nwhich is O(mlogn).\nIn other words, the amortized running time of performing a search, insertion,\nor deletion in a splay tree is O(logn),w h e r e nis the size of the splay tree at the\ntime. Thus, a splay tree can achieve logarithmic-time amortized performance for\nimplementing a sorted map ADT. This amortized performance matches the worst-case performance of AVL trees, (2,4)trees, and red-black trees, but it does so\nusing a simple binary tree that does not need any extra balance information storedat each of its nodes. In addition, splay trees have a number of other interestingproperties that are not shared by these other balanced search trees. We explore onesuch additional property in the following proposition (which is sometimes calledthe \u201cStatic Optimality\u201d proposition for splay trees):\nProposition 11.6:\nConsider a sequence of moperations on a splay tree, each one\na search, insertion, or deletion, starting from a splay tree Twith zero keys. Also, let\nf(i)denote the number of times the entry iis accessed in the splay tree, that is, its\nfrequency, and let ndenote the total number of entries. Assuming that each entry is\naccessed at least once, then the total running time for performing the sequence of\noperations is\nO/parenleftBigg\nm+n\n\u2211\ni=1f(i)log (m/f(i))/parenrightBigg\n.\nWe omit the proof of this proposition, but it is not as hard to justify as one might\nimagine. The remarkable thing is that this proposition states that the amortized\nrunning time of accessing an entry iisO(log (m/f(i))).", "502 Chapter 11. Search Trees\n11.5 (2,4) Trees\nIn this section, we consider a data structure known as a (2,4) tree . It is a particular\nexample of a more general structure known as a multiway search tree ,i nw h i c h\ninternal nodes may have more than two children. Other forms of multiway search\ntrees will be discussed in Section 15.3.\n11.5.1 Multiway Search Trees\nRecall that general trees are de\ufb01ned so that internal nodes may have many children.\nIn this section, we discuss how general trees can be used as multiway search trees.Map items stored in a search tree are pairs of the form (k,v),w h e r e kis the keyand\nvis the value associated with the key.\nDe\ufb01nition of a Multiway Search Tree\nLet wbe a node of an ordered tree. We say that wis ad-node ifwhas dchildren.\nWe de\ufb01ne a multiway search tree to be an ordered tree Tthat has the following\nproperties, which are illustrated in Figure 11.23a:\n\u2022Each internal node of Thas at least two children. That is, each internal node\nis a d-node such that d\u22652.\n\u2022Each internal d-node wofTwith children c1,..., cdstores an ordered set of\nd\u22121 key-value pairs (k1,v1),...,(kd\u22121,vd\u22121),w h e r e k1\u2264\u00b7\u00b7\u00b7\u2264 kd\u22121.\n\u2022Let us conventionally de\ufb01ne k0=\u2212\u221e and kd=+\u221e. For each item (k,v)\nstored at a node in the subtree of wrooted at ci,i=1,..., d,w eh a v et h a t\nki\u22121\u2264k\u2264ki.\nThat is, if we think of the set of keys stored at was including the special \ufb01ctitious\nkeys k0=\u2212\u221e and kd=+\u221e,t h e nak e y kstored in the subtree of Trooted at a\nchild node cimust be \u201cin between\u201d two keys stored at w. This simple viewpoint\ngives rise to the rule that a d-node stores d\u22121 regular keys, and it also forms the\nbasis of the algorithm for searching in a multiway search tree.\nBy the above de\ufb01nition, the external nodes of a multiway search do not store\nany data and serve only as \u201cplaceholders.\u201d These external nodes can be ef\ufb01cientlyrepresented by None references, as has been our convention with binary search\ntrees (Section 11.1). However, for the sake of exposition, we will discuss these\nas actual nodes that do not store anything. Based on this de\ufb01nition, there is an\ninteresting relationship between the number of key-value pairs and the number ofexternal nodes in a multiway search tree.\nProposition 11.7:\nAn n-item multiway search tree has n+1external nodes.\nWe leave the justi\ufb01cation of this proposition as an exercise (C-11.52).", "11.5. (2,4) Trees 503\n25\n11 1368 2 7 23 24 34 1 451 022\n17\n(a)\n6851 022\n25\n11 13 1723 24 27 34 1 4\n(b)\n2324\n1727 34 6825\n11 131451 022\n(c)\nFigure 11.23: (a) A multiway search tree T; (b) search path in Tfor key 12 (unsuc-\ncessful search); (c) search path in Tfor key 24 (successful search).", "504 Chapter 11. Search Trees\nSearching in a Multiway Tree\nSearching for an item with key kin a multiway search tree Tis simple. We perform\nsuch a search by tracing a path in Tstarting at the root. (See Figure 11.23b and c.)\nWhen we are at a d-node wduring this search, we compare the key kwith the keys\nk1,..., kd\u22121stored at w.I f k=kifor some i, the search is successfully completed.\nOtherwise, we continue the search in the child ciofwsuch that ki\u22121<k<ki.\n(Recall that we conventionally de\ufb01ne k0=\u2212\u221e and kd=+\u221e.) If we reach an\nexternal node, then we know that there is no item with key kinT, and the search\nterminates unsuccessfully.\nData Structures for Representing Multiway Search Trees\nIn Section 8.3.3, we discuss a linked data structure for representing a general tree.\nThis representation can also be used for a multiway search tree. When using ageneral tree to implement a multiway search tree, we must store at each node one\nor more key-value pairs associated with that node. That is, we need to store with w\na reference to some collection that stores the items for w.\nDuring a search for key kin a multiway search tree, the primary operation\nneeded when navigating a node is \ufb01nding the smallest key at that node that is greaterthan or equal to k. For this reason, it is natural to model the information at a\nnode itself as a sorted map, allowing use of the \ufb01nd\nge(k) method. We say such\na map serves as a secondary data structure to support the primary data structure\nrepresented by the entire multiway search tree. This reasoning may at \ufb01rst seemlike a circular argument, since we need a representation of a (secondary) ordered\nmap to represent a (primary) ordered map. We can avoid any circular dependence,however, by using the bootstrapping technique, where we use a simple solution to\na problem to create a new, more advanced solution.\nIn the context of a multiway search tree, a natural choice for the secondary\nstructure at each node is the SortedTableMap of Section 10.3.1. Because we want\nto determine the associated value in case of a match for key k, and otherwise the\ncorresponding child c\nisuch that ki\u22121<k<ki, we recommend having each key\nkiin the secondary structure map to the pair (vi,ci). With such a realization of a\nmultiway search tree T, processing a d-node wwhile searching for an item of T\nwith key kcan be performed using a binary search operation in O(logd)time. Let\ndmaxdenote the maximum number of children of any node of T,a n dl e t hdenote the\nheight of T. The search time in a multiway search tree is therefore O(hlogdmax ).\nIfdmaxis a constant, the running time for performing a search is O(h).\nThe primary ef\ufb01ciency goal for a multiway search tree is to keep the height as\nsmall as possible. We next discuss a strategy that caps dmaxat 4 while guaranteeing\na height hthat is logarithmic in n, the total number of items stored in the map.", "11.5. (2,4) Trees 505\n11.5.2 (2,4)-Tree Operations\nA multiway search tree that keeps the secondary data structures stored at each node\nsmall and also keeps the primary multiway tree balanced is the (2,4)tree,w h i c hi s\nsometimes called a 2-4 tree or 2-3-4 tree. This data structure achieves these goalsby maintaining two simple properties (see Figure 11.24):\nSize Property :Every internal node has at most four children.\nDepth Property :All the external nodes have the same depth.\n12\n17 11 678 3451 0 1 5\n13 14\nFigure 11.24: A(2,4)tree.\nAgain, we assume that external nodes are empty and, for the sake of simplicity,\nwe describe our search and update methods assuming that external nodes are real\nnodes, although this latter requirement is not strictly needed.\nEnforcing the size property for (2,4)trees keeps the nodes in the multiway\nsearch tree simple. It also gives rise to the alternative name \u201c2-3-4 tree,\u201d since itimplies that each internal node in the tree has 2, 3, or 4 children. Another implica-tion of this rule is that we can represent the secondary map stored at each internalnode using an unordered list or an ordered array, and still achieve O(1)-time perfor-\nmance for all operations (since d\nmax =4). The depth property, on the other hand,\nenforces an important bound on the height of a (2,4)tree.\nProposition 11.8: The height of a (2,4)tree storing nitems is O(logn).\nJusti\ufb01cation: Let hbe the height of a (2,4)tree Tstoring nitems. We justify\nthe proposition by showing the claim\n1\n2log (n+1)\u2264h\u2264log (n+1). (11.9)\nTo justify this claim note \ufb01rst that, by the size property, we can have at most\n4 nodes at depth 1, at most 42nodes at depth 2, and so on. Thus, the number of\nexternal nodes in Tis at most 4h. Likewise, by the depth property and the de\ufb01nition", "506 Chapter 11. Search Trees\nof a (2,4)tree, we must have at least 2 nodes at depth 1, at least 22nodes at depth\n2, and so on. Thus, the number of external nodes in Tis at least 2h. In addition, by\nProposition 11.7, the number of external nodes in Tisn+1. Therefore, we obtain\n2h\u2264n+1\u22644h.\nTaking the logarithm in base 2 of the terms for the above inequalities, we get that\nh\u2264log (n+1)\u22642h,\nwhich justi\ufb01es our claim (Formula 11.9) when terms are rearranged.\nProposition 11.8 states that the size and depth properties are suf\ufb01cient for keep-\ning a multiway tree balanced. Moreover, this proposition implies that performing\na search in a (2,4)tree takes O(logn)time and that the speci\ufb01c realization of the\nsecondary structures at the nodes is not a crucial design choice, since the maximumnumber of children d\nmaxis a constant.\nMaintaining the size and depth properties requires some effort after performing\ninsertions and deletions in a (2,4)tree, however. We discuss these operations next.\nInsertion\nTo insert a new item (k,v), with key k,i n t oa (2,4)tree T, we \ufb01rst perform a search\nfork. Assuming that Thas no item with key k, this search terminates unsuccessfully\nat an external node z.L e t wbe the parent of z. We insert the new item into node w\nand add a new child y(an external node) to won the left of z.\nOur insertion method preserves the depth property, since we add a new external\nnode at the same level as existing external nodes. Nevertheless, it may violate thesize property. Indeed, if a node wwas previously a 4-node, then it would become\na 5-node after the insertion, which causes the tree Tto no longer be a (2,4)tree.\nThis type of violation of the size property is called an over\ufb02ow at node w,a n di t\nmust be resolved in order to restore the properties of a (2,4)tree. Let c\n1,..., c5be\nthe children of w,a n dl e t k1,..., k4be the keys stored at w. To remedy the over\ufb02ow\nat node w, we perform a split operation on was follows (see Figure 11.25):\n\u2022Replace wwith two nodes w/primeand w/prime/prime,w h e r e\n\u25e6w/primeis a 3-node with children c1,c2,c3storing keys k1and k2\n\u25e6w/prime/primeis a 2-node with children c4,c5storing key k4.\n\u2022Ifwis the root of T, create a new root node u; else, let ube the parent of w.\n\u2022Insert key k3into uand make w/primeand w/prime/primechildren of u,s ot h a ti f wwas child\niofu,t h e nw/primeand w/prime/primebecome children iand i+1o f u, respectively.\nAs a consequence of a split operation on node w, a new over\ufb02ow may occur at the\nparent uofw. If such an over\ufb02ow occurs, it triggers in turn a split at node u.( S e e\nFigure 11.26.) A split operation either eliminates the over\ufb02ow or propagates it intothe parent of the current node. We show a sequence of insertions in a (2,4)tree in\nFigure 11.27.", "11.5. (2,4) Trees 507\nh1h2\nc3 c2 c1 c5u\nw\nk1k2k3k4\nc4k3\nc3 c2 c1 c5w\nk1k2 k4\nc4u\nh1 h2\nw/prime\nc2 c1 c4 c5k1k2 k4h1k3h2u\nw/prime/prime\nc3\n(a) (b) (c)\nFigure 11.25: A node split: (a) over\ufb02ow at a 5-node w; (b) the third key of winserted\ninto the parent uofw; (c) node wreplaced with a 3-node w/primeand a 2-node w/prime/prime.\n1312\n14 678 11 34105\n15 151712\n14 678 11 34105\n13\n(a) (b)\n678 11 13 14 171551 01 2\n34 13 14 17 11 678 3451 01 21 5\n(c) (d)\n12\n13 14 17 11 678 3451 0 1 5 15\n17 11 678 3412\n51 0\n13 14\n(e) (f)\nFigure 11.26: An insertion in a (2,4)tree that causes a cascading split: (a) before\nthe insertion; (b) insertion of 17, causing an over\ufb02ow; (c) a split; (d) after the split\na new over\ufb02ow occurs; (e) another split, creating a new root node; (f) \ufb01nal tree.", "508 Chapter 11. Search Trees\n4 46 6 412 61 24 15\n(a) (b) (c) (d)\n12\n46 1 512\n41 56\n(e) (f)\n6 1512\n43 1512\n34 56\n(g) (h)\n1512\n345\n612\n15 4 35\n6\n(i) (j)\n1012\n31 5 6 45\n31 512\n10 4 685\n(k) (l)\nFigure 11.27: A sequence of insertions into a (2,4)tree: (a) initial tree with one\nitem; (b) insertion of 6; (c) insertion of 12; (d) insertion of 15, which causes an\nover\ufb02ow; (e) split, which causes the creation of a new root node; (f) after the split;(g) insertion of 3; (h) insertion of 5, which causes an over\ufb02ow; (i) split; (j) after the\nsplit; (k) insertion of 10; (l) insertion of 8.", "11.5. (2,4) Trees 509\nAnalysis of Insertion in a (2,4) Tree\nBecause dmaxis at most 4, the original search for the placement of new key kuses\nO(1)time at each level, and thus O(logn)time overall, since the height of the tree\nisO(logn)by Proposition 11.8.\nThe modi\ufb01cations to a single node to insert a new key and child can be im-\nplemented to run in O(1)time, as can a single split operation. The number of\ncascading split operations is bounded by the height of the tree, and so that phase of\nthe insertion process also runs in O(logn)time. Therefore, the total time to perform\nan insertion in a (2,4)tree is O(logn).\nDeletion\nLet us now consider the removal of an item with key kfrom a (2,4)tree T.W eb e g i n\nsuch an operation by performing a search in Tfor an item with key k.R e m o v i n g\nan item from a (2,4)tree can always be reduced to the case where the item to be\nremoved is stored at a node wwhose children are external nodes. Suppose, for\ninstance, that the item with key kthat we wish to remove is stored in the ithitem\n(ki,vi)at a node zthat has only internal-node children. In this case, we swap the\nitem (ki,vi)with an appropriate item that is stored at a node wwith external-node\nchildren as follows (see Figure 11.28d):\n1. We \ufb01nd the rightmost internal node win the subtree rooted at the ithchild of\nz, noting that the children of node ware all external nodes.\n2. We swap the item (ki,vi)atzwith the last item of w.\nOnce we ensure that the item to remove is stored at a node wwith only external-\nnode children (because either it was already at wor we swapped it into w), we\nsimply remove the item from wand remove the ithexternal node of w.\nRemoving an item (and a child) from a node was described above preserves the\ndepth property, for we always remove an external child from a node wwith only\nexternal children. However, in removing such an external node, we may violate\nthe size property at w. Indeed, if wwas previously a 2-node, then it becomes a\n1-node with no items after the removal (Figure 11.28a and d), which is not allowed\nin a (2,4)tree. This type of violation of the size property is called an under\ufb02ow\nat node w. To remedy an under\ufb02ow, we check whether an immediate sibling of w\nis a 3-node or a 4-node. If we \ufb01nd such a sibling s, then we perform a transfer\noperation, in which we move a child of stow,ak e yo f sto the parent uofwand s,\na n dak e yo f utow. (See Figure 11.28b and c.) If whas only one sibling, or if both\nimmediate siblings of ware 2-nodes, then we perform a fusion operation, in which\nwe merge wwith a sibling, creating a new node w/prime, and move a key from the parent\nuofwtow/prime. (See Figure 11.28e and f.)", "510 Chapter 11. Search Trees\n68 1 7 13 141512\n11451 0 10\n815\n13 14 17 11u\nw12\n5\n6s\n(a) (b)\nw\n11 17 13 1415 106\n5812\nsu\n13 14 8 561 0\n1712\n1511\n(c) (d)\n1710\nw11\n13 1415\n8 56u\n6\n13 14 81 0w/prime15\n17u11\n5\n(e) (f)\n6\n14 81 01315\n17 511\n81 011\n17 1461 5\n5\n(g) (h)\nFigure 11.28: A sequence of removals from a (2,4)tree: (a) removal of 4, causing\nan under\ufb02ow; (b) a transfer operation; (c) after the transfer operation; (d) removal\nof 12, causing an under\ufb02ow; (e) a fusion operation; (f) after the fusion operation;\n(g) removal of 13; (h) after removing 13.", "11.5. (2,4) Trees 511\nA fusion operation at node wmay cause a new under\ufb02ow to occur at the parent\nuofw, which in turn triggers a transfer or fusion at u. (See Figure 11.29.) Hence,\nthe number of fusion operations is bounded by the height of the tree, which is\nO(logn)by Proposition 11.8. If an under\ufb02ow propagates all the way up to the root,\nthen the root is simply deleted. (See Figure 11.29c and d.)\n11\n17 81 06\n51415 6\nw\n81 0 1 711\nu\n515\n(a) (b)\n81 06u\nw\n17 15 511\n17 8 51 061 1\n15\n(c) (d)\nFigure 11.29: A propagating sequence of fusions in a (2,4)tree: (a) removal of 14,\nwhich causes an under\ufb02ow; (b) fusion, which causes another under\ufb02ow; (c) second\nfusion operation, which causes the root to be removed; (d) \ufb01nal tree.\nPerformance of (2,4) Trees\nThe asymptotic performance of a (2,4)tree is identical to that of an AVL tree (see\nTable 11.2) in terms of the sorted map ADT, with guaranteed logarithmic boundsfor most operations. The time complexity analysis for a (2,4)tree having nkey-\nvalue pairs is based on the following:\n\u2022The height of a (2,4)tree storing nentries is O(logn), by Proposition 11.8.\n\u2022A split, transfer, or fusion operation takes O(1)time.\n\u2022A search, insertion, or removal of an entry visits O(logn)nodes.\nThus, (2,4)trees provide for fast map search and update operations. (2,4)trees\nalso have an interesting relationship to the data structure we discuss next.", "512 Chapter 11. Search Trees\n11.6 Red-Black Trees\nAlthough AVL trees and (2,4)trees have a number of nice properties, they also\nhave some disadvantages. For instance, AVL trees may require many restructure\noperations (rotations) to be performed after a deletion, and (2,4)trees may require\nmany split or fusing operations to be performed after an insertion or removal. Thedata structure we discuss in this section, the red-black tree, does not have thesedrawbacks; it uses O(1)structural changes after an update in order to stay balanced.\nFormally, a red-black tree is a binary search tree (see Section 11.1) with nodes\ncolored red and black in a way that satis\ufb01es the following properties:\nRoot Property :The root is black.\nRed Property :The children of a red node (if any) are black.\nDepth Property :All nodes with zero or one children have the same black depth ,\nde\ufb01ned as the number of black ancestors. (Recall that a node is its own\nancestor).\nAn example of a red-black tree is shown in Figure 11.30.\n10 13 171512\n5\n3\n4\n8 671 1 14\nFigure 11.30: An example of a red-black tree, with \u201cred\u201d nodes drawn in white. The\ncommon black depth for this tree is 3.\nWe can make the red-black tree de\ufb01nition more intuitive by noting an interest-\ning correspondence between red-black trees and (2,4)trees (excluding their trivial\nexternal nodes). Namely, given a red-black tree, we can construct a corresponding(2,4)tree by merging every red node winto its parent, storing the entry from wat\nits parent, and with the children of wbecoming ordered children of the parent. For\nexample, the red-black tree in Figure 11.30 corresponds to the (2,4)tree from Fig-\nure 11.24, as illustrated in Figure 11.31. The depth property of the red-black treecorresponds to the depth property of the (2,4)tree since exactly one black node of\nthe red-black tree contributes to each node of the corresponding (2,4)tree.\nConversely, we can transform any (2,4)tree into a corresponding red-black tree\nby coloring each node wblack and then performing the following transformations,\nas illustrated in Figure 11.32.", "11.6. Red-Black Trees 513\n115\n1413 17 31512\n4\n8 6710\nFigure 11.31: An illustration that the red-black tree of Figure 11.30 corresponds to\nthe (2,4)tree of Figure 11.24, based on the highlighted grouping of red nodes with\ntheir black parents.\n\u2022Ifwis a 2-node, then keep the (black) children of was is.\n\u2022Ifwis a 3-node, then create a new red node y,g i v e w\u2019s last two (black)\nchildren to y, and make the \ufb01rst child of wand ybe the two children of w.\n\u2022Ifwis a 4-node, then create two new red nodes yand z,g i v e w\u2019s \ufb01rst two\n(black) children to y,g i v e w\u2019s last two (black) children to z,a n dm a k ey and\nzbe the two children of w.\nNotice that a red node always has a black parent in this construction.\nProposition 11.9: The height of a red-black tree storing nentries is O(logn).\n15\n\u2190\u219215\n(a)\n13 14\n\u2190\u21921314\n1413\nor\n(b)\n768\n\u2190\u21927\n8 6(c)\nFigure 11.32: Correspondence between nodes of a (2,4)tree and a red-black tree:\n(a) 2-node; (b) 3-node; (c) 4-node.", "514 Chapter 11. Search Trees\nJusti\ufb01cation: Let Tbe a red-black tree storing nentries, and let hbe the height\nofT. We justify this proposition by establishing the following fact:\nlog (n+1)\u22121\u2264h\u22642log (n+1)\u22122.\nLet dbe the common black depth of all nodes of Thaving zero or one children.\nLet T/primebe the (2,4)tree associated with T,a n dl e t h/primebe the height of T/prime(excluding\ntrivial leaves). Because of the correspondence between red-black trees and (2,4)\ntrees, we know that h/prime=d. Hence, by Proposition 11.8, d=h/prime\u2264log (n+1)\u22121. By\nthe red property, h\u22642d. Thus, we obtain h\u22642log (n+1)\u22122. The other inequality,\nlog (n+1)\u22121\u2264h, follows from Proposition 8.8 and the fact that Thas nnodes.\n11.6.1 Red-Black Tree Operations\nThe algorithm for searching in a red-black tree Tis the same as that for a standard\nbinary search tree (Section 11.1). Thus, searching in a red-black tree takes time\nproportional to the height of the tree, which is O(logn)by Proposition 11.9.\nThe correspondence between (2,4)trees and red-black trees provides important\nintuition that we will use in our discussion of how to perform updates in red-black\ntrees; in fact, the update algorithms for red-black trees can seem mysteriously com-\nplex without this intuition. Split and fuse operations of a (2,4)tree will be effec-\ntively mimicked by recoloring neighboring red-black tree nodes. A rotation withina red-black tree will be used to change orientations of a 3-node between the two\nforms shown in Figure 11.32(b).\nInsertion\nNow consider the insertion of a key-value pair (k,v)into a red-black tree T.T h e\nalgorithm initially proceeds as in a standard binary search tree (Section 11.1.3).\nNamely, we search for kinTuntil we reach a null subtree, and we introduce a new\nleaf xat that position, storing the item. In the special case that xis the only node\nofT, and thus the root, we color it black. In all other cases, we color xred. This\naction corresponds to inserting (k,v)into a node of the (2,4)tree T/primewith external\nchildren. The insertion preserves the root and depth properties of T,b u ti tm a y\nviolate the red property. Indeed, if xis not the root of Tand the parent yofxis\nred, then we have a parent and a child (namely, yand x) that are both red. Note that\nby the root property, ycannot be the root of T, and by the red property (which was\npreviously satis\ufb01ed), the parent zofymust be black. Since xand its parent are red,\nbut x\u2019s grandparent zis black, we call this violation of the red property a double\nredat node x. To remedy a double red, we consider two cases.", "11.6. Red-Black Trees 515\nCase 1: The Sibling s of y Is Black (or None) .(See Figure 11.33.) In this case,\nthe double red denotes the fact that we have added the new node to a cor-\nresponding 3-node of the (2,4)tree T/prime, effectively creating a malformed\n4-node. This formation has one red node ( y) that is the parent of another\nred node ( x), while we want it to have the two red nodes as siblings instead.\nTo \ufb01x this problem, we perform a trinode restructuring ofT. The trinode\nrestructuring is done by the operation restructure (x), which consists of the\nfollowing steps (see again Figure 11.33; this operation is also discussed inSection 11.2):\n\u2022Take node x, its parent y, and grandparent z, and temporarily relabel\nthem as a,b,a n d c, in left-to-right order, so that a,b,a n d cwill be\nvisited in this order by an inorder tree traversal.\n\u2022Replace the grandparent zwith the node labeled b, and make nodes a\nand cthe children of b, keeping inorder relationships unchanged.\nAfter performing the restructure (x)operation, we color bblack and we color\naand cred. Thus, the restructuring eliminates the double-red problem. No-\ntice that the portion of any path through the restructured part of the tree is\nincident to exactly one black node, both before and after the trinode restruc-\nturing. Therefore, the black depth of the tree is unaffected.\nz\nsxy\n102030\nxz\ny\ns1030\n20y\nxz\ns\n302010\nxz\ny\ns3010\n20\n(a)\nb\nac\n10 3020\n(b)\nFigure 11.33: Restructuring a red-black tree to remedy a double red: (a) the four\ncon\ufb01gurations for x,y,a n d zbefore restructuring; (b) after restructuring.", "516 Chapter 11. Search Trees\nCase 2: The Sibling s of y Is Red .(See Figure 11.34.) In this case, the double red\ndenotes an over\ufb02ow in the corresponding (2,4)tree T/prime. To \ufb01x the problem,\nwe perform the equivalent of a split operation. Namely, we do a recoloring :\nwe color yand sblack and their parent zred (unless zis the root, in which\ncase, it remains black). Notice that unless zis the root, the portion of any\npath through the affected part of the tree is incident to exactly one black\nnode, both before and after the recoloring. Therefore, the black depth of the\ntree is unaffected by the recoloring unless zis the root, in which case it is\nincreased by one.\nHowever, it is possible that the double-red problem reappears after such a\nrecoloring, albeit higher up in the tree T,s i n c ez may have a red parent. If\nthe double-red problem reappears at z, then we repeat the consideration of the\ntwo cases at z. Thus, a recoloring either eliminates the double-red problem\nat node x, or propagates it to the grandparent zofx. We continue going\nupTperforming recolorings until we \ufb01nally resolve the double-red problem\n(with either a \ufb01nal recoloring or a trinode restructuring). Thus, the numberof recolorings caused by an insertion is no more than half the height of treeT,t h a ti s , O(logn)by Proposition 11.9.\nz\ny\nxs\n1020 4030\n10 20 30 40\n(a)\nz\ny\nxs30\n20 40\n10 40 10 20...3 0...\n(b)\nFigure 11.34: Recoloring to remedy the double-red problem: (a) before recoloring\nand the corresponding 5-node in the associated (2,4)tree before the split; (b) after\nrecoloring and the corresponding nodes in the associated (2,4)tree after the split.\nAs further examples, Figures 11.35 and 11.36 show a sequence of insertion\noperations in a red-black tree.", "11.6. Red-Black Trees 517\n4 4\n7\n1274\n12 47\n(a) (b) (c) (d)\n1512 47\n1512 47\n12\n15 347\n15127\n5 34\n(e) (f) (g) (h)\n15127\n145 34\n12 15147\n5 34\n(i) (j)\n4\n181514\n12 5 37\n4\n181514\n12 5 37\n(k) (l)\nFigure 11.35: A sequence of insertions in a red-black tree: (a) initial tree; (b) inser-\ntion of 7; (c) insertion of 12, which causes a double red; (d) after restructuring; (e)\ninsertion of 15, which causes a double red; (f) after recoloring (the root remainsblack); (g) insertion of 3; (h) insertion of 5; (i) insertion of 14, which causes a\ndouble red; (j) after restructuring; (k) insertion of 18, which causes a double red;\n(l) after recoloring. (Continues in Figure 11.36.)", "518 Chapter 11. Search Trees\n4\n18314\n157\n12 5\n164\n18314\n167\n12 5\n15\n(m) (n)\n4\n15 18314\n167\n12 5\n174\n15 18314\n167\n12 5\n17\n(o) (p)\n1271 6\n1715\n34\n514\n18\n(q)\nFigure 11.36: A sequence of insertions in a red-black tree: (m) insertion of 16,\nwhich causes a double red; (n) after restructuring; (o) insertion of 17, which causes\na double red; (p) after recoloring there is again a double red, to be handled by arestructuring; (q) after restructuring. (Continued from Figure 11.35.)", "11.6. Red-Black Trees 519\nDeletion\nDeleting an item with key kfrom a red-black tree Tinitially proceeds as for a binary\nsearch tree (Section 11.1.3). Structurally, the process results in the removal a node\nthat has at most one child (either that originally containing key kor its inorder\npredecessor) and the promotion of its remaining child (if any).\nIf the removed node was red, this structural change does not affect the black\ndepths of any paths in the tree, nor introduce any red violations, and so the resultingtree remains a valid red-black tree. In the corresponding (2,4)tree T\n/prime, this case\ndenotes the shrinking of a 3-node or 4-node. If the removed node was black, thenit either had zero children or it had one child that was a red leaf (because the nullsubtree of the removed node has black height 0). In the latter case, the removednode represents the black part of a corresponding 3-node, and we restore the red-black properties by recoloring the promoted child to black.\nThe more complex case is when a (nonroot) black leaf is removed. In the cor-\nresponding (2,4)tree, this denotes the removal of an item from a 2-node. Without\nrebalancing, such a change results in a de\ufb01cit of one for the black depth along thepath leading to the deleted item. By necessity, the removed node must have a sib-ling whose subtree has black height 1 (given that this was a valid red-black treeprior to the deletion of the black leaf).\nTo remedy this scenario, we consider a more general setting with a node zthat\nis known to have two subtrees, T\nheavy and Tlight, such that the root of Tlight(if any) is\nblack and such that the black depth of Theavy is exactly one more than that of Tlight,\nas portrayed in Figure 11.37. In the case of a removed black leaf, zis the parent of\nthat leaf and Tlightis trivially the empty subtree that remains after the deletion. We\ndescribe the more general case of a de\ufb01cit because our algorithm for rebalancingthe tree will, in some cases, push the de\ufb01cit higher in the tree (just as the resolution\nof a deletion in a (2,4)tree sometimes cascades upward). We let ydenote the root\nofT\nheavy. (Such a node exists because Theavy has black height at least one.)\ny\nTheavyTlightz\nFigure 11.37: Portrayal of a de\ufb01cit between the black heights of subtrees of node z.\nThe gray color in illustrating yand zdenotes the fact that these nodes may be\ncolored either black or red.", "520 Chapter 11. Search Trees\nWe consider three possible cases to remedy a de\ufb01cit.\nCase 1: Node y Is Black and Has a Red Child x . (See Figure 11.38.)\nWe perform a trinode restructuring , as originally described in Section 11.2.\nThe operation restructure (x)takes the node x, its parent y, and grandparent\nz, labels them temporarily left to right as a,b,a n d c, and replaces zwith the\nnode labeled b, making it the parent of the other two. We color aand cblack,\nand give bthe former color of z.\nNotice that the path to Tlightin the result includes one additional black node\nafter the restructure, thereby resolving its de\ufb01cit. In contrast, the number\nof black nodes on paths to any of the other three subtrees illustrated in Fig-ure 11.38 remains unchanged.\nResolving this case corresponds to a transfer operation in the (2,4)tree T\n/prime\nbetween the two children of the node with z. The fact that yhas a red child\nassures us that it represents either a 3-node or a 4-node. In effect, the item\npreviously stored at zis demoted to become a new 2-node to resolve the\nde\ufb01ciency, while an item stored at yor its child is promoted to take the place\nof the item previously stored at z.\nyz\nx\nTlightx\nz y\nTlight10 30\n201030 20\nz\ny\nx\nTlighty\nz x\nTlight10 3020 30\n20\n10\nFigure 11.38: Resolving a black de\ufb01cit in Tlightby performing a trinode restructuring\nasrestructure (x). Two possible con\ufb01gurations are shown (two other con\ufb01gurations\nare symmetric). The gray color of zin the left \ufb01gures denotes the fact that this node\nmay be colored either red or black. The root of the restructured portion is giventhat same color, while the children of that node are both colored black in the result.", "11.6. Red-Black Trees 521\nCase 2: Node y Is Black and Both Children of y Are Black (or None) .\nResolving this case corresponds to a fusion operation in the corresponding\n(2,4)tree T/prime,a s ymust represent a 2-node. We do a recoloring ; we color\nyred, and, if zis red, we color it black. (See Figure 11.39). This does not\nintroduce any red violation, because ydoes not have a red child.\nIn the case that zwas originally red, and thus the parent in the corresponding\n(2,4)tree is a 3-node or 4-node, this recoloring resolves the de\ufb01cit. (See\nFigure 11.39a.) The path leading to Tlightincludes one additional black node\nin the result, while the recoloring did not affect the number of black nodes\non the path to the subtrees of Theavy.\nIn the case that zwas originally black, and thus the parent in the correspond-\ning (2,4)tree is a 2-node, the recoloring has not increased the number of\nblack nodes on the path to Tlight; in fact, it has reduced the number of black\nnodes on the path to Theavy. (See Figure 11.39b.) After this step, the two chil-\ndren of zwill have the same black height. However, the entire tree rooted at\nzhas become de\ufb01cient, thereby propogating the problem higher in the tree;\nwe must repeat consideration of all three cases at the parent of zas a remedy.\nz\ny\nTheavyTlightz\ny 30\n2030\n20\n(a)\nz\nTlighty y\nTheavyTlightz\n30\n20 2030\n(b)\nFigure 11.39: Resolving a black de\ufb01cit in Tlightby a recoloring operation: (a) when\nzis originally red, reversing the colors of yand zresolves the black de\ufb01cit in Tlight,\nending the process; (b) when zis originally black, recoloring ycauses the entire\nsubtree of zto have a black de\ufb01cit, requiring a cascading remedy.", "522 Chapter 11. Search Trees\nCase 3: N o d eyI sR e d . (See Figure 11.40.)\nBecause yis red and Theavy has black depth at least 1, zmust be black and the\ntwo subtrees of ymust each have a black root and a black depth equal to that\nofTheavy. In this case, we perform a rotation about yand z, and then recolor y\nblack and zred. This denotes a reorientation of a 3-node in the corresponding\n(2,4)tree T/prime.\nThis does not immediately resolve the de\ufb01cit, as the new subtree of zis an old\nsubtree of ywith black root y/primeand black height equal to that of the original\nTheavy. We reapply the algorithm to resolve the de\ufb01cit at z, knowing that the\nnew child y/prime, that is the root of Theavy is now black, and therefore that either\nCase 1 applies or Case 2 applies. Furthermore, the next application will be\nthe last, because Case 1 is always terminal and Case 2 will be terminal given\nthat zis red.\ny\nz\nTheavyy/prime\nTlightz\ny\nTheavyTlight30 2030 20\nFigure 11.40: A rotation and recoloring about red node yand black node z, assuming\na black de\ufb01cit at z. This amounts to a change of orientation in the corresponding\n3-node of a (2,4)tree. This operation does not affect the black depth of any paths\nthrough this portion of the tree. Furthermore, because ywas originally red, the\nnew subtree of zmust have a black root y/primeand must have black height equal to the\noriginal Theavy. Therefore, a black de\ufb01cit remains at node zafter the transformation.\nIn Figure 11.41, we show a sequence of deletions on a red-black tree. A dashed\nedge in those \ufb01gures, such as to the right of 7 in part (c), represents a branch with ablack de\ufb01ciency that has not yet been resolved. We illustrate a Case 1 restructuringin parts (c) and (d). We illustrate a Case 2 recoloring in parts (f) and (g). Finally,we show an example of a Case 3 rotation between parts (i) and (j), concluding with\na Case 2 recoloring in part (k).", "11.6. Red-Black Trees 523\n1271 6\n1715\n34\n514\n187\n516\n1814\n1715 12 4\n(a) (b)\n416\n1814\n17157\n5716\n1814\n17155\n4\n(c) (d)\n14\n16\n15 18 7 45 16\n1514\n7 45 5\n1514\n7 416\n(e) (f) (g)\n1614\n7 4514\n7 45 4\n75\n14 14\n75\n4\n(h) (i) (j) (k)\nFigure 11.41: A sequence of deletions from a red-black tree: (a) initial tree; (b) re-\nmoval of 3; (c) removal of 12, causing a black de\ufb01cit to the right of 7 (handled by\nrestructuring); (d) after restructuring; (e) removal of 17; (f) removal of 18, causing\na black de\ufb01cit to the right of 16 (handled by recoloring); (g) after recoloring; (h) re-\nmoval of 15; (i) removal of 16, causing a black de\ufb01cit to the right of 14 (handledinitially by a rotation); (j) after the rotation the black de\ufb01cit needs to be handled bya recoloring; (k) after the recoloring.", "524 Chapter 11. Search Trees\nPerformance of Red-Black Trees\nThe asymptotic performance of a red-black tree is identical to that of an AVL tree\nor a (2,4)tree in terms of the sorted map ADT, with guaranteed logarithmic time\nbounds for most operations. (See Table 11.2 for a summary of the AVL perfor-mance.) The primary advantage of a red-black tree is that an insertion or deletionrequires only a constant number of restructuring operations . (This is in contrast\nto AVL trees and (2,4)trees, both of which require a logarithmic number of struc-\ntural changes per map operation in the worst case.) That is, an insertion or deletion\nin a red-black tree requires logarithmic time for a search, and may require a loga-\nrithmic number of recoloring operations that cascade upward. Yet we show, in thefollowing propositions, that there are a constant number of rotations or restructureoperations for a single map operation.\nProposition 11.10:\nThe insertion of an item in a red-black tree storing nitems\ncan be done in O(logn)time and requires O(logn)recolorings and at most one\ntrinode restructuring.\nJusti\ufb01cation: Recall that an insertion begins with a downward search, the cre-\nation of a new leaf node, and then a potential upward effort to remedy a double-red\nviolation. There may be logarithmically many recoloring operations due to an up-ward cascading of Case 2 applications, but a single application of the Case 1 actioneliminates the double-red problem with a trinode restructuring. Therefore, at most\none restructuring operation is needed for a red-black tree insertion.\nProposition 11.11: The algorithm for deleting an item from a red-black tree with\nnitems takes O(logn)time and performs O(logn)recolorings and at most two\nrestructuring operations.\nJusti\ufb01cation: A deletion begins with the standard binary search tree deletion\nalgorithm, which requires time proportional to the height of the tree; for red-black\ntrees, that height is O(logn). The subsequent rebalancing takes place along an\nupward path from the parent of a deleted node.\nWe considered three cases to remedy a resulting black de\ufb01cit. Case 1 requires a\ntrinode restructuring operation, yet completes the process, so this case is applied at\nmost once. Case 2 may be applied logarithmically many times, but it only involves\na recoloring of up to two nodes per application. Case 3 requires a rotation, but thiscase can only apply once, because if the rotation does not resolve the problem, thevery next action will be a terminal application of either Case 1 or Case 2.\nIn the worst case, there will be O(logn)recolorings from Case 2, a single rota-\ntion from Case 3, and a trinode restructuring from Case 1.\n", "11.6. Red-Black Trees 525\n11.6.2 Python Implementation\nA complete implementation of a RedBlackTreeMap class is provided in Code Frag-\nments 11.15 through 11.17. It inherits from the standard TreeMap class and relies\non the balancing framework described in Section 11.2.1.\nWe begin, in Code Fragment 11.15, by overriding the de\ufb01nition of the nested\nNode class to introduce an additional Boolean \ufb01eld to denote the current color\nof a node. Our constructor intentionally sets the color of a new node to red to\nbe consistent with our approach for inserting items. We de\ufb01ne several additional\nutility functions, at the top of Code Fragment 11.16, that aid in setting the color of\nnodes and querying various conditions.\nWhen an element has been inserted as a leaf in the tree, the\n rebalance\n insert\nhook is called, allowing us the opportunity to modify the tree. The new node isred by default, so we need only look for the special case of the new node being\nthe root (in which case it should be colored black), or the possibility that we have\na double-red violation because the new node\u2019s parent is also red. To remedy suchviolations, we closely follow the case analysis described in Section 11.6.1.\nThe rebalancing after a deletion also follows the case analysis described in\nSection 11.6.1. An additional challenge is that by the time the\nrebalance\n hook is\ncalled, the old node has already been removed from the tree. That hook is invokedon the parent of the removed node. Some of the case analysis depends on knowing\nabout the properties of the removed node. Fortunately, we can reverse engineer thatinformation by relying on the red-black tree properties. In particular, if pdenotes\nthe parent of the removed node, it must be that:\n\u2022Ifphas no children, the removed node was a red leaf. (Exercise R-11.26.)\n\u2022Ifphas one child, the removed node was a black leaf, causing a de\ufb01cit,\nunless that one remaining child is a red leaf. (Exercise R-11.27.)\n\u2022Ifphas two children, the removed node was a black node with one red child,\nwhich was promoted. (Exercise R-11.28.)\n1classRedBlackTreeMap(TreeMap):\n2\u201d\u201d\u201dSorted map implementation using a red-black tree.\u201d\u201d\u201d\n3class\nNode(TreeMap.\n Node):\n4 \u201d\u201d\u201dNode class for red-black tree maintains bit that denotes color.\u201d\u201d\u201d\n5\n slots\n =\n_red\n # add additional data member to the Node class\n67 def\ninit\n(self,e l e m e n t ,p a r e n t = None,l e f t =None,r i g h t = None):\n8 super().\ninit\n(element, parent, left, right)\n9 self.\nred =True # new node red by default\nCode Fragment 11.15: Beginning of the RedBlackTreeMap class. (Continued in\nCode Fragment 11.16.)", "526 Chapter 11. Search Trees\n10 #------------------------- positional-based utility methods -------------------------\n11 # we consider a nonexistent child to be trivially black\n12def\nset\nred(self,p ) :p .\n node.\n red =True\n13def\nset\nblack(self,p ) :p .\n node.\n red =False\n14def\nset\ncolor(self,p ,m a k e\n red): p.\n node.\n red = make\n red\n15def\nis\nred(self,p ) :return pis not None and p.\nnode.\n red\n16def\nis\nred\nleaf(self,p ) :return self .\nis\nred(p) and self .is\nleaf(p)\n17\n18def\nget\nred\nchild(self,p ) :\n19 \u201d\u201d\u201dReturn a red child of p (or None if no such child).\u201d\u201d\u201d\n20 forchildin(self.left(p), self.right(p)):\n21 if self.\nis\nred(child):\n22 return child\n23 return None\n2425 #------------------------- support for insertions -------------------------\n26def\nrebalance\n insert(self,p ) :\n27 self.\nresolve\n red(p) # new node is always red\n2829def\nresolve\n red(self,p ) :\n30 if self.is\nroot(p):\n31 self.\nset\nblack(p) #m a k er o o tb l a c k\n32 else:\n33 parent = self.parent(p)\n34 if self.\nis\nred(parent): # double red problem\n35 uncle = self.sibling(parent)\n36 if not self .\nis\nred(uncle): # Case 1: misshapen 4-node\n37 middle = self.\nrestructure(p) # do trinode restructuring\n38 self.\nset\nblack(middle) # and then \ufb01x colors\n39 self.\nset\nred(self.left(middle))\n40 self.\nset\nred(self.right(middle))\n41 else: # Case 2: overfull 5-node\n42 grand = self.parent(parent)\n43 self.\nset\nred(grand) # grandparent becomes red\n44 self.\nset\nblack(self.left(grand)) # its children become black\n45 self.\nset\nblack(self.right(grand))\n46 self.\nresolve\n red(grand) # recur at red grandparent\nCode Fragment 11.16: Continuation of the RedBlackTreeMap class. (Continued\nfrom Code Fragment 11.15, and concluded in Code Fragment 11.17.)", "11.6. Red-Black Trees 527\n47 #------------------------- support for deletions -------------------------\n48def\nrebalance\n delete( self,p ) :\n49 iflen(self)= =1 :\n50 self.\nset\nblack(self.root()) # special case: ensure that root is black\n51 elifpis not None :\n52 n=self.num\n children(p)\n53 ifn= =1 : # de\ufb01cit exists unless child is a red leaf\n54 c=n e x t ( self.children(p))\n55 if not self .\nis\nred\nleaf(c):\n56 self.\n\ufb01x\nde\ufb01cit(p, c)\n57 elifn= =2 : # removed black node with red child\n58 if self.\n is\nred\nleaf(self .left(p)):\n59 self.\nset\nblack(self.left(p))\n60 else:\n61 self.\nset\nblack(self.right(p))\n62\n63def\n\ufb01x\nde\ufb01cit( self,z ,y ) :\n64 \u201d\u201d\u201dResolve black de\ufb01cit at z, where y is the root of z\n s heavier subtree.\u201d\u201d\u201d\n65 if not self .\nis\nred(y): # y is black; will apply Case 1 or 2\n66 x=self.\nget\nred\nchild(y)\n67 ifxis not None :# Case 1: y is black and has red child x; do \u201dtransfer\u201d\n68 old\ncolor = self.\nis\nred(z)\n69 middle = self.\nrestructure(x)\n70 self.\nset\ncolor(middle, old\n color) # middle gets old color of z\n71 self.\nset\nblack(self.left(middle)) # children become black\n72 self.\nset\nblack(self.right(middle))\n73 else:# Case 2: y is black, but no red children; recolor as \u201dfusion\u201d\n74 self.\nset\nred(y)\n75 if self.\n is\nred(z):\n76 self.\nset\nblack(z) # this resolves the problem\n77 elif not self .is\nroot(z):\n78 self.\n\ufb01x\nde\ufb01cit( self.parent(z), self.sibling(z)) # recur upward\n79 else:# Case 3: y is red; rotate misaligned 3-node and repeat\n80 self.\nrotate(y)\n81 self.\nset\nblack(y)\n82 self.\nset\nred(z)\n83 ifz= =self.right(y):\n84 self.\n\ufb01x\nde\ufb01cit(z, self.left(z))\n85 else:\n86 self.\n\ufb01x\nde\ufb01cit(z, self.right(z))\nCode Fragment 11.17: Conclusion of the RedBlackTreeMap class. (Continued from\nCode Fragment 11.16.)", "528 Chapter 11. Search Trees\n11.7 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-11.1 If we insert the entries (1,A),(2,B),(3,C),(4,D),a n d (5,E),i nt h i so r d e r ,\ninto an initially empty binary search tree, what will it look like?\nR-11.2 Insert, into an empty binary search tree, entries with keys 30, 40, 24, 58,\n48, 26, 11, 13 (in this order). Draw the tree after each insertion.\nR-11.3 How many different binary search trees can store the keys {1,2,3}?\nR-11.4 Dr. Amongus claims that the order in which a \ufb01xed set of entries is insertedinto a binary search tree does not matter\u2014the same tree results every time.\nGive a small example that proves he is wrong.\nR-11.5 Dr. Amongus claims that the order in which a \ufb01xed set of entries is inserted\ninto an A VL tree does not matter\u2014the same A VL tree results every time.Give a small example that proves he is wrong.\nR-11.6 Our implementation of the TreeMap.\nsubtree\n search utility, from Code\nFragment 11.4, relies on recursion. For a large unbalanced tree, Python\u2019s\ndefault limit on recursive depth may be prohibitive. Give an alternative\nimplementation of that method that does not rely on the use of recursion.\nR-11.7 Do the trinode restructurings in Figures 11.12 and 11.14 result in singleor double rotations?\nR-11.8 Draw the AVL tree resulting from the insertion of an entry with key 52into the AVL tree of Figure 11.14b.\nR-11.9 Draw the A VL tree resulting from the removal of the entry with key 62from the AVL tree of Figure 11.14b.\nR-11.10 Explain why performing a rotation in an n-node binary tree when using\nthe array-based representation of Section 8.3.2 takes \u03a9(n)time.\nR-11.11 Give a schematic \ufb01gure, in the style of Figure 11.13, showing the heightsof subtrees during a deletion operation in an AVL tree that triggers a tri-node restructuring for the case in which the two children of the node de-noted as ystart with equal heights. What is the net effect of the height of\nthe rebalanced subtree due to the deletion operation?\nR-11.12 Repeat the previous problem, considering the case in which y\u2019s children\nstart with different heights.", "11.7. Exercises 529\nR-11.13 The rules for a deletion in an AVL tree speci\ufb01cally require that when the\ntwo subtrees of the node denoted as yhave equal height, child xshould be\nchosen to be \u201caligned\u201d with y(so that xand yare both left children or both\nright children). To better understand this requirement, repeat Exercise R-\n11.11 assuming we picked the misaligned choice of x. Why might there\nbe a problem in restoring the AVL property with that choice?\nR-11.14 Perform the following sequence of operations in an initially empty splaytree and draw the tree after each set of operations.\na. Insert keys 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, in this order.\nb. Search for keys 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, in this order.\nc. Delete keys 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, in this order.\nR-11.15 What does a splay tree look like if its entries are accessed in increasingorder by their keys?\nR-11.16 Is the search tree of Figure 11.23(a) a (2,4)tree? Why or why not?\nR-11.17 An alternative way of performing a split at a node win a (2,4)tree is\nto partition winto w\n/primeand w/prime/prime, with w/primebeing a 2-node and w/prime/primea 3-node.\nWhich of the keys k1,k2,k3,o r k4do we store at w\u2019s parent? Why?\nR-11.18 Dr. Amongus claims that a (2,4)tree storing a set of entries will always\nhave the same structure, regardless of the order in which the entries areinserted. Show that he is wrong.\nR-11.19 Draw four different red-black trees that correspond to the same (2,4)tree.\nR-11.20 Consider the set of keys K={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15}.\na. Draw a (2,4)tree storing Kas its keys using the fewest number of\nnodes.\nb. Draw a (2,4)tree storing Kas its keys using the maximum number\nof nodes.\nR-11.21 Consider the sequence of keys (5,16,22,45,2,10,18,30,50,12,1).D r a w\nthe \nresult of inserting entries with these keys (in the given order) into\na. An initially empty (2,4)tree.\nb. An initially empty red-black tree.\nR-11.22 For the following statements about red-black trees, provide a justi\ufb01cation\nfor each true statement and a counterexample for each false one.\na. A subtree of a red-black tree is itself a red-black tree.\nb. A node that does not have a sibling is red.\nc. There is a unique (2,4)tree associated with a given red-black tree.\nd. There is a unique red-black tree associated with a given (2,4)tree.\nR-11.23 Explain why you would get the same output in an inorder listing of the\nentries in a binary search tree, T, independent of whether Tis maintained\nto be an AVL tree, splay tree, or red-black tree.", "530 Chapter 11. Search Trees\nR-11.24 Consider a tree Tstoring 100,000 entries. What is the worst-case height\nofTin the following cases?\na.Tis a binary search tree.\nb.Tis an AVL tree.\nc.Tis a splay tree.\nd.Tis a (2,4)tree.\ne.Tis a red-black tree.\nR-11.25 Draw an example of a red-black tree that is not an AVL tree.\nR-11.26 Let Tbe a red-black tree and let pbe the position of the parent of the\noriginal node that is deleted by the standard search tree deletion algorithm.\nProve that if phas zero children, the removed node was a red leaf.\nR-11.27 Let Tbe a red-black tree and let pbe the position of the parent of the\noriginal node that is deleted by the standard search tree deletion algorithm.Prove that if phas one child, the deletion has caused a black de\ufb01cit at p,\nexcept for the case when the one remaining child is a red leaf.\nR-11.28 Let Tbe a red-black tree and let pbe the position of the parent of the\noriginal node that is deleted by the standard search tree deletion algorithm.Prove that if phas two children, the removed node was black and had one\nred child.\nCreativity\nC-11.29 Explain how to use an AVL tree or a red-black tree to sort ncomparable\nelements in O(nlogn)time in the worst case.\nC-11.30 Can we use a splay tree to sort ncomparable elements in O(nlogn)time\nin the worst case ? Why or why not?\nC-11.31 Repeat Exercise C-10.28 for the TreeMap class.\nC-11.32 Show that any n-node binary tree can be converted to any other n-node\nbinary tree using O(n)rotations.\nC-11.33 For a key kthat is not found in binary search tree T, prove that both the\ngreatest key less than kand the least key greater than klie on the path\ntraced by the search for k.\nC-11.34 In Section 11.1.2 we claim that the \ufb01nd\nrange method of a binary search\ntree executes in O(s+h)time where sis the number of items found within\nthe range and his the height of the tree. Our implementation, in Code\nFragment 11.6 begins by searching for the starting key, and then repeat-edly calling the after method until reaching the end of the range. Each call\ntoafter is guaranteed to run in O(h)time. This suggests a weaker O(sh)\nbound for \ufb01nd\nrange , since it involves O(s)calls to after . Prove that this\nimplementation achieves the stronger O(s+h)bound.", "11.7. Exercises 531\nC-11.35 Describe how to perform an operation remove\n range(start, stop) that re-\nmoves all the items whose keys fall within range(start, stop) in a sorted\nmap that is implemented with a binary search tree T, and show that this\nmethod runs in time O(s+h),w h e r e sis the number of items removed\nand his the height of T.\nC-11.36 Repeat the previous problem using an AVL tree, achieving a running time\nofO(slogn). Why doesn\u2019t the solution to the previous problem trivially\nresult in an O(s+logn)algorithm for AVL trees?\nC-11.37 Suppose we wish to support a new method count\n range(start, stop) that\ndetermines how many keys of a sorted map fall in the speci\ufb01ed range. Wecould clearly implement this in O(s+h)time by adapting our approach to\n\ufb01nd\nrange . Describe how to modify the search tree structure to support\nO(h)worst-case time for count\n range .\nC-11.38 If the approach described in the previous problem were implemented aspart of the TreeMap class, what additional modi\ufb01cations (if any) would be\nnecessary to a subclass such as AVLTreeMap in order to maintain support\nfor the new method?\nC-11.39 Draw a schematic of an AVL tree such that a single remove operation\ncould require \u03a9(logn)trinode restructurings (or rotations) from a leaf to\nthe root in order to restore the height-balance property.\nC-11.40 In our AVL implementation, each node stores the height of its subtree,\nwhich is an arbitrarily large integer. The space usage for an AVL tree\ncan be reduced by instead storing the balance factor of a node, which is\nde\ufb01ned as the height of its left subtree minus the height of its right subtree.Thus, the balance factor of a node is always equal to \u22121, 0, or 1, except\nduring an insertion or removal, when it may become temporarily equal to\n\u22122o r +2. Reimplement the AVLTreeMap class storing balance factors\nrather than subtree heights.\nC-11.41 If we maintain a reference to the position of the leftmost node of a bi-nary search tree, then operation \ufb01nd\nmincan be performed in O(1)time.\nDescribe how the implementation of the other map methods need to bemodi\ufb01ed to maintain a reference to the leftmost position.\nC-11.42 If the approach described in the previous problem were implemented aspart of the TreeMap class, what additional modi\ufb01cations (if any) would\nbe necessary to a subclass such as AVLTreeMap in order to accurately\nmaintain the reference to the leftmost position?\nC-11.43 Describe a modi\ufb01cation to the binary search tree implementation havingworst-case O(1)-time performance for methods after(p) andbefore(p)\nwithout adversely affecting the asymptotics of any other methods.", "532 Chapter 11. Search Trees\nC-11.44 If the approach described in the previous problem were implemented as\npart of the TreeMap class, what additional modi\ufb01cations (if any) would\nbe necessary to a subclass such as AVLTreeMap in order to maintain the\nef\ufb01ciency?\nC-11.45 For a standard binary search tree, Table 11.1 claims O(h)-time perfor-\nmance for the delete(p) method. Explain why delete(p) would run in\nO(1)time if given a solution to Exercise C-11.43.\nC-11.46 Describe a modi\ufb01cation to the binary search tree data structure that would\nsupport the following two index-based operations for a sorted map in O(h)\ntime, where his the height of the tree.\nat\nindex(i) :Return the position pof the item at index iof a sorted map.\nindex\n of(p): Return the index iof the item at position pof a sorted map.\nC-11.47 Draw a splay tree, T1, together with the sequence of updates that produced\nit, and a red-black tree, T2, on the same set of ten entries, such that a\npreorder traversal of T1would be the same as a preorder traversal of T2.\nC-11.48 Show that the nodes that become temporarily unbalanced in an AVL treeduring an insertion may be nonconsecutive on the path from the newlyinserted node to the root.\nC-11.49 Show that at most one node in an AVL tree becomes temporarily un-balanced after the immediate deletion of a node as part of the standard\ndelitem\n map operation.\nC-11.50 Let Tand Ube (2,4)trees storing nand mentries, respectively, such\nthat all the entries in Thave keys less than the keys of all the entries in\nU. Describe an O(logn+logm)-time method for joining Tand Uinto a\nsingle tree that stores all the entries in Tand U.\nC-11.51 Repeat the previous problem for red-black trees Tand U.\nC-11.52 Justify Proposition 11.7.\nC-11.53 The Boolean indicator used to mark nodes in a red-black tree as being\n\u201cred\u201d or \u201cblack\u201d is not strictly needed when we have distinct keys. De-\nscribe a scheme for implementing a red-black tree without adding anyextra space to standard binary search tree nodes.\nC-11.54 Let Tbe a red-black tree storing nentries, and let kbe the key of an entry\ninT. Show how to construct from T,i n O(logn)time, two red-black trees\nT\n/primeand T/prime/prime, such that T/primecontains all the keys of Tless than k,a n d T/prime/prime\ncontains all the keys of Tgreater than k. This operation destroys T.\nC-11.55 Show that the nodes of any AVL tree Tcan be colored \u201cred\u201d and \u201cblack\u201d\nso that Tbecomes a red-black tree.", "11.7. Exercises 533\nC-11.56 The standard splaying step requires two passes, one downward pass to\n\ufb01nd the node xto splay, followed by an upward pass to splay the node\nx. Describe a method for splaying and searching for xin one downward\npass. Each substep now requires that you consider the next two nodes\nin the path down to x, with a possible zig substep performed at the end.\nDescribe how to perform the zig-zig, zig-zag, and zig steps.\nC-11.57 Consider a variation of splay trees, called half-splay trees , where splaying\na node at depth dstops as soon as the node reaches depth \u230ad/2\u230b. Perform\nan amortized analysis of half-splay trees.\nC-11.58 Describe a sequence of accesses to an n-node splay tree T,w h e r e nis odd,\nthat results in Tconsisting of a single chain of nodes such that the path\ndown Talternates between left children and right children.\nC-11.59 As a positional structure, our TreeMap implementation has a subtle \ufb02aw.\nA position instance passociated with an key-value pair (k,v)should re-\nmain valid as long as that item remains in the map. In particular, thatposition should be unaffected by calls to insert or delete other items in thecollection. Our algorithm for deleting an item from a binary search tree\nmay fail to provide such a guarantee, in particular because of our rule for\nusing the inorder predecessor of a key as a replacement when deleting akey that is located in a node with two children. Given an explicit series ofPython commands that demonstrates such a \ufb02aw.\nC-11.60 How might the TreeMap implementation be changed to avoid the \ufb02aw\ndescribed in the previous problem?\nProjects\nP-11.61 Perform an experimental study to compare the speed of our AVL tree,\nsplay tree, and red-black tree implementations for various sequences of\noperations.\nP-11.62 Redo the previous exercise, including an implementation of skip lists.(See Exercise P-10.53.)\nP-11.63 Implement the Map ADT using a (2,4)tree. (See Section 10.1.1.)\nP-11.64 Redo the previous exercise, including all methods of the Sorted Map ADT.\n(See Section 10.3.)\nP-11.65 Redo Exercise P-11.63 providing positional support, as we did for bi-\nnary search trees (Section 11.1.1), so as to include methods \ufb01rst() ,last() ,\nbefore(p) ,after(p) ,a n d\ufb01nd\nposition(k) . Each item should have a dis-\ntinct position in this abstraction, even though several items may be storedat a single node of a tree.", "534 Chapter 11. Search Trees\nP-11.66 Write a Python class that can take any red-black tree and convert it into its\ncorresponding (2,4)tree and can take any (2,4)tree and convert it into its\ncorresponding red-black tree.\nP-11.67 In describing multisets and multimaps in Section 10.5.3, we describe ageneral approach for adapting a traditional map by storing all duplicates\nwithin a secondary container as a value in the map. Give an alternativeimplementation of a multimap using a binary search tree such that eachentry of the map is stored at a distinct node of the tree. With the existenceof duplicates, we rede\ufb01ne the search tree property so that all items in the\nleft subtree of a position pwith key khave keys that are less than or equal\nto k, while all items in the right subtree of phave keys that are greater than\nor equal to k . Use the public interface given in Code Fragment 10.17.\nP-11.68 Prepare an implementation of splay trees that uses top-down splaying as\ndescribed in Exercise C-11.56. Perform extensive experimental studies tocompare its performance to the standard bottom-up splaying implementedin this chapter.\nP-11.69 The mergeable heap ADT is an extension of the priority queue ADT\nconsisting of operations add(k, v), min() ,remove\nmin() andmerge(h) ,\nwhere the merge(h) operations performs a union of the mergeable heap h\nwith the present one, incorporating all items into the current one whileemptying h. Describe a concrete implementation of the mergeable heap\nADT that achieves O(logn)performance for all its operations, where n\ndenotes the size of the resulting heap for the merge operation.\nP-11.70 Write a program that performs a simple n-body simulation, called \u201cJump-\ning Leprechauns.\u201d This simulation involves nleprechauns, numbered 1 to\nn. It maintains a gold value g\nifor each leprechaun i, which begins with\neach leprechaun starting out with a million dollars worth of gold, that is,\ngi=1000000 for each i=1,2,..., n. In addition, the simulation also\nmaintains, for each leprechaun, i, a place on the horizon, which is repre-\nsented as a double-precision \ufb02oating-point number, xi. In each iteration\nof the simulation, the simulation processes the leprechauns in order. Pro-cessing a leprechaun iduring this iteration begins by computing a new\nplace on the horizon for i, which is determined by the assignment\nx\ni=xi+rgi,\nwhere ris a random \ufb02oating-point number between \u22121 and 1. The lep-\nrechaun ithen steals half the gold from the nearest leprechauns on either\nside of him and adds this gold to his gold value, gi. Write a program that\ncan perform a series of iterations in this simulation for a given number,\nn, of leprechauns. You must maintain the set of horizon positions using a\nsorted map data structure described in this chapter.", "Chapter Notes 535\nChapter Notes\nSome of the data structures discussed in this chapter are extensively covered by Knuth\nin his Sorting and Searching book [65], and by Mehlhorn in [76]. A VL trees are due to\nAdel\u2019son-Vel\u2019skii and Landis [2], who invented this class of balanced search trees in 1962.\nBinary search trees, A VL trees, and hashing are described in Knuth\u2019s Sorting and Search-\ning[65] book. Average-height analyses for binary search trees can be found in the books by\nAho, Hopcroft, and Ullman [6] and Cormen, Leiserson, Rivest and Stein [29]. The hand-\nbook by Gonnet and Baeza-Yates [44] contains a number of theoretical and experimentalcomparisons among map implement ations. Aho, Hopcroft, and Ullman [5] discuss (2,3)\ntrees, which are similar to (2,4)trees. Red-black trees were de\ufb01ned by Bayer [10]. Vari-\nations and interesting properties of red-black trees are presented in a paper by Guibas and\nSedgewick [48]. The reader interested in learning more about different balanced tree datastructures is referred to the books by Mehlhorn [76] and Tarjan [95], and the book chapter\nby Mehlhorn and Tsakalidis [78]. Knuth [65] i s excellent additional r eading that includes\nearly approaches to balancing trees. Splay trees were invented by Sleator and Tarjan [89]\n(see also [95]).", "Chapter\n12Sorting and Selection\nContents\n1 2 . 1W h yS t u d yS o r t i n gA l g o r i t h m s ? ............... 5 3 7\n1 2 . 2M e r g e - S o r t........................... 5 3 8\n12.2.1 Divide-and-Conquer . . . . . . . . . . . . . . . . . . . . . 538\n12.2.2 Array-Based Implementation of Merge-Sort . . . . . . . . 54312.2.3 The Running Time of Merge-Sort . . . . . . . . . . . . . 544\n12.2.4 Merge-Sort and Recurrence Equations\u22c6..........5 4 6\n12.2.5 Alternative Implementations of Merge-Sort . . . . . . . . 547\n1 2 . 3Q u i c k - S o r t ........................... 5 5 0\n1 2 . 3 . 1R a n d o m i z e d Q u i c k - S o r t...................5 5 712.3.2 Additional Optimizations for Quick-Sort . . . . . . . . . . 559\n12.4 Studying Sorting through an Algorithmic Lens . . . . . . . 562\n1 2 . 4 . 1L o w e r B o u n df o r S o r t i n g ..................5 6 2\n12.4.2 Linear-Time Sorting: Bucket-Sort and Radix-Sort . . . . . 564\n1 2 . 5C o m p a r i n gS o r t i n gA l g o r i t h m s ................ 5 6 7\n1 2 . 6P y t h o n \u2019 sB u i l t - I nS o r t i n gF u n c t i o n s ............. 5 6 9\n1 2 . 6 . 1S o r t i n g A c c o r d i n g t o aK e y F u n c t i o n............5 6 9\n1 2 . 7S e l e c t i o n ............................ 5 7 1\n1 2 . 7 . 1P r u n e - a n d - S e a r c h......................5 7 11 2 . 7 . 2R a n d o m i z e d Q u i c k - S e l e c t..................5 7 21 2 . 7 . 3A n a l y z i n g R a n d o m i z e d Q u i c k - S e l e c t ............5 7 3\n1 2 . 8E x e r c i s e s ............................ 5 7 4\n", "12.1. Why Study Sorting Algorithms? 537\n12.1 Why Study Sorting Algorithms?\nMuch of this chapter focuses on algorithms for sorting a collection of objects.\nGiven a collection, the goal is to rearrange the elements so that they are orderedfrom smallest to largest (or to produce a new copy of the sequence with such anorder). As we did when studying priority queues (see Section 9.4), we assume that\nsuch a consistent order exists. In Python, the natural order of objects is typically\n1\nde\ufb01ned using the <operator having following properties:\n\u2022Irre\ufb02exive property :k/negationslash<k.\n\u2022Transitive property :i f k1<k2and k2<k3,t h e n k1<k3.\nThe transitive property is important as it allows us to infer the outcome of certain\ncomparisons without taking the time to perform those comparisons, thereby leadingto more ef\ufb01cient algorithms.\nSorting is among the most important, and well studied, of computing problems.\nData sets are often stored in sorted order, for example, to allow for ef\ufb01cient searcheswith the binary search algorithm (see Section 4.1.3). Many advanced algorithmsfor a variety of problems rely on sorting as a subroutine.\nPython has built-in support for sorting data, in the form of the sortmethod of\nthelistclass that rearranges the contents of a list, and the built-in sorted function\nthat produces a new list containing the elements of an arbitrary collection in sortedorder. Those built-in functions use advanced algorithms (some of which we willdescribe in this chapter), and they are highly optimized. A programmer shouldtypically rely on calls to the built-in sorting functions, as it is rare to have a specialenough circumstance to warrant implementing a sorting algorithm from scratch.\nWith that said, it remains important to have a deep understanding of sorting\nalgorithms. Most immediately, when calling the built-in function, it is good toknow what to expect in terms of ef\ufb01ciency and how that may depend upon theinitial order of elements or the type of objects that are being sorted. More generally,the ideas and approaches that have led to advances in the development of sorting\nalgorithm carry over to algorithm development in many other areas of computing.\nWe have introduced several sorting algorithms already in this book:\n\u2022Insertion-sort (see Sections 5.5.2, 7.5, and 9.4.1)\n\u2022Selection-sort (see Section 9.4.1)\n\u2022Bubble-sort (see Exercise C-7.38)\n\u2022Heap-sort (see Section 9.4.2)\nIn this chapter, we present four other sorting algorithms, called merge-sort ,\nquick-sort ,bucket-sort ,a n d radix-sort , and then discuss the advantages and disad-\nvantages of the various algorithms in Section 12.5.\n1In Section 12.6.1, we will explore another technique used in Python for sorting data according\nto an order other than the natural order de\ufb01ned by the <operator.", "538 Chapter 12. Sorting and Selection\n12.2 Merge-Sort\n12.2.1 Divide-and-Conquer\nThe \ufb01rst two algorithms we describe in this chapter, merge-sort and quick-sort, use\nrecursion in an algorithmic design pattern called divide-and-conquer .W e h a v e\nalready seen the power of recursion in describing algorithms in an elegant manner(see Chapter 4). The divide-and-conquer pattern consists of the following three\nsteps:\n1.Divide: If the input size is smaller than a certain threshold (say, one or two\nelements), solve the problem directly using a straightforward method and\nreturn the solution so obtained. Otherwise, divide the input data into two ormore disjoint subsets.\n2.Conquer: Recursively solve the subproblems associated with the subsets.\n3.Combine: Take the solutions to the subproblems and merge them into a so-\nlution to the original problem.\nUsing Divide-and-Conquer for Sorting\nWe will \ufb01rst describe the merge-sort algorithm at a high level, without focusing onwhether the data is an array-based (Python) list or a linked list; we will soon give\nconcrete implementations for each. To sort a sequence Swith nelements using the\nthree divide-and-conquer steps, the merge-sort algorithm proceeds as follows:\n1.Divide: IfShas zero or one element, return Simmediately; it is already\nsorted. Otherwise ( Shas at least two elements), remove all the elements\nfrom Sand put them into two sequences, S\n1and S2, each containing about\nhalf of the elements of S;t h a ti s , S1contains the \ufb01rst \u230an/2\u230belements of S,\nand S2contains the remaining \u2308n/2\u2309elements.\n2.Conquer: Recursively sort sequences S1and S2.\n3.Combine: Put back the elements into Sby merging the sorted sequences S1\nand S2into a sorted sequence.\nIn reference to the divide step, we recall that the notation \u230ax\u230bindicates the \ufb02oor of\nx, that is, the largest integer k, such that k\u2264x. Similarly, the notation \u2308x\u2309indicates\ntheceiling ofx, that is, the smallest integer m, such that x\u2264m.", "12.2. Merge-Sort 539\nWe can visualize an execution of the merge-sort algorithm by means of a binary\ntree T, called the merge-sort tree . Each node of Trepresents a recursive invocation\n(or call) of the merge-sort algorithm. We associate with each node vofTthe\nsequence Sthat is processed by the invocation associated with v. The children of\nnode vare associated with the recursive calls that process the subsequences S1and\nS2ofS. The external nodes of Tare associated with individual elements of S,\ncorresponding to instances of the algorithm that make no recursive calls.\nFigure 12.1 summarizes an execution of the merge-sort algorithm by showing\nthe input and output sequences processed at each node of the merge-sort tree. The\nstep-by-step evolution of the merge-sort tree is shown in Figures 12.2 through 12.4.\nThis algorithm visualization in terms of the merge-sort tree helps us analyze\nthe running time of the merge-sort algorithm. In particular, since the size of theinput sequence roughly halves at each recursive call of merge-sort, the height ofthe merge-sort tree is about log n(recall that the base of log is 2 if omitted).\n4585\n50 3124 17 31 63 45 96 5045 63 24 85 17 31 96 50\n17 31 96 50 85 24 63 45\n85 63 17 96 24\n(a)\n31 5024\n9685 17 31 45 63 50 9617 31 50 96 24 45 63 8545 31 24 17 50 63 85 96\n85 63 17 24 45\n(b)\nFigure 12.1: Merge-sort tree Tfor an execution of the merge-sort algorithm on\na sequence with 8 elements: (a) input sequences processed at each node of T;\n(b) output sequences generated at each node of T.", "540 Chapter 12. Sorting and Selection\n96 45 63 24 85 17 31 50 31 96 50\n85 24 63 4517\n(a) (b)\n50\n45 63\n8517 31 96\n2445 6317\n24\n8531 96 50\n(c) (d)\n4550\n8563\n2417 31 96\n246350 96 31 17\n8545\n(e) (f)\nFigure 12.2: Visualization of an execution of merge-sort. Each node of the tree\nrepresents a recursive call of merge-sort. The nodes drawn with dashed lines repre-\nsent calls that have not been made yet. The node drawn with thick lines representsthe current call. The empty nodes drawn with thin lines represent completed calls.The remaining nodes (drawn with thin lines and not empty) represent calls that are\nwaiting for a child invocation to return. (Continues in Figure 12.3.)", "12.2. Merge-Sort 541\n24 85 6350 96 31 17\n45\n63 4517 31 96 50\n24 85\n(g) (h)\n45\n6317 31 96 50\n24 85\n63\n4517 31 96 50\n24 85\n(i) (j)\n45 6317 31 96 50\n24 8517 31 96 50\n24 63 45 85\n(k) (l)\nFigure 12.3: Visualization of an execution of merge-sort. (Combined with Fig-\nures 12.2 and 12.4.)", "542 Chapter 12. Sorting and Selection\n24\n17 31 96 5085 63 45\n17 31 50 9685 63 45 24\n(m) (n)\n63 85 96 50 17 24 31 45 45 31 24 17 50 63 85 96\n(o) (p)\nFigure 12.4: Visualization of an execution of merge-sort (continued from Fig-\nure 12.3). Several invocations are omitted between (m) and (n). Note the merging\nof two halves performed in step (p).\nProposition 12.1: The merge-sort tree associated with an execution of merge-\nsort on a sequence of size nhas height \u2308logn\u2309.\nWe leave the justi\ufb01cation of Proposition 12.1 as a simple exercise (R-12.1). We\nwill use this proposition to analyze the running time of the merge-sort algorithm.\nHaving given an overview of merge-sort and an illustration of how it works,\nlet us consider each of the steps of this divide-and-conquer algorithm in more de-\ntail. Dividing a sequence of size ninvolves separating it at the element with index\n\u2308n/2\u2309, and recursive calls can be started by passing these smaller sequences as pa-\nrameters. The dif\ufb01cult step is combining the two sorted sequences into a singlesorted sequence. Thus, before we present our analysis of merge-sort, we need to\nsay more about how this is done.", "12.2. Merge-Sort 543\n12.2.2 Array-Based Implementation of Merge-Sort\nWe begin by focusing on the case when a sequence of items is represented as an\n(array-based) Python list. The merge function (Code Fragment 12.1) is responsible\nfor the subtask of merging two previously sorted sequences, S1and S2, with the\noutput copied into S. We copy one element during each pass of the while loop,\nconditionally determining whether the next element should be taken from S1orS2.\nThe divide-and-conquer merge-sort algorithm is given in Code Fragment 12.2.\nWe illustrate a step of the merge process in Figure 12.5. During the process,\nindex irepresents the number of elements of S1that have been copied to S, while\nindex jrepresents the number of elements of S2that have been copied to S. Assum-\ning S1and S2both have at least one uncopied element, we copy the smaller of the\ntwo elements being considered. Since i+jobjects have been previously copied,\nthe next element is placed in S[i+j]. (For example, when i+jis 0, the next ele-\nment is copied to S[0]). If we reach the end of one of the sequences, we must copy\nthe next element from the other.\n1defmerge(S1, S2, S):\n2\u201d\u201d\u201dMerge two sorted Python lists S1 and S2 into properly sized list S.\u201d\u201d\u201d\n3i=j=0\n4while i+j<len(S):\n5 ifj= =l e n ( S 2 ) or(i<len(S1) andS1[i]<S2[j]):\n6 S[i+j] = S1[i] # copy ith element of S1 as next item of S\n7 i+ =1\n8 else:\n9 S[i+j] = S2[j] # copy jth element of S2 as next item of S\n10 j+ =1\nCode Fragment 12.1: An implementation of the merge operation for Python\u2019s array-\nbased listclass.\nS1\nSS201234 6 5\n25 18 19 22 91 0\n9 2ji\ni+j31234 6 5\n11 12 14 2580\n15\n01234 6789 1 0 5 1 11 21 3\n8 35 SS1\nS2\n501234 6 5\n25 18 19 22 31 0\n9 2i\nj\ni+j1091234 6 5\n11 12 14 2580\n15\n01234 6789 1 0 5 1 11 21 3\n8 3\n(a) (b)\nFigure 12.5: A step in the merge of two sorted arrays for which S2[j]<S1[i].W e\nshow the arrays before the copy step in (a) and after it in (b).", "544 Chapter 12. Sorting and Selection\n1defmerge\n sort(S):\n2\u201d\u201d\u201dSort the elements of Python list S using the merge-sort algorithm.\u201d\u201d\u201d\n3n=l e n ( S )\n4ifn<2:\n5 return # list is already sorted\n6# divide\n7mid = n // 2\n8S1 = S[0:mid] #c o p yo f\ufb01 r s th a l f\n9S2 = S[mid:n] # copy of second half\n10 # conquer (with recursion)\n11 merge\n sort(S1) #s o r tc o p yo f\ufb01 r s th a l f\n12 merge\n sort(S2) # sort copy of second half\n13 #m e r g er e s u l t s\n14 merge(S1, S2, S) # merge sorted halves back into S\nCode Fragment 12.2: An implementation of the recursive merge-sort algorithm for\nPython\u2019s array-based listclass (using the merge function de\ufb01ned in Code Frag-\nment 12.1.\n12.2.3 The Running Time of Merge-Sort\nWe begin by analyzing the running time of the merge algorithm. Let n1and n2\nbe the number of elements of S1and S2, respectively. It is clear that the operations\nperformed inside each pass of the while loop take O(1)time. The key observation is\nthat during each iteration of the loop, one element is copied from either S1orS2into\nS(and that element is considered no further). Therefore, the number of iterations\nof the loop is n1+n2. Thus, the running time of algorithm merge isO(n1+n2).\nHaving analyzed the running time of the merge algorithm used to combine\nsubproblems, let us analyze the running time of the entire merge-sort algorithm,\nassuming it is given an input sequence of nelements. For simplicity, we restrict our\nattention to the case where nis a power of 2. We leave it to an exercise (R-12.3) to\nshow that the result of our analysis also holds when nis not a power of 2.\nWhen evaluating the merge-sort recursion, we rely on the analysis technique\nintroduced in Section 4.2. We account for the amount of time spent within eachrecursive call, but excluding any time spent waiting for successive recursive callsto terminate. In the case of our merge\nsort function, we account for the time to\ndivide the sequence into two subsequences, and the call to merge to combine the\ntwo sorted sequences, but we exclude the two recursive calls to merge\n sort.", "12.2. Merge-Sort 545\nA merge-sort tree T, as portrayed in Figures 12.2 through 12.4, can guide our\nanalysis. Consider a recursive call associated with a node vof the merge-sort tree T.\nThe divide step at node vis straightforward; this step runs in time proportional to\nthe size of the sequence for v, based on the use of slicing to create copies of the two\nlist halves. We have already observed that the merging step also takes time that is\nlinear in the size of the merged sequence. If we let idenote the depth of node v,\nthe time spent at node visO(n/2i), since the size of the sequence handled by the\nrecursive call associated with vis equal to n/2i.\nLooking at the tree Tmore globally, as shown in Figure 12.6, we see that, given\nour de\ufb01nition of \u201ctime spent at a node,\u201d the running time of merge-sort is equal tothe sum of the times spent at the nodes of T. Observe that Thas exactly 2\ninodes at\ndepth i. This simple observation has an important consequence, for it implies that\nthe overall time spent at all the nodes of Tat depth iisO(2i\u00b7n/2i),w h i c hi sO (n).\nBy Proposition 12.1, the height of Tis\u2308logn\u2309. Thus, since the time spent at each\nof the \u2308logn\u2309+1l e v e l so f TisO(n), we have the following result:\nProposition 12.2: Algorithm merge-sort sorts a sequence Sof size ninO(nlogn)\ntime, assuming two elements of Scan be compared in O(1)time.\nHeight Time per level\nTotal time: O(nlogn)O(n)O(n)\nO(logn)\nO(n)n\nn/2\nn/4 n/4 n/4 n/4n/2\nFigure 12.6: A visual analysis of the running time of merge-sort. Each node rep-\nresents the time spent in a particular recursive call, labeled with the size of itssubproblem.", "546 Chapter 12. Sorting and Selection\n12.2.4 Merge-Sort and Recurrence Equations \u22c6\nThere is another way to justify that the running time of the merge-sort algorithm is\nO(nlogn)(Proposition 12.2). Namely, we can deal more directly with the recursive\nnature of the merge-sort algorithm. In this section, we present such an analysis of\nthe running time of merge-sort, and in so doing, introduce the mathematical concept\nof arecurrence equation (also known as recurrence relation ).\nLet the function t(n)denote the worst-case running time of merge-sort on an\ninput sequence of size n. Since merge-sort is recursive, we can characterize func-\ntion t(n)by means of an equation where the function t(n)is recursively expressed\nin terms of itself. In order to simplify our characterization of t(n), let us restrict\nour attention to the case when nis a power of 2. (We leave the problem of showing\nthat our asymptotic characterization still holds in the general case as an exercise.)In this case, we can specify the de\ufb01nition of t(n)as\nt(n)=/braceleftbiggb ifn\u22641\n2t(n/2)+ cnotherwise.\nAn expression such as the one above is called a recurrence equation, since thefunction appears on both the left- and right-hand sides of the equal sign. Althoughsuch a characterization is correct and accurate, what we really desire is a big-Oh\ntype of characterization of t(n)that does not involve the function t(n)itself. That\nis, we want a closed-form characterization of t(n).\nWe can obtain a closed-form solution by applying the de\ufb01nition of a recurrence\nequation, assuming nis relatively large. For example, after one more application\nof the equation above, we can write a new recurrence for t(n)as\nt(n)= 2(2t(n/2\n2)+( cn/2)) + cn\n=22t(n/22)+2(cn/2)+ cn =22t(n/22)+2cn.\nIf we apply the equation again, we get t(n)=23t(n/23)+3cn. At this point, we\nshould see a pattern emerging, so that after applying this equation itimes, we get\nt(n)= 2it(n/2i)+ icn.\nThe issue that remains, then, is to determine when to stop this process. To see whento stop, recall that we switch to the closed form t(n)= bwhen n\u22641, which will\noccur when 2\ni=n. In other words, this will occur when i=logn.M a k i n g t h i s\nsubstitution, then, yields\nt(n)= 2lognt(n/2logn)+(logn)cn\n= nt(1)+ cnlogn\n= nb+cnlogn.\nThat is, we get an alternative justi\ufb01cation of the fact that t(n)isO(nlogn).", "12.2. Merge-Sort 547\n12.2.5 Alternative Implementations of Merge-Sort\nSorting Linked Lists\nThe merge-sort algorithm can easily be adapted to use any form of a basic queue\nas its container type. In Code Fragment 12.3, we provide such an implementation,based on use of the LinkedQueue class from Section 7.1.2. The O(nlogn)bound\nfor merge-sort from Proposition 12.2 applies to this implementation as well, since\neach basic operation runs in O(1)time when implemented with a linked list. We\nshow an example execution of this version of the merge algorithm in Figure 12.7.\n1defmerge(S1, S2, S):\n2\u201d\u201d\u201dMerge two sorted queue instances S1 and S2 into empty queue S.\u201d\u201d\u201d\n3while not S1.is\nempty( ) and not S2.is\nempty():\n4 ifS1.\ufb01rst( ) <S2.\ufb01rst():\n5 S.enqueue(S1.dequeue())\n6 else:\n7 S.enqueue(S2.dequeue())\n8while not S1.is\nempty(): # move remaining elements of S1 to S\n9 S.enqueue(S1.dequeue())\n10while not S2.is\nempty(): # move remaining elements of S2 to S\n11 S.enqueue(S2.dequeue())\n12\n13defmerge\n sort(S):\n14 \u201d\u201d\u201dSort the elements of queue S using the merge-sort algorithm.\u201d\u201d\u201d\n15 n=l e n ( S )\n16ifn<2:\n17 return # list is already sorted\n18 # divide\n19 S1 = LinkedQueue( ) # or any other queue implementation\n20 S2 = LinkedQueue()\n21while len(S1) <n/ /2 : # move the \ufb01rst n//2 elements to S1\n22 S1.enqueue(S.dequeue())\n23while not S.is\nempty(): # move the rest to S2\n24 S2.enqueue(S.dequeue())\n25 # conquer (with recursion)\n26 merge\n sort(S1) #s o r t\ufb01 r s th a l f\n27 merge\n sort(S2) # sort second half\n28 #m e r g er e s u l t s\n29 merge(S1, S2, S) # merge sorted halves back into S\nCode Fragment 12.3: An implementation of merge-sort using a basic queue.", "548 Chapter 12. Sorting and Selection\n24 45 63 85 S1\n17 31 50 96 S2\nS24 45 63 85 S1\n1731 50 96 S2\nS\n(a) (b)\n2445 63 85 S1\n1731 50 96 S2\nS 2445 63 85 S1\n1750 96 S2\nS 31\n(c) (d)\n2463 85 S1\n1750 96 S2\nS 31 45 2463 85 S1\n1796 S2\nS 31 45 50\n(e) (f)\n2485 S1\n1796 S2\nS 31 45 50 6324S1\n1796 S2\nS 31 45 50 63 85\n(g) (h)\n24S1\n17S2\nS 31 45 50 63 85 96\n(i)\nFigure 12.7: Example of an execution of the merge algorithm, as implemented in\nCode Fragment 12.3 using queues.", "12.2. Merge-Sort 549\nA Bottom-Up (Nonrecursive) Merge-Sort\nThere is a nonrecursive version of array-based merge-sort, which runs in O(nlogn)\ntime. It is a bit faster than recursive merge-sort in practice, as it avoids the extra\noverheads of recursive calls and temporary memory at each level. The main idea\nis to perform merge-sort bottom-up, performing the merges level by level going upthe merge-sort tree. Given an input array of elements, we begin by merging everysuccessive pair of elements into sorted runs of length two. We merge these runs intoruns of length four, merge these new runs into runs of length eight, and so on, until\nthe array is sorted. To keep the space usage reasonable, we deploy a second array\nthat stores the merged runs (swapping input and output arrays after each iteration).We give a Python implementation in Code Fragment 12.4. A similar bottom-upapproach can be used for sorting linked lists. (See Exercise C-12.29.)\n1defmerge(src, result, start, inc):\n2\u201d\u201d\u201dMerge src[start:start+inc] and src[start+inc:start+2\ninc] into result.\u201d\u201d\u201d\n3end1 = start+inc # boundary for run 1\n4end2 = min(start+2\n inc, len(src)) # boundary for run 2\n5x, y, z = start, start+inc, start # index into run 1, run 2, result\n6while x<end1andy<end2:\n7 ifsrc[x] <src[y]:\n8 result[z] = src[x]; x += 1 # copy from run 1 and increment x\n9 else:\n10 result[z] = src[y]; y += 1 # copy from run 2 and increment y\n11 z+ =1 # increment z to re\ufb02ect new result\n12ifx<end1:\n13 result[z:end2] = src[x:end1] # copy remainder of run 1 to output\n14elify<end2:\n15 result[z:end2] = src[y:end2] # copy remainder of run 2 to output\n1617defmerge\nsort(S):\n18 \u201d\u201d\u201dSort the elements of Python list S using the merge-sort algorithm.\u201d\u201d\u201d\n19 n=l e n ( S )\n20 logn = math.ceil(math.log(n,2))\n21 src, dest = S, [ None]\nn # make temporary storage for dest\n22foriin(2\nkforkinrange(logn)): # pass i creates all runs of length 2i\n23 forjinrange(0, n, 2\n i): # each pass merges two length i runs\n24 merge(src, dest, j, i)\n25 src, dest = dest, src # reverse roles of lists\n26ifSis not src:\n27 S[0:n] = src[0:n] # additional copy to get results to S\nCode Fragment 12.4: An implementation of the nonrecursive merge-sort algorithm.", "550 Chapter 12. Sorting and Selection\n12.3 Quick-Sort\nThe next sorting algorithm we discuss is called quick-sort . Like merge-sort, this\nalgorithm is also based on the divide-and-conquer paradigm, but it uses this tech-\nnique in a somewhat opposite manner, as all the hard work is done before the\nrecursive calls.\nHigh-Level Description of Quick-Sort\nThe quick-sort algorithm sorts a sequence Susing a simple recursive approach.\nThe main idea is to apply the divide-and-conquer technique, whereby we divide\nSinto subsequences, recur to sort each subsequence, and then combine the sorted\nsubsequences by a simple concatenation. In particular, the quick-sort algorithmconsists of the following three steps (see Figure 12.8):\n1.Divide: IfShas at least two elements (nothing needs to be done if Shas\nzero or one element), select a speci\ufb01c element xfrom S, which is called the\npivot . As is common practice, choose the pivot xto be the last element in S.\nRemove all the elements from Sand put them into three sequences:\n\u2022L, storing the elements in Sless than x\n\u2022E, storing the elements in Sequal to x\n\u2022G, storing the elements in Sgreater than x\nOf course, if the elements of Sare distinct, then Eholds just one element\u2014\nthe pivot itself.\n2.Conquer: Recursively sort sequences Land G.\n3.Combine: Put back the elements into Sin order by \ufb01rst inserting the elements\nofL, then those of E, and \ufb01nally those of G.\n2. Recur.1. Split using pivot x.\n3. Concatenate.2. Recur.\nG(>x) L(<x)E(=x)\nFigure 12.8: A visual schematic of the quick-sort algorithm.", "12.3. Quick-Sort 551\nLike merge-sort, the execution of quick-sort can be visualized by means of a bi-\nnary recursion tree, called the quick-sort tree . Figure 12.9 summarizes an execution\nof the quick-sort algorithm by showing the input and output sequences processed at\neach node of the quick-sort tree. The step-by-step evolution of the quick-sort treeis shown in Figures 12.10, 12.11, and 12.12.\nUnlike merge-sort, however, the height of the quick-sort tree associated with an\nexecution of quick-sort is linear in the worst case. This happens, for example, if thesequence consists of ndistinct elements and is already sorted. Indeed, in this case,\nthe standard choice of the last element as pivot yields a subsequence Lof size n\u22121,\nwhile subsequence Ehas size 1 and subsequence Ghas size 0. At each invocation\nof quick-sort on subsequence L, the size decreases by 1. Hence, the height of the\nquick-sort tree is n\u22121.\n45\n4563 24 85 17 31 96 50\n85 63 96 24 45 17 31\n24 85 63 17\n24 85\n(a)\n2431 63 85 9645 31 24 17 50 63 85 96\n17 24 45\n17 63 85\n24 8545\n(b)\nFigure 12.9: Quick-sort tree Tfor an execution of the quick-sort algorithm on a se-\nquence with 8 elements: (a) input sequences processed at each node of T; (b) output\nsequences generated at each node of T. The pivot used at each level of the recursion\nis shown in bold.", "552 Chapter 12. Sorting and Selection\n63 24 85 17 31 96 50 45 24 31 85 63 96 50 45 17\n(a) (b)\n85 63 96 50\n45 17 24 3163 96 50\n45 31 24 1785\n(c) (d)\n63 96 50\n45\n24 173185 63 96 50\n45 31\n172485\n(e) (f)\nFigure 12.10: Visualization of quick-sort. Each node of the tree represents a re-\ncursive call. The nodes drawn with dashed lines represent calls that have not been\nmade yet. The node drawn with thick lines represents the running invocation. The\nempty nodes drawn with thin lines represent terminated calls. The remaining nodes\nrepresent suspended calls (that is, active invocations that are waiting for a child in-vocation to return). Note the divide steps performed in (b), (d), and (f). (Continuesin Figure 12.11.)", "12.3. Quick-Sort 553\n63 96 50\n45 31\n241785 63 96 50\n45 31\n172485\n(g) (h)\n63 96 50\n45 31\n17\n2485 63 96 50\n45 31\n172485\n(i) (j)\n63 96 50\n45 31\n17 2485 85 63 96 50\n45 31 17 24\n(k) (l)\nFigure 12.11: Visualization of an execution of quick-sort. Note the concatenation\nstep performed in (k). (Continues in Figure 12.12.)", "554 Chapter 12. Sorting and Selection\n85 63 96 50\n17 24 31\n4585 63 96 50\n45 31 17 24\n(m) (n)\n85 63 96 50\n24 31 17 4563 96 50 24 31 17 45 85\n(o) (p)\n85 63 45 17 31 24 50 96 50 24 31 17 45 63 85 96\n(q) (r)\nFigure 12.12: Visualization of an execution of quick-sort. Several invocations be-\ntween (p) and (q) have been omitted. Note the concatenation steps performed in (o)\nand (r). (Continued from Figure 12.11.)", "12.3. Quick-Sort 555\nPerforming Quick-Sort on General Sequences\nIn Code Fragment 12.5, we give an implementation of the quick-sort algorithm\nthat works on any sequence type that operates as a queue. This particular version\nrelies on the LinkedQueue class from Section 7.1.2; we provide a more streamlined\nimplementation of quick-sort using an array-based sequence in Section 12.3.2.\nOur implementation chooses the \ufb01rst item of the queue as the pivot (since it\nis easily accessible), and then it divides sequence Sinto queues L,E,a n d Gof\nelements that are respectively less than, equal to, and greater than the pivot. Wethen recur on the Land Glists, and transfer elements from the sorted lists L,E,\nand Gback to S. All of the queue operations run in O(1)worst-case time when\nimplemented with a linked list.\n1defquick\nsort(S):\n2\u201d\u201d\u201dSort the elements of queue S using the quick-sort algorithm.\u201d\u201d\u201d\n3n=l e n ( S )\n4ifn<2:\n5 return # list is already sorted\n6# divide\n7p=S . \ufb01 r s t () # using \ufb01rst as arbitrary pivot\n8L = LinkedQueue()\n9E = LinkedQueue()\n10 G = LinkedQueue()\n11while not S.is\nempty(): # divide S into L, E, and G\n12 ifS.\ufb01rst( ) <p:\n13 L.enqueue(S.dequeue())\n14 elifp<S.\ufb01rst():\n15 G.enqueue(S.dequeue())\n16 else: # S.\ufb01rst() must equal pivot\n17 E.enqueue(S.dequeue())\n18 # conquer (with recursion)\n19 quick\n sort(L) # sort elements less than p\n20 quick\n sort(G) # sort elements greater than p\n21 # concatenate results\n22while not L.is\nempty():\n23 S.enqueue(L.dequeue())\n24while not E.is\nempty():\n25 S.enqueue(E.dequeue())\n26while not G.is\nempty():\n27 S.enqueue(G.dequeue())\nCode Fragment 12.5: Quick-sort for a sequence Simplemented as a queue.", "556 Chapter 12. Sorting and Selection\nRunning Time of Quick-Sort\nWe can analyze the running time of quick-sort with the same technique used for\nmerge-sort in Section 12.2.3. Namely, we can identify the time spent at each nodeof the quick-sort tree Tand sum up the running times for all the nodes.\nExamining Code Fragment 12.5, we see that the divide step and the \ufb01nal con-\ncatenation of quick-sort can be implemented in linear time. Thus, the time spentat a node vofTis proportional to the input size s(v)ofv, de\ufb01ned as the size of\nthe sequence handled by the invocation of quick-sort associated with node v.S i n c e\nsubsequence Ehas at least one element (the pivot), the sum of the input sizes of the\nchildren of vis at most s(v)\u22121.\nLet s\nidenote the sum of the input sizes of the nodes at depth ifor a particular\nquick-sort tree T. Clearly, s0=n, since the root rofTis associated with the entire\nsequence. Also, s1\u2264n\u22121, since the pivot is not propagated to the children of r.\nMore generally, it must be that si<si\u22121since the elements of the subsequences at\ndepth iall come from distinct subsequences at depth i\u22121, and at least one element\nfrom depth i\u22121 does not propagate to depth ibecause it is in a set E(in fact, one\nelement from each node at depth i\u22121 does not propagate to depth i).\nWe can therefore bound the overall running time of an execution of quick-sort\nasO(n\u00b7h)where his the overall height of the quick-sort tree Tfor that execution.\nUnfortunately, in the worst case, the height of a quick-sort tree is \u0398(n), as observed\nin Section 12.3. Thus, quick-sort runs in O(n2)worst-case time. Paradoxically,\nif we choose the pivot as the last element of the sequence, this worst-case behav-ior occurs for problem instances when sorting should be easy\u2014if the sequence is\nalready sorted.\nGiven its name, we would expect quick-sort to run quickly, and it often does\nin practice. The best case for quick-sort on a sequence of distinct elements oc-\ncurs when subsequences Land Ghave roughly the same size. In that case, as\nwe saw with merge-sort, the tree has height O(logn)and therefore quick-sort runs\ninO(nlogn)time; we leave the justi\ufb01cation of this fact as an exercise (R-12.10).\nMore so, we can observe an O(nlogn)running time even if the split between L\nand Gis not as perfect. For example, if every divide step caused one subsequence\nto have one-fourth of those elements and the other to have three-fourths of the\nelements, the height of the tree would remain O(logn)and thus the overall perfor-\nmance O(nlogn).\nWe will see in the next section that introducing randomization in the choice of\na pivot will makes quick-sort essentially behave in this way on average, with an\nexpected running time that is O(nlogn).", "12.3. Quick-Sort 557\n12.3.1 Randomized Quick-Sort\nOne common method for analyzing quick-sort is to assume that the pivot will al-\nways divide the sequence in a reasonably balanced manner. We feel such an as-sumption would presuppose knowledge about the input distribution that is typically\nnot available, however. For example, we would have to assume that we will rarely\nbe given \u201calmost\u201d sorted sequences to sort, which are actually common in manyapplications. Fortunately, this assumption is not needed in order for us to matchour intuition to quick-sort\u2019s behavior.\nIn general, we desire some way of getting close to the best-case running time\nfor quick-sort. The way to get close to the best-case running time, of course, is forthe pivot to divide the input sequence Salmost equally. If this outcome were to\noccur, then it would result in a running time that is asymptotically the same as thebest-case running time. That is, having pivots close to the \u201cmiddle\u201d of the set of\nelements leads to an O(nlogn)running time for quick-sort.\nPicking Pivots at Random\nSince the goal of the partition step of the quick-sort method is to divide the sequence\nSwith suf\ufb01cient balance, let us introduce randomization into the algorithm and pick\nas the pivot a random element of the input sequence. That is, instead of picking\nthe pivot as the \ufb01rst or last element of S, we pick an element of Sat random as the\npivot, keeping the rest of the algorithm unchanged. This variation of quick-sort iscalled randomized quick-sort . The following proposition shows that the expected\nrunning time of randomized quick-sort on a sequence with nelements is O(nlogn).\nThis expectation is taken over all the possible random choices the algorithm makes,and is independent of any assumptions about the distribution of the possible inputsequences the algorithm is likely to be given.\nProposition 12.3:\nThe expected running time of randomized quick-sort on a se-\nquence Sof size nisO(nlogn).\nJusti\ufb01cation: We assume two elements of Scan be compared in O(1)time.\nConsider a single recursive call of randomized quick-sort, and let ndenote the size\nof the input for this call. Say that this call is \u201cgood\u201d if the pivot chosen is such that\nsubsequences Land Ghave size at least n/4 and at most 3 n/4 each; otherwise, a\ncall is \u201cbad.\u201d\nNow, consider the implications of our choosing a pivot uniformly at random.\nNote that there are n/2 possible good choices for the pivot for any given call of\nsize nof the randomized quick-sort algorithm. Thus, the probability that any call is\ngood is 1 /2. Note further that a good call will at least partition a list of size ninto\ntwo lists of size 3 n/4a n d n/4, and a bad call could be as bad as producing a single\ncall of size n\u22121.", "558 Chapter 12. Sorting and Selection\nNow consider a recursion trace for randomized quick-sort. This trace de\ufb01nes a\nbinary tree, T, such that each node in Tcorresponds to a different recursive call on\na subproblem of sorting a portion of the original list.\nSay that a node vinTis in size group iif the size of v\u2019s subproblem is greater\nthan (3/4)i+1nand at most (3/4)in. Let us analyze the expected time spent working\non all the subproblems for nodes in size group i. By the linearity of expectation\n(Proposition B.19), the expected time for working on all these subproblems is the\nsum of the expected times for each one. Some of these nodes correspond to goodcalls and some correspond to bad calls. But note that, since a good call occurs with\nprobability 1 /2, the expected number of consecutive calls we have to make before\ngetting a good call is 2. Moreover, notice that as soon as we have a good call for\na node in size group i, its children will be in size groups higher than i. Thus, for\nany element xfrom in the input list, the expected number of nodes in size group i\ncontaining xin their subproblems is 2. In other words, the expected total size of all\nthe subproblems in size group iis 2n. Since the nonrecursive work we perform for\nany subproblem is proportional to its size, this implies that the total expected timespent processing subproblems for nodes in size group iisO(n).\nThe number of size groups is log\n4/3n, since repeatedly multiplying by 3 /4i s\nthe same as repeatedly dividing by 4 /3. That is, the number of size groups is\nO(logn). Therefore, the total expected running time of randomized quick-sort is\nO(nlogn). (See Figure 12.13.)\nIf fact, we can show that the running time of randomized quick-sort is O(nlogn)\nwith high probability. (See Exercise C-12.54.)\nO(n)\nO(n)\nO(n)\nO(nlogn)p e rs i z eg r o u pExpected time\nsize groupsNumber of\nO(logn)\nTotal expected time:size group 1\nsize group 2size group 0s(r)\ns(b)\ns(e) s(f) s(c) s(d)s(a)\nFigure 12.13: A visual time analysis of the quick-sort tree T. Each node is shown\nlabeled with the size of its subproblem.", "12.3. Quick-Sort 559\n12.3.2 Additional Optimizations for Quick-Sort\nAn algorithm is in-place if it uses only a small amount of memory in addition\nto that needed for the original input. Our implementation of heap-sort, from Sec-\ntion 9.4.2, is an example of such an in-place sorting algorithm. Our implementationof quick-sort from Code Fragment 12.5 does not qualify as in-place because we useadditional containers L,E,a n d Gwhen dividing a sequence Swithin each recursive\ncall. Quick-sort of an array-based sequence can be adapted to be in-place, and suchan optimization is used in most deployed implementations.\nPerforming the quick-sort algorithm in-place requires a bit of ingenuity, how-\never, for we must use the input sequence itself to store the subsequences for all therecursive calls. We show algorithm inplace\nquick\n sort, which performs in-place\nquick-sort, in Code Fragment 12.6. Our implementation assumes that the input\nsequence, S, is given as a Python list of elements. In-place quick-sort modi\ufb01es\nthe input sequence using element swapping and does not explicitly create subse-\nquences. Instead, a subsequence of the input sequence is implicitly represented bya range of positions speci\ufb01ed by a leftmost index aand a rightmost index b.T h e\n1definplace\nquick\n sort(S, a, b):\n2\u201d\u201d\u201dSort the list from S[a] to S[b] inclusive using the quick-sort algorithm.\u201d\u201d\u201d\n3ifa>=b :return # range is trivially sorted\n4pivot = S[b] # last element of range is pivot\n5left = a # will scan rightward\n6right = b \u22121 # will scan leftward\n7while left<=r i g h t :\n8 # scan until reaching value equal or larger than pivot (or right marker)\n9 while left<=r i g h t andS[left] <pivot:\n10 left += 1\n11 # scan until reaching value equal or smaller than pivot (or left marker)\n12 while left<=r i g h t andpivot<S[right]:\n13 right\u2212=1\n14 ifleft<=r i g h t : # scans did not strictly cross\n15 S[left], S[right] = S[right], S[left] #s w a pv a l u e s\n16 left, right = left + 1, right \u22121 # shrink range\n1718 # put pivot into its \ufb01nal place (currently marked by left index)\n19 S[left], S[b] = S[b], S[left]\n20 # make recursive calls\n21 inplace\nquick\n s o r t ( S ,a ,l e f t \u22121)\n22 inplace\n quick\n sort(S, left + 1, b)\nCode Fragment 12.6: In-place quick-sort for a Python list S.", "560 Chapter 12. Sorting and Selection\ndivide step is performed by scanning the array simultaneously using local variables\nleft, which advances forward, and right , which advances backward, swapping pairs\nof elements that are in reverse order, as shown in Figure 12.14. When these two\nindices pass each other, the division step is complete and the algorithm completesby recurring on these two sublists. There is no explicit \u201ccombine\u201d step, because theconcatenation of the two sublists is implicit to the in-place use of the original list.\nIt is worth noting that if a sequence has duplicate values, we are not explicitly\ncreating three sublists L,E,a n d G, as in our original quick-sort description. We in-\nstead allow elements equal to the pivot (other than the pivot itself) to be dispersedacross the two sublists. Exercise R-12.11 explores the subtlety of our implementa-\ntion in the presence of duplicate keys, and Exercise C-12.33 describes an in-place\nalgorithm that strictly partitions into three sublists L,E,a n d G.\n24 63 45 17 31 96 50\nl85\nr(a)\n24 63 45 17 31 96 50\nl85\nr(b)\n24 63 45 17 85 96 50\nl31\nr(c)\n24 63 45 17 85 96 50\nr31\nl(d)\n24 17 45 63 85 96 50 31\nl,r\n(e)\nr<31 24 17 45 63 85 96 50\nl\n(f)\n24 17 45 31 85 96 63 50\n(g)\nFigure 12.14: Divide step of in-place quick-sort, using index las shorthand for iden-\nti\ufb01erleft, and index ras shorthand for identi\ufb01er right . Index lscans the sequence\nfrom left to right, and index rscans the sequence from right to left. A swap is per-\nformed when lis at an element as large as the pivot and ris at an element as small\nas the pivot. A \ufb01nal swap with the pivot, in part (f), completes the divide step.", "12.3. Quick-Sort 561\nAlthough the implementation we describe in this section for dividing the se-\nquence into two pieces is in-place, we note that the complete quick-sort algorithm\nneeds space for a stack proportional to the depth of the recursion tree, which inthis case can be as large as n\u22121. Admittedly, the expected stack depth is O(logn),\nwhich is small compared to n. Nevertheless, a simple trick lets us guarantee the\nstack size is O(logn). The main idea is to design a nonrecursive version of in-place\nquick-sort using an explicit stack to iteratively process subproblems (each of whichcan be represented with a pair of indices marking subarray boundaries). Each iter-ation involves popping the top subproblem, splitting it in two (if it is big enough),and pushing the two new subproblems. The trick is that when pushing the new\nsubproblems, we should \ufb01rst push the larger subproblem and then the smaller one.\nIn this way, the sizes of the subproblems will at least double as we go down thestack; hence, the stack can have depth at most O(logn). We leave the details of this\nimplementation as an exercise (P-12.56).\nPivot Selection\nOur implementation in this section blindly picks the last element as the pivot at eachlevel of the quick-sort recursion. This leaves it susceptible to the \u0398(n\n2)-time worst\ncase, most notably when the original sequence is already sorted, reverse sorted, ornearly sorted.\nAs described in Section 12.3.1, this can be improved upon by using a randomly\nchosen pivot for each partition step. In practice, another common technique for\nchoosing a pivot is to use the median of tree values, taken respectively from the\nfront, middle, and tail of the array. This median-of-three heuristic will more often\nchoose a good pivot and computing a median of three may require lower overheadthan selecting a pivot with a random number generator. For larger data sets, themedian of more than three potential pivots might be computed.\nHybrid Approaches\nAlthough quick-sort has very good performance on large data sets, it has ratherhigh overhead on relatively small data sets. For example, the process of quick-sorting a sequence of eight elements, as illustrated in Figures 12.10 through 12.12,\ninvolves considerable bookkeeping. In practice, a simple algorithm like insertion-\nsort (Section 7.5) will execute faster when sorting such a short sequence.\nIt is therefore common, in optimized sorting implementations, to use a hybrid\napproach, with a divide-and-conquer algorithm used until the size of a subsequencefalls below some threshold (perhaps 50 elements); insertion-sort can be directlyinvoked upon portions with length below the threshold. We will further discusssuch practical considerations in Section 12.5, when comparing the performance of\nvarious sorting algorithms.", "562 Chapter 12. Sorting and Selection\n12.4 Studying Sorting through an Algorithmic Lens\nRecapping our discussions on sorting to this point, we have described several meth-\nods with either a worst case or expected running time of O(nlogn)on an input se-\nquence of size n. These methods include merge-sort and quick-sort, described in\nthis chapter, as well as heap-sort (Section 9.4.2). In this section, we study sortingas an algorithmic problem, addressing general issues about sorting algorithms.\n12.4.1 Lower Bound for Sorting\nA natural \ufb01rst question to ask is whether we can sort any faster than O(nlogn)\ntime. Interestingly, if the computational primitive used by a sorting algorithm is thecomparison of two elements, this is in fact the best we can do\u2014comparison-basedsorting has an \u03a9(nlogn)worst-case lower bound on its running time. (Recall the\nnotation \u03a9(\u00b7)from Section 3.3.1.) To focus on the main cost of comparison-based\nsorting, let us only count comparisons, for the sake of a lower bound.\nSuppose we are given a sequence S=( x\n0,x1,..., xn\u22121)that we wish to sort,\nand assume that all the elements of Sare distinct (this is not really a restriction\nsince we are deriving a lower bound). We do not care if Sis implemented as an\narray or a linked list, for the sake of our lower bound, since we are only countingcomparisons. Each time a sorting algorithm compares two elements x\niand xj(that\nis, it asks, \u201cis xi<xj?\u201d), there are two outcomes: \u201cyes\u201d or \u201cno.\u201d Based on the result\nof this comparison, the sorting algorithm may perform some internal calculations(which we are not counting here) and will eventually perform another comparisonbetween two other elements of S, which again will have two outcomes. Therefore,\nwe can represent a comparison-based sorting algorithm with a decision tree T(re-\ncall Example 8.6). That is, each internal node vinTcorresponds to a comparison\nand the edges from position vto its children correspond to the computations result-\ning from either a \u201cyes\u201d or \u201cno\u201d answer. It is important to note that the hypotheticalsorting algorithm in question probably has no explicit knowledge of the tree T.T h e\ntree simply represents all the possible sequences of comparisons that a sorting algo-\nrithm might make, starting from the \ufb01rst comparison (associated with the root) and\nending with the last comparison (associated with the parent of an external node).\nEach possible initial order, or permutation, of the elements in Swill cause\nour hypothetical sorting algorithm to execute a series of comparisons, traversing apath in Tfrom the root to some external node. Let us associate with each external\nnode vinT, then, the set of permutations of Sthat cause our sorting algorithm to\nend up in v. The most important observation in our lower-bound argument is that\neach external node vinTcan represent the sequence of comparisons for at most\none permutation of S. The justi\ufb01cation for this claim is simple: If two different", "12.4. Studying Sorting through an Algorithmic Lens 563\npermutations P1and P2ofSare associated with the same external node, then there\nare at least two objects xiand xj, such that xiis before xjinP1but xiis after xj\ninP2. At the same time, the output associated with vmust be a speci\ufb01c reordering\nofS, with either xiorxjappearing before the other. But if P1and P2both cause the\nsorting algorithm to output the elements of Sin this order, then that implies there is\na way to trick the algorithm into outputting xiand xjin the wrong order. Since this\ncannot be allowed by a correct sorting algorithm, each external node of Tmust be\nassociated with exactly one permutation of S. We use this property of the decision\ntree associated with a sorting algorithm to prove the following result:\nProposition 12.4: The running time of any comparison-based algorithm for sort-\ning an n-element sequence is \u03a9(nlogn)in the worst case.\nJusti\ufb01cation: The running time of a comparison-based sorting algorithm must\nbe greater than or equal to the height of the decision tree Tassociated with this\nalgorithm, as described above. (See Figure 12.15.) By the argument above, each\nexternal node in Tmust be associated with one permutation of S. Moreover, each\npermutation of Smust result in a different external node of T. The number of\npermutations of nobjects is n!=n(n\u22121)(n\u22122)\u00b7\u00b7\u00b72\u00b71. Thus, Tmust have at\nleast n! external nodes. By Proposition 8.8, the height of Tis at least log (n!).T h i s\nimmediately justi\ufb01es the proposition, because there are at least n/2 terms that are\ngreater than or equal to n/2 in the product n!; hence,\nlog (n!)\u2265log/parenleftbigg/parenleftBign\n2/parenrightBign\n2/parenrightbigg\n=n\n2logn\n2,\nwhich is \u03a9(nlogn).\n(i.e., worst-case running time)Minimum Height\nlog (n!)\nn!xk<xl? xg<xh?xc<xd?\nxm<xn?xa<xb?\nxe<xf?xi<xj?\nFigure 12.15: Visualizing the lower bound for comparison-based sorting.", "564 Chapter 12. Sorting and Selection\n12.4.2 Linear-Time Sorting: Bucket-Sort and Radix-Sort\nIn the previous section, we showed that \u03a9(nlogn)time is necessary, in the worst\ncase, to sort an n-element sequence with a comparison-based sorting algorithm. A\nnatural question to ask, then, is whether there are other kinds of sorting algorithms\nthat can be designed to run asymptotically faster than O(nlogn)time. Interest-\ningly, such algorithms exist, but they require special assumptions about the inputsequence to be sorted. Even so, such scenarios often arise in practice, such as whensorting integers from a known range or sorting character strings, so discussing themis worthwhile. In this section, we consider the problem of sorting a sequence of en-\ntries, each a key-value pair, where the keys have a restricted type.\nBucket-Sort\nConsider a sequence Sofnentries whose keys are integers in the range [0,N\u22121],\nfor some integer N\u22652, and suppose that Sshould be sorted according to the keys\nof the entries. In this case, it is possible to sort SinO(n+N)time. It might seem\nsurprising, but this implies, for example, that if NisO(n), then we can sort Sin\nO(n)time. Of course, the crucial point is that, because of the restrictive assumption\nabout the format of the elements, we can avoid using comparisons.\nThe main idea is to use an algorithm called bucket-sort , which is not based on\ncomparisons, but on using keys as indices into a bucket array Bthat has cells in-\ndexed from 0 to N\u22121. An entry with key kis placed in the \u201cbucket\u201d B[k],w h i c h\nitself is a sequence (of entries with key k). After inserting each entry of the input\nsequence Sinto its bucket, we can put the entries back into Sin sorted order by enu-\nmerating the contents of the buckets B[0],B[1],..., B[N\u22121]in order. We describe\nthe bucket-sort algorithm in Code Fragment 12.7.\nAlgorithm bucketSort(S) :\nInput: Sequence Sof entries with integer keys in the range [0,N\u22121]\nOut\nput: Sequence Ssorted in nondecreasing order of the keys\nletBbe an array of Nsequences, each of which is initially empty\nforeach entry einSdo\nk=the key of e\nremove efromSand insert it at the end of bucket (sequence) B[k]\nfori=0t oN\u22121do\nforeach entry ein sequence B[i]do\nremove efromB[i]and insert it at the end of S\nCode Fragment 12.7: Bucket-sort.", "12.4. Studying Sorting through an Algorithmic Lens 565\nIt is easy to see that bucket-sort runs in O(n+N)time and uses O(n+N)\nspace. Hence, bucket-sort is ef\ufb01cient when the range Nof values for the keys is\nsmall compared to the sequence size n, say N=O(n)orN=O(nlogn). Still, its\nperformance deteriorates as Ngrows compared to n.\nAn important property of the bucket-sort algorithm is that it works correctly\neven if there are many different elements with the same key. Indeed, we described\nit in a way that anticipates such occurrences.\nStable Sorting\nWhen sorting key-value pairs, an important issue is how equal keys are handled. LetS=( ( k\n0,v0),..., (kn\u22121,vn\u22121))be a sequence of such entries. We say that a sorting\nalgorithm is stable if, for any two entries (ki,vi)and (kj,vj)ofSsuch that ki=kj\nand (ki,vi)precedes (kj,vj)inSbefore sorting (that is, i<j), entry (ki,vi)also\nprecedes entry (kj,vj)after sorting. Stability is important for a sorting algorithm\nbecause applications may want to preserve the initial order of elements with thesame key.\nOur informal description of bucket-sort in Code Fragment 12.7 guarantees sta-\nbility as long as we ensure that all sequences act as queues, with elements processedand removed from the front of a sequence and inserted at the back. That is, wheninitially placing elements of Sinto buckets, we should process Sfrom front to back,\nand add each element to the end of its bucket. Subsequently, when transferring el-ements from the buckets back to S, we should process each B[i]from front to back,\nwith those elements added to the end of S.\nRadix-Sort\nOne of the reasons that stable sorting is so important is that it allows the bucket-sort\napproach to be applied to more general contexts than to sort integers. Suppose, for\nexample, that we want to sort entries with keys that are pairs (k,l),w h e r e kand l\nare integers in the range [0,N\u22121], for some integer N\u22652. In a context such as this,\nit is common to de\ufb01ne an order on these keys using the lexicographic (dictionary)\nconvention, where (k1,l1)<(k2,l2)ifk1<k2or if k1=k2and l1<l2(see page 15).\nThis is a pairwise version of the lexicographic comparison function, which can beapplied to equal-length character strings, or to tuples of length d.\nTheradix-sort algorithm sorts a sequence Sof entries with keys that are pairs,\nby applying a stable bucket-sort on the sequence twice; \ufb01rst using one componentof the pair as the key when ordering and then using the second component. Butwhich order is correct? Should we \ufb01rst sort on the k\u2019s (the \ufb01rst component) and\nthen on the l\u2019s (the second component), or should it be the other way around?", "566 Chapter 12. Sorting and Selection\nTo gain intuition before answering this question, we consider the following\nexample.\nExample 12.5: Consider the following sequence S(we show only the keys):\nS=( (3,3),(1,5),(2,5),(1,2),(2,3),(1,7),(3,2),(2,2)).\nIf we sort Sstably on the \ufb01rst component, then we get the sequence\nS1=( (1,5),(1,2),(1,7),(2,5),(2,3),(2,2),(3,3),(3,2)).\nIf we then stably sort this sequence S1using the second component, we get the\nsequence\nS1,2=( (1,2),(2,2),(3,2),(2,3),(3,3),(1,5),(2,5),(1,7)),\nwhich is unfortunately not a sorted sequence. On the other hand, if we \ufb01rst stably\nsort Susing the second component, then we get the sequence\nS2=( (1,2),(3,2),(2,2),(3,3),(2,3),(1,5),(2,5),(1,7)).\nIf we then stably sort sequence S2using the \ufb01rst component, we get the sequence\nS2,1=( (1,2),(1,5),(1,7),(2,2),(2,3),(2,5),(3,2),(3,3)),\nwhich is indeed sequence Slexicographically ordered.\nSo, from this example, we are led to believe that we should \ufb01rst sort using\nthe second component and then again using the \ufb01rst component. This intuition is\nexactly right. By \ufb01rst stably sorting by the second component and then again bythe \ufb01rst component, we guarantee that if two entries are equal in the second sort(by the \ufb01rst component), then their relative order in the starting sequence (which\nis sorted by the second component) is preserved. Thus, the resulting sequence is\nguaranteed to be sorted lexicographically every time. We leave to a simple exercise(R-12.18) the determination of how this approach can be extended to triples andother d-tuples of numbers. We can summarize this section as follows:\nProposition 12.6:\nLet Sbe a sequence of nkey-value pairs, each of which has a\nkey (k1,k2,..., kd),w h e r e kiis an integer in the range [0,N\u22121]for some integer\nN\u22652. We can sort Slexicographically in time O(d(n+N))using radix-sort.\nRadix sort can be applied to any key that can be viewed as a composite of\nsmaller pieces that are to be sorted lexicographically. For example, we can applyit to sort character strings of moderate length, as each individual character can berepresented as an integer value. (Some care is needed to properly handle strings\nwith varying lengths.)", "12.5. Comparing Sorting Algorithms 567\n12.5 Comparing Sorting Algorithms\nAt this point, it might be useful for us to take a moment and consider all the algo-\nrithms we have studied in this book to sort an n-element sequence.\nConsidering Running Time and Other Factors\nWe have studied several methods, such as insertion-sort, and selection-sort, thathave O(n\n2)-time behavior in the average and worst case. We have also studied sev-\neral methods with O(nlogn)-time behavior, including heap-sort, merge-sort, and\nquick-sort. Finally, the bucket-sort and radix-sort methods run in linear time forcertain types of keys. Certainly, the selection-sort algorithm is a poor choice in anyapplication, since it runs in O(n\n2)time even in the best case. But, of the remaining\nsorting algorithms, which is the best?\nAs with many things in life, there is no clear \u201cbest\u201d sorting algorithm from\nthe remaining candidates. There are trade-offs involving ef\ufb01ciency, memory usage,and stability. The sorting algorithm best suited for a particular application dependson the properties of that application. In fact, the default sorting algorithm usedby computing languages and systems has evolved greatly over time. We can offer\nsome guidance and observations, therefore, based on the known properties of the\n\u201cgood\u201d sorting algorithms.\nInsertion-Sort\nIf implemented well, the running time of insertion-sort isO(n+m),w h e r e mis\nthe number of inversions (that is, the number of pairs of elements out of order).\nThus, insertion-sort is an excellent algorithm for sorting small sequences (say, lessthan 50 elements), because insertion-sort is simple to program, and small sequencesnecessarily have few inversions. Also, insertion-sort is quite effective for sorting\nsequences that are already \u201calmost\u201d sorted. By \u201calmost,\u201d we mean that the number\nof inversions is small. But the O(n\n2)-time performance of insertion-sort makes it a\npoor choice outside of these special contexts.\nHeap-Sort\nHeap-sort , on the other hand, runs in O(nlogn)time in the worst case, which is\noptimal for comparison-based sorting methods. Heap-sort can easily be made to ex-ecute in-place, and is a natural choice on small- and medium-sized sequences, wheninput data can \ufb01t into main memory. However, heap-sort tends to be outperformedby both quick-sort and merge-sort on larger sequences. A standard heap-sort does\nnot provide a stable sort, because of the swapping of elements.", "568 Chapter 12. Sorting and Selection\nQuick-Sort\nAlthough its O(n2)-time worst-case performance makes quick-sort susceptible in\nreal-time applications where we must make guarantees on the time needed to com-\nplete a sorting operation, we expect its performance to be O(nlogn)-time, and ex-\nperimental studies have shown that it outperforms both heap-sort and merge-sort on\nmany tests. Quick-sort does not naturally provide a stable sort, due to the swappingof elements during the partitioning step.\nFor decades quick-sort was the default choice for a general-purpose, in-memory\nsorting algorithm. Quick-sort was included as the qsort sorting utility provided in C\nlanguage libraries, and was the basis for sorting utilities on Unix operating systems\nfor many years. It was also the standard algorithm for sorting arrays in Java through\nversion 6 of that language. (We discuss Java7 below.)\nMerge-Sort\nMerge-sort runs in O(nlogn)time in the worst case. It is quite dif\ufb01cult to make\nmerge-sort run in-place for arrays, and without that optimization the extra overheadof allocate a temporary array, and copying between the arrays is less attractive thanin-place implementations of heap-sort and quick-sort for sequences that can \ufb01t en-\ntirely in a computer\u2019s main memory. Even so, merge-sort is an excellent algorithm\nfor situations where the input is strati\ufb01ed across various levels of the computer\u2019smemory hierarchy (e.g., cache, main memory, external memory). In these contexts,the way that merge-sort processes runs of data in long merge streams makes the best\nuse of all the data brought as a block into a level of memory, thereby reducing the\ntotal number of memory transfers.\nThe GNU sorting utility (and most current versions of the Linux operating sys-\ntem) relies on a multiway merge-sort variant. Since 2003, the standard sortmethod\nof Python\u2019s listclass has been a hybrid approach named Tim-sort (designed by Tim\nPeters), which is essentially a bottom-up merge-sort that takes advantage of someinitial runs in the data while using insertion-sort to build additional runs. Tim-sort\nhas also become the default algorithm for sorting arrays in Java7.\nBucket-Sort and Radix-Sort\nFinally, if an application involves sorting entries with small integer keys, character\nstrings, or d-tuples of keys from a discrete range, then bucket-sort orradix-sort is\nan excellent choice, for it runs in O(d(n+N))time, where [0,N\u22121]is the range of\ninteger keys (and d=1 for bucket sort). Thus, if d(n+N)is signi\ufb01cantly \u201cbelow\u201d\nthe nlognfunction, then this sorting method should run faster than even quick-sort,\nheap-sort, or merge-sort.", "12.6. Python\u2019s Built-In Sorting Functions 569\n12.6 Python\u2019s Built-In Sorting Functions\nPython provides two built-in ways to sort data. The \ufb01rst is the sortmethod of the\nlistclass. As an example, suppose that we de\ufb01ne the following list:\ncolors = [\n red\n,\ngreen\n ,\nblue\n ,\ncyan\n ,\nmagenta\n ,\nyellow\n ]\nThat method has the effect of reordering the elements of the list into order, as de-\n\ufb01ned by the natural meaning of the <operator for those elements. In the above\nexample, within elements that are strings, the natural order is de\ufb01ned alphabeti-cally. Therefore, after a call to colors.sort() , the order of the list would become:\n[\nblue\n ,\ncyan\n ,\ngreen\n ,\nmagenta\n ,\nred\n,\nyellow\n ]\nPython also supports a built-in function, named sorted , that can be used to\nproduce a new ordered list containing the elements of any existing iterable con-\ntainer. Going back to our original example, the syntax sorted(colors) would return\na new list of those colors, in alphabetical order, while leaving the contents of the\noriginal list unchanged. This second form is more general because it can be ap-plied to any iterable object as a parameter; for example, sorted(\ngreen\n )returns\n[\ne\n,\ne\n,\ng\n,\nn\n,\nr\n].\n12.6.1 Sorting According to a Key Function\nThere are many situations in which we wish to sort a list of elements, but accordingto some order other than the natural order de\ufb01ned by the <operator. For example,\nwe might wish to sort a list of strings from shortest to longest (rather than alphabet-ically). Both of Python\u2019s built-in sort functions allow a caller to control the notionof order that is used when sorting. This is accomplished by providing, as an op-tional keyword parameter, a reference to a secondary function that computes a key\nfor each element of the primary sequence; then the primary elements are sortedbased on the natural order of their keys. (See pages 27 and 28 of Section 1.5.1 fora discussion of this technique in the context of the built-in minandmax functions.)\nA key function must be a one-parameter function that accepts an element as a\nparameter and returns a key. For example, we could use the built-in lenfunction\nwhen sorting strings by length, as a call len(s) for string sreturns its length. To sort\nourcolors list based on length, we use the syntax colors.sort(key=len) to mutate\nthe list or sorted(colors, key=len) to generate a new ordered list, while leaving the\noriginal alone. When sorted with the length function as a key, the contents are:\n[\nred\n,\nblue\n ,\ncyan\n ,\ngreen\n ,\nyellow\n ,\nmagenta\n ]\nThese built-in functions also support a keyword parameter, reverse , that can be\nset toTrue to cause the sort order to be from largest to smallest.", "570 Chapter 12. Sorting and Selection\nDecorate-Sort-Undecorate Design Pattern\nPython\u2019s support for a key function when sorting is implemented using what is\nknown as the decorate-sort-undecorate design pattern . It proceeds in 3 steps:\n1. Each element of the list is temporarily replaced with a \u201cdecorated\u201d version\nthat includes the result of the key function applied to the element.\n2. The list is sorted based upon the natural order of the keys (Figure 12.16).3. The decorated elements are replaced by the original elements.\n1 0 23456 3 44 blue 5 green\nred yellow7 magenta\ncyan\nFigure 12.16: A list of \u201cdecorated\u201d strings, using their lengths as decoration. This\nlist has been sorted by those keys.\nAlthough there is already built-in support for Python, if we were to implement\nsuch a strategy ourselves, a natural way to represent a \u201cdecorated\u201d element is usingthe same composition strategy that we used for representing key-value pairs within\na priority queue. Code Fragment 9.1 of Section 9.2.1 includes just such an\nItem\nclass, de\ufb01ned so that the <operator for items relies upon the given keys. With such\na composition, we could trivially adapt any sorting algorithm to use the decorate-\nsort-undecorate pattern, as demonstrated in Code Fragment 12.8 with merge-sort.\n1defdecorated\n merge\n sort(data, key= None):\n2\u201d\u201d\u201dDemonstration of the decorate-sort-undecorate pattern.\u201d\u201d\u201d\n3ifkeyis not None :\n4 forjinrange(len(data)):\n5 data[j] =\n Item(key(data[j]), data[j]) # decorate each element\n6merge\n sort(data) # sort with existing algorithm\n7ifkeyis not None :\n8 forjinrange(len(data)):\n9 data[j] = data[j].\n value # undecorate each element\nCode Fragment 12.8: An approach for implementing the decorate-sort-undecorate\npattern based upon the array-based merge-sort of Code Fragment 12.1. The\n Item\nclass is identical to that which was used in the PriorityQueueBase class. (See Code\nFragment 9.1.)", "12.7. Selection 571\n12.7 Selection\nAs important as it is, sorting is not the only interesting problem dealing with a total\norder relation on a set of elements. There are a number of applications in whichwe are interested in identifying a single element in terms of its rank relative tothe sorted order of the entire set. Examples include identifying the minimum and\nmaximum elements, but we may also be interested in, say, identifying the median\nelement, that is, the element such that half of the other elements are smaller and the\nremaining half are larger. In general, queries that ask for an element with a givenrank are called order statistics .\nDe\ufb01ning the Selection Problem\nIn this section, we discuss the general order-statistic problem of selecting the kth\nsmallest element from an unsorted collection of ncomparable elements. This is\nknown as the selection problem. Of course, we can solve this problem by sorting\nthe collection and then indexing into the sorted sequence at index k\u22121. Using\nthe best comparison-based sorting algorithms, this approach would take O(nlogn)\ntime, which is obviously an overkill for the cases where k=1o r k=n(or even\nk=2,k=3,k=n\u22121, or k=n\u22125), because we can easily solve the selection\nproblem for these values of kinO(n)time. Thus, a natural question to ask is\nwhether we can achieve an O(n)running time for all values of k(including the\ninteresting case of \ufb01nding the median, where k=\u230an/2\u230b).\n12.7.1 Prune-and-Search\nWe can indeed solve the selection problem in O(n)time for any value of k.M o r e -\nover, the technique we use to achieve this result involves an interesting algorithmic\ndesign pattern. This design pattern is known as prune-and-search ordecrease-\nand-conquer . In applying this design pattern, we solve a given problem that is\nde\ufb01ned on a collection of nobjects by pruning away a fraction of the nobjects\nand recursively solving the smaller problem. When we have \ufb01nally reduced theproblem to one de\ufb01ned on a constant-sized collection of objects, we then solve\nthe problem using some brute-force method. Returning back from all the recursivecalls completes the construction. In some cases, we can avoid using recursion, inwhich case we simply iterate the prune-and-search reduction step until we can ap-ply a brute-force method and stop. Incidentally, the binary search method described\nin Section 4.1.3 is an example of the prune-and-search design pattern.", "572 Chapter 12. Sorting and Selection\n12.7.2 Randomized Quick-Select\nIn applying the prune-and-search pattern to \ufb01nding the kthsmallest element in an\nunordered sequence of nelements, we describe a simple and practical algorithm,\nknown as randomized quick-select . This algorithm runs in O(n)expected time,\ntaken over all possible random choices made by the algorithm; this expectation\ndoes not depend whatsoever on any randomness assumptions about the input dis-tribution. We note though that randomized quick-select runs in O(n\n2)time in the\nworst case , the justi\ufb01cation of which is left as an exercise (R-12.24). We also\nprovide an exercise (C-12.55) for modifying randomized quick-select to de\ufb01ne adeterministic selection algorithm that runs in O(n)worst-case time. The existence\nof this deterministic algorithm is mostly of theoretical interest, however, since theconstant factor hidden by the big-Oh notation is relatively large in that case.\nSuppose we are given an unsorted sequence Sofncomparable elements to-\ngether with an integer k\u2208[1,n]. At a high level, the quick-select algorithm for\n\ufb01nding the k\nthsmallest element in Sis similar to the randomized quick-sort algo-\nrithm described in Section 12.3.1. We pick a \u201cpivot\u201d element from Sat random and\nuse this to subdivide Sinto three subsequences L,E,a n d G, storing the elements of\nSless than, equal to, and greater than the pivot, respectively. In the prune step, we\ndetermine which of these subsets contains the desired element, based on the valueofkand the sizes of those subsets. We then recur on the appropriate subset, noting\nthat the desired element\u2019s rank in the subset may differ from its rank in the full set.An implementation of randomized quick-select is shown in Code Fragment 12.9.\n1defquick\nselect(S, k):\n2\u201d\u201d\u201dReturn the kth smallest element of list S, for k from 1 to len(S).\u201d\u201d\u201d\n3iflen(S) == 1:\n4 return S[0]\n5pivot = random.choice(S) # pick random pivot element from S\n6L=[ x forxinSifx<pivot] # elements less than pivot\n7E=[ x forxinSifx= =p i v o t ] # elements equal to pivot\n8G=[ x forxinSifpivot<x] # elements greater than pivot\n9ifk<= len(L):\n10 return quick\n select(L, k) # kth smallest lies in L\n11elifk<=l e n ( L )+l e n ( E ) :\n12 return pivot # kth smallest equal to pivot\n13else:\n14 j=k \u2212len(L) \u2212len(E) # new selection parameter\n15 return quick\n select(G, j) # kth smallest is jth in G\nCode Fragment 12.9: Randomized quick-select algorithm.", "12.7. Selection 573\n12.7.3 Analyzing Randomized Quick-Select\nShowing that randomized quick-select runs in O(n)time requires a simple prob-\nabilistic argument. The argument is based on the linearity of expectation ,w h i c h\nstates that if Xand Yare random variables and cis a number, then\nE(X+Y)= E(X)+ E(Y) and E(cX)= cE(X),\nwhere we use E(Z)to denote the expected value of the expression Z.\nLet t(n)be the running time of randomized quick-select on a sequence of size n.\nSince this algorithm depends on random events, its running time, t(n), is a random\nvariable. We want to bound E(t(n)), the expected value of t(n). Say that a recursive\ninvocation of our algorithm is \u201cgood\u201d if it partitions Sso that the size of each of L\nand Gis at most 3 n/4. Clearly, a recursive call is good with probability at least 1 /2.\nLet g(n)denote the number of consecutive recursive calls we make, including the\npresent one, before we get a good one. Then we can characterize t(n)using the\nfollowing recurrence equation :\nt(n)\u2264bn\u00b7g(n)+ t(3n/4),\nwh\nere b\u22651 is a constant. Applying the linearity of expectation for n>1, we get\nE(t(n))\u2264E(bn\u00b7g(n)+ t(3n/4)) = bn\u00b7E(g(n)) + E(t(3n/4)).\nSince a recursive call is good with probability at least 1 /2, and whether a recursive\ncall is good or not is independent of its parent call being good, the expected value\nofg(n)is at most the expected number of times we must \ufb02ip a fair coin before it\ncomes up \u201cheads.\u201d That is, E(g(n))\u22642. Thus, if we let T(n)be shorthand for\nE(t(n)), then we can write the case for n>1a s\nT(n)\u2264T(3n/4)+2bn.\nT\no convert this relation into a closed form, let us iteratively apply this inequality\nassuming nis large. So, for example, after two applications,\nT(n)\u2264T((3/4)2n)+2b(3/4)n+2bn.\nAt this point, we should see that the general case is\nT(n)\u22642bn\u00b7\u2308log4/3n\u2309\n\u2211\ni=0(3/4)i.\nIn other words, the expected running time is at most 2 bntimes a geometric sum\nwhose base is a positive number less than 1. Thus, by Proposition 3.5, T(n)isO(n).\nProposition 12.7: The expected running time of randomized quick-select on a\nsequence Sof size nisO(n), assuming two elements of Scan be compared in O(1)\ntime.", "574 Chapter 12. Sorting and Selection\n12.8 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-12.1 Give a complete justi\ufb01cation of Proposition 12.1.\nR-12.2 In the merge-sort tree shown in Figures 12.2 through 12.4, some edges are\ndrawn as arrows. What is the meaning of a downward arrow? How about\nan upward arrow?\nR-12.3 Show that the running time of the merge-sort algorithm on an n-element\nsequence is O(nlogn),e v e nw h e n nis not a power of 2.\nR-12.4 Is our array-based implementation of merge-sort given in Section 12.2.2stable? Explain why or why not.\nR-12.5 Is our linked-list-based implementation of merge-sort given in Code Frag-ment 12.3 stable? Explain why or why not.\nR-12.6 An algorithm that sorts key-value entries by key is said to be straggling\nif, any time two entries e\niand ejhave equal keys, but eiappears before ej\nin the input, then the algorithm places eiafter ejin the output. Describe a\nchange to the merge-sort algorithm in Section 12.2 to make it straggling.\nR-12.7 Suppose we are given two n-element sorted sequences Aand Beach with\ndistinct elements, but potentially some elements that are in both sequences.Describe an O(n)-time method for computing a sequence representing the\nunion A\u222aB(with no duplicates) as a sorted sequence.\nR-12.8 Suppose we modify the deterministic version of the quick-sort algorithmso that, instead of selecting the last element in an n-element sequence as\nthe pivot, we choose the element at index \u230an/2\u230b. What is the running time\nof this version of quick-sort on a sequence that is already sorted?\nR-12.9 Consider a modi\ufb01cation of the deterministic version of the quick-sort al-gorithm where we choose the element at index \u230an/2\u230bas our pivot. De-\nscribe the kind of sequence that would cause this version of quick-sort to\nrun in \u03a9(n\n2)time.\nR-12.10 Show that the best-case running time of quick-sort on a sequence of size\nnwith distinct elements is \u03a9(nlogn).\nR-12.11 Suppose function inplace\n quick\n sortis executed on a sequence with du-\nplicate elements. Prove that the algorithm still correctly sorts the inputsequence. What happens in the partition step when there are elementsequal to the pivot? What is the running time of the algorithm if all the\ninput elements are equal?", "12.8. Exercises 575\nR-12.12 If the outermost while loop of our implementation of inplace\n quick\n sort\n(line 7 of Code Fragment 12.6) were changed to use condition left<right\n(rather than left<=r i g h t ), there would be a \ufb02aw. Explain the \ufb02aw and\ngive a speci\ufb01c input sequence on which such an implementation fails.\nR-12.13 If the conditional at line 14 of our inplace\n quick\n sortimplementation of\nCode Fragment 12.6 were changed to use condition left<right (rather\nthanleft<=r i g h t ), there would be a \ufb02aw. Explain the \ufb02aw and give a\nspeci\ufb01c input sequence on which such an implementation fails.\nR-12.14 Following our analysis of randomized quick-sort in Section 12.3.1, show\nthat the probability that a given input element xbelongs to more than\n2log nsubproblems in size group iis at most 1 /n2.\nR-12.15 Of the n! possible inputs to a given comparison-based sorting algorithm,\nwhat is the absolute maximum number of inputs that could be correctlysorted with just ncomparisons?\nR-12.16 Jonathan has a comparison-based sorting algorithm that sorts the \ufb01rst k\nelements of a sequence of size ninO(n)time. Give a big-Oh characteri-\nzation of the biggest that kcan be?\nR-12.17 Is the bucket-sort algorithm in-place? Why or why not?\nR-12.18 Describe a radix-sort method for lexicographically sorting a sequence Sof\ntriplets (k,l,m),w h e r e k,l,a n d mare integers in the range [0,N\u22121],f o r\nsome N\u22652. How could this scheme be extended to sequences of d-tuples\n(k\n1,k2,..., kd), where each kiis an integer in the range [0,N\u22121]?\nR-12.19 Suppose Sis a sequence of nvalues, each equal to 0 or 1. How long will\nit take to sort Swith the merge-sort algorithm? What about quick-sort?\nR-12.20 Suppose Sis a sequence of nvalues, each equal to 0 or 1. How long will\nit take to sort Sstably with the bucket-sort algorithm?\nR-12.21 Given a sequence Sofnvalues, each equal to 0 or 1, describe an in-place\nmethod for sorting S.\nR-12.22 Give an example input list that requires merge-sort and heap-sort to takeO(nlogn)time to sort, but insertion-sort runs in O(n)time. What if you\nreverse this list?\nR-12.23 What is the best algorithm for sorting each of the following: general com-\nparable objects, long character strings, 32-bit integers, double-precision\n\ufb02oating-point numbers, and bytes? Justify your answer.\nR-12.24 Show that the worst-case running time of quick-select on an n-element\nsequence is \u03a9(n\n2).", "576 Chapter 12. Sorting and Selection\nCreativity\nC-12.25 Linda claims to have an algorithm that takes an input sequence Sand\nproduces an output sequence Tthat is a sorting of the nelements in S.\na. Give an algorithm, is\nsorted , that tests in O(n)time if Tis sorted.\nb. Explain why the algorithm is\nsorted is not suf\ufb01cient to prove a par-\nticular output Tto Linda\u2019s algorithm is a sorting of S.\nc. Describe what additional information Linda\u2019s algorithm could out-\nput so that her algorithm\u2019s correctness could be established on any\ngiven Sand TinO(n)time.\nC-12.26 Describe and analyze an ef\ufb01cient method for removing all duplicates from\na collection Aofnelements.\nC-12.27 Augment the PositionalList class (see Section 7.4) to support a method\nnamed merge with the following behavior. If AandBarePositionalList\ninstances whose elements are sorted, the syntax A.merge(B) should merge\nall elements of BintoAso that Aremains sorted and Bbecomes empty.\nYour implementation must accomplish the merge by relinking existingnodes; you are not to create any new nodes.\nC-12.28 Augment the PositionalList class (see Section 7.4) to support a method\nnamed sort that sorts the elements of a list by relinking existing nodes;\nyou are not to create any new nodes. You may use your choice of sortingalgorithm.\nC-12.29 Implement a bottom-up merge-sort for a collection of items by placingeach item in its own queue, and then repeatedly merging pairs of queuesuntil all items are sorted within a single queue.\nC-12.30 Modify our in-place quick-sort implementation of Code Fragment 12.6 tobe a randomized version of the algorithm, as discussed in Section 12.3.1.\nC-12.31 Consider a version of deterministic quick-sort where we pick as our pivotthe median of the dlast elements in the input sequence of nelements, for\na \ufb01xed, constant odd number d\u22653. What is the asymptotic worst-case\nrunning time of quick-sort in this case?\nC-12.32 Another way to analyze randomized quick-sort is to use a recurrence\nequation . In this case, we let T(n)denote the expected running time\nof randomized quick-sort, and we observe that, because of the worst-casepartitions for good and bad splits, we can write\nT(n)\u22641\n2(T(3n/4)+ T(n/4)) +1\n2(T(n\u22121)) + bn,\nwhere bnis the time needed to partition a list for a given pivot and concate-\nnate the result sublists after the recursive calls return. Show, by induction,\nthat T(n)isO(nlogn).", "12.8. Exercises 577\nC-12.33 Our high-level description of quick-sort describes partitioning the ele-\nments into three sets L,E,a n d G, having keys less than, equal to, or\ngreater than the pivot, respectively. However, our in-place quick-sort im-\nplementation of Code Fragment 12.6 does not gather all elements equalto the pivot into a set E. An alternative strategy for an in-place, three-\nway partition is as follows. Loop through the elements from left to right\nmaintaining indices i,j,a n dkand the invariant that all elements of slice\nS[0:i] are strictly less than the pivot, all elements of slice S[i:j] are equal\nto the pivot, and all elements of slice S[j:k] are strictly greater than the\npivot; elements of S[k:n] are yet unclassi\ufb01ed. In each pass of the loop,\nclassify one additional element, performing a constant number of swaps\nas needed. Implement an in-place quick-sort using this strategy.\nC-12.34 Suppose we are given an n-element sequence Ssuch that each element in S\nrepresents a different vote for president, where each vote is given as an in-teger representing a particular candidate, yet the integers may be arbitrar-\nily large (even if the number of candidates is not). Design an O(nlogn)-\ntime algorithm to see who wins the election Srepresents, assuming the\ncandidate with the most votes wins.\nC-12.35 Consider the voting problem from Exercise C-12.34, but now suppose thatwe know the number k<nof candidates running, even though the integer\nIDs for those candidates can be arbitrarily large. Describe an O(nlogk)-\ntime algorithm for determining who wins the election.\nC-12.36 Consider the voting problem from Exercise C-12.34, but now suppose theintegers 1 to kare used to identify k<ncandidates. Design an O(n)-time\nalgorithm to determine who wins the election.\nC-12.37 Show that any comparison-based sorting algorithm can be made to be\nstable without affecting its asymptotic running time.\nC-12.38 Suppose we are given two sequences Aand Bofnelements, possibly\ncontaining duplicates, on which a total order relation is de\ufb01ned. Describean ef\ufb01cient algorithm for determining if Aand Bcontain the same set of\nelements. What is the running time of this method?\nC-12.39 G\niven an array Aofnintegers in the range [0,n2\u22121], describe a simple\nmethod for sorting AinO(n)time.\nC-12.40 Let S1,S2,..., Skbekdifferent sequences whose elements have integer\nkeys in the range [0,N\u22121], for some parameter N\u22652. Describe an algo-\nrithm that produces krespective sorted sequences in O(n+N)time, were\nndenotes the sum of the sizes of those sequences.\nC-12.41 Given a sequence Sofnelements, on which a total order relation is de-\n\ufb01ned, describe an ef\ufb01cient method for determining whether there are two\nequal elements in S. What is the running time of your method?", "578 Chapter 12. Sorting and Selection\nC-12.42 Let Sbe a sequence of nelements on which a total order relation is de-\n\ufb01ned. Recall that an inversion inSi sap a i ro fe l e m e n t s xand ysuch\nthat xappears before yinSbut x>y. Describe an algorithm running in\nO(nlogn)time for determining the number of inversions in S.\nC-12.43 Let Sbe a sequence of nintegers. Describe a method for printing out all\nthe pairs of inversions in SinO(n+k)time, where kis the number of such\ninversions.\nC-12.44 Let Sbe a random permutation of ndistinct integers. Argue that the ex-\npected running time of insertion-sort on Sis\u03a9(n2). (Hint: Note that half\nof the elements ranked in the top half of a sorted version of Sare expected\nto be in the \ufb01rst half of S.)\nC-12.45 Let Aand Bbe two sequences of nintegers each. Given an integer m,\ndescribe an O(nlogn)-time algorithm for determining if there is an integer\nainAa n da ni n t e g e r binBsuch that m=a+b.\nC-12.46 Given a set of nintegers, describe and analyze a fast method for \ufb01nding\nthe\u2308logn\u2309integers closest to the median.\nC-12.47 Bob has a set Aofnnuts and a set Bofnbolts, such that each nut in A\nhas a unique matching bolt in B. Unfortunately, the nuts in Aall look the\nsame, and the bolts in Ball look the same as well. The only kind of a\ncomparison that Bob can make is to take a nut-bolt pair (a,b), such that a\nis in Aand bis in B, and test it to see if the threads of aare larger, smaller,\nor a perfect match with the threads of b. Describe and analyze an ef\ufb01cient\nalgorithm for Bob to match up all of his nuts and bolts.\nC-12.48 Our quick-select implementation can be made more space-ef\ufb01cient by ini-\ntially computing only the counts for sets L,E,a n d G, creating only the new\nsubset that will be needed for recursion. Implement such a version.\nC\n-12.49 Describe an in-place version of the quick-select algorithm in pseudo-code,\nassuming that you are allowed to modify the order of elements.\nC-12.50 Show how to use a deterministic O(n)-time selection algorithm to sort a\nsequence of nelements in O(nlogn)worst-case time.\nC-12.51 Given an unsorted sequence Sofncomparable elements, and an integer k,\ngive an O(nlogk)expected-time algorithm for \ufb01nding the O(k)elements\nthat have rank \u2308n/k\u2309,2\u2308n/k\u2309,3\u2308n/k\u2309, and so on.\nC-12.52 Space aliens have given us a function, alien\nsplit, that can take a sequence\nSofnintegers and partition SinO(n)time into sequences S1,S2,..., Skof\nsize at most \u2308n/k\u2309each, such that the elements in Siare less than or equal\nto every element in Si+1,f o r i=1,2,..., k\u22121, for a \ufb01xed number, k<n.\nShow how to use alien\nsplit to sort SinO(nlogn/logk)time.\nC-12.53 Read documenation of the reverse keyword parameter of Python\u2019s sorting\nfunctions, and describe how the decorate-sort-undecorate paradigm couldbe used to implement it, without assuming anything about the key type.", "12.8. Exercises 579\nC-12.54 Show that randomized quick-sort runs in O(nlogn)time with probability\nat least 1 \u22121/n, that is, with high probability , by answering the following:\na. For each input element x,d e \ufb01 n e Ci,j(x)t ob ea0 /1 random variable\nthat is 1 if and only if element xis in j+1 subproblems that belong\nto size group i. Argue why we need not de\ufb01ne Ci,jfor j>n.\nb. Let Xi,jb ea0 /1 random variable that is 1 with probability 1 /2j,\nindependent of any other events, and let L=\u2308log4/3n\u2309. Argue why\n\u2211L\u22121\ni=0\u2211n\nj=0Ci,j(x)\u2264\u2211L\u22121\ni=0\u2211n\nj=0Xi,j.\nc. Show that the expected value of \u2211L\u22121\ni=0\u2211n\nj=0Xi,jis(2\u22121/2n)L.\nd. Show that the probability that \u2211L\ni=0\u2211n\nj=0Xi,j>4Lis at most 1 /n2,\nusing the Chernoff bound that states that if Xis the sum of a \ufb01nite\nnumber of independent 0/1 random variables with expected value\n\u03bc>0, then Pr (X>2\u03bc)<(4/e)\u2212\u03bc,w h e r e e=2.71828128 ....\ne. Argue why the previous claim proves randomized quick-sort runs in\nO(nlogn)time with probability at least 1 \u22121/n.\nC-12.55 We can make the quick-select algorithm deterministic, by choosing thepivot of an n-element sequence as follows:\nPartition the set Sinto\u2308n/5\u2309groups of size 5 each (except pos-\nsibly for one group). Sort each little set and identify the medianelement in this set. From this set of \u2308n/5\u2309\u201cbaby\u201d medians, ap-\nply the selection algorithm recursively to \ufb01nd the median of thebaby medians. Use this element as the pivot and proceed as inthe quick-select algorithm.\nShow that this deterministic quick-select algorithm runs in O(n)time by\nanswering the following questions (please ignore \ufb02oor and ceiling func-tions if that simpli\ufb01es the mathematics, for the asymptotics are the sameeither way):\na. How many baby medians are less than or equal to the chosen pivot?\nHow many are greater than or equal to the pivot?\nb. For each baby median less than or equal to the pivot, how many\nother elements are less than or equal to the pivot? Is the same truefor those greater than or equal to the pivot?\nc. Argue why the method for \ufb01nding the deterministic pivot and using\nit to partition Stakes O(n)time.\nd. Based on these estimates, write a recurrence equation to bound the\nworst-case running time t(n)for this selection algorithm (note that in\nthe worst case there are two recursive calls\u2014one to \ufb01nd the median\nof the baby medians and one to recur on the larger of Land G).\ne. Using this recurrence equation, show by induction that t(n)isO(n).", "580 Chapter 12. Sorting and Selection\nProjects\nP-12.56 Implement a nonrecursive, in-place version of the quick-sort algorithm, as\ndescribed at the end of Section 12.3.2.\nP-12.57 Experimentally compare the performance of in-place quick-sort and a ver-sion of quick-sort that is not in-place.\nP-12.58 Perform a series of benchmarking tests on a version of merge-sort andquick-sort to determine which one is faster. Your tests should includesequences that are \u201crandom\u201d as well as \u201calmost\u201d sorted.\nP-12.59 Implement deterministic and randomized versions of the quick-sort al-gorithm and perform a series of benchmarking tests to see which one isfaster. Your tests should include sequences that are very \u201crandom\u201d lookingas well as ones that are \u201calmost\u201d sorted.\nP-12.60 Implement an in-place version of insertion-sort and an in-place version ofquick-sort. Perform benchmarking tests to determine the range of valuesofnwhere quick-sort is on average better than insertion-sort.\nP-12.61 Design and implement a version of the bucket-sort algorithm for sortinga list of nentries with integer keys taken from the range [0,N\u22121],f o r\nN\u22652. The algorithm should run in O(n+N)time.\nP-12.62 Design and implement an animation for one of the sorting algorithms de-scribed in this chapter. Your animation should illustrate the key properties\nof this algorithm in an intuitive manner.\nChapter Notes\nKnuth\u2019s classic text on Sorting and Searching [65] contains an extensive history of the\nsorting problem and algorithms for solving it. Huang and Langston [53] show how to\nmerge two sorted lists in-place in linear time. Th e standard quick-sort algorithm is due\nto Hoare [51]. Several optimizations for quick-sort are described by Bentley and McIl-\nroy [16]. More information about randomization, including Chernoff bounds, can be foundin the appendix and the book by Motwani and Raghavan [80]. The quick-sort analysisgiven in this chapter is a comb ination of the analysis given in an earlier Java edition of this\nbook and the analysis of Kleinberg and Tardos [60]. Exercise C-12.32 is due to Littman.\nGonnet and Baeza-Yates [44] analyze and compare experimentally several sorting algo-\nrithms. The term \u201cprune-and-search\u201d comes originally from the computational geometry\nliterature (such as in the work of Clarkson [ 26] and Megiddo [75]). The term \u201cdecrease-\nand-conquer\u201d is from Levitin [70].", "Chapter\n13Text Processing\nContents\n13.1 Abundance of Digitized Text . . . . ............. 5 8 2\n13.1.1 Notations for Strings and the Python str Class . . . . . . . 583\n1 3 . 2P a t t e r n - M a t c h i n gA l g o r i t h m s................. 5 8 4\n1 3 . 2 . 1B r u t e F o r c e .........................5 8 4\n1 3 . 2 . 2T h e B o y e r - M o o r e A l g o r i t h m ................5 8 6\n1 3 . 2 . 3T h e K n u t h - M o r r i s - P r a t t A l g o r i t h m.............5 9 0\n1 3 . 3D y n a m i cP r o g r a m m i n g .................... 5 9 4\n1 3 . 3 . 1M a t r i x C h a i n - P r o d u c t....................5 9 413.3.2 DNA and Text Sequence Alignment . . . . . . . . . . . . 597\n1 3 . 4T e x tC o m p r e s s i o na n dt h eG r e e d yM e t h o d......... 6 0 1\n1 3 . 4 . 1T h e H u \ufb00 m a n C o d i n g A l g o r i t h m ..............6 0 2\n1 3 . 4 . 2T h e G r e e d y M e t h o d.....................6 0 3\n1 3 . 5T r i e s ............................... 6 0 4\n1 3 . 5 . 1S t a n d a r d T r i e s........................6 0 4\n1 3 . 5 . 2C o m p r e s s e d T r i e s ......................6 0 8\n1 3 . 5 . 3S u \ufb03 x T r i e s .........................6 1 0\n1 3 . 5 . 4S e a r c h E n g i n e I n d e x i n g...................6 1 2\n1 3 . 6E x e r c i s e s ............................ 6 1 3\n", "582 Chapter 13. Text Processing\n13.1 Abundance of Digitized Text\nDespite the wealth of multimedia information, text processing remains one of the\ndominant functions of computers. Computer are used to edit, store, and displaydocuments, and to transport documents over the Internet. Furthermore, digital sys-\ntems are used to archive a wide range of textual information, and new data is being\ngenerated at a rapidly increasing pace. A large corpus can readily surpass a petabyteof data (which is equivalent to a thousand terabytes, or a million gigabytes). Com-mon examples of digital collections that include textual information are:\n\u2022Snapshots of the World Wide Web, as Internet document formats HTML andXML are primarily text formats, with added tags for multimedia content\n\u2022All documents stored locally on a user\u2019s computer\n\u2022Email archives\n\u2022Customer reviews\n\u2022Compilations of status updates on social networking sites such as Facebook\n\u2022Feeds from microblogging sites such as Twitter and Tumblr\nThese collections include written text from hundreds of international languages.\nFurthermore, there are large data sets (such as DNA) that can be viewed computa-\ntionally as \u201cstrings\u201d even though they are not language.\nIn this chapter we explore some of the fundamental algorithms that can be used\nto ef\ufb01ciently analyze and process large textual data sets. In addition to having\ninteresting applications, text-processing algorithms also highlight some important\nalgorithmic design patterns.\nWe begin by examining the problem of searching for a pattern as a substring\nof a larger piece of text, for example, when searching for a word in a document.\nThe pattern-matching problem gives rise to the brute-force method , which is often\ninef\ufb01cient but has wide applicability.\nNext, we introduce an algorithmic technique known as dynamic programming ,\nwhich can be applied in certain settings to solve a problem in polynomial time that\nappears at \ufb01rst to require exponential time to solve. We demonstrate the applicationon this technique to the problem of \ufb01nding partial matches between strings that may\nbe similar but not perfectly aligned. This problem arises when making suggestions\nfor a misspelled word, or when trying to match related genetic samples.\nBecause of the massive size of textual data sets, the issue of compression is\nimportant, both in minimizing the number of bits that need to be communicated\nthrough a network and to reduce the long-term storage requirements for archives.\nFor text compression, we can apply the greedy method , which often allows us to\napproximate solutions to hard problems, and for some problems (such as in textcompression) actually gives rise to optimal algorithms.\nFinally, we examine several special-purpose data structures that can be used to\nbetter organize textual data in order to support more ef\ufb01cient run-time queries.", "13.1. Abundance of Digitized Text 583\n13.1.1 Notations for Strings and the Python str Class\nWe use character strings as a model for text when discuss algorithms for text pro-\ncessing. Character strings can come from a wide variety of sources, includingscienti\ufb01c, linguistic, and Internet applications. Indeed, the following are examplesof such strings:\nS =\"CGTAAACTGCTTTAATCAAACGC\"\nT =\"http://www.wiley.com\"\nThe \ufb01rst string, S, comes from DNA applications, and the second string, T,i st h e\nInternet address (URL) for the publisher of this book. We refer to Appendix A foran overview of the operations supported by Python\u2019s strclass.\nTo allow fairly general notions of a string in our algorithm descriptions, we\nonly assume that characters of a string come from a known alphabet ,w h i c hw e\ndenote as \u03a3. For example, in the context of DNA, there are four symbols in the\nstandard alphabet, \u03a3={A,C,G,T}. This alphabet \u03a3can, of course, be a subset of\nthe ASCII or Unicode character sets, but it could also be something more general.\nAlthough we assume that an alphabet has a \ufb01xed \ufb01nite size, denoted as |\u03a3|,t h a t\nsize can be nontrivial, as with Python\u2019s treatment of the Unicode alphabet, whichallows for more than a million distinct characters. We therefore consider the impact\nof|\u03a3|in our asymptotic analysis of text-processing algorithms.\nSeveral string-processing operations involve breaking large strings into smaller\nstrings. In order to be able to speak about the pieces that result from such oper-\nations, we will rely on Python\u2019s indexing andslicing notations. For the sake of\nnotation, we let Sdenote a string of length n. In that case, we let S[j]refer to the\ncharacter at index jfor 0\u2264j\u2264n\u22121. We let notation S[j:k] for 0\u2264j\u2264k\u2264ndenote\nthe slice (or substring )o fSconsisting of characters S[j]up to and including S[k\u22121],\nbut not S[k]. By this de\ufb01nition, note that substring S [ j:j+m ] has length mand\nthat substring S\n [j:j]is trivially the null string , having length 0. In accordance with\nPython conventions, the substring S[j:k] is also the null string when k<j.\nIn order to distinguish some special kinds of substrings, let us refer to any\nsubstring of the form S[0:k] for 0\u2264k\u2264nas apre\ufb01x ofS; such a pre\ufb01x results in\nPython when the \ufb01rst index is omitted from slice notation, as in S[:k] . Similarly,\nany substring of the form S[j:n] for 0\u2264j\u2264nis asuf\ufb01x ofS; such a suf\ufb01x results\nin Python when the second index is omitted from slice notation, as in S[j:].F o r\nexample, if we again take Sto be the string of DNA given above, then \"CGTAA\" is\nap r e \ufb01 xo f S,\"CGC\" is a suf\ufb01x of S,a n d \"C\"is both a pre\ufb01x and suf\ufb01x of S.N o t e\nthat the null string is a pre\ufb01x and a suf\ufb01x of any string.", "584 Chapter 13. Text Processing\n13.2 Pattern-Matching Algorithms\nIn the classic pattern-matching problem, we are given a textstring Tof length n\nand a pattern string Pof length m, and want to \ufb01nd whether Pis a substring of T.\nIf so, we may want to \ufb01nd the lowest index jwithin Tat which Pbegins, such that\nT[j:j+m] equals P, or perhaps to \ufb01nd allindices of Tat which pattern Pbegins.\nThe pattern-matching problem is inherent to many behaviors of Python\u2019s str\nclass, such as Pi nT ,T.\ufb01nd(P) ,T.index(P) ,T.count(P), and is a subtask of more\ncomplex behaviors such as T.partition(P) ,T.split(P) ,a n dT.replace(P, Q) .\nIn this section, we present three pattern-matching algorithms (with increasing\nlevels of dif\ufb01culty). For simplicity, we model the outward semantics of our func-\ntions upon the \ufb01ndmethod of the string class, returning the lowest index at which\nthe pattern begins, or \u22121 if the pattern is not found.\n13.2.1 Brute Force\nThe brute-force algorithmic design pattern is a powerful technique for algorithm\ndesign when we have something we wish to search for or when we wish to optimizesome function. When applying this technique in a general situation, we typicallyenumerate all possible con\ufb01gurations of the inputs involved and pick the best of allthese enumerated con\ufb01gurations.\nIn applying this technique to design a brute-force pattern-matching algorithm,\nwe derive what is probably the \ufb01rst algorithm that we might think of for solving\nthe problem\u2014we simply test all the possible placements of Prelative to T.A n\nimplementation of this algorithm is shown in Code Fragment 13.1.\n1def\ufb01nd\nbrute(T, P):\n2\u201d\u201d\u201dReturn the lowest index of T at which substring P begins (or else -1).\u201d\u201d\u201d\n3n, m = len(T), len(P) # introduce convenient notations\n4foriinrange(n\u2212 m+1): # try every potential starting index within T\n5 k=0 # an index into pattern P\n6 while k<mandT[i + k] == P[k]: # kth character of P matches\n7 k+ =1\n8 ifk= =m : # if we reached the end of pattern,\n9 return i # substring T[i:i+m] matches P\n10return \u22121 # failed to \ufb01nd a match starting with any i\nCode Fragment 13.1: An implementation of brute-force pattern-matching algo-\nrithm.", "13.2. Pattern-Matching Algorithms 585\nPerformance\nThe analysis of the brute-force pattern-matching algorithm could not be simpler.\nIt consists of two nested loops, with the outer loop indexing through all possible\nstarting indices of the pattern in the text, and the inner loop indexing through eachcharacter of the pattern, comparing it to its potentially corresponding characterin the text. Thus, the correctness of the brute-force pattern-matching algorithmfollows immediately from this exhaustive search approach.\nThe running time of brute-force pattern matching in the worst case is not good,\nhowever, because, for each candidate index in T, we can perform up to mcharacter\ncomparisons to discover that Pdoes not match Tat the current index. Referring to\nCode Fragment 13.1, we see that the outer forloop is executed at most n\u2212m+1\ntimes, and the inner while loop is executed at most mtimes. Thus, the worst-case\nrunning time of the brute-force method is O(nm).\nExample 13.1:\nSuppose we are given the text string\nT= \"abacaabaccabacabaabb\"\nand the pattern string\nP= \"abacab\"\nFigure 13.1 illustrates the execution of the brute-force pattern-matching algorithm\nonTand P.\na\nbabc\nc10\n23 22b\n11 comparisons not shownbaaa b c7Text:\nPattern:\nb aaaa b c6 5 4 3 2 1\naabaaa b c89\nb c27 26 25 24baaab b a a a b a c c a b a a cb a\nFigure 13.1: Example run of the brute-force pattern-matching algorithm. The algo-\nrithm performs 27 character comparisons, indicated above with numerical labels.", "586 Chapter 13. Text Processing\n13.2.2 The Boyer-Moore Algorithm\nAt \ufb01rst, it might seem that it is always necessary to examine every character in Tin\norder to locate a pattern Pas a substring or to rule out its existence. But this is not\nalways the case. The Boyer-Moore pattern-matching algorithm, which we study in\nthis section, can sometimes avoid comparisons between Pand a sizable fraction of\nthe characters in T. In this section, we describe a simpli\ufb01ed version of the original\nalgorithm by Boyer and Moore.\nThe main idea of the Boyer-Moore algorithm is to improve the running time of\nthe brute-force algorithm by adding two potentially time-saving heuristics. Roughly\nstated, these heuristics are as follows:\nLooking-Glass Heuristic :When testing a possible placement of Pagainst T,b e -\ngin the comparisons from the end of Pand move backward to the front of P.\nCharacter-Jump Heuristic :During the testing of a possible placement of Pwithin\nT, a mismatch of text character T[i]=c with the corresponding pattern char-\nacterP[k] is handled as follows. If cis not contained anywhere in P,t h e n\nshiftPcompletely past T[i](for it cannot match any character in P). Other-\nwise, shift Puntil an occurrence of character cinPgets aligned with T[i].\nWe will formalize these heuristics shortly, but at an intuitive level, they work as an\nintegrated team. The looking-glass heuristic sets up the other heuristic to allow usto avoid comparisons between Pand whole groups of characters in T. In this case at\nleast, we can get to the destination faster by going backwards, for if we encounter a\nmismatch during the consideration of Pat a certain location in T, then we are likely\nto avoid lots of needless comparisons by signi\ufb01cantly shifting Prelative to Tusing\nthe character-jump heuristic. The character-jump heuristic pays off big if it can be\napplied early in the testing of a potential placement of Pagainst T. Figure 13.2\ndemonstrates a few simple applications of these heuristics.\nsi\u00b7\nPattern:\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 s\u00b7 \u00b7 \u00b7 e \u00b7 \u00b7\u00b7 \u00b7\u00b7 \u00b7 Text: \u00b7\u00b7 \u00b7\nh u ssi\nh u ssi\nh us\nFigure 13.2: A simple example demonstrating the intuition of the Boyer-Moore\npattern-matching algorithm. The original comparison results in a mismatch withcharacter eof the text. Because that character is nowhere in the pattern, the entire\npattern is shifted beyond its location. The second comparison is also a mismatch,but the mismatched character soccurs elsewhere in the pattern. The pattern is next\nshifted so that its last occurrence of sis aligned with the corresponding sin the\ntext. The remainder of the process is not illustrated in this \ufb01gure.", "13.2. Pattern-Matching Algorithms 587\nThe example of Figure 13.2 is rather basic, because it only involves mismatches\nwith the last character of the pattern. More generally, when a match is found for\nthat last character, the algorithm continues by trying to extend the match with thesecond-to-last character of the pattern in its current alignment. That process contin-ues until either matching the entire pattern, or \ufb01nding a mismatch at some interiorposition of the pattern.\nIf a mismatch is found, and the mismatched character of the text does not occur\nin the pattern, we shift the entire pattern beyond that location, as originally illus-\ntrated in Figure 13.2. If the mismatched character occurs elsewhere in the pattern,\nwe must consider two possible subcases depending on whether its last occurrenceis before or after the character of the pattern that was aligned with the mismatched.Those two cases are illustrated in Figure 13.3.\n(a)\n\u00b7\n\u00b7 b\u00b7 \u00b7\nk ji/prime\n\u00b7a\u00b7\u00b7 \u00b7\n\u00b7 b\u00b7 \u00b7\nj+1m\u2212(j+1)Text:\nPattern:\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 a\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\u00b7 \u00b7\ni\na\u00b7\n(b)\u00b7\nkji/prime\n\u00b7\u00b7 b\u00b7 \u00b7 a\n\u00b7\u00b7 b\u00b7 \u00b7 a\nkm\u2212kText:\nPattern:\u00b7\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 a\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\u00b7 \u00b7\ni\nFigure 13.3: Additional rules for the character-jump heuristic of the Boyer-Moore\nalgorithm. We let irepresent the index of the mismatched character in the text, k\nrepresent the corresponding index in the pattern, and jrepresent the index of the\nlast occurrence of T[i]within the pattern. We distinguish two cases: (a) j<k,\nin which case we shift the pattern by k\u2212junits, and thus, index iadvances by\nm\u2212(j+1)units; (b) j>k, in which case we shift the pattern by one unit, and\nindex iadvances by m\u2212kunits.\nIn the case of Figure 13.3(b), we slide the pattern only one unit. It would\nbe more productive to slide it rightward until \ufb01nding another occurrence of mis-\nmatched character T[i]in the pattern, but we do not wish to take time to search for", "588 Chapter 13. Text Processing\nanother occurrence. The ef\ufb01ciency of the Boyer-Moore algorithm relies on creat-\ning a lookup table that quickly determines where a mismatched character occurs\nelsewhere in the pattern. In particular, we de\ufb01ne a function last (c)as\n\u2022Ifcis inP,last (c)is the index of the last (rightmost) occurrence of cinP.\nOtherwise, we conventionally de\ufb01ne last (c)=\u22121.\nIf we assume that the alphabet is of \ufb01xed, \ufb01nite size, and that characters can be\nconverted to indices of an array (for example, by using their character code), the\nlastfunction can be easily implemented as a lookup table with worst-case O(1)-\ntime access to the value last (c). However, the table would have length equal to the\nsize of the alphabet (rather than the size of the pattern), and time would be required\nto initialize the entire table.\nWe prefer to use a hash table to represent the lastfunction, with only those\ncharacters from the pattern occurring in the structure. The space usage for thisapproach is proportional to the number of distinct alphabet symbols that occur in\nthe pattern, and thus O(m). The expected lookup time remains independent of the\nproblem (although the worst-case bound is O(m)). Our complete implementation\nof the Boyer-Moore pattern-matching algorithm is given in Code Fragment 13.2.\n1def\ufb01nd\nboyer\n moore(T, P):\n2\u201d\u201d\u201dReturn the lowest index of T at which substring P begins (or else -1).\u201d\u201d\u201d\n3n, m = len(T), len(P) # introduce convenient notations\n4ifm= =0 : return 0 # trivial search for empty string\n5last = {} # build \u2019last\u2019 dictionary\n6forkinrange(m):\n7 l a s t [P [ k ]]=k # later occurrence overwrites\n8# align end of pattern at index m-1 of text\n9i=m \u22121 # an index into T\n10 k=m \u22121 # an index into P\n11while i<n:\n12 ifT[i] == P[k]: # a matching character\n13 ifk= =0 :\n14 return i # pattern begins at index i of text\n15 else:\n16 i\u2212=1 # examine previous character\n17 k\u2212=1 #o fb o t hTa n dP\n18 else:\n19 j = last.get(T[i], \u22121) # last(T[i]) is -1 if not found\n20 i+ =m \u2212min(k, j + 1) # case analysis for jump step\n21 k=m \u22121 # restart at end of pattern\n22return \u22121\nCode Fragment 13.2: An implementation of the Boyer-Moore algorithm.", "13.2. Pattern-Matching Algorithms 589\nThe correctness of the Boyer-Moore pattern-matching algorithm follows from\nthe fact that each time the method makes a shift, it is guaranteed not to \u201cskip\u201d over\nany possible matches. For last( c)is the location of the lastoccurrence of cinP.\nIn Figure 13.4, we illustrate the execution of the Boyer-Moore pattern-matchingalgorithm on an input string similar to Example 13.1.\nc\nabc d\nlast (c)\n453\u2212 1\na c d a b a a cb aabc Text:\nPattern: b aaa b c1\nbaaa b c2 3 4\nbaaa b c5\nbaaa b c6baaa b c7baaa b c8 9 10 11 12 13b b a a a b a\nFigure 13.4: An illustration of the Boyer-Moore pattern-matching algorithm, in-\ncluding a summary of the last (c)function. The algorithm performs 13 character\ncomparisons, which are indicated with numerical labels.\nPerformance\nIf using a traditional lookup table, the worst-case running time of the Boyer-Moorealgorithm is O(nm +|\u03a3|). Namely, the computation of the lastfunction takes time\nO(m+|\u03a3|), and the actual search for the pattern takes O(nm)time in the worst case,\nthe same as the brute-force algorithm. (With a hash table, the dependence on |\u03a3|is\nremoved.) An example of a text-pattern pair that achieves the worst case is\nT = n/bracehtipdownleft\n/bracehtipupright/bracehtipupleft\n /bracehtipdownrightaaaaaa \u00b7\u00b7\u00b7a\nP = bm\u22121/bracehtipdownleft\n/bracehtipupright/bracehtipupleft\n/bracehtipdownrightaa\u00b7\u00b7\u00b7a\nThe worst-case performance, however, is unlikely to be achieved for English text,for, in that case, the Boyer-Moore algorithm is often able to skip large portionsof text. Experimental evidence on English text shows that the average number of\ncomparisons done per character is 0 .24 for a \ufb01ve-character pattern string.\nWe have actually presented a simpli\ufb01ed version of the Boyer-Moore algorithm.\nThe original algorithm achieves running time O(n+m+|\u03a3|)by using an alternative\nshift heuristic to the partially matched text string, whenever it shifts the pattern\nmore than the character-jump heuristic. This alternative shift heuristic is based on\napplying the main idea from the Knuth-Morris-Pratt pattern-matching algorithm,\nwhich we discuss next.", "590 Chapter 13. Text Processing\n13.2.3 The Knuth-Morris-Pratt Algorithm\nIn examining the worst-case performances of the brute-force and Boyer-Moore\npattern-matching algorithms on speci\ufb01c instances of the problem, such as that given\nin Example 13.1, we should notice a major inef\ufb01ciency. For a certain alignment ofthe pattern, if we \ufb01nd several matching characters but then detect a mismatch, weignore all the information gained by the successful comparisons after restarting\nwith the next incremental placement of the pattern.\nThe Knuth-Morris-Pratt (or \u201cKMP\u201d) algorithm, discussed in this section, avoids\nthis waste of information and, in so doing, it achieves a running time of O(n+m),\nwhich is asymptotically optimal. That is, in the worst case any pattern-matching\nalgorithm will have to examine all the characters of the text and all the charactersof the pattern at least once. The main idea of the KMP algorithm is to precom-\npute self-overlaps between portions of the pattern so that when a mismatch occurs\nat one location, we immediately know the maximum amount to shift the patternbefore continuing the search. A motivating example is shown in Figure 13.5.\na \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\na m\namto n i maag a lto ni maag a l\u00b7 t\namto n i maag a lText:\nPattern:aa m a g l a m c\nFigure 13.5: A motivating example for the Knuth-Morris-Pratt algorithm. If a mis-\nmatch occurs at the indicated location, the pattern could be shifted to the second\nalignment, without explicit need to recheck the partial match with the pre\ufb01x ama.\nIf the mismatched character is not an l, then the next potential alignment of the\npattern can take advantage of the common a.\nThe Failure Function\nTo implement the KMP algorithm, we will precompute a failure function ,f,t h a t\nindicates the proper shift of Pupon a failed comparison. Speci\ufb01cally, the failure\nfunction f(k)is de\ufb01ned as the length of the longest pre\ufb01x of Pthat is a suf\ufb01x\nofP[1:k+1] (note that we did notinclude P[0] here, since we will shift at least\none unit). Intuitively, if we \ufb01nd a mismatch upon character P[k+1] , the function\nf(k)tells us how many of the immediately preceding characters can be reused to\nrestart the pattern. Example 13.2 describes the value of the failure function for the\nexample pattern from Figure 13.5.", "13.2. Pattern-Matching Algorithms 591\nExample 13.2: Consider the pattern P=\"amalgamation\" from Figure 13.5.\nThe Knuth-Morris-Pratt (KMP) failure function, f(k), for the string Pis as shown\nin the following table:\nk\n 0 1 2345 6 7891 01 1\nP[k]\namalgamat i o n\nf(k)\n0 0 1001 2 300 0 0\nImplementation\nOur implementation of the KMP pattern-matching algorithm is shown in Code\nFragment 13.3. It relies on a utility function, compute\n kmp\nfail, discussed on\nthe next page, to compute the failure function ef\ufb01ciently.\nThe main part of the KMP algorithm is its while loop, each iteration of which\nperforms a comparison between the character at index jinTand the character at\nindex kinP. If the outcome of this comparison is a match, the algorithm moves on\nto the next characters in both Tand P(or reports a match if reaching the end of the\npattern). If the comparison failed, the algorithm consults the failure function for anew candidate character in P, or starts over with the next index in Tif failing on\nthe \ufb01rst character of the pattern (since nothing can be reused).\n1def\ufb01nd\nkmp(T, P):\n2\u201d\u201d\u201dReturn the lowest index of T at which substring P begins (or else -1).\u201d\u201d\u201d\n3n, m = len(T), len(P) # introduce convenient notations\n4ifm= =0 : return 0 # trivial search for empty string\n5fail = compute\n kmp\nfail(P) # rely on utility to precompute\n6j=0 # index into text\n7k=0 # index into pattern\n8while j<n:\n9 ifT[j] == P[k]: # P[0:1+k] matched thus far\n10 ifk= =m \u22121: # match is complete\n11 return j\u2212m+1\n12 j+ =1 # try to extend match\n13 k+ =1\n14 elifk>0:\n15 k=f a i l [ k \u22121] # reuse su\ufb03x of P[0:k]\n16 else:\n17 j+ =1\n18return \u22121 # reached end without match\nCode Fragment 13.3: An implementation of the KMP pattern-matching algorithm.\nThecompute\n kmp\nfailutility function is given in Code Fragment 13.4.", "592 Chapter 13. Text Processing\nConstructing the KMP Failure Function\nTo construct the failure function, we use the method shown in Code Fragment 13.4,\nwhich is a \u201cbootstrapping\u201d process that compares the pattern to itself as in the KMPalgorithm. Each time we have two characters that match, we set f(j)= k+1. Note\nthat since we have j>kthroughout the execution of the algorithm, f(k\u22121)is\nalways well de\ufb01ned when we need to use it.\n1defcompute\nkmp\nfail(P):\n2\u201d\u201d\u201dUtility that computes and returns KMP\n fail\nlist.\u201d\u201d\u201d\n3m=l e n ( P )\n4fail = [0]\n m # by default, presume overlap of 0 everywhere\n5j=1\n6k=0\n7while j<m: # compute f(j) during this pass, if nonzero\n8 ifP[j] == P[k]: # k + 1 characters match thus far\n9 f a i l [ j ]=k+1\n10 j+ =1\n11 k+ =1\n12 elifk>0: # k follows a matching pre\ufb01x\n13 k=f a i l [ k \u22121]\n14 else: # no match found starting at j\n15 j+ =1\n16return fail\nCode Fragment 13.4: An implementation of the compute\n kmp\nfailutility in sup-\nport of the KMP pattern-matching algorithm. Note how the algorithm uses theprevious values of the failure function to ef\ufb01ciently compute new values.\nPerformance\nExcluding the computation of the failure function, the running time of the KMPalgorithm is clearly proportional to the number of iterations of the while loop. For\nthe sake of the analysis, let us de\ufb01ne s=j\u2212k. Intuitively, sis the total amount by\nwhich the pattern Phas been shifted with respect to the text T. Note that throughout\nthe execution of the algorithm, we have s\u2264n. One of the following three cases\noccurs at each iteration of the loop.\n\u2022IfT[j]= P[k],t h e n jand keach increase by 1, and thus, sdoes not change.\n\u2022IfT[j]/negationslash=P[k]and k>0, then jdoes not change and sincreases by at least 1,\nsince in this case schanges from j\u2212ktoj\u2212f(k\u22121), which is an addition\nofk\u2212f(k\u22121), which is positive because f(k\u22121)<k.\n\u2022IfT[j]/negationslash=P[k]and k=0,\n then jincreases by 1 and sincreases by 1, since k\ndoes not change.", "13.2. Pattern-Matching Algorithms 593\nThus, at each iteration of the loop, either jorsincreases by at least 1 (possibly\nboth); hence, the total number of iterations of the while loop in the KMP pattern-\nmatching algorithm is at most 2 n. Achieving this bound, of course, assumes that\nwe have already computed the failure function for P.\nThe algorithm for computing the failure function runs in O(m)time. Its analysis\nis analogous to that of the main KMP algorithm, yet with a pattern of length m\ncompared to itself. Thus, we have:\nProposition 13.3: The Knuth-Morris-Pratt algorithm performs pattern matching\non a text string of length nand a pattern string of length minO(n+m)time.\nThe correctness of this algorithm follows from the de\ufb01nition of the failure func-\ntion. Any comparisons that are skipped are actually unnecessary, for the failure\nfunction guarantees that all the ignored comparisons are redundant\u2014they would\ninvolve comparing the same matching characters over again.\nIn Figure 13.6, we illustrate the execution of the KMP pattern-matching algo-\nrithm on the same input strings as in Example 13.1. Note the use of the failure\nfunction to avoid redoing one of the comparisons between a character of the pat-tern and a character of the text. Also note that the algorithm performs fewer overall\ncomparisons than the brute-force algorithm run on the same strings (Figure 13.1).\nThe failure function:\nk\n012345\nP[k]\nabacab\nf(k)\n001012\na\nabc13\nbaaa b c19 18 17 16 15 14no comparison\nperformedText:\nPattern: b aaa b c6 5 4 3 2 1b b a a a b a c c a b a a cb aabc\nbaaa b c7\nbaaa b c8 9 10 11 12\nbaa\nFigure 13.6: An illustration of the KMP pattern-matching algorithm. The primary\nalgorithm performs 19 character comparisons, which are indicated with numericallabels. (Additional comparisons would be performed during the computation of the\nfailure function.)", "594 Chapter 13. Text Processing\n13.3 Dynamic Programming\nIn this section, we discuss the dynamic programming algorithm-design technique.\nThis technique is similar to the divide-and-conquer technique (Section 12.2.1), in\nthat it can be applied to a wide variety of different problems. Dynamic program-\nming can often be used to take problems that seem to require exponential time and\nproduce polynomial-time algorithms to solve them. In addition, the algorithms thatresult from applications of the dynamic programming technique are usually quitesimple\u2014often needing little more than a few lines of code to describe some nested\nloops for \ufb01lling in a table.\n13.3.1 Matrix Chain-Product\nRather than starting out with an explanation of the general components of the dy-\nnamic programming technique, we begin by giving a classic, concrete example.\nSuppose we are given a collection of ntwo-dimensional matrices for which we\nwish to compute the mathematical product\nA=A0\u00b7A1\u00b7A2\u00b7\u00b7\u00b7An\u22121,\nwhere Aiis a di\u00d7di+1matrix, for i=0,1,2,..., n\u22121. In the standard matrix\nmultiplication algorithm (which is the one we will use), to multiply a d\u00d7e-matrix B\ntimes an e\u00d7f-matrix C, we compute the product, A,a s\nA[i][j]=e\u22121\n\u2211\nk=0B[i][k]\u00b7C[k][j].\nThis de\ufb01nition implies that matrix multiplication is associative, that is, it impliesthat B\u00b7(C\u00b7D)=( B\u00b7C)\u00b7D. Thus, we can parenthesize the expression for Aany\nway we wish and we will end up with the same answer. However, we will not\nnecessarily perform the same number of primitive (that is, scalar) multiplicationsin each parenthesization, as is illustrated in the following example.\nExample 13.4:\nLet Bbe a 2\u00d710-matrix, let Cbe a 10\u00d750-matrix, and let Dbe\na50\u00d720-matrix. Computing B\u00b7(C\u00b7D)requires 2\u00b710\u00b720+10\u00b750\u00b720 =10400\nmultiplications, whereas computing (B\u00b7C)\u00b7Drequires 2\u00b710\u00b750+2\u00b750\u00b720=3000\nmultiplications.\nThematrix chain-product problem is to determine the parenthesization of the\nexpression de\ufb01ning the product Athat minimizes the total number of scalar mul-\ntiplications performed. As the example above illustrates, the differences between\nparenthesizations can be dramatic, so \ufb01nding a good solution can result in signi\ufb01-\ncant speedups.", "13.3. Dynamic Programming 595\nDe\ufb01ning Subproblems\nOne way to solve the matrix chain-product problem is to simply enumerate all the\npossible ways of parenthesizing the expression for Aand determine the number\nof multiplications performed by each one. Unfortunately, the set of all differentparenthesizations of the expression for Ais equal in number to the set of all dif-\nferent binary trees that have nleaves. This number is exponential in n. Thus, this\nstraightforward (\u201cbrute-force\u201d) algorithm runs in exponential time, for there are anexponential number of ways to parenthesize an associative arithmetic expression.\nWe can signi\ufb01cantly improve the performance achieved by the brute-force al-\ngorithm, however, by making a few observations about the nature of the matrix\nchain-product problem. The \ufb01rst is that the problem can be split into subproblems .\nIn this case, we can de\ufb01ne a number of different subproblems, each of which is tocompute the best parenthesization for some subexpression A\ni\u00b7Ai+1\u00b7\u00b7\u00b7Aj. As a con-\ncise notation, we use Ni,jto denote the minimum number of multiplications needed\nto compute this subexpression. Thus, the original matrix chain-product problemcan be characterized as that of computing the value of N\n0,n\u22121. This observation\nis important, but we need one more in order to apply the dynamic programming\ntechnique.\nCharacterizing Optimal Solutions\nThe other important observation we can make about the matrix chain-product prob-lem is that it is possible to characterize an optimal solution to a particular subprob-lem in terms of optimal solutions to its subproblems. We call this property thesubproblem optimality condition.\nIn the case of the matrix chain-product problem, we observe that, no mat-\nter how we parenthesize a subexpression, there has to be some \ufb01nal matrix mul-tiplication that we perform. That is, a full parenthesization of a subexpression\nA\ni\u00b7Ai+1\u00b7\u00b7\u00b7Ajhas to be of the form (Ai\u00b7\u00b7\u00b7Ak)\u00b7(Ak+1\u00b7\u00b7\u00b7Aj),f o rs o m e k\u2208{ i,i+\n1,..., j\u22121}. Moreover, for whichever kis the correct one, the products (Ai\u00b7\u00b7\u00b7Ak)\nand (Ak+1\u00b7\u00b7\u00b7Aj)must also be solved optimally. If this were not so, then there would\nbe a global optimal that had one of these subproblems solved suboptimally. But thisis impossible, since we could then reduce the total number of multiplications by re-\nplacing the current subproblem solution by an optimal solution for the subproblem.This observation implies a way of explicitly de\ufb01ning the optimization problem forN\ni,jin terms of other optimal subproblem solutions. Namely, we can compute Ni,j\nby considering each place kwhere we could put the \ufb01nal multiplication and taking\nthe minimum over all such choices.", "596 Chapter 13. Text Processing\nDesigning a Dynamic Programming Algorithm\nWe can therefore characterize the optimal subproblem solution, Ni,j,a s\nNi,j=min\ni\u2264k<j{Ni,k+Nk+1,j+didk+1dj+1},\nwhere Ni,i=0, since no work is needed for a single matrix. That is, Ni,jis the\nminimum, taken over all possible places to perform the \ufb01nal multiplication, of the\nnumber of multiplications needed to compute each subexpression plus the number\nof multiplications needed to perform the \ufb01nal matrix multiplication.\nNotice that there is a sharing of subproblems going on that prevents us from\ndividing the problem into completely independent subproblems (as we would need\nto do to apply the divide-and-conquer technique). We can, nevertheless, use the\nequation for Ni,jto derive an ef\ufb01cient algorithm by computing Ni,jvalues in a\nbottom-up fashion, and storing intermediate solutions in a table of Ni,jvalues. We\ncan begin simply enough by assigning Ni,i=0f o ri =0,1,..., n\u22121. We can then\napply the general equation for Ni,jto compute Ni,i+1values, since they depend only\nonNi,iand Ni+1,i+1values that are available. Given the Ni,i+1values, we can then\ncompute the Ni,i+2values, and so on. Therefore, we can build Ni,jvalues up from\npreviously computed values until we can \ufb01nally compute the value of N0,n\u22121,w h i c h\nis the number that we are searching for. A Python implementation of this dynamic\nprogramming solution is given in Code Fragment 13.5; we use techniques from\nSection 5.6 for representing a multidimensional table in Python.\n1defmatrix\n chain(d):\n2\u201d\u201d\u201dd is a list of n+1 numbers such that size of kth matrix is d[k]-by-d[k+1].\n3\n4Return an n-by-n table such that N[i][j] represents the minimum number of\n5multiplications needed to compute the product of Ai through Aj inclusive.\n6\u201d\u201d\u201d\n7n=l e n ( d ) \u22121 # number of matrices\n8N = [[0]\n nforiinrange(n)] # initialize n-by-n result to zero\n9forbinrange(1, n): # number of products in subchain\n10 foriinrange(n \u2212b): # start of subchain\n11 j=i+b # end of subchain\n12 N[i][j] = min(N[i][k]+N[k+1][j]+d[i]\n d[k+1]\n d[j+1] forkinrange(i,j))\n13return N\nCode Fragment 13.5: Dynamic programming algorithm for the matrix chain-\nproduct problem.\nThus, we can compute N0,n\u22121with an algorithm that consists primarily of three\nnested loops (the third of which computes the min term). Each of these loops\niterates at most ntimes per execution, with a constant amount of additional work\nwithin. Therefore, the total running time of this algorithm is O(n3).", "13.3. Dynamic Programming 597\n13.3.2 DNA and Text Sequence Alignment\nA common text-processing problem, which arises in genetics and software engi-\nneering, is to test the similarity between two text strings. In a genetics application,\nthe two strings could correspond to two strands of DNA, for which we want to com-\npute similarities. Likewise, in a software engineering application, the two stringscould come from two versions of source code for the same program, for which wewant to determine changes made from one version to the next. Indeed, determining\nthe similarity between two strings is so common that the Unix and Linux operating\nsystems have a built-in program, named diff, for comparing text \ufb01les.\nGiven a string X=x\n0x1x2\u00b7\u00b7\u00b7xn\u22121,asubsequence ofXis any string that is of\nthe form xi1xi2\u00b7\u00b7\u00b7xik,w h e r e ij<ij+1; that is, it is a sequence of characters that are\nnot necessarily contiguous but are nevertheless taken in order from X. For example,\nthe string AAAG is a subsequence of the string CGA\n TA A\n TTG\n AGA .\nThe DNA and text similarity problem we address here is the longest common\nsubsequence (LCS) problem. In this problem, we are given two character strings,\nX=x0x1x2\u00b7\u00b7\u00b7xn\u22121and Y=y0y1y2\u00b7\u00b7\u00b7ym\u22121, over some alphabet (such as the alpha-\nbet{A,C,G,T}common in computational genetics) and are asked to \ufb01nd a longest\nstring Sthat is a subsequence of both Xand Y. One way to solve the longest\ncommon subsequence problem is to enumerate all subsequences of Xand take the\nlargest one that is also a subsequence of Y. Since each character of Xis either in\nor not in a subsequence, there are potentially 2ndifferent subsequences of X, each\nof which requires O(m)time to determine whether it is a subsequence of Y. Thus,\nthis brute-force approach yields an exponential-time algorithm that runs in O(2nm)\ntime, which is very inef\ufb01cient. Fortunately, the LCS problem is ef\ufb01ciently solvable\nusing dynamic programming .\nThe Components of a Dynamic Programming Solution\nAs mentioned above, the dynamic programming technique is used primarily for\noptimization problems, where we wish to \ufb01nd the \u201cbest\u201d way of doing something.\nWe can apply the dynamic programming technique in such situations if the problemhas certain properties:\nSimple Subproblems: There has to be some way of repeatedly breaking the global\noptimization problem into subproblems. Moreover, there should be a way to\nparameterize subproblems with just a few indices, like i,j,k, and so on.\nSubproblem Optimization: An optimal solution to the global problem must be a\ncomposition of optimal subproblem solutions.\nSubproblem Overlap: Optimal solutions to unrelated subproblems can contain\nsubproblems in common.", "598 Chapter 13. Text Processing\nApplying Dynamic Programming to the LCS Problem\nRecall that in the LCS problem, we are given two character strings, Xand Y,o f\nlength nand m, respectively, and are asked to \ufb01nd a longest string Sthat is a sub-\nsequence of both Xand Y.S i n c e Xand Yare character strings, we have a natural\nset of indices with which to de\ufb01ne subproblems\u2014indices into the strings Xand Y.\nLet us de\ufb01ne a subproblem, therefore, as that of computing the value Lj,k,w h i c h\nwe will use to denote the length of a longest string that is a subsequence of both\npre\ufb01xes X[0:j]and Y[0:k]. This de\ufb01nition allows us to rewrite Lj,kin terms of\noptimal subproblem solutions. This de\ufb01nition depends on which of two cases we\nare in. (See Figure 13.7.)\nL10,12=1+L9,11 L9,11=max (L9,10,L8,11)\n6789 1 0 1 1GTT CC TAAT A\nCA TAA TTG GA GA0123456789\nX=\n0Y=\n12345 56789 1 0GTT CC TAAT\nCA TAA TTG G GA012345678\nX=\n0Y=\n1234\n(a) (b)\nFigure 13.7: The two cases in the longest common subsequence algorithm for com-\nputing Lj,k:( a ) xj\u22121=yk\u22121;( b ) xj\u22121/negationslash=yk\u22121.\n\u2022xj\u22121=yk\u22121. In this case, we have a match between the last character of\nX[0:j]and the last character of Y[0:k]. We claim that this character be-\nlongs to a longest common subsequence of X[0:j]and Y[0:k]. To justify\nthis claim, let us suppose it is not true. There has to be some longest com-\nmon subsequence xa1xa2...xac=yb1yb2...ybc.I f xac=xj\u22121orybc=yk\u22121,\nthen we get the same sequence by setting ac=j\u22121a n d bc=k\u22121. Alter-\nnately, if xac/negationslash=xj\u22121and ybc/negationslash=yk\u22121, then we can get an even longer common\nsubsequence by adding xj\u22121=yk\u22121to the end. Thus, a longest common\nsubsequence of X[0:j]and Y[0:k]ends with xj\u22121. Therefore, we set\nLj,k=1+Lj\u22121,k\u22121ifxj\u22121=yk\u22121.\n\u2022xj\u22121/negationslash=yk\u22121. In this case, we cannot have a common subsequence that in-\ncludes both xj\u22121and yk\u22121. That is, we can have a common subsequence end\nwith xj\u22121or one that ends with yk\u22121(or possibly neither), but certainly not\nboth. Therefore, we set\nLj,k=max{Lj\u22121,k,Lj,k\u22121}ifxj\u22121/negationslash=yk\u22121.\nWe note that because slice Y[0:0 ]is the empty string, Lj,0=0f o r j=0,1,..., n;\nsimilarly, because slice X[0:0 ]is the empty string, L0,k=0f o r k=0,1,..., m.", "13.3. Dynamic Programming 599\nThe LCS Algorithm\nThe de\ufb01nition of Lj,ksatis\ufb01es subproblem optimization, for we cannot have a\nlongest common subsequence without also having longest common subsequences\nfor the subproblems. Also, it uses subproblem overlap, because a subproblem so-lution L\nj,kcan be used in several other problems (namely, the problems Lj+1,k,\nLj,k+1,a n d Lj+1,k+1). Turning this de\ufb01nition of Lj,kinto an algorithm is actually\nquite straightforward. We create an (n+1)\u00d7(m+1)array, L, de\ufb01ned for 0 \u2264j\u2264n\nand 0 \u2264k\u2264m. We initialize all entries to 0, in particular so that all entries of the\nform Lj,0and L0,kare zero. Then, we iteratively build up values in Luntil we have\nLn,m, the length of a longest common subsequence of Xand Y. W eg i v eaP y t h o n\nimplementation of this algorithm in Code Fragment 13.6.\n1defLCS(X, Y):\n2\u201d\u201d\u201dReturn table such that L[j][k] is length of LCS for X[0:j] and Y[0:k].\u201d\u201d\u201d\n3n, m = len(X), len(Y) # introduce convenient notations\n4L = [[0]\n (m+1) forkinrange(n+1)] # (n+1) x (m+1) table\n5forjinrange(n):\n6 forkinrange(m):\n7 ifX[j] == Y[k]: # align this match\n8 L[j+1][k+1] = L[j][k] + 1\n9 else: # choose to ignore one character\n10 L[j+1][k+1] = max(L[j][k+1], L[j+1][k])\n11return L\nCode Fragment 13.6: Dynamic programming algorithm for the LCS problem.\nThe running time of the algorithm of the LCS algorithm is easy to analyze,\nfor it is dominated by two nested forloops, with the outer one iterating ntimes\nand the inner one iterating mtimes. Since the if-statement and assignment inside\nthe loop each requires O(1)primitive operations, this algorithm runs in O(nm)\ntime. Thus, the dynamic programming technique can be applied to the longestcommon subsequence problem to improve signi\ufb01cantly over the exponential-timebrute-force solution to the LCS problem.\nTheLCS function of Code Fragment 13.6 computes the length of the longest\ncommon subsequence (stored as L\nn,m), but not the subsequence itself. Fortunately,\nit is easy to extract the actual longest common subsequence if given the completetable of L\nj,kvalues computed by the LCS function. The solution can be recon-\nstructed back to front by reverse engineering the calculation of length Ln,m.A ta n y\nposition Lj,k,i f xj=yk, then the length is based on the common subsequence as-\nsociated with length Lj\u22121,k\u22121, followed by common character xj. We can record xj\nas part of the sequence, and then continue the analysis from Lj\u22121,k\u22121.I f xj/negationslash=yk,", "600 Chapter 13. Text Processing\nthen we can move to the larger of Lj,k\u22121and Lj\u22121,k. We continue this process until\nreaching some Lj,k=0 (for example, if jorkis 0 as a boundary case). A Python\nimplementation of this strategy is given in Code Fragment 13.7. This function con-\nstructs a longest common subsequence in O(n+m)additional time, since each pass\nof thewhile loop decrements either jork(or both). An illustration of the algorithm\nfor computing the longest common subsequence is given in Figure 13.8.\n1defLCS\ns o l u t i o n ( X ,Y ,L ) :\n2\u201d\u201d\u201dReturn the longest common substring of X and Y, given LCS table L.\u201d\u201d\u201d\n3solution = [ ]\n4j,k = len(X), len(Y)\n5while L[j][k] >0: # common characters remain\n6 ifX[j\u22121] == Y[k \u22121]:\n7 solution.append(X[j \u22121])\n8 j\u2212=1\n9 k\u2212=1\n10 elifL[j\u22121][k]>= L[j][k \u22121]:\n11 j\u2212=1\n12 else:\n13 k\u2212=1\n14return\n .join(reversed(solution)) # return left-to-right version\nCode Fragment 13.7: Reconstructing the longest common subsequence.\n11111111111\n0011222222222\n00112223333330111222333333011122233333301112223444440112233344555\n0112334555556\n0112340\n55566601122344445560\n123456789\n10 4123456789 1 0 1 1 1 2\n0 000000000000\n00\n6 7 8 9 10 11GTT CC TAAT A\nCA TAA TTG GA GA Y=0123456789\n0X=\n12345\nFigure 13.8: Illustration of the algorithm for constructing a longest common subse-\nquence from the array L. A diagonal step on the highlighted path represents the use\nof a common character (with that character\u2019s respective indices in the sequences\nhighlighted in the margins).", "13.4. Text Compression and the Greedy Method 601\n13.4 Text Compression and the Greedy Method\nIn this section, we consider an important text-processing task, text compression .\nIn this problem, we are given a string Xde\ufb01ned over some alphabet, such as the\nASCII or Unicode character sets, and we want to ef\ufb01ciently encode Xinto a small\nbinary string Y(using only the characters 0 and 1). Text compression is useful in\nany situation where we wish to reduce bandwidth for digital communications, so\nas to minimize the time needed to transmit our text. Likewise, text compression isuseful for storing large documents more ef\ufb01ciently, so as to allow a \ufb01xed-capacitystorage device to contain as many documents as possible.\nThe method for text compression explored in this section is the Huffman code .\nStandard encoding schemes, such as ASCII, use \ufb01xed-length binary strings to en-code characters (with 7 or 8 bits in the traditional or extended ASCII systems,respectively). The Unicode system was originally proposed as a 16-bit \ufb01xed-length representation, although common encodings reduce the space usage by al-lowing common groups of characters, such as those from the ASCII system, with\nfewer bits. The Huffman code saves space over a \ufb01xed-length encoding by using\nshort code-word strings to encode high-frequency characters and long code-wordstrings to encode low-frequency characters. Furthermore, the Huffman code usesa variable-length encoding speci\ufb01cally optimized for a given string Xover any al-\nphabet. The optimization is based on the use of character frequencies ,w h e r ew e\nhave, for each character c, a count f(c)of the number of times cappears in the\nstring X.\nTo encode the string X, we convert each character in Xto a variable-length\ncode-word, and we concatenate all these code-words in order to produce the en-\ncoding Yfor X. In order to avoid ambiguities, we insist that no code-word in our\nencoding be a pre\ufb01x of another code-word in our encoding. Such a code is called\napre\ufb01x code , and it simpli\ufb01es the decoding of Yto retrieve X. (See Figure 13.9.)\nEven with this restriction, the savings produced by a variable-length pre\ufb01x codecan be signi\ufb01cant, particularly if there is a wide variance in character frequencies\n(as is the case for natural language text in almost every written language).\nHuffman\u2019s algorithm for producing an optimal variable-length pre\ufb01x code for\nXis based on the construction of a binary tree Tthat represents the code. Each\nedge in Trepresents a bit in a code-word, with an edge to a left child representing\na \u201c0\u201d and an edge to a right child representing a \u201c1.\u201d Each leaf vis associated\nwith a speci\ufb01c character, and the code-word for that character is de\ufb01ned by the\nsequence of bits associated with the edges in the path from the root of Ttov.( S e e\nFigure 13.9.) Each leaf vhas a frequency ,f(v), which is simply the frequency in\nXof the character associated with v. In addition, we give each internal node vinT\na frequency, f(v), that is the sum of the frequencies of all the leaves in the subtree\nrooted at v.", "602 Chapter 13. Text Processing\n(a)\nCharacter\n a\nb\nd\ne\nf\nh\ni\nk\nn\no\nr\ns\nt\nu\nv\nFrequency\n 9\n5\n1\n3\n7\n3\n1\n1\n1\n4\n1\n5\n1\n2\n1\n1\n(b)46\n5\nk\n1i\n12\no\n1248\nt\n2\ns\n115\nn\n47\nf\n34\nv1u\n125\nb\n12\nh\n1d\n312\ne\n727 19\na\n510\n9\nr\nFigure 13.9: An illustration of an example Huffman code for the input string\nX=\"a fast runner need never be afraid of the dark\" : (a) frequency\nof each character of X; (b) Huffman tree Tfor string X. The code for a character c\nis obtained by tracing the path from the root of Tto the leaf where cis stored, and\nassociating a left child with 0 and a right child with 1. For example, the code for\n\u201cr\u201d is 011, and the code for \u201ch\u201d is 10111.\n13.4.1 The Hu\ufb00man Coding Algorithm\nThe Huffman coding algorithm begins with each of the ddistinct characters of the\nstring Xto encode being the root node of a single-node binary tree. The algorithm\nproceeds in a series of rounds. In each round, the algorithm takes the two binarytrees with the smallest frequencies and merges them into a single binary tree. Itrepeats this process until only one tree is left. (See Code Fragment 13.8.)\nEach iteration of the while loop in Huffman\u2019s algorithm can be implemented\ninO(logd)time using a priority queue represented with a heap. In addition, each\niteration takes two nodes out of Qand adds one in, a process that will be repeated\nd\u22121 times before exactly one node is left in Q. Thus, this algorithm runs in\nO(n+dlogd)time. Although a full justi\ufb01cation of this algorithm\u2019s correctness is\nbeyond our scope here, we note that its intuition comes from a simple idea\u2014any\noptimal code can be converted into an optimal code in which the code-words for the\ntwo lowest-frequency characters, aand b, differ only in their last bit. Repeating the\nargument for a string with aand breplaced by a character c, gives the following:\nProposition 13.5:\nHuffman\u2019s algorithm constructs an optimal pre\ufb01x code for a\nstring of length nwith ddistinct characters in O(n+dlogd)time.", "13.4. Text Compression and the Greedy Method 603\nAlgorithm Hu\ufb00man( X):\nInput: String Xof length nwith ddistinct characters\nOutput: Coding tree for X\nCompute the frequency f(c)of each character cofX.\nInitialize a priority queue Q.\nfor each character cinXdo\nCreate a single-node binary tree Tstoring c.\nInsert Tinto Qwith key f(c).\nwhile len(Q)>1do\n(f1,T1)= Q.remove\n min ()\n(f2,T2)= Q.remove\n min ()\nCreate a new binary tree Twith left subtree T1and right subtree T2.\nInsert Tinto Qwith key f1+f2.\n(f,T)= Q.remove\n min ()\nreturn tree T\nCode Fragment 13.8: Huffman coding algorithm.\n13.4.2 The Greedy Method\nHuffman\u2019s algorithm for building an optimal encoding is an example application\nof an algorithmic design pattern called the greedy method . This design pattern is\napplied to optimization problems, where we are trying to construct some structure\nwhile minimizing or maximizing some property of that structure.\nThe general formula for the greedy method pattern is almost as simple as that\nfor the brute-force method. In order to solve a given optimization problem usingthe greedy method, we proceed by a sequence of choices. The sequence starts\nfrom some well-understood starting condition, and computes the cost for that ini-tial condition. The pattern then asks that we iteratively make additional choicesby identifying the decision that achieves the best cost improvement from all ofthe choices that are currently possible. This approach does not always lead to an\noptimal solution.\nBut there are several problems that it does work for, and such problems are said\nto possess the greedy-choice property. This is the property that a global optimal\ncondition can be reached by a series of locally optimal choices (that is, choices\nthat are each the current best from among the possibilities available at the time),starting from a well-de\ufb01ned starting condition. The problem of computing an opti-mal variable-length pre\ufb01x code is just one example of a problem that possesses the\ngreedy-choice property.", "604 Chapter 13. Text Processing\n13.5 Tries\nThe pattern-matching algorithms presented in Section 13.2 speed up the search in\na text by preprocessing the pattern (to compute the failure function in the Knuth-Morris-Pratt algorithm or the last function in the Boyer-Moore algorithm). In thissection, we take a complementary approach, namely, we present string searchingalgorithms that preprocess the text. This approach is suitable for applications where\na series of queries is performed on a \ufb01xed text, so that the initial cost of preprocess-\ning the text is compensated by a speedup in each subsequent query (for example, aWeb site that offers pattern matching in Shakespeare\u2019s Hamlet or a search engine\nthat offers Web pages on the Hamlet topic).\nAtrie(pronounced \u201ctry\u201d) is a tree-based data structure for storing strings in\norder to support fast pattern matching. The main application for tries is in infor-mation retrieval. Indeed, the name \u201ctrie\u201d comes from the word \u201cre trieval.\u201d In an\ninformation retrieval application, such as a search for a certain DNA sequence in agenomic database, we are given a collection Sof strings, all de\ufb01ned using the same\nalphabet. The primary query operations that tries support are pattern matching andpre\ufb01x matching . The latter operation involves being given a string X, and looking\nfor all the strings in Sthat contain Xas a pre\ufb01x.\n13.5.1 Standard Tries\nLet Sbe a set of sstrings from alphabet \u03a3such that no string in Sis a pre\ufb01x\nof another string. A standard trie for Sis an ordered tree Twith the following\nproperties (see Figure 13.10):\n\u2022Each node of T, except the root, is labeled with a character of \u03a3.\n\u2022The children of an internal node of Thave distinct labels.\n\u2022Thas sleaves, each associated with a string of S, such that the concatenation\nof the labels of the nodes on the path from the root to a leaf vofTyields the\nstring of Sassociated with v.\nThus, a trie Trepresents the strings of Swith paths from the root to the leaves\nofT. Note the importance of assuming that no string in Sis a pre\ufb01x of another\nstring. This ensures that each string of Sis uniquely associated with a leaf of T.\n(This is similar to the restriction for pre\ufb01x codes with Huffman coding, as describedin Section 13.4.) We can always satisfy this assumption by adding a special char-\nacter that is not in the original alphabet \u03a3at the end of each string.\nAn internal node in a standard trie Tcan have anywhere between 1 and |\u03a3|\nchildren. There is an edge going from the root rto one of its children for each\ncharacter that is \ufb01rst in some string in the collection S. In addition, a path from the\nroot of Tto an internal node vat depth kcorresponds to a k-character pre\ufb01x X[0:k]", "13.5. Tries 605\nb\ne\nli\nld\nlyue\nc\nko arls\nlt\nl\np\nFigure 13.10: Standard trie for the strings {bear, bell, bid, bull, buy, sell, stock,\nstop}.\nof a string XofS. In fact, for each character cthat can follow the pre\ufb01x X[0:k]in\na string of the set S, there is a child of vlabeled with character c. In this way, a trie\nconcisely stores the common pre\ufb01xes that exist among a set of strings.\nAs a special case, if there are only two characters in the alphabet, then the\ntrie is essentially a binary tree, with some internal nodes possibly having only one\nchild (that is, it may be an improper binary tree). In general, although it is possiblethat an internal node has up to |\u03a3|children, in practice the average degree of such\nnodes is likely to be much smaller. For example, the trie shown in Figure 13.10 has\nseveral internal nodes with only one child. On larger data sets, the average degree\nof nodes is likely to get smaller at greater depths of the tree, because there maybe fewer strings sharing the common pre\ufb01x, and thus fewer continuations of thatpattern. Furthermore, in many languages, there will be character combinations that\nare unlikely to naturally occur.\nThe following proposition provides some important structural properties of a\nstandard trie:\nProposition 13.6:\nA standard trie storing a collection Sofsstrings of total length\nnfrom an alphabet \u03a3has the following properties:\n\u2022The height of Tis equal to the length of the longest string in S.\n\u2022Every internal node of Thas at most |\u03a3|children.\n\u2022Thas sleaves\n\u2022The number of nodes of Tis at most n+1.\nThe worst case for the number of nodes of a trie occurs when no two strings\nshare a common nonempty pre\ufb01x; that is, except for the root, all internal nodes\nhave one child.", "606 Chapter 13. Text Processing\nA trie Tfor a set Sof strings can be used to implement a set or map whose keys\nare the strings of S. Namely, we perform a search in Tfor a string Xby tracing\ndown from the root the path indicated by the characters in X. If this path can be\ntraced and terminates at a leaf node, then we know Xis a key in the map. For\nexample, in the trie in Figure 13.10, tracing the path for \u201cbull\u201d ends up at a leaf.\nIf the path cannot be traced or the path can be traced but terminates at an internal\nnode, then Xis not a key in the map. In the example in Figure 13.10, the path for\n\u201cbet\u201d cannot be traced and the path for \u201cbe\u201d ends at an internal node. Neither such\nword is in the map.\nIt is easy to see that the running time of the search for a string of length mis\nO(m\u00b7|\u03a3|), because we visit at most m+1 nodes of Tand we spend O(|\u03a3|)time\nat each node determining the child having the subsequent character as a label. The\nO(|\u03a3|)upper bound on the time to locate a child with a given label is achievable,\neven if the children of a node are unordered, since there are at most |\u03a3|children.\nWe can improve the time spent at a node to be O(log|\u03a3|)or expected O(1),b y\nmapping characters to children using a secondary search table or hash table at each\nnode, or by using a direct lookup table of size |\u03a3|at each node, if |\u03a3|is suf\ufb01ciently\nsmall (as is the case for DNA strings). For these reasons, we typically expect a\nsearch for a string of length mto run in O(m)time.\nFrom the discussion above, it follows that we can use a trie to perform a spe-\ncial type of pattern matching, called word matching , where we want to determine\nwhether a given pattern matches one of the words of the text exactly. Word match-\ning differs from standard pattern matching because the pattern cannot match anarbitrary substring of the text\u2014only one of its words. To accomplish this, each\nword of the original document must be added to the trie. (See Figure 13.11.) A\nsimple extension of this scheme supports pre\ufb01x-matching queries. However, ar-bitrary occurrences of the pattern in the text (for example, the pattern is a propersuf\ufb01x of a word or spans two words) cannot be ef\ufb01ciently performed.\nTo construct a standard trie for a set Sof strings, we can use an incremental\nalgorithm that inserts the strings one at a time. Recall the assumption that no stringofSis\na pre\ufb01x of another string. To insert a string Xinto the current trie T,w e\ntrace the path associated with XinT, creating a new chain of nodes to store the\nremaining characters of Xwhen we get stuck. The running time to insert Xwith\nlength mis similar to a search, with worst-case O(m\u00b7|\u03a3|)performance, or expected\nO(m)if using secondary hash tables at each node. Thus, constructing the entire trie\nfor set Stakes expected O(n)time, where nis the total length of the strings of S.\nThere is a potential space inef\ufb01ciency in the standard trie that has prompted the\ndevelopment of the compressed trie , which is also known (for historical reasons)\nas the Patricia trie . Namely, there are potentially a lot of nodes in the standard trie\nthat have only one child, and the existence of such nodes is a waste. We discuss thecompressed trie next.", "13.5. Tries 607\ne a bear ? sell0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\nstock17 18 19 20 21\n!22\nsee a bull ?23 24 25 26 27 28 29 30 31 32 33 34\nbuy stock !35 36 37 38 39 40 41 42 43 44 45\nbid46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68\nstock ! bid stock !\nhear the bell ? stop !69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88se\n(a)\n690,24\nl\n12l\n84p\n17,40,51,62lee\nc\nko arhs\nt e\na\nrb\nei\nd\n67 8l\n47,58u\n30y\n36\nll\n(b)\nFigure 13.11: Word matching with a standard trie: (a) text to be searched (articles\nand prepositions, which are also known as stop words , excluded); (b) standard trie\nfor the words in the text, with leaves augmented with indications of the index at\nwhich the given work begins in the text. For example, the leaf for the word stock\nnotes that the word begins at indices 17, 40, 51, and 62 of the text.", "608 Chapter 13. Text Processing\n13.5.2 Compressed Tries\nAcompressed trie is similar to a standard trie but it ensures that each internal node\nin the trie has at least two children. It enforces this rule by compressing chains of\nsingle-child nodes into individual edges. (See Figure 13.12.) Let Tbe a standard\ntrie. We say that an internal node vofTisredundant ifvhas one child and is not\nthe root. For example, the trie of Figure 13.10 has eight redundant nodes. Let usalso say that a chain of k\u22652 edges,\n(v\n0,v1)(v1,v2)\u00b7\u00b7\u00b7(vk\u22121,vk),\nisredundant if:\n\u2022viis redundant for i=1,..., k\u22121.\n\u2022v0and vkare not redundant.\nWe can transform Tinto a compressed trie by replacing each redundant chain\n(v0,v1)\u00b7\u00b7\u00b7(vk\u22121,vk)ofk\u22652 edges into a single edge (v0,vk), relabeling vkwith\nthe concatenation of the labels of nodes v1,..., vk.\ns\nto\npb\ncke id\nar y llu\nllell\nFigure 13.12: Compressed trie for the strings {bear, bell, bid, bull, buy, sell, stock,\nstop}. (Compare this with the standard trie shown in Figure 13.10.) In addition to\ncompression at the leaves, notice the internal node with label toshared by words\nstock andstop.\nThus, nodes in a compressed trie are labeled with strings, which are substrings\nof strings in the collection, rather than with individual characters. The advantage ofa compressed trie over a standard trie is that the number of nodes of the compressedtrie is proportional to the number of strings and not to their total length, as shownin the following proposition (compare with Proposition 13.6).\nProposition 13.7:\nA compressed trie storing a collection Sofsstrings from an\nalphabet of size dhas the following properties:\n\u2022Every internal node of Thas at least two children and most dchildren.\n\u2022Thas sleaves nodes.\n\u2022The number of nodes of TisO(s).", "13.5. Tries 609\nThe attentive reader may wonder whether the compression of paths provides\nany signi\ufb01cant advantage, since it is offset by a corresponding expansion of the\nnode labels. Indeed, a compressed trie is truly advantageous only when it is used asanauxiliary index structure over a collection of strings already stored in a primary\nstructure, and is not required to actually store all the characters of the strings in thecollection.\nSuppose, for example, that the collection Sof strings is an array of strings S[0],\nS[1],...,S[s\u22121]. Instead of storing the label Xof a node explicitly, we represent\nit implicitly by a combination of three integers (i,j:k), such that X=S[i][j:k];\nthat is, Xis the slice of S[i]consisting of the characters from the j\nthup to but\nnot including the kth. (See the example in Figure 13.13. Also compare with the\nstandard trie of Figure 13.11.)\nS[2]=\nS[3]=S[4]=\nS[5]=\nS[6]= S[9]=S[8]=S[7]= S[0]=\nS[1]=\nk tcose l lbe r asee\nl l0123\nbu l lr\nbuyh\nbide\nst p oa\nbe0123 01234\ns\n(a)\n5,2:33,1:3\n0,2:3 3,3:50,1:27,0:4 1,0:1\n9,3:40,0:1\n6,1:3 4,1:2\n4,2:41,1:2\n1,2:4 8 ,2:4 2,2:4\n(b)\nFigure 13.13: (a) Collection Sof strings stored in an array. (b) Compact represen-\ntation of the compressed trie for S.\nThis additional compression scheme allows us to reduce the total space for the\ntrie itself from O(n)for the standard trie to O(s)for the compressed trie, where n\nis the total length of the strings in Sand sis the number of strings in S.W e m u s t\nstill store the different strings in S, of course, but we nevertheless reduce the space\nfor the trie.\nSearching in a compressed trie is not necessarily faster than in a standard tree,\nsince there is still need to compare every character of the desired pattern with the\npotentially multi-character labels while traversing paths in the trie.", "610 Chapter 13. Text Processing\n13.5.3 Su\ufb03x Tries\nOne of the primary applications for tries is for the case when the strings in the\ncollection Sare all the suf\ufb01xes of a string X. Such a trie is called the suf\ufb01x trie (also\nknown as a suf\ufb01x tree orposition tree ) of string X. For example, Figure 13.14a\nshows the suf\ufb01x trie for the eight suf\ufb01xes of string \u201cminimize.\u201d For a suf\ufb01x trie, the\ncompact representation presented in the previous section can be further simpli\ufb01ed.\nNamely, the label of each vertex is a pair (j,k)indicating the string X[j:k].( S e e\nFigure 13.14b.) To satisfy the rule that no suf\ufb01x of Xis a pre\ufb01x of another suf\ufb01x,\nwe can add a special character, denoted with $, that is not in the original alphabet \u03a3\nat the end of X(and thus to every suf\ufb01x). That is, if string Xhas length n, we build\na trie for the set of nstrings X[j:n],f o r j=0,..., n\u22121.\nSaving Space\nUsing a suf\ufb01x trie allows us to save space over a standard trie by using several spacecompression techniques, including those used for the compressed trie.\nThe advantage of the compact representation of tries now becomes apparent for\nsuf\ufb01x tries. Since the total length of the suf\ufb01xes of a string Xof length nis\n1+2+\u00b7\u00b7\u00b7+n=n(n+1)\n2,\nstoring all the suf\ufb01xes of Xexplicitly would take O(n2)space. Even so, the suf-\n\ufb01x trie represents these strings implicitly in O(n)space, as formally stated in the\nfollowing proposition.\nProposition 13.8: The compact representation of a suf\ufb01x trie Tfor a string Xof\nlength nuses O(n)space.\nConstruction\nWe can construct the suf\ufb01x trie for a string of length nwith an incremental algo-\nrithm like the one given in Section 13.5.1. This construction takes O(|\u03a3|n2)time\nbecause the total length of the suf\ufb01xes is quadratic in n. However, the (compact)\nsuf\ufb01x trie for a string of length ncan be constructed in O(n)time with a specialized\nalgorithm, different from the one for general tries. This linear-time construction\nalgorithm is fairly complex, however, and is not reported here. Still, we can takeadvantage of the existence of this fast construction algorithm when we want to use\na suf\ufb01x trie to solve other problems.", "13.5. Tries 611\ne\nzeze\nmizei\nnimize ze nimizemi nimize\n(a)\n0:2 6:8\n6:8 2:8 2:82:8 1:2\n6:87:8\n4:8\ne01234567\nminimiz\n(b)\nFigure 13.14: (a) Suf\ufb01x trie Tfor the string X=\"minimize\" . (b) Compact repre-\nsentation of T, where pair j:kdenotes slice X[j:k]in the reference string.\nUsing a Su\ufb03x Trie\nThe suf\ufb01x trie Tfor a string Xcan be used to ef\ufb01ciently perform pattern-matching\nqueries on text X. Namely, we can determine whether a pattern Pis a substring\nofXby trying to trace a path associated with PinT.Pis a substring of Xif and\nonly if such a path can be traced. The search down the trie Tassumes that nodes in\nTstore some additional information, with respect to the compact representation of\nthe suf\ufb01x trie:\nIf node vhas label (j,k)and Yis the string of length yassociated with\nthe path from the root to v(included), then X[k\u2212y:k]= Y.\nThis property ensures that we can easily compute the start index of the pattern in\nthe text when a match occurs.", "612 Chapter 13. Text Processing\n13.5.4 Search Engine Indexing\nThe World Wide Web contains a huge collection of text documents (Web pages).\nInformation about these pages are gathered by a program called a Web crawler ,\nwhich then stores this information in a special dictionary database. A Web search\nengine allows users to retrieve relevant information from this database, thereby\nidentifying relevant pages on the Web containing given keywords. In this section,we present a simpli\ufb01ed model of a search engine.\nInverted Files\nThe core information stored by a search engine is a dictionary, called an inverted\nindex orinverted \ufb01le , storing key-value pairs (w,L),w h e r e wis a word and Lis\na collection of pages containing word w. The keys (words) in this dictionary are\ncalled index terms and should be a set of vocabulary entries and proper nouns as\nlarge as possible. The elements in this dictionary are called occurrence lists and\nshould cover as many Web pages as possible.\nWe can ef\ufb01ciently implement an inverted index with a data structure consisting\nof the following:\n1. An array storing the occurrence lists of the terms (in no particular order).\n2. A compressed trie for the set of index terms, where each leaf stores the index\nof the occurrence list of the associated term.\nThe reason for storing the occurrence lists outside the trie is to keep the size of the\ntrie data structure suf\ufb01ciently small to \ufb01t in internal memory. Instead, because oftheir large total size, the occurrence lists have to be stored on disk.\nWith our data structure, a query for a single keyword is similar to a word-\nmatching query (Section 13.5.1). Namely, we \ufb01nd the keyword in the trie and wereturn the associated occurrence list.\nWhen multiple keywords are given and the desired output are the pages con-\ntaining allthe given keywords, we retrieve the occurrence list of each keyword\nusing the trie and return their intersection. To facilitate the intersection computa-tion, each occurrence list should be implemented with a sequence sorted by addressor with a map, to allow ef\ufb01cient set operations.\nIn addition to the basic task of returning a list of pages containing given key-\nwords, search engines provide an important additional service by ranking the pages\nreturned by relevance. Devising fast and accurate ranking algorithms for search\nengines is a major challenge for computer researchers and electronic commerce\ncompanies.", "13.6. Exercises 613\n13.6 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-13.1 List the pre\ufb01xes of the string P=\"aaabbaaa\" that are also suf\ufb01xes of P.\nR-13.2 What is the longest (proper) pre\ufb01x of the string \"cgtacgttcgtacg\" that\nis also a suf\ufb01x of this string?\nR-13.3 Draw a \ufb01gure illustrating the comparisons done by brute-force pattern\nmatching for the text \"aaabaadaabaaa\" and pattern \"aabaaa\".\nR-13.4 Repeat the previous problem for the Boyer-Moore algorithm, not countingthe comparisons made to compute the last (c)function.\nR-13.5 Repeat Exercise R-13.3 for the Knuth-Morris-Pratt algorithm, not count-ing the comparisons made to compute the failure function.\nR-13.6 Compute a map representing the lastfunction used in the Boyer-Moore\npattern-matching algorithm for characters in the pattern string:\n\"the quick brown fox jumped over a lazy cat\".\nR-13.7 Compute a table representing the Knuth-Morris-Pratt failure function forthe pattern string \"cgtacgttcgtac\".\nR-13.8 What is the best way to multiply a chain of matrices with dimensions that\nare 10 \u00d75, 5\u00d72, 2\u00d720, 20 \u00d712, 12 \u00d74, and 4 \u00d760? Show your work.\nR-13.9 In Figure 13.8, we illustrate that GTTTAA is a longest common subse-\nquence for the given strings Xand Y. However, that answer is not unique.\nGive another common subsequence of Xand Yhaving length six.\nR-13.10 Show the longest common subsequence array Lfor the two strings:\nX =\"skullandbones\"\nY =\"lullabybabies\"\nWhat is a longest common subsequence between these strings?\nR-13.11 Draw the frequency array and Huffman tree for the following string:\n\"dogs do not spot hot pots or cats\".\nR-13.12 Draw a standard trie for the following set of strings:\n{abab, baba, ccccc, bbaaaa, caa, bbaacc, cbcc, cbca }.\nR-13.13 Draw a compressed trie for the strings given in the previous problem.\nR-13.14 Draw the compact representation of the suf\ufb01x trie for the string:\n\"minimize minime\".", "614 Chapter 13. Text Processing\nCreativity\nC-13.15 Describe an example of a text Tof length nand a pattern Pof length\nmsuch that force the brute-force pattern-matching algorithm achieves a\nrunning time that is \u03a9(nm).\nC-13.16 Adapt the brute-force pattern-matching algorithm in order to implement a\nfunction, r\ufb01nd\nbrute(T,P) , that returns the index at which the rightmost\noccurrence of pattern Pwithin text T,i fa n y .\nC-13.17 Redo the previous problem, adapting the Boyer-Moore pattern-matching\nalgorithm appropriately to implement a function r\ufb01nd\nboyer\n moore(T,P) .\nC-13.18 Redo Exercise C-13.16, adapting the Knuth-Morris-Pratt pattern-matching\nalgorithm appropriately to implement a function r\ufb01nd\nkmp(T,P) .\nC-13.19 Thecount method of Python\u2019s strclass reports the maximum number of\nnonoverlapping occurrences of a pattern within a string. For example, the\ncall\n abababa\n .count(\n aba\n)returns 2 (not 3). Adapt the brute-force\npattern-matching algorithm to implement a function, count\n brute(T,P) ,\nwith similar outcome.\nC-13.20 Redo the previous problem, adapting the Boyer-Moore pattern-matchingalgorithm in order to implement a function count\nboyer\n moore(T,P) .\nC-13.21 Redo Exercise C-13.19, adapting the Knuth-Morris-Pratt pattern-matchingalgorithm appropriately to implement a function count\nkmp(T,P) .\nC-13.22 Give a justi\ufb01cation of why the compute\n kmp\nfailfunction (Code Frag-\nment 13.4) runs in O(m)time on a pattern of length m.\nC-13.23 Let Tbe a text of length n,a n dl e t Pbe a pattern of length m. Describe an\nO(n+m)-time method for \ufb01nding the longest pre\ufb01x of Pthat is a substring\nofT.\nC-13.24 Say that a pattern Pof length mis acircular substring of a text Tof length\nn>mifPis a (normal) substring of T,o ri f Pis equal to the concatenation\nof a suf\ufb01x of Tand a pre\ufb01x of T, that is, if there is an index 0 \u2264k<m,\nsuch that P=T[n\u2212m+k:n]+T[0:k].G i v ea n O(n+m)-time algorithm\nfor determining whether Pis a circular substring of T.\nC-13.25 The Knuth-Morris-Pratt pattern-matching algorithm can be modi\ufb01ed torun faster on binary strings by rede\ufb01ning the failure function as:\nf(k)=the largest j<ksuch\nthat P[0:j]/hatwidepjis a suf\ufb01x of P[1:k+1],\nwhere /hatwidepjdenotes the complement of the jthbit of P. Describe how to\nmodify the KMP algorithm to be able to take advantage of this new failurefunction and also give a method for computing this failure function. Showthat this method makes at most ncomparisons between the text and the\npattern (as opposed to the 2 ncomparisons needed by the standard KMP\nalgorithm given in Section 13.2.3).", "13.6. Exercises 615\nC-13.26 Modify the simpli\ufb01ed Boyer-Moore algorithm presented in this chapter\nusing ideas from the KMP algorithm so that it runs in O(n+m)time.\nC-13.27 Design an ef\ufb01cient algorithm for the matrix chain multiplication problem\nthat outputs a fully parenthesized expression for how to multiply the ma-trices in the chain using the minimum number of operations.\nC-13.28 A native Australian named Anatjari wishes to cross a desert carrying onlya single water bottle. He has a map that marks all the watering holes alongthe way. Assuming he can walk kmiles on one bottle of water, design an\nef\ufb01cient algorithm for determining where Anatjari should re\ufb01ll his bottlein order to make as few stops as possible. Argue why your algorithm iscorrect.\nC-13.29 Describe an ef\ufb01cient greedy algorithm for making change for a speci\ufb01edvalue using a minimum number of coins, assuming there are four denomi-nations of coins (called quarters, dimes, nickels, and pennies), with values25, 10, 5, and 1, respectively. Argue why your algorithm is correct.\nC-13.30 Give an example set of denominations of coins so that a greedy change-making algorithm will not use the minimum number of coins.\nC-13.31 In the art gallery guarding problem we are given a line Lthat repre-\nsents a long hallway in an art gallery. We are also given a set X=\n{x\n0,x1,..., xn\u22121}of real numbers that specify the positions of paintings\nin this hallway. Suppose that a single guard can protect all the paintingswithin distance at most 1 of his or her position (on both sides). Designan algorithm for \ufb01nding a placement of guards that uses the minimum\nnumber of guards to guard all the paintings with positions in X.\nC-13.32 Let Pbe a convex polygon, a triangulation ofPis an addition of diag-\nonals connecting the vertices of Pso that each interior face is a triangle.\nTheweight of a triangulation is the sum of the lengths of the diagonals.\nAssuming that we can compute lengths and add and compare them in con-\nstant time, give an ef\ufb01cient algorithm for computing a minimum-weighttriangulation of P.\nC-13.33 Let Tbe a text string of length n. Describe an O(n)-time method for\n\ufb01nding the longest pre\ufb01x of Tthat is a substring of the reversal of T.\nC-13.34 Describe an ef\ufb01cient algorithm to \ufb01nd the longest palindrome that is a\nsuf\ufb01x of a string Tof length n. Recall that a palindrome is a string that is\nequal to its reversal. What is the running time of your method?\nC-13.35 Given a sequence S=( x\n0,x1,..., xn\u22121)of numbers, describe an O(n2)-\ntime algorithm for \ufb01nding a longest subsequence T=( xi0,xi1,..., xik\u22121)\nof numbers, such that ij<ij+1and xij>xij+1.T h a t i s , Tis a longest\ndecreasing subsequence of S.\nC-13.36 Give an ef\ufb01cient algorithm for determining if a pattern Pis a subsequence\n(not substring) of a text T. What is the running time of your algorithm?", "616 Chapter 13. Text Processing\nC-13.37 De\ufb01ne the edit distance between two strings Xand Yof length nand m,\nrespectively, to be the number of edits that it takes to change Xinto Y.A n\nedit consists of a character insertion, a character deletion, or a character\nreplacement. For example, the strings \"algorithm\" and\"rhythm\" have\nedit distance 6. Design an O(nm)-time algorithm for computing the edit\ndistance between Xand Y.\nC-13.38 Let Xand Ybe strings of length nand m, respectively. De\ufb01ne B(j,k)to\nbe the length of the longest common substring of the suf\ufb01x X[n\u2212j:n]and\nthe suf\ufb01x Y[m\u2212k:m]. Design an O(nm)-time algorithm for computing all\nthe values of B(j,k)for j=1,..., nand k=1,..., m.\nC-13.39 Anna has just won a contest that allows her to take npieces of candy out\nof a candy store for free. Anna is old enough to realize that some candy is\nexpensive, while other candy is relatively cheap, costing much less. The\njars of candy are numbered 0, 1, ...,m\u22121 , so that jar jhas njpieces in\nit, with a price of cjper piece. Design an O(n+m)-time algorithm that\nallows Anna to maximize the value of the pieces of candy she takes for\nher winnings. Show that your algorithm produces the maximum value for\nAnna.\nC-13.40 Let three integer arrays, A,B,a n d C, be given, each of size n. Given an\narbitrary integer k, design an O(n2logn)-time algorithm to determine if\nthere exist numbers, ainA,binB,a n d cinC, such that k=a+b+c.\nC-13.41 Give an O(n2)-time algorithm for the previous problem.\nC-13.42 Given a string Xof length nand a string Yof length m, describe an O(n+\nm)-time algorithm for \ufb01nding the longest pre\ufb01x of Xthat is a suf\ufb01x of Y.\nC-13.43 Give an ef\ufb01cient algorithm for deleting a string from a standard trie andanalyze its running time.\nC-13.44 Give an ef\ufb01cient algorithm for deleting a string from a compressed trie\nand analyze its running time.\nC-13.45 Describe an algorithm for constructing the compact representation of asuf\ufb01x trie, given its noncompact representation, and analyze its runningtime.\nProjects\nP-13.46 Use the LCS algorithm to compute the best sequence alignment between\nsome DNA strings, which you can get online from GenBank.\nP-13.47 Write a program that takes two character strings (which could be, for ex-\nample, representations of DNA strands) and computes their edit distance,\nshowing the corresponding pieces. (See Exercise C-13.37.)", "13.6. Exercises 617\nP-13.48 Perform an experimental analysis of the ef\ufb01ciency (number of character\ncomparisons performed) of the brute-force and KMP pattern-matching al-\ngorithms for varying-length patterns.\nP-13.49 Perform an experimental analysis of the ef\ufb01ciency (number of charac-ter comparisons performed) of the brute-force and Boyer-Moore pattern-matching algorithms for varying-length patterns.\nP-13.50 Perform an experimental comparison of the relative speeds of the brute-force, KMP, and Boyer-Moore pattern-matching algorithms. Document\nthe relative running times on large text documents that are then searched\nusing varying-length patterns.\nP-13.51 Experiment with the ef\ufb01ciency of the \ufb01nd method of Python\u2019s strclass\nand develop a hypothesis about which pattern-matching algorithm it uses.Try using inputs that are likely to cause both best-case and worst-case\nrunning times for various algorithms. Describe your experiments and your\nconclusions.\nP-13.52 Implement a compression and decompression scheme that is based onHuffman coding.\nP-13.53 Create a class that implements a standard trie for a set of ASCII strings.The class should have a constructor that takes a list of strings as an argu-ment, and the class should have a method that tests whether a given string\nis stored in the trie.\nP-13.54 Create a class that implements a compressed trie for a set of ASCII strings.\nThe class should have a constructor that takes a list of strings as an argu-ment, and the class should have a method that tests whether a given stringis stored in the trie.\nP-13.55 Create a class that implements a pre\ufb01x trie for an ASCII string. The class\nshould have a constructor that takes a string as an argument, and a method\nfor pattern matching on the string.\nP-13.56 Implement the simpli\ufb01ed search engine described in Section 13.5.4 forthe pages of a small Web site. Use all the words in the pages of the siteas index terms, excluding stop words such as articles, prepositions, andpronouns.\nP-13.57 Implement a search engine for the pages of a small Web site by adding\na page-ranking feature to the simpli\ufb01ed search engine described in Sec-\ntion 13.5.4. Your page-ranking feature should return the most relevantpages \ufb01rst. Use all the words in the pages of the site as index terms, ex-cluding stop words, such as articles, prepositions, and pronouns.", "618 Chapter 13. Text Processing\nChapter Notes\nThe KMP algorithm is described by Knuth, Mo rris, and Pratt in their journal article [66],\nand Boyer and Moore describe their algor ithm in a journal article published the same\nyear [18]. In their article, however, Knuth et al. [66] also prove that the Boyer-Moore\nalgorithm runs in linear time. More recently, Cole [27] shows that the Boyer-Moore algo-\nrithm makes at most 3 ncharacter comparisons in the worst case, and this bound is tight.\nAll of the algorithms discussed above are also discussed in the book chapter by Aho [4],\nalbeit in a more theoretical framework, includi ng the methods for regular-expression pat-\ntern matching. The reader interested in further study of string pattern-matching algorithms\nis referred to the book by Stephen [90] and the book chapters by Aho [4], and Crochemore\nand Lecroq [30].\nDynamic programming was developed in the operations research community and for-\nmalized by Bellman [13].\nThe trie was invented by Morrison [79] and is discussed extensively in the classic\nSorting and Searching book by Knuth [65]. The name \u201cPatricia\u201d is short for \u201cPractical\nAlgorithm to Retrieve Information Coded in Alphanumeric\u201d [79]. McCreight [73] shows\nhow to construct suf\ufb01x tries in linear time. An introduction to the \ufb01eld of information\nretrieval, which includes a discussion of s earch engines for the Web, is provided in the\nbook by Baeza-Yates and Ribeiro-Neto [8].", "Chapter\n14Graph Algorithms\nContents\n1 4 . 1G r a p h s ............................. 6 2 0\n1 4 . 1 . 1T h e G r a p hA D T.......................6 2 6\n1 4 . 2D a t aS t r u c t u r e sf o rG r a p h s.................. 6 2 7\n1 4 . 2 . 1E d g e L i s t S t r u c t u r e .....................6 2 8\n1 4 . 2 . 2A d j a c e n c y L i s t S t r u c t u r e ..................6 3 01 4 . 2 . 3A d j a c e n c y M a p S t r u c t u r e..................6 3 2\n1 4 . 2 . 4A d j a c e n c y M a t r i x S t r u c t u r e.................6 3 3\n14.2.5 Python Implementation . . . . . . . . . . . . . . . . . . . 634\n1 4 . 3G r a p hT r a v e r s a l s........................ 6 3 8\n1 4 . 3 . 1D e p t h - F i r s t S e a r c h .....................6 3 9\n14.3.2 DFS Implementation and Extensions . . . . . . . . . . . . 644\n1 4 . 3 . 3B r e a d t h - F i r s t S e a r c h ....................6 4 8\n1 4 . 4T r a n s i t i v eC l o s u r e ....................... 6 5 1\n1 4 . 5D i r e c t e dA c y c l i cG r a p h s.................... 6 5 5\n1 4 . 5 . 1T o p o l o g i c a l O r d e r i n g ....................6 5 5\n1 4 . 6S h o r t e s tP a t h s......................... 6 5 9\n1 4 . 6 . 1W e i g h t e d G r a p h s ......................6 5 91 4 . 6 . 2D i j k s t r a \u2019 s A l g o r i t h m.....................6 6 1\n1 4 . 7M i n i m u mS p a n n i n gT r e e s................... 6 7 0\n14.7.1 Prim-Jarn \u00b4 \u0131k A l g o r i t h m ...................6 7 2\n1 4 . 7 . 2K r u s k a l \u2019 s A l g o r i t h m.....................6 7 614.7.3 Disjoint Partitions and Union-Find Structures . . . . . . . 681\n1 4 . 8E x e r c i s e s ............................ 6 8 6\n", "620 Chapter 14. Graph Algorithms\n14.1 Graphs\nAgraph is a way of representing relationships that exist between pairs of objects.\nThat is, a graph is a set of objects, called vertices, together with a collection of\npairwise connections between them, called edges. Graphs have applications in\nmodeling many domains, including mapping, transportation, computer networks,\nand electrical engineering. By the way, this notion of a \u201cgraph\u201d should not beconfused with bar charts and function plots, as these kinds of \u201cgraphs\u201d are unrelatedto the topic of this chapter.\nViewed abstractly, a graph Gis simply a set Vofvertices and a collection E\nof pairs of vertices from V, called edges . Thus, a graph is a way of representing\nconnections or relationships between pairs of objects from some set V. Incidentally,\nsome books use different terminology for graphs and refer to what we call verticesasnodes and what we call edges as arcs. We use the terms \u201cvertices\u201d and \u201cedges.\u201d\nEdges in a graph are either directed orundirected . An edge (u,v)is said to\nbedirected from utovif the pair (u,v)is ordered, with upreceding v. An edge\n(u,v)i ss a i dt ob e undirected if the pair (u,v)is not ordered. Undirected edges are\nsometimes denoted with set notation, as {u,v}, but for simplicity we use the pair\nnotation (u,v), noting that in the undirected case (u,v)is the same as (v,u). Graphs\nare typically visualized by drawing the vertices as ovals or rectangles and the edgesas segments or curves connecting pairs of ovals and rectangles. The following aresome examples of directed and undirected graphs.\nExample 14.1:\nWe can visualize collaborations among the researchers of a cer-\ntain discipline by constructing a graph whose vertices are associated with the re-\nsearchers themselves, and whose edges connect pairs of vertices associated with\nresearchers who have coauthored a paper or book. (See Figure 14.1.) Such edges\nare undirected because coauthorship is a symmetric relation; that is, if Ahas coau-\nthored something with B,t h e n Bnecessarily has coauthored something with A.\nChiangGoldwasser\nTamassia GoodrichGarg Snoeyink\nTollis\nVitter Preparata\nFigure 14.1: Graph of coauthorship among some authors.", "14.1. Graphs 621\nExample 14.2: We can associate with an object-oriented program a graph whose\nvertices represent the classes de\ufb01ned in the program, and whose edges indicate\ninheritance between classes. There is an edge from a vertex vto a vertex uif\nthe class for vinherits from the class for u. Such edges are directed because the\ninheritance relation only goes in one direction (that is, it is asymmetric ).\nIf all the edges in a graph are undirected, then we say the graph is an undirected\ngraph . Likewise, a directed graph , also called a digraph , is a graph whose edges\nare all directed. A graph that has both directed and undirected edges is often called\namixed graph . Note that an undirected or mixed graph can be converted into a\ndirected graph by replacing every undirected edge (u,v)by the pair of directed\nedges (u,v)and (v,u). It is often useful, however, to keep undirected and mixed\ngraphs represented as they are, for such graphs have several applications, as in thefollowing example.\nExample 14.3:\nA city map can be modeled as a graph whose vertices are intersec-\ntions or dead ends, and whose edges are stretches of streets without intersections.\nThis graph has both undirected edges, which correspond to stretches of two-way\nstreets, and directed edges, which correspond to stretches of one-way streets. Thus,\nin this way, a graph modeling a city map is a mixed graph.\nExample 14.4: Physical examples of graphs are present in the electrical wiring\nand plumbing networks of a building. Such networks can be modeled as graphs,\nwhere each connector, \ufb01xture, or outlet is viewed as a vertex, and each uninter-\nrupted stretch of wire or pipe is viewed as an edge. Such graphs are actually com-\nponents of much larger graphs, namely the local power and water distribution net-\nworks. Depending on the speci\ufb01c aspects of these graphs that we are interested in,\nwe may consider their edges as undirected or directed, for, in principle, water can\n\ufb02ow in a pipe and current can \ufb02ow in a wire in either direction.\nThe two vertices joined by an edge are called the end vertices (orendpoints )\nof the edge. If an edge is directed, its \ufb01rst endpoint is its origin and the other is the\ndestination of the edge. Two vertices uand vare said to be adjacent if there is an\nedge whose end vertices are uand v. A ne d g ei ss a i dt ob e incident to a vertex if\nthe vertex is one of the edge\u2019s endpoints. The outgoing edges of a vertex are the\ndirected edges whose origin is that vertex. The incoming edges of a vertex are the\ndirected edges whose destination is that vertex. The degree of a vertex v, denoted\ndeg (v), is the number of incident edges of v.T h e in-degree andout-degree of a\nvertex vare the number of the incoming and outgoing edges of v, and are denoted\nindeg (v)and outdeg (v), respectively.", "622 Chapter 14. Graph Algorithms\nExample 14.5: We can study air transportation by constructing a graph G, called\na\ufb02ight network , whose vertices are associated with airports, and whose edges\nare associated with \ufb02ights. (See Figure 14.2.) In graph G, the edges are directed\nbecause a given \ufb02ight has a speci\ufb01c travel direction. The endpoints of an edge ein\nGcorrespond respectively to the origin and destination of the \ufb02ight corresponding\ntoe. Two airports are adjacent in Gif there is a \ufb02ight that \ufb02ies between them,\nand an edge eis incident to a vertex vinGif the \ufb02ight for e\ufb02ies to or from the\nairport for v. The outgoing edges of a vertex vcorrespond to the outbound \ufb02ights\nfrom v\u2019s airport, and the incoming edges correspond to the inbound \ufb02ights to v\u2019s\nairport. Finally, the in-degree of a vertex vofGcorresponds to the number of\ninbound \ufb02ights to v\u2019s airport, and the out-degree of a vertex vinGcorresponds to\nthe number of outbound \ufb02ights.\nORD\nMIANW 35\nAA 903DL 247DL 335\nAA 49\nAA 411AA 523UA 120UA 877SW 45\nAA 1387\nDFW\nLAXSFOBOS\nJFK\nFigure 14.2: Example of a directed graph representing a \ufb02ight network. The end-\npoints of edge UA 120 are LAX and ORD; hence, LAX and ORD are adjacent.\nThe in-degree of DFW is 3, and the out-degree of DFW is 2.\nThe de\ufb01nition of a graph refers to the group of edges as a collection , not a\nset, thus allowing two undirected edges to have the same end vertices, and for two\ndirected edges to have the same origin and the same destination. Such edges arecalled parallel edges ormultiple edges . A \ufb02ight network can contain parallel edges\n(Example 14.5), such that multiple edges between the same pair of vertices couldindicate different \ufb02ights operating on the same route at different times of the day.Another special type of edge is one that connects a vertex to itself. Namely, we say\nthat an edge (undirected or directed) is a self-loop if its two endpoints coincide. A\nself-loop may occur in a graph associated with a city map (Example 14.3), where\nit would correspond to a \u201ccircle\u201d (a curving street that returns to its starting point).\nWith few exceptions, graphs do not have parallel edges or self-loops. Such\ngraphs are said to be simple . Thus, we can usually say that the edges of a simple\ngraph are a setof vertex pairs (and not just a collection). Throughout this chapter,\nwe assume that a graph is simple unless otherwise speci\ufb01ed.", "14.1. Graphs 623\nApath is a sequence of alternating vertices and edges that starts at a vertex and\nends at a vertex such that each edge is incident to its predecessor and successor\nvertex. A cycle is a path that starts and ends at the same vertex, and that includes at\nleast one edge. We say that a path is simple if each vertex in the path is distinct, and\nwe say that a cycle is simple if each vertex in the cycle is distinct, except for the\n\ufb01rst and last one. A directed path is a path such that all edges are directed and are\ntraversed along their direction. A directed cycle is similarly de\ufb01ned. For example,\nin Figure 14.2, (BOS, NW 35, JFK, AA 1387, DFW) is a directed simple path, and(LAX, UA 120, ORD, UA 877, DFW, AA 49, LAX) is a directed simple cycle.Note that a directed graph may have a cycle consisting of two edges with opposite\ndirection between the same pair of vertices, for example (ORD, UA 877, DFW,\nDL 335, ORD) in Figure 14.2. A directed graph is acyclic if it has no directed\ncycles. For example, if we were to remove the edge UA 877 from the graph inFigure 14.2, the remaining graph is acyclic. If a graph is simple, we may omit theedges when describing path Por cycle C, as these are well de\ufb01ned, in which case\nPis a list of adjacent vertices and Cis a cycle of adjacent vertices.\nExample 14.6:\nGiven a graph Grepresenting a city map (see Example 14.3), we\ncan model a couple driving to dinner at a recommended restaurant as traversing a\npath though G. If they know the way, and do not accidentally go through the same\nintersection twice, then they traverse a simple path in G. Likewise, we can model\nthe entire trip the couple takes, from their home to the restaurant and back, as a\ncycle. If they go home from the restaurant in a completely different way than how\nthey went, not even going through the same intersection twice, then their entire\nround trip is a simple cycle. Finally, if they travel along one-way streets for their\nentire trip, we can model their night out as a directed cycle.\nGiven vertices uand vof a (directed) graph G, we say that ureaches v,a n d\nthat visreachable from u,i fGhas a (directed) path from utov. In an undirected\ngraph, the notion of reachability is symmetric, that is to say, ureaches vif an only\nifvreaches u. However, in a directed graph, it is possible that ureaches vbut vdoes\nnot reach u, because a directed path must be traversed according to the respective\ndirections of the edges. A graph is connected if, for any two vertices, there is a path\nbetween them. A directed graph /vectorGisstrongly connected if for any two vertices u\nand vof/vectorG,ureaches vand vreaches u. (See Figure 14.3 for some examples.)\nAsubgraph of a graph Gis a graph Hwhose vertices and edges are subsets of\nthe vertices and edges of G, respectively. A spanning subgraph ofGis a subgraph\nofGthat contains all the vertices of the graph G.I f a g r a p h Gis not connected,\nits maximal connected subgraphs are called the connected components ofG.A\nforest is a graph without cycles. A treeis a connected forest, that is, a connected\ngraph without cycles. A spanning tree of a graph is a spanning subgraph that is a\ntree.\n(Note that this de\ufb01nition of a tree is somewhat different from the one given in\nChapter 8, as there is not necessarily a designated root.)", "624 Chapter 14. Graph Algorithms\nDFW\nMIAORD\nJFKBOS\nSFO\nLAXDFWSFO\nMIAORD\nJFKBOS\nLAX\n(a) (b)\nJFKBOS\nLAXDFWORD\nMIASFO\nDFW\nMIAORD\nJFKBOS\nSFO\nLAX\n(c) (d)\nFigure 14.3: Examples of reachability in a directed graph: (a) a directed path from\nBOS to LAX is highlighted; (b) a directed cycle (ORD, MIA, DFW, LAX, ORD) is\nhighlighted; its vertices induce a strongly connected subgraph; (c) the subgraph ofthe vertices and edges reachable from ORD is highlighted; (d) the removal of the\ndashed edges results in an acyclic directed graph.\nExample 14.7:\nPerhaps the most talked about graph today is the Internet, which\ncan be viewed as a graph whose vertices are computers and whose (undirected)\nedges are communication connections between pairs of computers on the Inter-\nnet. The computers and the connections between them in a single domain, like\nwiley.com , form a subgraph of the Internet. If this subgraph is connected, then two\nusers on computers in this domain can send email to one another without having\ntheir information packets ever leave their domain. Suppose the edges of this sub-\ngraph form a spanning tree. This implies that, if even a single connection goes\ndown (for example, because someone pulls a communication cable out of the back\nof a computer in this domain), then this subgraph will no longer be connected.", "14.1. Graphs 625\nIn the propositions that follow, we explore a few important properties of graphs.\nProposition 14.8: IfGis a graph with medges and vertex set V,t h e n\n\u2211\nvinVdeg (v)=2m.\nJusti\ufb01cation: An edge (u,v)is counted twice in the summation above; once by\nits endpoint uand once by its endpoint v. Thus, the total contribution of the edges\nto the degrees of the vertices is twice the number of edges.\nProposition 14.9: IfGis a directed graph with medges and vertex set V,t h e n\n\u2211\nvinVindeg (v)= \u2211\nvinVoutdeg (v)= m.\nJusti\ufb01cation: In a directed graph, an edge (u,v)contributes one unit to the\nout-degree of its origin uand one unit to the in-degree of its destination v. Thus,\nthe total contribution of the edges to the out-degrees of the vertices is equal to the\nnumber of edges, and similarly for the in-degrees.\nWe next show that a simple graph with nvertices has O(n2)edges.\nProposition 14.10: Let Gbe a simple graph with nvertices and medges. If Gis\nundirected, then m\u2264n(n\u22121)/2,a n di f Gis directed, then m\u2264n(n\u22121).\nJusti\ufb01cation: Suppose that Gis undirected. Since no two edges can have the\nsame endpoints and there are no self-loops, the maximum degree of a vertex in G\nisn\u22121 in this case. Thus, by Proposition 14.8, 2 m\u2264n(n\u22121). Now suppose that\nGis directed. Since no two edges can have the same origin and destination, and\nthere are no self-loops, the maximum in-degree of a vertex in Gisn\u22121 in this case.\nThus, by Proposition 14.9, m\u2264n(n\u22121).\nThere are a number of simple properties of trees, forests, and connected graphs.\nProposition 14.11: Let Gbe an undirected graph with nvertices and medges.\n\u2022IfGis connected, then m\u2265n\u22121.\n\u2022IfGis a tree, then m=n\u22121.\n\u2022IfGis a forest, then m\u2264n\u22121.", "626 Chapter 14. Graph Algorithms\n14.1.1 The Graph ADT\nA graph is a collection of vertices and edges. We model the abstraction as a com-\nbination of three data types: Vertex ,Edge ,a n dGraph .AVertex is a lightweight\nobject that stores an arbitrary element provided by the user (e.g., an airport code);\nwe assume it supports a method, element() , to retrieve the stored element. An\nEdge also stores an associated object (e.g., a \ufb02ight number, travel distance, cost),\nretrieved with the element() method. In addition, we assume that an Edge supports\nthe following methods:\nendpoints() :Return a tuple (u,v)such that vertex uis the origin of\nthe edge and vertex vis the destination; for an undirected\ngraph, the orientation is arbitrary.\nopposite(v) :Assuming vertex vis one endpoint of the edge (either\norigin or destination), return the other endpoint.\nThe primary abstraction for a graph is the Graph ADT. We presume that a graph\ncan be either undirected ordirected , with the designation declared upon construc-\ntion; recall that a mixed graph can be represented as a directed graph, modeling\nedge{u,v}as a pair of directed edges (u,v)and (v,u).T h e Graph ADT includes\nthe following methods:\nvertex\n count() :Return the number of vertices of the graph.\nvertices() :Return an iteration of all the vertices of the graph.\nedge\ncount() :Return the number of edges of the graph.\nedges() :Return an iteration of all the edges of the graph.\nget\nedge(u,v) :Return the edge from vertex uto vertex v, if one exists;\notherwise return None . For an undirected graph, there is\nno difference between get\nedge(u,v) andget\nedge(v,u) .\ndegree(v, out=True) :For an undirected graph, return the number of edges inci-dent to vertex v. For a directed graph, return the number\nof outgoing (resp. incoming) edges incident to vertex v,\nas designated by the optional parameter.\nincident\nedges(v, out=True) :Return an iteration of all edges incident to vertex v.I n\nthe case of a directed graph, report outgoing edges bydefault; report incoming edges if the optional parameteris set to False .\ninsert\nvertex(x=None) :Create and return a new Vertex storing element x.\ninsert\n edge(u, v, x=None) :Create and return a new Edge from vertex uto vertex v,\nstoring element x(None by default).\nremove\n vertex(v) :Remove vertex vand all its incident edges from the graph.\nremove\n edge(e) :Remove edge efrom the graph.", "14.2. Data Structures for Graphs 627\n14.2 Data Structures for Graphs\nIn this section, we introduce four data structures for representing a graph. In each\nrepresentation, we maintain a collection to store the vertices of a graph. However,\nthe four representations differ greatly in the way they organize the edges.\n\u2022In an edge list , we maintain an unordered list of all edges. This minimally\nsuf\ufb01ces, but there is no ef\ufb01cient way to locate a particular edge (u,v),o rt h e\nset of all edges incident to a vertex v.\n\u2022In an adjacency list , we maintain, for each vertex, a separate list containing\nthose edges that are incident to the vertex. The complete set of edges can\nbe determined by taking the union of the smaller sets, while the organizationallows us to more ef\ufb01ciently \ufb01nd all edges incident to a given vertex.\n\u2022Anadjacency map is very similar to an adjacency list, but the secondary\ncontainer of all edges incident to a vertex is organized as a map, rather thanas a list, with the adjacent vertex serving as a key. This allows for access toa speci\ufb01c edge (u,v)inO(1)expected time.\n\u2022Anadjacency matrix provides worst-case O(1)access to a speci\ufb01c edge\n(u,v)by maintaining an n\u00d7nmatrix, for a graph with nvertices. Each\nentry is dedicated to storing a reference to the edge (u,v)for a particular pair\nof vertices uand v; if no such edge exists, the entry will be None .\nA summary of the performance of these structures is given in Table 14.1. We\ngive further explanation of the structures in the remainder of this section.\nOperation\n Edge List\n Adj. List\n Adj. Map\n Adj. Matrix\nvertex\n count()\n O(1)\n O(1)\n O(1)\n O(1)\nedge\ncount()\n O(1)\n O(1)\n O(1)\n O(1)\nvertices()\n O(n)\n O(n)\n O(n)\n O(n)\nedges()\n O(m)\n O(m)\n O(m)\n O(m)\nget\nedge(u,v)\n O(m)\n O(min (du,dv))\n O(1)exp.\n O(1)\ndegree(v)\n O(m)\n O(1)\n O(1)\n O(n)\nincident\n edges(v)\n O(m)\n O(dv)\n O(dv)\n O(n)\ninsert\n vertex(x)\n O(1)\n O(1)\n O(1)\n O(n2)\nremove\n vertex(v)\n O(m)\n O(dv)\n O(dv)\n O(n2)\ninsert\n edge(u,v,x)\n O(1)\n O(1)\n O(1)exp.\n O(1)\nremove\n edge(e)\n O(1)\n O(1)\n O(1)exp.\n O(1)\nTable 14.1: A summary of the running times for the methods of the graph ADT, us-\ning the graph representations discussed in this section. We let ndenote the number\nof vertices, mthe number of edges, and dvthe degree of vertex v. Note that the\nadjacency matrix uses O(n2)space, while all other structures use O(n+m)space.", "628 Chapter 14. Graph Algorithms\n14.2.1 Edge List Structure\nTheedge list structure is possibly the simplest, though not the most ef\ufb01cient, rep-\nresentation of a graph G. All vertex objects are stored in an unordered list V,a n d\nall edge objects are stored in an unordered list E. We illustrate an example of the\nedge list structure for a graph Gin Figure 14.4.\nhe g\nvu\nwzf\nze\nf\ng\nhVE\nvu\nw\n(a) (b)\nFigure 14.4: (a) A graph G; (b) schematic representation of the edge list structure\nforG. Notice that an edge object refers to the two vertex objects that correspond to\nits endpoints, but that vertices do not refer to incident edges.\nTo support the many methods of the Graph ADT (Section 14.1), we assume the\nfollowing additional features of an edge list representation. Collections Vand Eare\nrepresented with doubly linked lists using our PositionalList class from Chapter 7.\nVertex Objects\nThe vertex object for a vertex vstoring element xhas instance variables for:\n\u2022A reference to element x, to support the element() method.\n\u2022A reference to the position of the vertex instance in the list V, thereby allow-\ning vto be ef\ufb01ciently removed from Vif it were removed from the graph.\nEdge ObjectsThe edge object for an edge estoring element xhas instance variables for:\n\u2022A reference to element x, to support the element() method.\n\u2022References to the vertex objects associated with the endpoint vertices of e.\nThese allow the edge instance to provide constant-time support for methods\nendpoints() andopposite(v) .\n\u2022A reference to the position of the edge instance in list E, thereby allowing e\nto be ef\ufb01ciently removed from Eif it were removed from the graph.", "14.2. Data Structures for Graphs 629\nPerformance of the Edge List Structure\nThe performance of an edge list structure in ful\ufb01lling the graph ADT is summarized\nin Table 14.2. We begin by discussing the space usage, which is O(n+m)for\nrepresenting a graph with nvertices and medges. Each individual vertex or edge\ninstance uses O(1)space, and the additional lists Vand Euse space proportional\nto their number of entries.\nIn terms of running time, the edge list structure does as well as one could hope\nin terms of reporting the number of vertices or edges, or in producing an iteration\nof those vertices or edges. By querying the respective list VorE,t h evertex\n count\nandedge\ncount methods run in O(1)time, and by iterating through the appropriate\nlist, the methods vertices andedges run respectively in O(n)and O(m)time.\nThe most signi\ufb01cant limitations of an edge list structure, especially when com-\npared to the other graph representations, are the O(m)running times of methods\nget\nedge(u,v) ,degree(v) ,a n d incident\n edges(v) . The problem is that with all\nedges of the graph in an unordered list E, the only way to answer those queries\nis through an exhaustive inspection of all edges. The other data structures intro-\nduced in this section will implement these methods more ef\ufb01ciently.\nFinally, we consider the methods that update the graph. It is easy to add a new\nvertex or a new edge to the graph in O(1)time. For example, a new edge can be\nadded to the graph by creating an Edge instance storing the given element as data,\nadding that instance to the positional list E, and recording its resulting Position\nwithin Eas an attribute of the edge. That stored position can later be used to\nlocate and remove this edge from EinO(1)time, and thus implement the method\nremove\n edge(e)\nIt is worth discussing why the remove\n vertex(v) method has a running time of\nO(m). As stated in the graph ADT, when a vertex vis removed from the graph, all\nedges incident to vmust also be removed (otherwise, we would have a contradiction\nof edges that refer to vertices that are not part of the graph). To locate the incident\nedges to the vertex, we must examine all edges of E.\nOperation\n Running Time\nvertex\n count(), edge\n count()\n O(1)\nvertices()\n O(n)\nedges()\n O(m)\nget\nedge(u,v), degree(v), incident\n edges(v)\n O(m)\ninsert\n vertex(x), insert\n edge(u,v,x), remove\n edge(e)\n O(1)\nremove\n vertex(v)\n O(m)\nTable 14.2: Running times of the methods of a graph implemented with the edge\nlist structure. The space used is O(n+m),w h e r e nis the number of vertices and m\nis the number of edges.", "630 Chapter 14. Graph Algorithms\n14.2.2 Adjacency List Structure\nIn contrast to the edge list representation of a graph, the adjacency list structure\ngroups the edges of a graph by storing them in smaller, secondary containers that\nare associated with each individual vertex. Speci\ufb01cally, for each vertex v,w em a i n -\ntain a collection I(v), called the incidence collection ofv, whose entries are edges\nincident to v. (In the case of a directed graph, outgoing and incoming edges can be\nrespectively stored in two separate collections, Iout(v)and Iin(v).) Traditionally, the\nincidence collection I(v)for a vertex vis a list, which is why we call this way of\nrepresenting a graph the adjacency list structure.\nWe require that the primary structure for an adjacency list maintain the col-\nlection Vof vertices in a way so that we can locate the secondary structure I(v)\nfor a given vertex vinO(1)time. This could be done by using a positional list\nto represent V, with each Vertex instance maintaining a direct reference to its I(v)\nincidence collection; we illustrate such an adjacency list structure of a graph in Fig-\nure 14.5. If vertices can be uniquely numbered from 0 to n\u22121, we could instead\nuse a primary array-based structure to access the appropriate secondary lists.\nThe primary bene\ufb01t of an adjacency list is that the collection I(v)contains ex-\nactly those edges that should be reported by the method incident\n edges(v) .T h e r e -\nfore, we can implement this method by iterating the edges of I(v)inO(deg (v))\ntime, where deg (v)is the degree of vertex v. This is the best possible outcome for\nany graph representation, because there are deg (v)edges to be reported.\nhe g\nvu\nwzf fh\nhg e\nf e\ngu\nv\nw\nzV\n(a) (b)\nFigure 14.5: (a) An undirected graph G; (b) a schematic representation of the ad-\njacency list structure for G. Collection Vis the primary list of vertices, and each\nvertex has an associated list of incident edges. Although not diagrammed as such,we presume that each edge of the graph is represented with a unique Edge instance\nthat maintains references to its endpoint vertices.", "14.2. Data Structures for Graphs 631\nPerformance of the Adjacency List Structure\nTable 14.3 summarizes the performance of the adjacency list structure implemen-\ntation of a graph, assuming that the primary collection Vand all secondary collec-\ntions I(v)are implemented with doubly linked lists.\nAsymptotically, the space requirements for an adjacency list are the same as\nan edge list structure, using O(n+m)space for a graph with nvertices and m\nedges. The primary list of vertices uses O(n)space. The sum of the lengths of\nall secondary lists is O(m), for reasons that were formalized in Propositions 14.8\nand 14.9. In short, an undirected edge (u,v)is referenced in both I(u)and I(v),b u t\nits presence in the graph results in only a constant amount of additional space.\nWe have already noted that the incident\n edges(v) method can be achieved in\nO(deg (v))time based on use of I(v). We can achieve the degree(v) method of\nthe graph ADT to use O(1)time, assuming collection I(v)can report its size in\nsimilar time. To locate a speci\ufb01c edge for implementing get\nedge(u,v) , we can\nsearch through either I(u)and I(v). By choosing the smaller of the two, we get\nO(min (deg (u),deg (v)))running time.\nThe rest of the bounds in Table 14.3 can be achieved with additional care. To\nef\ufb01ciently support deletions of edges, an edge (u,v)would need to maintain a ref-\nerence to its positions within both I(u)and I(v), so that it could be deleted from\nthose collections in O(1)time. To remove a vertex v, we must also remove any\nincident edges, but at least we can locate those edges in O(deg (v))time.\nThe easiest way to support edges() inO(m)andcount\n edges() inO(1)is to\nmaintain an auxiliary list Eof edges, as in the edge list representation. Otherwise,\nwe can implement the edges method in O(n+m)time by accessing each secondary\nlist and reporting its edges, taking care not to report an undirected edge (u,v)twice.\nOperation\n Running Time\nvertex\n count(), edge\n count()\n O(1)\nvertices()\n O(n)\nedges()\n O(m)\nget\nedge(u,v)\n O(min (deg (u),deg (v)))\ndegree(v)\n O(1)\nincident\n edges(v)\n O(deg (v))\ninsert\n vertex(x), insert\n edge(u,v,x)\n O(1)\nremove\n edge(e)\n O(1)\nremove\n vertex(v)\n O(deg (v))\nTable 14.3: Running times of the methods of a graph implemented with the adja-\ncency list structure. The space used is O(n+m),w h e r e nis the number of vertices\nand mis the number of edges.", "632 Chapter 14. Graph Algorithms\n14.2.3 Adjacency Map Structure\nIn the adjacency list structure, we assume that the secondary incidence collections\nare implemented as unordered linked lists. Such a collection I(v)uses space pro-\nportional to O(deg (v)), allows an edge to be added or removed in O(1)time, and\nallows an iteration of all edges incident to vertex vinO(deg (v))time. However,\nthe best implementation of get\nedge(u,v) requires O(min (deg (u),deg (v)))time,\nbecause we must search through either I(u)orI(v).\nWe can improve the performance by using a hash-based map to implement I(v)\nfor each vertex v. Speci\ufb01cally, we let the opposite endpoint of each incident edge\nserve as a key in the map, with the edge structure serving as the value. We call such\na graph representation an adjacency map . (See Figure 14.6.) The space usage for\nan adjacency map remains O(n+m), because I(v)uses O(deg (v))space for each\nvertex v, as with the adjacency list.\nThe advantage of the adjacency map, relative to an adjacency list, is that the\nget\nedge(u,v) method can be implemented in expected O(1)time by searching for\nvertex uas a key in I(v), or vice versa. This provides a likely improvement over the\nadjacency list, while retaining the worst-case bound of O(min (deg (u),deg (v))).\nIn comparing the performance of adjacency map to other representations (see\nTable 14.1), we \ufb01nd that it essentially achieves optimal running times for all meth-\nods, making it an excellent all-purpose choice as a graph representation.\nhe g\nvu\nwzf gh\nw\nhuuw v\ng e\nf ew\nvuz\nfv\nw\nzV\n(a) (b)\nFigure 14.6: (a) An undirected graph G; (b) a schematic representation of the ad-\njacency map structure for G. Each vertex maintains a secondary map in which\nneighboring vertices serve as keys, with the connecting edges as associated values.\nAlthough not diagrammed as such, we presume that there is a unique Edge instance\nfor each edge of the graph, and that it maintains references to its endpoint vertices.", "14.2. Data Structures for Graphs 633\n14.2.4 Adjacency Matrix Structure\nTheadjacency matrix structure for a graph Gaugments the edge list structure with\na matrix A(that is, a two-dimensional array, as in Section 5.6), which allows us to\nlocate an edge between a given pair of vertices in worst-case constant time. In the\nadjacency matrix representation, we think of the vertices as being the integers in\nthe set {0,1,..., n\u22121}and the edges as being pairs of such integers. This allows\nus to store references to edges in the cells of a two-dimensional n\u00d7narray A.\nSpeci\ufb01cally, the cell A[i,j]holds a reference to the edge (u,v), if it exists, where u\nis the vertex with index iand vis the vertex with index j. If there is no such edge,\nthen A[i,j]=None . We note that array Ais symmetric if graph Gis undirected, as\nA[i,j]= A[j,i]for all pairs iand j. (See Figure 14.7.)\nThe most signi\ufb01cant advantage of an adjacency matrix is that any edge (u,v)\ncan be accessed in worst-case O(1)time; recall that the adjacency map supports\nthat operation in O(1)expected time. However, several operation are less ef\ufb01cient\nwith an adjacency matrix. For example, to \ufb01nd the edges incident to vertex v,w e\nmust presumably examine all nentries in the row associated with v; recall that an\nadjacency list or map can locate those edges in optimal O(deg (v))time. Adding or\nremoving vertices from a graph is problematic, as the matrix must be resized.\nFurthermore, the O(n2)space usage of an adjacency matrix is typically far\nworse than the O(n+m)space required of the other representations. Although,\nin the worst case, the number of edges in a dense graph will be proportional to\nn2, most real-world graphs are sparse . In such cases, use of an adjacency matrix\nis inef\ufb01cient. However, if a graph is dense, the constants of proportionality of an\nadjacency matrix can be smaller than that of an adjacency list or map. In fact, ifedges do not have auxiliary data, a Boolean adjacency matrix can use one bit peredge slot, such that A[i,j]=True if and only if associated (u,v)is an edge.\nhe g\nvu\nwzf h0\n1\n2\n30123\nu\nv\nw\nze\neg\ngf\nfh\n(a) (b)\nFigure 14.7: (a) An undirected graph G; (b) a schematic representation of the aux-\niliary adjacency matrix structure for G,i nw h i c h nvertices are mapped to indices 0\nton\u22121. Although not diagrammed as such, we presume that there is a unique Edge\ninstance for each edge, and that it maintains references to its endpoint vertices. We\nalso assume that there is a secondary edge list (not pictured), to allow the edges()\nmethod to run in O(m)time, for a graph with medges.", "634 Chapter 14. Graph Algorithms\n14.2.5 Python Implementation\nIn this section, we provide an implementation of the Graph ADT. Our implementa-\ntion will support directed or undirected graphs, but for ease of explanation, we \ufb01rstdescribe it in the context of an undirected graph.\nWe use a variant of the adjacency map representation. For each vertex v,w e\nuse a Python dictionary to represent the secondary incidence map I(v).H o w e v e r ,\nwe do not explicitly maintain lists Vand E, as originally described in the edge list\nrepresentation. The list Vis replaced by a top-level dictionary Dthat maps each\nvertex vto its incidence map I(v); note that we can iterate through all vertices by\ngenerating the set of keys for dictionary D. By using such a dictionary Dto map\nvertices to the secondary incidence maps, we need not maintain references to thoseincidence maps as part of the vertex structures. Also, a vertex does not need toexplicitly maintain a reference to its position in D, because it can be determined\ninO(1)expected time. This greatly simpli\ufb01es our implementation. However, a\nconsequence of our design is that some of the worst-case running time bounds forthe graph ADT operations, given in Table 14.1, become expected bounds. Rather\nthan maintain list E, we are content with taking the union of the edges found in the\nvarious incidence maps; technically, this runs in O(n+m)time rather than strictly\nO(m)time, as the dictionary Dhas nkeys, even if some incidence maps are empty.\nOur implementation of the graph ADT is given in Code Fragments 14.1 through\n14.3. Classes Vertex andEdge , given in Code Fragment 14.1, are rather simple,\nand can be nested within the more complex Graph class. Note that we de\ufb01ne the\nhash\n method for both Vertex andEdge so that those instances can be used as\nkeys in Python\u2019s hash-based sets and dictionaries. The rest of the Graph class is\ngiven in Code Fragments 14.2 and 14.3. Graphs are undirected by default, but canbe declared as directed with an optional parameter to the constructor.\nInternally, we manage the directed case by having two different top-level dictio-\nnary instances,\noutgoing and\nincoming , such that\n outgoing[v] maps to another\ndictionary representing Iout(v),a n d\n incoming[v] maps to a representation of Iin(v).\nIn order to unify our treatment of directed and undirected graphs, we continue touse the\noutgoing and\nincoming identi\ufb01ers in the undirected case, yet as aliases\nto the same dictionary. For convenience, we de\ufb01ne a utility named is\ndirected to\nallow us to distinguish between the two cases.\nFor methods degree andincident\n edges , which each accept an optional param-\neter to differentiate between the outgoing and incoming orientations, we choose theappropriate map before proceeding. For method insert\nvertex , we always initial-\nize\noutgoing[v] to an empty dictionary for new vertex v. In the directed case, we\nindependently initialize\n incoming[v] as well. For the undirected case, that step is\nunnecessary as\n outgoing and\nincoming are aliases. We leave the implementations\nof methods remove\n vertex andremove\n edge as exercises (C-14.37 and C-14.38).", "14.2. Data Structures for Graphs 635\n1#------------------------- nested Vertex class -------------------------\n2classVertex:\n3 \u201d\u201d\u201dLightweight vertex structure for a graph.\u201d\u201d\u201d\n4\n slots\n =\n_element\n5\n6 def\n init\n(self,x ) :\n7 \u201d\u201d\u201dDo not call constructor directly. Use Graph\n si n s e r t\n vertex(x).\u201d\u201d\u201d\n8 self.\nelement = x\n9\n10 defelement( self):\n11 \u201d\u201d\u201dReturn element associated with this vertex.\u201d\u201d\u201d\n12 return self .\nelement\n13\n14 def\n hash\n (self): # will allow vertex to be a map/set key\n15 return hash(id( self))\n1617 #------------------------- nested Edge class -------------------------\n18classEdge:\n19 \u201d\u201d\u201dLightweight edge structure for a graph.\u201d\u201d\u201d\n20\nslots\n =\n_origin\n ,\n_destination\n ,\n_element\n2122 def\ninit\n(self,u ,v ,x ) :\n23 \u201d\u201d\u201dDo not call constructor directly. Use Graph\n si n s e r t\n edge(u,v,x).\u201d\u201d\u201d\n24 self.\norigin = u\n25 self.\ndestination = v\n26 self.\nelement = x\n27\n28 defendpoints( self):\n29 \u201d\u201d\u201dReturn (u,v) tuple for vertices u and v.\u201d\u201d\u201d\n30 return (self.\norigin, self.\ndestination)\n31\n32 defopposite( self,v ) :\n33 \u201d\u201d\u201dReturn the vertex that is opposite v on this edge.\u201d\u201d\u201d\n34 return self .\ndestination ifvis self .\noriginelse self .\norigin\n3536 defelement( self):\n37 \u201d\u201d\u201dReturn element associated with this edge.\u201d\u201d\u201d\n38 return self .\nelement\n3940 def\nhash\n (self): # will allow edge to be a map/set key\n41 return hash( ( self.\norigin, self.\ndestination) )\nCode Fragment 14.1: Vertex andEdge classes (to be nested within Graph class).", "636 Chapter 14. Graph Algorithms\n1classGraph:\n2\u201d\u201d\u201dRepresentation of a simple graph using an adjacency map.\u201d\u201d\u201d\n3\n4def\n init\n(self, directed= False):\n5 \u201d\u201d\u201dCreate an empty graph (undirected, by default).\n6\n7 Graph is directed if optional paramter is set to True.\n8 \u201d\u201d\u201d\n9 self.\noutgoing = {}\n10 # only create second map for directed graph; use alias for undirected\n11 self.\nincoming = {}if directed else self .\noutgoing\n12\n13defis\ndirected( self):\n14 \u201d\u201d\u201dReturn True if this is a directed graph; False if undirected.\n15\n16 Property is based on the original declaration of the graph, not its contents.\n17 \u201d\u201d\u201d\n18 return self .\nincoming is not self .\noutgoing # directed if maps are distinct\n19\n20defvertex\n count(self):\n21 \u201d\u201d\u201dReturn the number of vertices in the graph.\u201d\u201d\u201d\n22 return len(self.\noutgoing)\n2324defvertices( self):\n25 \u201d\u201d\u201dReturn an iteration of all vertices of the graph.\u201d\u201d\u201d\n26 return self .\noutgoing.keys()\n2728defedge\ncount(self):\n29 \u201d\u201d\u201dReturn the number of edges in the graph.\u201d\u201d\u201d\n30 total = sum(len( self.\noutgoing[v]) forvin self.\n outgoing)\n31 # for undirected graphs, make sure not to double-count edges\n32 return totalif self.is\ndirected( ) elsetotal // 2\n33\n34defedges(self):\n35 \u201d\u201d\u201dReturn a set of all edges of the graph.\u201d\u201d\u201d\n36 result = set() # avoid double-reporting edges of undirected graph\n37 forsecondary\n mapin self.\n outgoing.values():\n38 result.update(secondary\n map.values()) # add edges to resulting set\n39 return result\nCode Fragment 14.2: Graph class de\ufb01nition (continued in Code Fragment 14.3).", "14.2. Data Structures for Graphs 637\n40defget\nedge(self,u ,v ) :\n41 \u201d\u201d\u201dReturn the edge from u to v, or None if not adjacent.\u201d\u201d\u201d\n42 return self .\noutgoing[u].get(v) # returns None if v not adjacent\n43\n44defdegree( self, v, outgoing= True):\n45 \u201d\u201d\u201dReturn number of (outgoing) edges incident to vertex v in the graph.\n46\n47 If graph is directed, optional parameter used to count incoming edges.\n48 \u201d\u201d\u201d\n49 adj =self.\noutgoing ifoutgoing else self .\nincoming\n50 return len(adj[v])\n5152defincident\nedges(self, v, outgoing=True ):\n53 \u201d\u201d\u201dReturn all (outgoing) edges incident to vertex v in the graph.\n5455 If graph is directed, optional parameter used to request incoming edges.\n56 \u201d\u201d\u201d\n57 adj =self.\noutgoing ifoutgoing else self .\nincoming\n58 foredgeinadj[v].values():\n59 yieldedge\n6061definsert\nvertex( self,x =None):\n62 \u201d\u201d\u201dInsert and return a new Vertex with element x.\u201d\u201d\u201d\n63 v=self.Vertex(x)\n64 self.\noutgoing[v] = {}\n65 if self.is\ndirected():\n66 self.\nincoming[v] = {} # need distinct map for incoming edges\n67 return v\n6869definsert\nedge(self,u ,v ,x = None):\n70 \u201d\u201d\u201dInsert and return a new Edge from u to v with auxiliary element x.\u201d\u201d\u201d\n71 e=self.Edge(u, v, x)\n72 self.\noutgoing[u][v] = e\n73 self.\nincoming[v][u] = e\nCode Fragment 14.3: Graph class de\ufb01nition (continued from Code Fragment 14.2).\nWe omit error-checking of parameters for brevity.", "638 Chapter 14. Graph Algorithms\n14.3 Graph Traversals\nGreek mythology tells of an elaborate labyrinth that was built to house the mon-\nstrous Minotaur, which was part bull and part man. This labyrinth was so complex\nthat neither beast nor human could escape it. No human, that is, until the Greekhero, Theseus, with the help of the king\u2019s daughter, Ariadne, decided to implementagraph traversal algorithm. Theseus fastened a ball of thread to the door of the\nlabyrinth and unwound it as he traversed the twisting passages in search of the\nmonster. Theseus obviously knew about good algorithm design, for, after \ufb01nding\nand defeating the beast, Theseus easily followed the string back out of the labyrinthto the loving arms of Ariadne.\nFormally, a traversal is a systematic procedure for exploring a graph by exam-\nining all of its vertices and edges. A traversal is ef\ufb01cient if it visits all the verticesand edges in time proportional to their number, that is, in linear time.\nGraph traversal algorithms are key to answering many fundamental questions\nabout graphs involving the notion of reachability , that is, in determining how to\ntravel from one vertex to another while following paths of a graph. Interesting\nproblems that deal with reachability in an undirected graph Ginclude the following:\n\u2022Computing a path from vertex uto vertex v, or reporting that no such path\nexists.\n\u2022Given a start vertex sofG, computing, for every vertex vofG, a path with\nthe minimum number of edges between sand v, or reporting that no such\npath exists.\n\u2022Testing whether Gis connected.\n\u2022Computing a spanning tree of G,i fGis connected.\n\u2022Computing the connected components of G.\n\u2022Computing a cycle in G, or reporting that Ghas no cycles.\nInteresting problems that deal with reachability in a directed graph /vectorGinclude the\nfollowing:\n\u2022Computing a directed path from vertex uto vertex v, or reporting that no such\npath exists.\n\u2022Finding all the vertices of /vectorGthat are reachable from a given vertex s.\n\u2022Determine whether /vectorGis acyclic.\n\u2022Determine whether /vectorGis strongly connected.\nIn the remainder of this section, we present two ef\ufb01cient graph traversal algo-\nrithms, called depth-\ufb01rst search andbreadth-\ufb01rst search , respectively.", "14.3. Graph Traversals 639\n14.3.1 Depth-First Search\nThe \ufb01rst traversal algorithm we consider in this section is depth-\ufb01rst search (DFS).\nDepth-\ufb01rst search is useful for testing a number of properties of graphs, including\nwhether there is a path from one vertex to another and whether or not a graph is\nconnected.\nDepth-\ufb01rst search in a graph Gis analogous to wandering in a labyrinth with\na string and a can of paint without getting lost. We begin at a speci\ufb01c startingvertex sinG, which we initialize by \ufb01xing one end of our string to sand painting\nsas \u201cvisited.\u201d The vertex sis now our \u201ccurrent\u201d vertex\u2014call our current vertex u.\nWe then traverse Gby considering an (arbitrary) edge (u,v)incident to the current\nvertex u. If the edge (u,v)leads us to a vertex vthat is already visited (that is,\npainted), we ignore that edge. If, on the other hand, (u,v)leads to an unvisited\nvertex v, then we unroll our string, and go to v. We then paint vas \u201cvisited,\u201d and\nmake it the current vertex, repeating the computation above. Eventually, we will getto a \u201cdead end,\u201d that is, a current vertex vsuch that all the edges incident to vlead\nto vertices already visited. To get out of this impasse, we roll our string back up,backtracking along the edge that brought us to v, going back to a previously visited\nvertex u.W et h e nm a k e uour current vertex and repeat the computation above for\nany edges incident to uthat we have not yet considered. If all of u\u2019s incident edges\nlead to visited vertices, then we again roll up our string and backtrack to the vertexwe came from to get to u, and repeat the procedure at that vertex. Thus, we continue\nto backtrack along the path that we have traced so far until we \ufb01nd a vertex that has\nyet unexplored edges, take one such edge, and continue the traversal. The process\nterminates when our backtracking leads us back to the start vertex s, and there are\nno more unexplored edges incident to s.\nThe pseudo-code for a depth-\ufb01rst search traversal starting at a vertex u(see\nCode Fragment 14.4) follows our analogy with string and paint. We use recursion\nto implement the string analogy, and we assume that we have a mechanism (the\npaint analogy) to determine whether a vertex or edge has been previously explored.\nAlgorithm DFS(G,u) : {We assume uhas already been marked as visited }\nInput: Ag r a p h Gand\na vertex uofG\nOutput: A collection of vertices reachable from u, with their discovery edges\nforeach outgoing edge e=(u,v)ofudo\nifvertex vhas not been visited then\nMark vertex vas visited (via edge e).\nRecursively call DFS(G,v) .\nCode Fragment 14.4: TheDFS algorithm.", "640 Chapter 14. Graph Algorithms\nClassifying Graph Edges with DFS\nAn execution of depth-\ufb01rst search can be used to analyze the structure of a graph,\nbased upon the way in which edges are explored during the traversal. The DFSprocess naturally identi\ufb01es what is known as the depth-\ufb01rst search tree rooted at\na starting vertex s. Whenever an edge e=(u,v)is used to discover a new vertex v\nduring the DFS algorithm of Code Fragment 14.4, that edge is known as a discovery\nedge ortree edge , as oriented from utov. All other edges that are considered during\nthe execution of DFS are known as nontree edges , which take us to a previously\nvisited vertex. In the case of an undirected graph, we will \ufb01nd that all nontree edges\nthat are explored connect the current vertex to one that is an ancestor of it in the\nDFS tree. We will call such an edge a back edge . When performing a DFS on a\ndirected graph, there are three possible kinds of nontree edges:\n\u2022back edges , which connect a vertex to an ancestor in the DFS tree\n\u2022forward edges , which connect a vertex to a descendant in the DFS tree\n\u2022cross edges , which connect a vertex to a vertex that is neither its ancestor nor\nits descendant.\nAn example application of the DFS algorithm on a directed graph is shown in\nFigure 14.8, demonstrating each type of nontree edge. An example application ofthe DFS algorithm on an undirected graph is shown in Figure 14.9.\nBOS\nJFKORD\nMIASFO\nLAXDFW\n41\n5\n2\n3\n67\nSFO\nMIAJFK\nDFWBOS\nORD\nLAX\n(a) (b)\nFigure 14.8: An example of a DFS in a directed graph, starting at vertex (BOS):\n(a) intermediate step, where, for the \ufb01rst time, a considered edge leads to an al-ready visited vertex (DFW); (b) the completed DFS. The tree edges are shownwith thick lines, the back edges are shown with dashed lines, and the forward and\ncross edges are shown with dotted lines. The order in which the vertices are vis-\nited is indicated by a label next to each vertex. The edge (ORD,DFW) is a backedge, but (DFW,ORD) is a forward edge. Edge (BOS,SFO) is a forward edge, and(SFO,LAX) is a cross edge.", "14.3. Graph Traversals 641\nAC D\nEFG H\nIJ K L\nMNO PB AC D\nEFG H\nIJ K L\nMNO PB\n(a) (b)\nAC D\nEFG H\nIJ K L\nMNO PB AC D\nEFG H\nIJ K L\nMNO PB\n(c) (d)\nAC D\nEFG H\nIJ K L\nMNO PB AC D\nEFG H\nIJ K L\nMNO PB\n(e) (f)\nFigure 14.9: Example of depth-\ufb01rst search traversal on an undirected graph starting\nat vertex A. We assume that a vertex\u2019s adjacencies are considered in alphabetical\norder. Visited vertices and explored edges are highlighted, with discovery edges\ndrawn as solid lines and nontree (back) edges as dashed lines: (a) input graph;(b) path of tree edges, traced from A until back edge (G,C) is examined; (c) reach-\ning F, which is a dead end; (d) after backtracking to I, resuming with edge (I,M),\nand hitting another dead end at O; (e) after backtracking to G, continuing with edge(G,L), and hitting another dead end at H; (f) \ufb01nal result.", "642 Chapter 14. Graph Algorithms\nProperties of a Depth-First Search\nThere are a number of observations that we can make about the depth-\ufb01rst search\nalgorithm, many of which derive from the way the DFS algorithm partitions the\nedges of a graph Ginto groups. We begin with the most signi\ufb01cant property.\nProposition 14.12: Let Gbe an undirected graph on which a DFS traversal start-\ning at a vertex shas been performed. Then the traversal visits all vertices in the\nconnected component of s, and the discovery edges form a spanning tree of the\nconnected component of s.\nJusti\ufb01cation: Suppose there is at least one vertex wins\u2019s connected component\nnot visited, and let vbe the \ufb01rst unvisited vertex on some path from stow(we may\nhave v=w). Since vis the \ufb01rst unvisited vertex on this path, it has a neighbor u\nthat was visited. But when we visited u, we must have considered the edge (u,v);\nhence, it cannot be correct that vis unvisited. Therefore, there are no unvisited\nvertices in s\u2019s connected component.\nSince we only follow a discovery edge when we go to an unvisited vertex, we\nwill never form a cycle with such edges. Therefore, the discovery edges form a\nconnected subgraph without cycles, hence a tree. Moreover, this is a spanningtree because, as we have just seen, the depth-\ufb01rst search visits each vertex in the\nconnected component of s.\nProposition 14.13: Let/vectorGbe a directed graph. Depth-\ufb01rst search on /vectorGstarting at\na vertex svisits all the vertices of /vectorGthat are reachable from s. Also, the DFS tree\ncontains directed paths from sto every vertex reachable from s.\nJusti\ufb01cation: Let Vsbe the subset of vertices of /vectorGvisited by DFS starting at\nvertex s. We want to show that Vscontains sand every vertex reachable from s\nbelongs to Vs. Suppose now, for the sake of a contradiction, that there is a vertex w\nreachable from sthat is not in Vs. Consider a directed path from stow,a n dl e t (u,v)\nbe the \ufb01rst edge on such a path taking us out of Vs,t h a ti s , uis in Vsbut vis not\ninVs. When DFS reaches u, it explores all the outgoing edges of u, and thus must\nreach also vertex vvia edge (u,v). Hence, vshould be in Vs, and we have obtained\na contradiction. Therefore, Vsmust contain every vertex reachable from s.\nWe prove the second fact by induction on the steps of the algorithm. We claim\nthat each time a discovery edge (u,v)is identi\ufb01ed, there exists a directed path from\nstovin the DFS tree. Since umust have previously been discovered, there exists\na path from stou, so by appending the edge (u,v)to that path, we have a directed\npath from stov.\nNote that since back edges always connect a vertex vto a previously visited\nvertex u, each back edge implies a cycle in G, consisting of the discovery edges\nfrom utovplus the back edge (u,v).", "14.3. Graph Traversals 643\nRunning Time of Depth-First Search\nIn terms of its running time, depth-\ufb01rst search is an ef\ufb01cient method for traversing\na graph. Note that DFS is called at most once on each vertex (since it gets marked\nas visited), and therefore every edge is examined at most twice for an undirected\ngraph, once from each of its end vertices, and at most once in a directed graph,from its origin vertex. If we let n\ns\u2264nbe the number of vertices reachable from\na vertex s,a n d ms\u2264mbe the number of incident edges to those vertices, a DFS\nstarting at sruns in O(ns+ms)time, provided the following conditions are satis\ufb01ed:\n\u2022The graph is represented by a data structure such that creating and iterating\nthrough the incident\n edges(v) takes O(deg (v))time, and the e.opposite(v)\nmethod takes O(1)time. The adjacency list structure is one such structure,\nbut the adjacency matrix structure is not.\n\u2022We have a way to \u201cmark\u201d a vertex or edge as explored, and to test if a vertexor edge has been explored in O(1)time. We discuss ways of implementing\nDFS to achieve this goal in the next section.\nGiven the assumptions above, we can solve a number of interesting problems.\nProposition 14.14:\nLet Gbe an undirected graph with nvertices and medges. A\nDFS traversal of Gcan be performed in O(n+m)time, and can be used to solve\nthe following problems in O(n+m)time:\n\u2022Computing a path between two given vertices of G, if one exists.\n\u2022Testing whether Gis connected.\n\u2022Computing a spanning tree of G,i fGis connected.\n\u2022Computing the connected components of G.\n\u2022Computing a cycle in G, or reporting that Ghas no cycles.\nProposition 14.15: Let/vectorGbe a directed graph with nvertices and medges. A\nDFS traversal of /vectorGcan be performed in O(n+m)time, and can be used to solve\nthe following problems in O(n+m)time:\n\u2022Computing a directed path between two given vertices of /vectorG, if one exists.\n\u2022Computing the set of vertices of /vectorGthat are reachable from a given vertex s.\n\u2022Testing whether /vectorGis strongly connected.\n\u2022Computing a directed cycle in /vectorG, or reporting that /vectorGis acyclic.\n\u2022Computing the transitive closure of/vectorG(see Section 14.4).\nThe justi\ufb01cation of Propositions 14.14 and 14.15 is based on algorithms that\nuse slightly modi\ufb01ed versions of the DFS algorithm as subroutines. We will exploresome of those extensions in the remainder of this section.", "644 Chapter 14. Graph Algorithms\n14.3.2 DFS Implementation and Extensions\nWe begin by providing a Python implementation of the basic depth-\ufb01rst search\nalgorithm, originally described with pseudo-code in Code Fragment 14.4. Our DFS\nfunction is presented in Code Fragment 14.5.\n1defDFS(g, u, discovered):\n2\u201d\u201d\u201dPerform DFS of the undiscovered portion of Graph g starting at Vertex u.\n34discovered is a dictionary mapping each vertex to the edge that was used to\n5discover it during the DFS. (u should be \u201ddiscovered\u201d prior to the call.)\n6Newly discovered vertices will be added to the dictionary as a result.\n7\u201d\u201d\u201d\n8foreing.incident\nedges(u): # for every outgoing edge from u\n9 v=e . o p p o s i t e ( u )\n10 ifvnot in discovered: # v is an unvisited vertex\n11 discovered[v] = e # e is the tree edge that discovered v\n12 DFS(g, v, discovered) # recursively explore from v\nCode Fragment 14.5: Recursive implementation of depth-\ufb01rst search on a graph,\nstarting at a designated vertex u.\nIn order to track which vertices have been visited, and to build a representation\nof the resulting DFS tree, our implementation introduces a third parameter, nameddiscovered . This parameter should be a Python dictionary that maps a vertex of the\ngraph to the tree edge that was used to discover that vertex. As a technicality, weassume that the source vertex uoccurs as a key of the dictionary, with None as its\nvalue. Thus, a caller might start the traversal as follows:\nresult = {u:None} # a new dictionary, with u trivially discovered\nDFS(g, u, result)\nThe dictionary serves two purposes. Internally, the dictionary provides a mecha-nism for recognizing visited vertices, as they will appear as keys in the dictionary.Externally, the DFS function augments this dictionary as it proceeds, and thus the\nvalues within the dictionary are the DFS tree edges at the conclusion of the process.\nBecause the dictionary is hash-based, the test, \u201c ifvnot in discovered ,\u201d and\nthe record-keeping step, \u201c discovered[v] = e ,\u201d run in O(1)expected time, rather\nthan worst-case time. In practice, this is a compromise we are willing to accept,but it does violate the formal analysis of the algorithm, as given on page 643. If wecould assume that vertices could be numbered from 0 to n\u22121, then those numbers\ncould be used as indices into an array-based lookup table rather than a hash-basedmap. Alternatively, we could store each vertex\u2019s discovery status and associated\ntree edge directly as part of the vertex instance.", "14.3. Graph Traversals 645\nReconstructing a Path from utov\nWe can use the basic DFS function as a tool to identify the (directed) path lead-\ning from vertex utov,i f vis reachable from u. This path can easily be recon-\nstructed from the information that was recorded in the discovery dictionary during\nthe traversal. Code Fragment 14.6 provides an implementation of a secondary func-tion that produces an ordered list of vertices on the path from utov.\nTo reconstruct the path, we begin at the endof the path, examining the discovery\ndictionary to determine what edge was used to reach vertex v, and then what the\nother endpoint of that edge is. We add that vertex to a list, and then repeat theprocess to determine what edge was used to discover it. Once we have traced thepath all the way back to the starting vertex u, we can reverse the list so that it is\nproperly oriented from utov, and return it to the caller. This process takes time\nproportional to the length of the path, and therefore it runs in O(n)time (in addition\nto the time originally spent calling DFS).\n1defconstruct\npath(u, v, discovered):\n2path = [ ] # empty path by default\n3ifvindiscovered:\n4 # we build list from v to u and then reverse it at the end\n5 path.append(v)\n6 walk = v\n7 while walkis not u:\n8 e = discovered[walk] # \ufb01nd edge leading to walk\n9 parent = e.opposite(walk)\n10 path.append(parent)\n11 walk = parent\n12 path.reverse( ) # reorient path from u to v\n13return path\nCode Fragment 14.6: Function to reconstruct a directed path from utov,g i v e nt h e\ntrace of discovery from a DFS started at u. The function returns an ordered list of\nvertices on the path.\nTesting for Connectivity\nWe can use the basic DFS function to determine whether a graph is connected. In\nthe case of an undirected graph, we simply start a depth-\ufb01rst search at an arbitraryvertex and then test whether len(discovered) equals nat the conclusion. If the graph\nis connected, then by Proposition 14.12, all vertices will have been discovered;conversely, if the graph is not connected, there must be at least one vertex vthat is\nnot reachable from u, and that will not be discovered.", "646 Chapter 14. Graph Algorithms\nFor directed graph, /vectorG, we may wish to test whether it is strongly connected ,t h a t\nis, whether for every pair of vertices uand v, both ureaches vand vreaches u.I fw e\nstart an independent call to DFS from each vertex, we could determine whether this\nwas the case, but those ncalls when combined would run in O(n(n+m)).H o w e v e r ,\nwe can determine if /vectorGis strongly connected much faster than this, requiring only\ntwo depth-\ufb01rst searches.\nWe begin by performing a depth-\ufb01rst search of our directed graph /vectorGstarting at\nan arbitrary vertex s. If there is any vertex of /vectorGthat is not visited by this traversal,\nand is not reachable from s, then the graph is not strongly connected. If this \ufb01rst\ndepth-\ufb01rst search visits each vertex of /vectorG, we need to then check whether sis reach-\nable from all other vertices. Conceptually, we can accomplish this by making a\ncopy of graph /vectorG, but with the orientation of all edges reversed. A depth-\ufb01rst search\nstarting at sin the reversed graph will reach every vertex that could reach sin the\noriginal. In practice, a better approach than making a new graph is to reimplement\na version of the DFS method that loops through all incoming edges to the current\nvertex, rather than all outgoing edges. Since this algorithm makes just two DFS\ntraversals of /vectorG, it runs in O(n+m)time.\nComputing all Connected Components\nWhen a graph is not connected, the next goal we may have is to identify all of the\nconnected components of an undirected graph, or the strongly connected compo-\nnents of a directed graph. We begin by discussing the undirected case.\nIf an initial call to DFS fails to reach all vertices of a graph, we can restart a\nnew call to DFS at one of those unvisited vertices. An implementation of such a\ncomprehensive DFS\nallmethod is given in Code Fragment 14.7.\n1defDFS\ncomplete(g):\n2\u201d\u201d\u201dPerform DFS for entire graph and return forest as a dictionary.\n34Result maps each vertex v to the edge that was used to discover it.\n5(Vertices that are roots of a DFS tree are mapped to None.)\n6\u201d\u201d\u201d\n7forest = {}\n8foruing.vertices():\n9 ifunot in forest:\n10 forest[u] = None # u will be the root of a tree\n11 DFS(g, u, forest)\n12return forest\nCode Fragment 14.7: Top-level function that returns a DFS forest for an entire\ngraph.", "14.3. Graph Traversals 647\nAlthough the DFS\ncomplete function makes multiple calls to the original DFS\nfunction, the total time spent by a call to DFS\ncomplete isO(n+m). For an undi-\nrected graph, recall from our original analysis on page 643 that a single call to\nDFS starting at vertex sruns in time O(ns+ms)where nsis the number of vertices\nreachable from s,a n d msis the number of incident edges to those vertices. Because\neach call to DFS explores a different component, the sum of ns+msterms is n+m.\nThe O(n+m)total bound applies to the directed case as well, even though the sets\nof reachable vertices are not necessarily disjoint. However, because the same dis-covery dictionary is passed as a parameter to all DFS calls, we know that the DFSsubroutine is called once on each vertex, and then each outgoing edge is explored\nonly once during the process.\nTheDFS\ncomplete function can be used to analyze the connected components\nof an undirected graph. The discovery dictionary it returns represents a DFS forest\nfor the entire graph. We say this is a forest rather than a tree, because the graph may\nnot be connected. The number of connected components can be determined by thenumber of vertices in the discovery dictionary that have None as their discovery\nedge (those are roots of DFS trees). A minor modi\ufb01cation to the core DFS methodcould be used to tag each vertex with a component number when it is discovered.\n(See Exercise C-14.44.)\nThe situation is more complex for \ufb01nding strongly connected components of\na directed graph. There exists an approach for computing those components inO(n+m)time, making use of two separate depth-\ufb01rst search traversals, but the\ndetails are beyond the scope of this book.\nDetecting Cycles with DFS\nFor both undirected and directed graphs, a cycle exists if and only if a back edge\nexists relative to the DFS traversal of that graph. It is easy to see that if a back edge\nexists, a cycle exists by taking the back edge from the descendant to its ancestorand then following the tree edges back to the descendant. Conversely, if a cycleexists in the graph, there must be a back edge relative to a DFS (although we do not\nprove this fact here).\nAlgorithmically, detecting a back edge in the undirected case is easy, because\nall edges are either tree edges or back edges. In the case of a directed graph, ad-ditional modi\ufb01cations to the core DFS implementation are needed to properly cat-\negorize a nontree edge as a back edge. When a directed edge is explored leadingto a previously visited vertex, we must recognize whether that vertex is an ancestorof the current vertex. This requires some additional bookkeeping, for example, bytagging vertices upon which a recursive call to DFS is still active. We leave details\nas an exercise (C-14.43).", "648 Chapter 14. Graph Algorithms\n14.3.3 Breadth-First Search\nThe advancing and backtracking of a depth-\ufb01rst search, as described in the previ-\nous section, de\ufb01nes a traversal that could be physically traced by a single personexploring a graph. In this section, we consider another algorithm for traversinga connected component of a graph, known as a breadth-\ufb01rst search (BFS). The\nBFS algorithm is more akin to sending out, in all directions, many explorers who\ncollectively traverse a graph in coordinated fashion.\nA BFS proceeds in rounds and subdivides the vertices into levels . BFS starts\nat vertex s, which is at level 0. In the \ufb01rst round, we paint as \u201cvisited,\u201d all vertices\nadjacent to the start vertex s\u2014these vertices are one step away from the beginning\nand are placed into level 1. In the second round, we allow all explorers to go\ntwo steps (i.e., edges) away from the starting vertex. These new vertices, whichare adjacent to level 1 vertices and not previously assigned to a level, are placed\ninto level 2 and marked as \u201cvisited.\u201d This process continues in similar fashion,\nterminating when no new vertices are found in a level.\nA Python implementation of BFS is given in Code Fragment 14.8. We follow\na convention similar to that of DFS (Code Fragment 14.5), using a discovered dic-\ntionary both to recognize visited vertices, and to record the discovery edges of theBFS tree. We illustrate a BFS traversal in Figure 14.10.\n1defBFS(g, s, discovered):\n2\u201d\u201d\u201dPerform BFS of the undiscovered portion of Graph g starting at Vertex s.\n34discovered is a dictionary mapping each vertex to the edge that was used to\n5discover it during the BFS (s should be mapped to None prior to the call).\n6Newly discovered vertices will be added to the dictionary as a result.\n7\u201d\u201d\u201d\n8level = [s] # \ufb01rst level includes only s\n9while len(level) >0:\n10 next\nlevel = [ ] # prepare to gather newly found vertices\n11 foruinlevel:\n12 foreing.incident\n edges(u): # for every outgoing edge from u\n13 v=e . o p p o s i t e ( u )\n14 ifvnot in discovered: # v is an unvisited vertex\n15 discovered[v] = e # e is the tree edge that discovered v\n16 next\nlevel.append(v) # v will be further considered in next pass\n17 level = next\n level # relabel \u2019next\u2019 level to become current\nCode Fragment 14.8: Implementation of breadth-\ufb01rst search on a graph, starting at\na designated vertex s.", "14.3. Graph Traversals 649\nFH\nIJ K L\nMNO PABC\nED\nG0\nB\nIJ K L\nMNO PCD\nG F EA\nH01\n(a) (b)\nA\nJKL\nMNO PBC\nED\nH G F\nI012\nKL\nMNO PIH G FABCD\nE\nJ23 01\n(c) (d)\nFGH\nIJ K L\nMNO PABCD\nE\n4123 0\nFGH\nIJ K L\nMNO PABCD\nE\n4123 0\n5\n(e) (f)\nFigure 14.10: Example of breadth-\ufb01rst search traversal, where the edges incident to\na vertex are considered in alphabetical order of the adjacent vertices. The discovery\nedges are shown with solid lines and the nontree (cross) edges are shown withdashed lines: (a) starting the search at A; (b) discovery of level 1; (c) discovery oflevel 2; (d) discovery of level 3; (e) discovery of level 4; (f) discovery of level 5.", "650 Chapter 14. Graph Algorithms\nWhen discussing DFS, we described a classi\ufb01cation of nontree edges being\neither back edges , which connect a vertex to one of its ancestors, forward edges ,\nwhich connect a vertex to one of its descendants, or cross edges , which connect a\nvertex to another vertex that is neither its ancestor nor its descendant. For BFS on\nan undirected graph, all nontree edges are cross edges (see Exercise C-14.47), andfor BFS on a directed graph, all nontree edges are either back edges or cross edges\n(see Exercise C-14.48).\nThe BFS traversal algorithm has a number of interesting properties, some of\nwhich we explore in the proposition that follows. Most notably, a path in a breadth-\n\ufb01rst search tree rooted at vertex sto any other vertex vis guaranteed to be the\nshortest such path from stovin terms of the number of edges.\nProposition 14.16:\nLet Gbe an undirected or directed graph on which a BFS\ntraversal starting at vertex shas been performed. Then\n\u2022The traversal visits all vertices of Gthat are reachable from s.\n\u2022For each vertex vat level i, the path of the BFS tree Tbetween sand vhas i\nedges, and any other path of Gfrom stovhas at least iedges.\n\u2022If(u,v)is an edge that is not in the BFS tree, then the level number of vcan\nbe at most 1greater than the level number of u.\nWe leave the justi\ufb01cation of this proposition as an exercise (C-14.50).\nThe analysis of the running time of BFS is similar to the one of DFS, with\nthe algorithm running in O(n+m)time, or more speci\ufb01cally, in O(ns+ms)time\nifnsis the number of vertices reachable from vertex s,a n d ms\u2264mis the number\nof incident edges to those vertices. To explore the entire graph, the process can\nbe restarted at another vertex, akin to the DFS\ncomplete function of Code Frag-\nment 14.7. Also, the actual path from vertex sto vertex vcan be reconstructed\nusing the construct\n path function of Code Fragment 14.6\nProposition 14.17: Let Gbe a graph with nvertices and medges represented\nwith the adjacency list structure. A BFS traversal of Gtakes O(n+m)time.\nAlthough our implementation of BFS in Code Fragment 14.8 progresses level\nby level, the BFS algorithm can also be implemented using a single FIFO queueto represent the current fringe of the search. Starting with the source vertex in the\nqueue, we repeatedly remove the vertex from the front of the queue and insert anyof its unvisited neighbors to the back of the queue. (See Exercise C-14.51.)\nIn comparing the capabilities of DFS and BFS, both can be used to ef\ufb01ciently\n\ufb01nd the set of vertices that are reachable from a given source, and to determine pathsto those vertices. However, BFS guarantees that those paths use as few edges aspossible. For an undirected graph, both algorithms can be used to test connectivity,to identify connected components, or to locate a cycle. For directed graphs, theDFS algorithm is better suited for certain tasks, such as \ufb01nding a directed cycle in\nthe graph, or in identifying the strongly connected components.", "14.4. Transitive Closure 651\n14.4 Transitive Closure\nWe have seen that graph traversals can be used to answer basic questions of reach-\nability in a directed graph. In particular, if we are interested in knowing whether\nthere is a path from vertex uto vertex vin a graph, we can perform a DFS or BFS\ntraversal starting at uand observe whether vis discovered. If representing a graph\nwith an adjacency list or adjacency map, we can answer the question of reachability\nforuand vinO(n+m)time (see Propositions 14.15 and 14.17).\nIn certain applications, we may wish to answer many reachability queries more\nef\ufb01ciently, in which case it may be worthwhile to precompute a more convenientrepresentation of a graph. For example, the \ufb01rst step for a service that computes\ndriving directions from an origin to a destination might be to assess whether the\ndestination is reachable. Similarly, in an electricity network, we may wish to beable to quickly determine whether current \ufb02ows from one particular vertex to an-other. Motivated by such applications, we introduce the following de\ufb01nition. The\ntransitive closure of a directed graph /vectorGis itself a directed graph /vectorG\n\u2217such that the\nvertices of /vectorG\u2217are the same as the vertices of /vectorG,a n d/vectorG\u2217has an edge (u,v), when-\never/vectorGhas a directed path from utov(including the case where (u,v)is an edge of\nthe original /vectorG).\nIf a graph is represented as an adjacency list or adjacency map, we can compute\nits transitive closure in O(n(n+m))time by making use of ngraph traversals, one\nfrom each starting vertex. For example, a DFS starting at vertex ucan be used to\ndetermine all vertices reachable from u, and thus a collection of edges originating\nwith uin the transitive closure.\nIn the remainder of this section, we explore an alternative technique for comput-\ning the transitive closure of a directed graph that is particularly well suited for when\na directed graph is represented by a data structure that supports O(1)-time lookup\nfor the get\nedge(u,v) method (for example, the adjacency-matrix structure). Let /vectorG\nbe a directed graph with nvertices and medges. We compute the transitive closure\nof/vectorGin a series of rounds. We initialize /vectorG0=/vectorG. We also arbitrarily number the\nvertices of /vectorGasv1,v2,..., vn. We then begin the computation of the rounds, begin-\nning with round 1. In a generic round k, we construct directed graph /vectorGkstarting\nwith/vectorGk=/vectorGk\u22121and adding to /vectorGkthe directed edge (vi,vj)if directed graph /vectorGk\u22121\ncontains both the edges (vi,vk)and (vk,vj). In this way, we will enforce a simple\nrule embodied in the proposition that follows.\nProposition 14.18: For i=1,..., n, directed graph /vectorGkhas an edge (vi,vj)if and\nonly if directed graph /vectorGhas a directed path from vitovj, whose intermediate\nvertices (if any) are in the set {v1,..., vk}. In particular, /vectorGnis equal to /vectorG\u2217,t h e\ntransitive closure of /vectorG.", "652 Chapter 14. Graph Algorithms\nProposition 14.18 suggests a simple algorithm for computing the transitive clo-\nsure of /vectorGthat is based on the series of rounds to compute each /vectorGk. This algorithm\nis known as the Floyd-Warshall algorithm , and its pseudo-code is given in Code\nFragment 14.9. We illustrate an example run of the Floyd-Warshall algorithm in\nFigure 14.11.\nAlgorithm FloydWarshall( /vectorG):\nInput: A directed graph /vectorGwith nvertices\nOutput: The transitive closure /vectorG\u2217of/vectorG\nletv1,v2,..., vnbe an arbitrary numbering of the vertices of /vectorG\n/vectorG0=/vectorG\nfor k=1t o ndo\n/vectorGk=/vectorGk\u22121\nfor all i,jin{1,..., n}with i/negationslash=jand i,j/negationslash=kdo\nifboth edges (vi,vk)and (vk,vj)are in /vectorGk\u22121then\nadd edge (vi,vj)to/vectorGk(if it is not already present)\nreturn /vectorGn\nCode Fragment 14.9: Pseudo-code for the Floyd-Warshall algorithm. This algo-\nrithm computes the transitive closure /vectorG\u2217ofGby incrementally computing a series\nof directed graphs /vectorG0,/vectorG1,...,/vectorGn,f o r k=1,..., n.\nFrom this pseudo-code, we can easily analyze the running time of the Floyd-\nWarshall algorithm assuming that the data structure representing Gsupports meth-\nodsget\nedge andinsert\n edge inO(1)time. The main loop is executed ntimes and\nthe inner loop considers each of O(n2)pairs of vertices, performing a constant-time\ncomputation for each one. Thus, the total running time of the Floyd-Warshall al-\ngorithm is O(n3). From the description and analysis above we may immediately\nderive the following proposition.\nProposition 14.19: Let/vectorGbe a directed graph with nvertices, and let /vectorGbe repre-\nsented by a data structure that supports lookup and update of adjacency information\ninO(1)time. Then the Floyd-Warshall algorithm computes the transitive closure\n/vectorG\u2217of/vectorGinO(n3)time.\nPerformance of the Floyd-Warshall Algorithm\nAsymptotically, the O(n3)running time of the Floyd-Warshall algorithm is no bet-\nter than that achieved by repeatedly running DFS, once from each vertex, to com-\npute the reachability. However, the Floyd-Warshall algorithm matches the asymp-totic bounds of the repeated DFS when a graph is dense, or when a graph is sparse\nbut represented as an adjacency matrix. (See Exercise R-14.12.)", "14.4. Transitive Closure 653\nv7\nv2v6v4\nv1\nv5v3DFW\nMIASFOORD\nJFKBOS\nLAXv7\nv2v6v4\nv1\nv5v3\nMIASFOORD\nJFK\nLAXBOS\nDFW\n(a) (b)\nv7\nv2v6v4\nv1\nv5v3LAX\nMIASFOORD\nJFKBOS\nDFWv7\nv2v6v4\nv5v3\nv1JFK\nLAXDFWBOS\nSFOORD\nMIA\n(c) (d)\nv4\nv1\nv5v7\nv3v2v6\nSFOORD\nMIALAXBOS\nJFK\nDFWv4\nv1\nv5v7\nv3v2v6\nLAXBOS\nJFK\nDFWSFOORD\nMIA\n(e) (f)\nFigure 14.11: Sequence of directed graphs computed by the Floyd-Warshall algo-\nrithm: (a) initial directed graph /vectorG=/vectorG0and numbering of the vertices; (b) directed\ngraph /vectorG1;( c )/vectorG2;( d )/vectorG3;( e )/vectorG4;( f )/vectorG5. Note that /vectorG5=/vectorG6=/vectorG7. If directed\ngraph /vectorGk\u22121has the edges (vi,vk)and (vk,vj), but not the edge (vi,vj), in the draw-\ning of directed graph /vectorGk, we show edges (vi,vk)and (vk,vj)with dashed lines, and\nedge (vi,vj)with a thick line. For example, in (b) existing edges (MIA,LAX) and\n(LAX,ORD) result in new edge (MIA,ORD).", "654 Chapter 14. Graph Algorithms\nThe importance of the Floyd-Warshall algorithm is that it is much easier to\nimplement than DFS, and much faster in practice because there are relatively few\nlow-level operations hidden within the asymptotic notation. The algorithm is par-ticularly well suited for the use of an adjacency matrix, as a single bit can be usedto designate the reachability modeled as an edge (u,v)in the transitive closure.\nHowever, note that repeated calls to DFS results in better asymptotic perfor-\nmance when the graph is sparse and represented using an adjacency list or adja-cency map. In that case, a single DFS runs in O(n+m)time, and so the transitive\nclosure can be computed in O(n\n2+nm)time, which is preferable to O(n3).\nPython Implementation\nWe conclude with a Python implementation of the Floyd-Warshall algorithm, aspresented in Code Fragment 14.10. Although the original algorithm is describedusing a series of directed graphs /vectorG\n0,/vectorG1,...,/vectorGn, we create a single copy of the\noriginal graph (using the deepcopy method of Python\u2019s copy module) and then re-\npeatedly add new edges to the closure as we progress through rounds of the Floyd-Warshall algorithm.\nThe algorithm requires a canonical numbering of the graph\u2019s vertices; therefore,\nwe create a list of the vertices in the closure graph, and subsequently index that list\nfor our order. Within the outermost loop, we must consider all pairs iand j. Finally,\nwe optimize by only iterating through all values of jafter we have veri\ufb01ed that i\nhas been chosen such that (v\ni,vk)exists in the current version of our closure.\n1def\ufb02oyd\nwarshall(g):\n2\u201d\u201d\u201dReturn a new graph that is the transitive closure of g.\u201d\u201d\u201d\n3closure = deepcopy(g) # imported from copy module\n4verts = list(closure.vertices()) # make indexable list\n5n=l e n ( v e r t s )\n6forkinrange(n):\n7 foriinrange(n):\n8 # verify that edge (i,k) exists in the partial closure\n9 ifi! =k andclosure.get\n edge(verts[i],verts[k]) is not None :\n10 forjinrange(n):\n11 # verify that edge (k,j) exists in the partial closure\n12 ifi! =j! =k andclosure.get\n edge(verts[k],verts[j]) is not None :\n13 # if (i,j) not yet included, add it to the closure\n14 ifclosure.get\n edge(verts[i],verts[j]) is None :\n15 closure.insert\n edge(verts[i],verts[j])\n16return closure\nCode Fragment 14.10: Python implementation of the Floyd-Warshall algorithm.", "14.5. Directed Acyclic Graphs 655\n14.5 Directed Acyclic Graphs\nDirected graphs without directed cycles are encountered in many applications.\nSuch a directed graph is often referred to as a directed acyclic graph ,o rDAG ,\nfor short. Applications of such graphs include the following:\n\u2022Prerequisites between courses of a degree program.\n\u2022Inheritance between classes of an object-oriented program.\n\u2022Scheduling constraints between the tasks of a project.\nWe explore this latter application further in the following example:\nExample 14.20: In order to manage a large project, it is convenient to break it up\ninto a collection of smaller tasks. The tasks, however, are rarely independent, be-\ncause scheduling constraints exist between them. (For example, in a house building\nproject, the task of ordering nails obviously precedes the task of nailing shingles\nto the roof deck.) Clearly, scheduling constraints cannot have circularities, because\nthey would make the project impossible. (For example, in order to get a job you\nneed to have work experience, but in order to get work experience you need to have\na job.) The scheduling constraints impose restrictions on the order in which the\ntasks can be executed. Namely, if a constraint says that task amust be completed\nbefore task bis started, then amust precede bin the order of execution of the tasks.\nThus, if we model a feasible set of tasks as vertices of a directed graph, and we\nplace a directed edge from utovwhenever the task for umust be executed before\nthe task for v, then we de\ufb01ne a directed acyclic graph.\n14.5.1 Topological Ordering\nThe example above motivates the following de\ufb01nition. Let /vectorGbe a directed graph\nwith nvertices. A topological ordering of/vectorGis an ordering v1,..., vnof the vertices\nof/vectorGsuch that for every edge (vi,vj)of/vectorG, it is the case that i<j. That is, a topo-\nlogical ordering is an ordering such that any directed path in /vectorGtraverses vertices in\nincreasing order. Note that a directed graph may have more than one topological\nordering. (See Figure 14.12.)\nProposition 14.21: /vectorGhas a topological ordering if and only if it is acyclic.\nJusti\ufb01cation: The necessity (the \u201conly if\u201d part of the statement) is easy to\ndemonstrate. Suppose /vectorGis topologically ordered. Assume, for the sake of a con-\ntradiction, that /vectorGhas a cycle consisting of edges (vi0,vi1),(vi1,vi2),..., (vik\u22121,vi0).\nBecause of the topological ordering, we must have i0<i1<\u00b7\u00b7\u00b7<ik\u22121<i0,w h i c h\nis clearly impossible. Thus, /vectorGmust be acyclic.", "656 Chapter 14. Graph Algorithms\n1\n832\n7654\nFC\nD\nGB\nHEA2\n863\n7541\nFC\nD\nGB\nHEA\n(a) (b)\nFigure 14.12: Two topological orderings of the same acyclic directed graph.\nWe now argue the suf\ufb01ciency of the condition (the \u201cif\u201d part). Suppose /vectorGis\nacyclic. We will give an algorithmic description of how to build a topological\nordering for /vectorG.S i n c e /vectorGis acyclic, /vectorGmust have a vertex with no incoming edges\n(that is, with in-degree 0). Let v1be such a vertex. Indeed, if v1did not exist,\nthen in tracing a directed path from an arbitrary start vertex, we would eventuallyencounter a previously visited vertex, thus contradicting the acyclicity of /vectorG.I fw e\nremove v\n1from/vectorG, together with its outgoing edges, the resulting directed graph is\nstill acyclic. Hence, the resulting directed graph also has a vertex with no incomingedges, and we let v\n2be such a vertex. By repeating this process until the directed\ngraph becomes empty, we obtain an ordering v1,..., vnof the vertices of /vectorG. Because\nof the construction above, if (vi,vj)is an edge of /vectorG,t h e n vimust be deleted before\nvjcan be deleted, and thus, i<j. Therefore, v1,..., vnis a topological ordering.\nProposition 14.21\u2019s justi\ufb01cation suggests an algorithm for computing a topo-\nlogical ordering of a directed graph, which we call topological sorting . We present\na Python implementation of the technique in Code Fragment 14.11, and an exampleexecution of the algorithm in Figure 14.13. Our implementation uses a dictionary,\nnamed incount , to map each vertex vto a counter that represents the current number\nof incoming edges to v, excluding those coming from vertices that have previously\nbeen added to the topological order. Technically, a Python dictionary provides O(1)\nexpected time access to entries, rather than worst-case time; as was the case with\nour graph traversals, this could be converted to worst-case time if vertices could be\nindexed from 0 to n\u22121, or if we store the counter as an element of a vertex.\nAs a side effect, the topological sorting algorithm of Code Fragment 14.11\nalso tests whether the given directed graph /vectorGis acyclic. Indeed, if the algorithm\nterminates without ordering all the vertices, then the subgraph of the vertices thathave not been ordered must contain a directed cycle.", "14.5. Directed Acyclic Graphs 657\n1deftopological\n sort(g):\n2\u201d\u201d\u201dReturn a list of verticies of directed acyclic graph g in topological order.\n3\n4If graph g has a cycle, the result will be incomplete.\n5\u201d\u201d\u201d\n6topo = [ ] # a list of vertices placed in topological order\n7ready = [ ] # list of vertices that have no remaining constraints\n8incount = {} # keep track of in-degree for each vertex\n9foruing.vertices():\n10 incount[u] = g.degree(u, False)# parameter requests incoming degree\n11 ifincount[u] == 0: # if u has no incoming edges,\n12 ready.append(u) # it is free of constraints\n13while len(ready) >0:\n14 u = ready.pop( ) # u is free of constraints\n15 topo.append(u) # add u to the topological order\n16 foreing.incident\n edges(u): # consider all outgoing neighbors of u\n17 v=e . o p p o s i t e ( u )\n18 incount[v] \u2212=1 # v has one less constraint without u\n19 ifincount[v] == 0:\n20 ready.append(v)\n21return topo\nCode Fragment 14.11: Python implementation for the topological sorting algorithm.\n(We show an example execution of this algorithm in Figure 14.13.)\nPerformance of Topological Sorting\nProposition 14.22: Let/vectorGbe a directed graph with nvertices and medges, using\nan adjacency list representation. The topological sorting algorithm runs in O(n+m)\ntime using O(n)auxiliary space, and either computes a topological ordering of /vectorG\nor fails to include some vertices, which indicates that /vectorGhas a directed cycle.\nJusti\ufb01cation: The initial recording of the nin-degrees uses O(n)time based\non the degree method. Say that a vertex uisvisited by the topological sorting al-\ngorithm when uis removed from the ready list. A vertex ucan be visited only\nwhenincount(u) is 0, which implies that all its predecessors (vertices with outgo-\ning edges into u) were previously visited. As a consequence, any vertex that is on\na directed cycle will never be visited, and any other vertex will be visited exactlyonce. The algorithm traverses all the outgoing edges of each visited vertex once, so\nits running time is proportional to the number of outgoing edges of the visited ver-tices. In accordance with Proposition 14.9, the running time is (n+m). Regarding\nthe space usage, observe that containers topo ,ready ,a n dincount have at most one\nentry per vertex, and therefore use O(n)space.\n", "658 Chapter 14. Graph Algorithms\n10\n323\n10\n2\nGC\nHD\nFB\nEA\n00\n321 21\n2 E FCAB\nD\nG\nH0\n220 221\n1 E FCAB\nD\nG\nH\n(a) (b) (c)\n0 1\n213 22\n1 E FCAB\nD\nG\nH 213 104\n21\nFC\nDAB\nG\nHE\n213 04\n2\n51\nCB\nD\nFA\nG\nHE\n(d) (e) (f)\n1034\n61\n2\n5 E F\nHGDCAB\n7\n03 62\n514\nC\nG\nHB A\nD\nF E\n84\n73 61\n2\n5 E F\nHGDCAB\n(g) (h) (i)\nFigure 14.13: Example of a run of algorithm topological\n sort (Code Frag-\nment 14.11). The label near a vertex shows its current incount value, and its\neventual rank in the resulting topological order. The highlighted vertex is one\nwithincount equal to zero that will become the next vertex in the topological or-\nder. Dashed lines denote edges that have already been examined and which are nolonger re\ufb02ected in the incount values.", "14.6. Shortest Paths 659\n14.6 Shortest Paths\nAs we saw in Section 14.3.3, the breadth-\ufb01rst search strategy can be used to \ufb01nd a\nshortest path from some starting vertex to every other vertex in a connected graph.This approach makes sense in cases where each edge is as good as any other, but\nthere are many situations where this approach is not appropriate.\nFor example, we might want to use a graph to represent the roads between\ncities, and we might be interested in \ufb01nding the fastest way to travel cross-country.\nIn this case, it is probably not appropriate for all the edges to be equal to each other,\nfor some inter-city distances will likely be much larger than others. Likewise, we\nmight be using a graph to represent a computer network (such as the Internet), andwe might be interested in \ufb01nding the fastest way to route a data packet betweentwo computers. In this case, it again may not be appropriate for all the edges tobe equal to each other, for some connections in a computer network are typically\nmuch faster than others (for example, some edges might represent low-bandwidth\nconnections, while others might represent high-speed, \ufb01ber-optic connections). Itis natural, therefore, to consider graphs whose edges are not weighted equally.\n14.6.1 Weighted Graphs\nAweighted graph is a graph that has a numeric (for example, integer) label w(e)\nassociated with each edge e, called the weight of edge e.F o r e=( u,v),w el e t\nnotation w(u,v)= w(e). We show an example of a weighted graph in Figure 14.14.\nBOS\nJFK\nMIAORD\nDFWSFO\nLAX2704\n1846867\n740\n125810908021464\n337\n23421235\n1121187\nFigure 14.14: A weighted graph whose vertices represent major U.S. airports and\nwhose edge weights represent distances in miles. This graph has a path from JFK toLAX of total weight 2,777 (going through ORD and DFW). This is the minimum-\nweight path in the graph from JFK to LAX.", "660 Chapter 14. Graph Algorithms\nDe\ufb01ning Shortest Paths in a Weighted Graph\nLet Gbe a weighted graph. The length (or weight) of a path is the sum of the\nweights of the edges of P.T h a ti s ,i f P=( ( v0,v1),(v1,v2),..., (vk\u22121,vk)), then the\nlength of P, denoted w(P),i sd e \ufb01 n e da s\nw(P)=k\u22121\n\u2211\ni=0w(vi,vi+1).\nThedistance from a vertex uto a vertex vinG, denoted d(u,v), is the length of a\nminimum-length path (also called shortest path ) from utov, if such a path exists.\nPeople often use the convention that d(u,v)=\u221eif there is no path at all from\nutovinG. Even if there is a path from utovinG, however, if there is a cycle\ninGwhose total weight is negative, the distance from utovmay not be de\ufb01ned.\nFor example, suppose vertices in Grepresent cities, and the weights of edges in\nGrepresent how much money it costs to go from one city to another. If someone\nwere willing to actually pay us to go from say JFK to ORD, then the \u201ccost\u201d of the\nedge (JFK,ORD) would be negative. If someone else were willing to pay us to gofrom ORD to JFK, then there would be a negative-weight cycle in Gand distances\nwould no longer be de\ufb01ned. That is, anyone could now build a path (with cycles)inGfrom any city Ato another city Bthat \ufb01rst goes to JFK and then cycles as\nmany times as he or she likes from JFK to ORD and back, before going on to B.\nThe existence of such paths would allow us to build arbitrarily low negative-costpaths (and, in this case, make a fortune in the process). But distances cannot bearbitrarily low negative numbers. Thus, any time we use edge weights to representdistances, we must be careful not to introduce any negative-weight cycles.\nSuppose we are given a weighted graph G, and we are asked to \ufb01nd a shortest\npath from some vertex sto each other vertex in G, viewing the weights on the edges\nas distances. In this section, we explore ef\ufb01cient ways of \ufb01nding all such shortestpaths, if they exist. The \ufb01rst algorithm we discuss is for the simple, yet common,case when all the edge weights in Gare nonnegative (that is, w(e)\u22650 for each edge\neofG); hence, we know in advance that there are no negative-weight cycles in G.\nRecall that the special case of computing a shortest path when all weights are equal\nto one was solved with the BFS traversal algorithm presented in Section 14.3.3.\nThere is an interesting approach for solving this single-source problem based\non\nthegreedy method design pattern (Section 13.4.2). Recall that in this pattern we\nsolve the problem at hand by repeatedly selecting the best choice from among those\navailable in each iteration. This paradigm can often be used in situations where weare trying to optimize some cost function over a collection of objects. We can addobjects to our collection, one at a time, always picking the next one that optimizes\nthe function from among those yet to be chosen.", "14.6. Shortest Paths 661\n14.6.2 Dijkstra\u2019s Algorithm\nThe main idea in applying the greedy method pattern to the single-source shortest-\npath problem is to perform a \u201cweighted\u201d breadth-\ufb01rst search starting at the sourcevertex s. In particular, we can use the greedy method to develop an algorithm that\niteratively grows a \u201ccloud\u201d of vertices out of s, with the vertices entering the cloud\nin order of their distances from s. Thus, in each iteration, the next vertex chosen\nis the vertex outside the cloud that is closest to s. The algorithm terminates when\nno more vertices are outside the cloud (or when those outside the cloud are not\nconnected to those within the cloud), at which point we have a shortest path from\nsto every vertex of Gthat is reachable from s. This approach is a simple, but\nnevertheless powerful, example of the greedy method design pattern. Applying thegreedy method to the single-source, shortest-path problem, results in an algorithmknown as Dijkstra\u2019s algorithm .\nEdge Relaxation\nLet us de\ufb01ne a label D[v]for each vertex vinV, which we use to approximate the\ndistance in Gfrom stov. The meaning of these labels is that D[v]will always store\nthe length of the best path we have found so far from stov. Initially, D[s]=0a n d\nD[v]=\u221efor each v/negationslash=s, and we de\ufb01ne the set C, which is our \u201c cloud \u201d of vertices,\nto initially be the empty set. At each iteration of the algorithm, we select a vertex\nunot in Cwith smallest D[u]label, and we pull uinto C. (In general, we will use\na priority queue to select among the vertices outside the cloud.) In the very \ufb01rst\niteration we will, of course, pull sinto C. Once a new vertex uis pulled into C,w e\nthen update the label D[v]of each vertex vthat is adjacent to uand is outside of\nC, to re\ufb02ect the fact that there may be a new and better way to get to vvia u.T h i s\nupdate operation is known as a relaxation procedure, for it takes an old estimate\nand checks if it can be improved to get closer to its true value. The speci\ufb01c edgerelaxation operation is as follows:\nEdge Relaxation :\nifD[u]+w(u,v)<D[v]th\n en\nD[v]= D[u]+w(u,v)\nAlgorithm Description and Example\nWe give the pseudo-code for Dijkstra\u2019s algorithm in Code Fragment 14.12, and\nillustrate several iterations of Dijkstra\u2019s algorithm in Figures 14.15 through 14.17.", "662 Chapter 14. Graph Algorithms\nAlgorithm ShortestPath( G,s):\nInput: A weighted graph Gwith nonnegative edge weights, and a distinguished\nvertex sofG.\nOutput: The length of a shortest path from stovfor each vertex vofG.\nInitialize D[s]=0a n d D[v]=\u221efor each vertex v/negationslash=s.\nLet a priority queue Qcontain all the vertices of Gusing the Dlabels as keys.\nwhile Qis not empty do\n{pull a new vertex uinto the cloud }\nu=value returned by Q.remove\n min ()\nforeach vertex vadjacent to usuch that vis in Qdo\n{perform the relaxation procedure on edge (u,v)}\nifD[u]+w(u,v)<D[v]then\nD[v]= D[u]+w(u,v)\nChange to D[v]the key of vertex vinQ.\nreturn the label D[v]of each vertex v\nCode Fragment 14.12: Pseudo-code for Dijkstra\u2019s algorithm, solving the single-\nsource shortest-path problem.\n3371846187849\n1258\n1090867\n144\n9466212704\n184\n23421235740\n1391\n1121PVD\n1464802\nBWI\nDFW\nLAXORD\nMIASFOBOS\nJFK\u221e\u221e\n\u221e\u221e\n0\u221e\n\u221e\n\u221e\u221e1464621740\n1391\n11219461842704\n23421235802\n3371846187849 PVD\n1258\n1090867\n144\nDFWJFK\nMIAORD\nBWI\nLAXBOS\nSFO\u221e\n946621\n184\n0\n\u221e\u221e\n\u221e\n\u221e\n(a) (b)\nFigure 14.15: An execution of Dijkstra\u2019s algorithm on a weighted graph. The start\nvertex is BWI. A box next to each vertex vstores the label D[v]. The edges of\nthe shortest-path tree are drawn as thick arrows, and for each vertex uoutside the\n\u201ccloud\u201d we show the current best edge for pulling in uwith a thick line. (Continues\nin Figure 14.16.)", "14.6. Shortest Paths 663\n3371846187\n9461842704\n2342PVD\n1235802849\n1258\n1090867\n144\n621740\n1391\n11211464\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n946371\n184621\n\u221e\n1575328\n0\n\u221e802\n3371846144\n9461842704\n23421235187849\n1258\n1090867\n621740\n1391\n11211464PVD\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n9461575184621\n\u221e\n0371\n328\n\u221e\n(c) (d)\n144\n9461842704\n23421235802\n3371846187849\n1258\n1090867\n621740\n1391\n1121PVD\n1464\nDFW\nLAXORD\nMIASFOBOS\nJFK\nBWI\n\u221e328\n3075\n946371\n1575184\n0621\n144PVD\n1464\n11211391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946SFO\nLAXORD\nMIABOS\nJFK\nBWI\nDFW0621328\n1423371\n2467\n\u221e184\n946\n(e) (f)\n23421235802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n11211464144\n9462704\n184\nDFW\nLAXORDBOS\nJFK\nBWISFO\nMIA\n946371\n2467621\n1423\n32880328\n184\n802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\n1235\n2342\nLAXORDBOS\nJFK\nBWISFO\nMIADFW\n946371\n1423621\n2467\n26580328\n184\n(g) (h)\nFigure 14.16: An example execution of Dijkstra\u2019s algorithm. (Continued from Fig-\nure 14.15; continued in Figure 14.17.)", "664 Chapter 14. Graph Algorithms\n23421235802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\nLAXORDBOS\nJFK\nBWI\nMIADFWSFO\n946371\n2658621\n14232467\n0328\n184\n802\n3371846187849\n1258\n1090867\n621PVD\n740\n1391\n1121144\n94614641842704\n23421235ORDBOS\nJFK\nBWI\nMIADFWSFO\nLAX\n946371\n2467621\n1423\n26580328\n184\n(i) (j)\nFigure 14.17: An example execution of Dijkstra\u2019s algorithm. (Continued from Fig-\nure 14.16.)\nWhy It Works\nThe interesting aspect of the Dijkstra algorithm is that, at the moment a vertex u\nis pulled into C, its label D[u]stores the correct length of a shortest path from v\ntou. Thus, when the algorithm terminates, it will have computed the shortest-path\ndistance from sto every vertex of G. That is, it will have solved the single-source\nshortest-path problem.\nIt is probably not immediately clear why Dijkstra\u2019s algorithm correctly \ufb01nds the\nshortest path from the start vertex sto each other vertex uin the graph. Why is it\nthat the distance from stouis equal to the value of the label D[u]at the time vertex\nuis removed from the priority queue Qand added to the cloud C? The answer\nto this question depends on there being no negative-weight edges in the graph, for\nit allows the greedy method to work correctly, as we show in the proposition that\nfollows.\nProposition 14.23: In Dijkstra\u2019s algorithm, whenever a vertex vis pulled into the\ncloud, the label D[v]is equal to d(s,v), the length of a shortest path from stov.\nJusti\ufb01cation: Suppose that D[v]>d(s,v)for some vertex vinV,a n dl e t z\nbe the \ufb01rst vertex the algorithm pulled into the cloud C(that is, removed from\nQ) such that D[z]>d(s,z). There is a shortest path Pfrom stoz(for otherwise\nd(s,z)=\u221e=D[z]). Let us therefore consider the moment when zis pulled into\nC,a n dl e t ybe the \ufb01rst vertex of P(when going from stoz) that is not in Cat this\nmoment. Let xbe the predecessor of yin path P(note that we could have x=s).\n(See Figure 14.18.) We know, by our choice of y,t h a t xis already in Cat this point.", "14.6. Shortest Paths 665\nthe \ufb01rst \u201cwrong\u201d vertex picked\nzpicked implies\nthat D[z]\u2264D[y]\nPD[z]>d(s,z)C\nD[y]= d(s,y)yz\ns\nxD[x]= d(s,x)\nFigure 14.18: A schematic illustration for the justi\ufb01cation of Proposition 14.23.\nMoreover, D[x]= d(s,x),s i n c e zis the \ufb01rst incorrect vertex. When xwas pulled\ninto C, we tested (and possibly updated) D[y]so that we had at that point\nD[y]\u2264D[x]+w(x,y)= d(s,x)+ w(x,y).\nBut since yis the next vertex on the shortest path from stoz, this implies that\nD[y]= d(s,y).\nBut we are now at the moment when we are picking z, not y,t oj o i n C; hence,\nD[z]\u2264D[y].\nIt should be clear that a subpath of a shortest path is itself a shortest path. Hence,\nsince yis on the shortest path from stoz,\nd(s,y)+ d(y,z)= d(s,z).\nM\noreover, d(y,z)\u22650 because there are no negative-weight edges. Therefore,\nD[z]\u2264D[y]= d(s,y)\u2264d(s,y)+ d(y,z)= d(s,z).\nBut this contradicts the de\ufb01nition of z; hence, there can be no such vertex z.\nThe Running Time of Dijkstra\u2019s Algorithm\nIn this section, we analyze the time complexity of Dijkstra\u2019s algorithm. We denote\nwith nand mthe number of vertices and edges of the input graph G, respectively.\nWe assume that the edge weights can be added and compared in constant time.\nBecause of the high level of the description we gave for Dijkstra\u2019s algorithm inCode Fragment 14.12, analyzing its running time requires that we give more detailson its implementation. Speci\ufb01cally, we should indicate the data structures used and\nhow they are implemented.", "666 Chapter 14. Graph Algorithms\nLet us \ufb01rst assume that we are representing the graph Gusing an adjacency\nlist or adjacency map structure. This data structure allows us to step through the\nvertices adjacent to uduring the relaxation step in time proportional to their number.\nTherefore, the time spent in the management of the nested forloop, and the number\nof iterations of that loop, is\n\u2211\nuinVGoutdeg (u),\nwhich is O(m)by Proposition 14.9. The outer while loop executes O(n)times,\nsince a new vertex is added to the cloud during each iteration. This still does notsettle all the details for the algorithm analysis, however, for we must say more about\nhow to implement the other principal data structure in the algorithm\u2014the priority\nqueue Q.\nReferring back to Code Fragment 14.12 in search of priority queue operations,\nwe \ufb01nd that nvertices are originally inserted into the priority queue; since these are\nthe only insertions, the maximum size of the queue is n. In each of niterations of\nthewhile loop, a call to remove\nminis made to extract the vertex uwith smallest\nDlabel from Q. Then, for each neighbor vofu, we perform an edge relaxation,\nand may potentially update the key of vin the queue. Thus, we actually need an\nimplementation of an adaptable priority queue (Section 9.5), in which case the key\nof a vertex vis changed using the method update (/lscript,k),w h e r e /lscriptis the locator for\nthe priority queue entry associated with vertex v. In the worst case, there could be\none such update for each edge of the graph. Overall, the running time of Dijkstra\u2019salgorithm is bounded by the sum of the following:\n\u2022ninsertions into Q.\n\u2022ncalls to the remove\nminmethod on Q.\n\u2022mcalls to the update method on Q.\nIfQis an adaptable priority queue implemented as a heap, then each of the\nabove operations run in O(logn), and so the overall running time for Dijkstra\u2019s\nalgorithm is O((n+m)logn). Note that if we wish to express the running time as a\nfunction of nonly, then it is O(n2logn)in the worst case.\nLet us now consider an alternative implementation for the adaptable priority\nqueue Qusing an unsorted sequence. (See Exercise P-9.58.) This, of course, re-\nquires that we spend O(n)time to extract the minimum element, but it affords\nvery fast key updates, provided Qsupports location-aware entries (Section 9.5.1).\nSpeci\ufb01cally, we can implement each key update done in a relaxation step in O(1)\ntime\u2014we simply change the key value once we locate the entry in Qto update.\nHence, this implementation results in a running time that is O(n2+m), which can\nbe simpli\ufb01ed to O(n2)since Gis simple.", "14.6. Shortest Paths 667\nComparing the Two Implementations\nWe have two choices for implementing the adaptable priority queue with location-\naware entries in Dijkstra\u2019s algorithm: a heap implementation, which yields a run-\nning time of O((n+m)logn), and an unsorted sequence implementation, which\nyields a running time of O(n2). Since both implementations would be fairly simple\nto code, they are about equal in terms of the programming sophistication needed.\nThese two implementations are also about equal in terms of the constant factors intheir worst-case running times. Looking only at these worst-case times, we prefer\nthe heap implementation when the number of edges in the graph is small (that is,\nwhen m<n\n2/logn), and we prefer the sequence implementation when the number\nof edges is large (that is, when m>n2/logn).\nProposition 14.24: Given a weighted graph Gwith nvertices and medges, such\nthat the weight of each edge is nonnegative, and a vertex sofG, Dijkstra\u2019s algorithm\ncan compute the distance from sto all other vertices of Gin the better of O(n2)or\nO((n+m)logn)time.\nWe note that an advanced priority queue implementation, known as a Fibonacci\nheap , can be used to implement Dijkstra\u2019s algorithm in O(m+nlogn)time.\nProgramming Dijkstra\u2019s Algorithm in Python\nHaving given a pseudo-code description of Dijkstra\u2019s algorithm, let us now present\nPython code for performing Dijkstra\u2019s algorithm, assuming we are given a graph\nwhose edge elements are nonnegative integer weights. Our implementation of thealgorithm is in the form of a function, shortest\npath\nlengths , that takes a graph and\na designated source vertex as parameters. (See Code Fragment 14.13.) It returns a\ndictionary, named cloud , mapping each vertex vthat is reachable from the source\nto its shortest-path distance d(s,v). We rely on our AdaptableHeapPriorityQueue\ndeveloped in Section 9.5.2 as an adaptable priority queue.\nAs we have done with other algorithms in this chapter, we rely on dictionaries\nto map vertices to associated data (in this case, mapping vto its distance bound\nD[v]and its adaptable priority queue locator). The expected O(1)-time access to\nelements of these dictionaries could be converted to worst-case bounds, either by\nnumbering vertices from 0 to n\u22121 to use as indices into a list, or by storing the\ninformation within each vertex\u2019s element.\nThe pseudo-code for Dijkstra\u2019s algorithm begins by assigning d[v]=\u221efor\neach vother than the source. We rely on the special value \ufb02oat(\ninf\n)in Python\nto provide a numeric value that represents positive in\ufb01nity. However, we avoid in-cluding vertices with this \u201cin\ufb01nite\u201d distance in the resulting cloud that is returnedby the function. The use of this numeric limit could be avoided altogether by wait-\ning to add a vertex to the priority queue until after an edge that reaches it is relaxed.\n(See Exercise C-14.64.)", "668 Chapter 14. Graph Algorithms\n1defshortest\n path\nlengths(g, src):\n2\u201d\u201d\u201dCompute shortest-path distances from src to reachable vertices of g.\n3\n4Graph g can be undirected or directed, but must be weighted such that\n5e.element() returns a numeric weight for each edge e.\n67Return dictionary mapping each reachable vertex to its distance from src.\n8\u201d\u201d\u201d\n9d={} # d[v] is upper bound from s to v\n10 cloud = {} # map reachable v to its d[v] value\n11 pq = AdaptableHeapPriorityQueue( ) #v e r t e xvw i l lh a v ek e yd [ v ]\n12 pqlocator = {} # map from vertex to its pq locator\n1314 # for each vertex v of the graph, add an entry to the priority queue, with\n15 # the source having distance 0 and all others having in\ufb01nite distance\n16forving.vertices():\n17 ifvissrc:\n18 d[v] = 0\n19 else:\n20 d[v] = \ufb02oat(\ninf\n) # syntax for positive in\ufb01nity\n21 pqlocator[v] = pq.add(d[v], v) # save locator for future updates\n2223while not pq.is\nempty():\n24 key, u = pq.remove\n min()\n25 cloud[u] = key # its correct d[u] value\n26 delpqlocator[u] # u is no longer in pq\n27 foreing.incident\n edges(u): # outgoing edges (u,v)\n28 v=e . o p p o s i t e ( u )\n29 ifvnot in cloud:\n30 # perform relaxation step on edge (u,v)\n31 wgt = e.element()\n32 ifd[u] + wgt <d[v]: # better path to v?\n33 d[v] = d[u] + wgt # update the distance\n34 pq.update(pqlocator[v], d[v], v) # update the pq entry\n3536return cloud # only includes reachable vertices\nCode Fragment 14.13: Python implementation of Dijkstra\u2019s algorithm for comput-\ning the shortest-path distances from a single source. We assume that e.element()\nfor edge erepresents the weight of that edge.", "14.6. Shortest Paths 669\nReconstructing the Shortest-Path Tree\nOur pseudo-code description of Dijkstra\u2019s algorithm in Code Fragment 14.12, and\nour implementation in Code Fragment 14.13, computes the value d[v], for each ver-\ntex v, that is the length of the shortest path from the source vertex stov.H o w e v e r ,\nthose forms of the algorithm do not explicitly compute the actual paths that achievethose distances. The collection of all shortest paths emanating from source scan be\ncompactly represented by what is known as the shortest-path tree . The paths form\na rooted tree because if a shortest path from stovpasses through an intermediate\nvertex u, it must begin with a shortest path from stou.\nIn this section, we demonstrate that the shortest-path tree rooted at source s\ncan be reconstructed in O(n+m)time, given the set of d[v]values produced by\nDijkstra\u2019s algorithm using sas the source. As we did when representing the DFS\nand BFS trees, we will map each vertex v/negationslash=sto a parent u(possibly, u=s), such\nthat uis the vertex immediately before von a shortest path from stov.I f uis the\nvertex just before von the shortest path from stov, it must be that\nd[u]+w(u,v)= d[v].\nConversely, if the above equation is satis\ufb01ed, then the shortest path from stou,\nfollowed by the edge (u,v)is\na shortest path to v.\nOur implementation in Code Fragment 14.14 reconstructs the tree based on this\nlogic, testing all incoming edges to each vertex v, looking for a (u,v)that satis\ufb01es\nthe key equation. The running time is O(n+m), as we consider each vertex and all\nincoming edges to those vertices. (See Proposition 14.9.)\n1defshortest\n path\ntree(g, s, d):\n2\u201d\u201d\u201dReconstruct shortest-path tree rooted at vertex s, given distance map d.\n34Return tree as a map from each reachable vertex v (other than s) to the\n5edge e=(u,v) that is used to reach v from its parent u in the tree.\n6\u201d\u201d\u201d\n7tree = {}\n8forvind:\n9 ifvis not s:\n10 foreing.incident\nedges(v, False): # consider INCOMING edges\n11 u=e . o p p o s i t e ( v )\n12 wgt = e.element()\n13 ifd[v] == d[u] + wgt:\n14 tree[v] = e # edge e is used to reach v\n15return tree\nCode Fragment 14.14: Python function that reconstructs the shortest paths, based\non knowledge of the single-source distances.", "670 Chapter 14. Graph Algorithms\n14.7 Minimum Spanning Trees\nSuppose we wish to connect all the computers in a new of\ufb01ce building using the\nleast amount of cable. We can model this problem using an undirected, weightedgraph Gwhose vertices represent the computers, and whose edges represent all\nthe possible pairs (u,v)of computers, where the weight w(u,v)of edge (u,v)is\nequal to the amount of cable needed to connect computer uto computer v.R a t h e r\nthan computing a shortest-path tree from some particular vertex v, we are interested\ninstead in \ufb01nding a tree Tthat contains all the vertices of Gand has the minimum\ntotal weight over all such trees. Algorithms for \ufb01nding such a tree are the focus ofthis section.\nProblem De\ufb01nition\nGiven an undirected, weighted graph G, we are interested in \ufb01nding a tree Tthat\ncontains all the vertices in Gand minimizes the sum\nw(T)= \u2211\n(u,v)inTw(u,v).\nA tree, such as this, that contains every vertex of a connected graph Gis said to\nbe a spanning tree , and the problem of computing a spanning tree Twith smallest\ntotal weight is known as the minimum spanning tree (orMST ) problem.\nThe development of ef\ufb01cient algorithms for the minimum spanning tree prob-\nlem predates the modern notion of computer science itself. In this section, we\ndiscuss two classic algorithms for solving the MST problem. These algorithms\nare both applications of the greedy method , which, as was discussed brie\ufb02y in the\nprevious section, is based on choosing objects to join a growing collection by it-eratively picking an object that minimizes some cost function. The \ufb01rst algorithmwe discuss is the Prim-Jarn \u00b4 \u0131k algorithm, which grows the MST from a single root\nvertex, much in the same way as Dijkstra\u2019s shortest-path algorithm. The secondalgorithm we discuss is Kruskal\u2019s algorithm, which \u201cgrows\u201d the MST in clustersby considering edges in nondecreasing order of their weights.\nIn order to simplify the description of the algorithms, we assume, in the follow-\ning, that the input graph Gis undirected (that is, all its edges are undirected) and\nsimple (that is, it has no self-loops and no parallel edges). Hence, we denote theedges of Gas unordered vertex pairs (u,v).\nBefore we discuss the details of these algorithms, however, let us give a crucial\nfact about minimum spanning trees that forms the basis of the algorithms.", "14.7. Minimum Spanning Trees 671\nA Crucial Fact about Minimum Spanning Trees\nThe two MST algorithms we discuss are based on the greedy method, which in this\ncase depends crucially on the following fact. (See Figure 14.19.)\nV1 V2e\nmin-weight\n\u201cbridge\u201d edgeeBelongs to a Minimum Spanning Tree\nFigure 14.19: An illustration of the crucial fact about minimum spanning trees.\nProposition 14.25: Let Gbe a weighted connected graph, and let V1and V2be a\npartition of the vertices of Ginto two disjoint nonempty sets. Furthermore, let ebe\nan edge in Gwith minimum weight from among those with one endpoint in V1and\nthe other in V2. There is a minimum spanning tree Tthat has eas one of its edges.\nJusti\ufb01cation: Let Tbe a minimum spanning tree of G.I f Tdoes not contain\nedge e, the addition of etoTmust create a cycle. Therefore, there is some edge\nf/negationslash=eof this cycle that has one endpoint in V1and the other in V2. Moreover, by\nthe choice of e,w(e)\u2264w(f). If we remove ffrom T\u222a{e}, we obtain a spanning\ntree whose total weight is no more than before. Since Twas a minimum spanning\ntree, this new tree must also be a minimum spanning tree.\nIn fact, if the weights in Gare distinct, then the minimum spanning tree is\nunique; we leave the justi\ufb01cation of this less crucial fact as an exercise (C-14.65).\nIn addition, note that Proposition 14.25 remains valid even if the graph Gcon-\ntains negative-weight edges or negative-weight cycles, unlike the algorithms we\npresented for shortest paths.", "672 Chapter 14. Graph Algorithms\n14.7.1 Prim-Jarn \u00b4 \u0131k Algorithm\nIn the Prim-Jarn \u00b4 \u0131k algorithm, we grow a minimum spanning tree from a single\ncluster starting from some \u201croot\u201d vertex s. The main idea is similar to that of\nDijkstra\u2019s algorithm. We begin with some vertex s, de\ufb01ning the initial \u201ccloud\u201d of\nvertices C. Then, in each iteration, we choose a minimum-weight edge e=(u,v),\nconnecting a vertex uin the cloud Cto a vertex voutside of C. The vertex vis\nthen brought into the cloud Cand the process is repeated until a spanning tree is\nformed. Again, the crucial fact about minimum spanning trees comes into play,\nfor by always choosing the smallest-weight edge joining a vertex inside Cto one\noutside C, we are assured of always adding a valid edge to the MST.\nTo ef\ufb01ciently implement this approach, we can take another cue from Dijkstra\u2019s\nalgorithm. We maintain a label D[v]for each vertex voutside the cloud C,s ot h a t\nD[v]stores the weight of the minimum observed edge for joining vto the cloud\nC. (In Dijkstra\u2019s algorithm, this label measured the full path length from starting\nvertex stov, including an edge (u,v).) These labels serve as keys in a priority\nqueue used to decide which vertex is next in line to join the cloud. We give thepseudo-code in Code Fragment 14.15.\nAlgorithm PrimJarnik( G):\nInput: An undirected, weighted, connected graph Gwith nvertices and medges\nOutput: A minimum spanning tree TforG\nPick any vertex sofG\nD[s]=0\nforeach vertex v/negationslash=sdo\nD[v]=\u221e\nInitialize T=\u2205.\nInitialize\na priority queue Qwith an entry (D[v],(v,None ))for each vertex v,\nwhere D[v]is the key in the priority queue, and (v,None )is the associated value.\nwhile Qis not empty do\n(u,e)=value returned by Q.remove\n min ()\nConnect vertex utoTusing edge e.\nforeach edge e/prime=(u,v)such that vis in Qdo\n{check if edge (u,v)better connects vtoT}\nifw(u,v)<D[v]then\nD[v]= w(u,v)\nChange the key of vertex vinQtoD[v].\nChange the value of vertex vinQto(v,e/prime).\nreturn the tree T\nCode Fragment 14.15: The Prim-Jarn \u00b4 \u0131k algorithm for the MST problem.", "14.7. Minimum Spanning Trees 673\nAnalyzing the Prim-Jarn \u00b4 \u0131k Algorithm\nThe implementation issues for the Prim-Jarn \u00b4 \u0131k algorithm are similar to those for\nDijkstra\u2019s algorithm, relying on an adaptable priority queue Q(Section 9.5.1).\nWe initially perform ninsertions into Q, later perform nextract-min operations,\nand may update a total of mpriorities as part of the algorithm. Those steps are\nthe primary contributions to the overall running time. With a heap-based priority\nqueue, each operation runs in O(logn)time, and the overall time for the algorithm\nisO((n+m)logn),w h i c hi s O(mlogn)for a connected graph. Alternatively, we\ncan achieve O(n2)running time by using an unsorted list as a priority queue.\nIllustrating the Prim-Jarn \u00b4 \u0131k Algorithm\nWe illustrate the Prim-Jarn \u00b4 \u0131k algorithm in Figures 14.20 through 14.21.\n187\n1391740\n621867\n1090849\n1846\n802\n12352704\n184144\n337\n23421258\n946PVD\n1464\n1121JFK\nBWIORD\nMIALAXDFWSFOBOS\n144\n1391740\n621867\n1090849\n1846\n802\n12352704\n184\n337\n23421258\n946187PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW\n(a) (b)\n1090621867\n849\n187\n1846\n802\n12352704\n184144\n337\n23421258\n946740\n1391PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW187740867\n1090849\n1846\n802\n12352704\n184144\n337\n23421258\n1391621\n946PVD\n1464\n1121SFOBOS\nJFK\nBWIORD\nMIALAXDFW\n(c) (d)\nFigure 14.20: An illustration of the Prim-Jarn \u00b4 \u0131k MST algorithm, starting with vertex\nPVD. (Continues in Figure 14.21.)", "674 Chapter 14. Graph Algorithms\n9461391740867\n10901258849\n187\n1846\n802\n12352704\n184144\n337\n2342621PVD\n1464\n1121DFWBOS\nJFK\nBWIORD\nMIASFO\nLAX1464621867\n10901258849\n187\n1846\n802\n12352704\n184\n946144\n337\n23421121PVD\n1391740\nDFWBOS\nJFK\nBWIORD\nMIASFO\nLAX\n(e) (f)\n12351391740\n621867\n10901258849\n187\n1846\n8022704\n184\n946144\n337\n2342PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n12351391740\n621867\n10901258849\n187\n1846\n802\n23422704\n184\n946144\n337PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n(g) (h)\n3371391740\n621867\n10901258849\n187\n1846\n802\n1235\n23422704\n184\n946144PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121LAXBOS\nJFK\nBWIORD\nMIADFWSFO\n(i) (j)\nFigure 14.21: An illustration of the Prim-Jarn \u00b4 \u0131k MST algorithm. (Continued from\nFigure 14.20.)", "14.7. Minimum Spanning Trees 675\nPython Implementation\nCode Fragment 14.16 presents a Python implementation of the Prim-Jarn \u00b4 \u0131k algo-\nrithm. The MST is returned as an unordered list of edges.\n1defMST\n PrimJarnik(g):\n2\u201d\u201d\u201dCompute a minimum spanning tree of weighted graph g.\n3\n4Return a list of edges that comprise the MST (in arbitrary order).\n5\u201d\u201d\u201d\n6d={} # d[v] is bound on distance to tree\n7tree = [ ] # list of edges in spanning tree\n8pq = AdaptableHeapPriorityQueue( ) # d[v] maps to value (v, e=(u,v))\n9pqlocator = {} # map from vertex to its pq locator\n10\n11 # for each vertex v of the graph, add an entry to the priority queue, with\n12 # the source having distance 0 and all others having in\ufb01nite distance\n13forving.vertices():\n14 iflen(d) == 0: # this is the \ufb01rst node\n15 d[v] = 0 # make it the root\n16 else:\n17 d[v] = \ufb02oat(\ninf\n) # positive in\ufb01nity\n18 pqlocator[v] = pq.add(d[v], (v, None))\n1920while not pq.is\nempty():\n21 key,value = pq.remove\n min()\n22 u,edge = value # unpack tuple from pq\n23 delpqlocator[u] # u is no longer in pq\n24 ifedgeis not None :\n25 tree.append(edge) # add edge to tree\n26 forlinking.incident\n edges(u):\n27 v = link.opposite(u)\n28 ifvinpqlocator: # thus v not yet in tree\n29 # see if edge (u,v) better connects v to the growing tree\n30 wgt = link.element()\n31 ifwgt<d[v]: # better edge to v?\n32 d[v] = wgt # update the distance\n33 pq.update(pqlocator[v], d[v], (v, link)) # update the pq entry\n34return tree\nCode Fragment 14.16: Python implementation of the Prim-Jarn \u00b4 \u0131k algorithm for the\nminimum spanning tree problem.", "676 Chapter 14. Graph Algorithms\n14.7.2 Kruskal\u2019s Algorithm\nIn this section, we introduce Kruskal\u2019s algorithm for constructing a minimum span-\nning tree. While the Prim-Jarn \u00b4 \u0131k algorithm builds the MST by growing a single tree\nuntil it spans the graph, Kruskal\u2019s algorithm maintains a forest of clusters, repeat-\nedly merging pairs of clusters until a single cluster spans the graph.\nInitially, each vertex is by itself in a singleton cluster. The algorithm then\nconsiders each edge in turn, ordered by increasing weight. If an edge econnects\ntwo different clusters, then eis added to the set of edges of the minimum spanning\ntree, and the two clusters connected by eare merged into a single cluster. If, on the\nother hand, econnects two vertices that are already in the same cluster, then eis\ndiscarded. Once the algorithm has added enough edges to form a spanning tree, it\nterminates and outputs this tree as the minimum spanning tree.\nWe give pseudo-code for Kruskal\u2019s MST algorithm in Code Fragment 14.17\nand we show an example of this algorithm in Figures 14.22, 14.23, and 14.24.\nAlgorithm Kruskal( G):\nInput: A simple connected weighted graph Gwith nvertices and medges\nOutput: A minimum spanning tree TforG\nforeach vertex vinGdo\nDe\ufb01ne an elementary cluster C(v)={v}.\nInitialize a priority queue Qto contain all edges in G,u s i n gt h ew e i g h t sa sk e y s .\nT=\u2205 {Twill ultimately contain the edges of the MST }\nwhile Thas fewer than n\u22121 edges do\n(u,v)=value returned by Q.remove\n min ()\nLet C(u)be the cluster containing u,a n dl e t C(v)be the cluster containing v.\nifC(u)/negationslash=C(v)then\nAdd edge (u,v)toT.\nMerge C(u)and C(v)into one cluster.\nreturn tree T\nCode Fragment 14.17: Kruskal\u2019s algorithm for the MST problem.\nAs was the case with the Prim-Jarn \u00b4 \u0131k algorithm, the correctness of Kruskal\u2019s al-\ngorithm is based upon the crucial fact about minimum spanning trees from Propo-\nsition 14.25. Each time Kruskal\u2019s algorithm adds an edge (u,v)to the minimum\nspanning tree T, we can de\ufb01ne a partitioning of the set of vertices V(as in the\nproposition) by letting V1be the cluster containing vand letting V2contain the rest\nof the vertices in V. This clearly de\ufb01nes a disjoint partitioning of the vertices of\nVand, more importantly, since we are extracting edges from Qin order by their\nweights, emust be a minimum-weight edge with one vertex in V1and the other in\nV2. Thus, Kruskal\u2019s algorithm always adds a valid minimum spanning tree edge.", "14.7. Minimum Spanning Trees 677\n144\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n946621\n184PVD\n1464\n1121BWI\nLAXBOS\nJFK\nMIAORD\nSFO\nDFW184PVD\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n144\n946621\n1464\n1121LAXJFKBOS\nSFO\nMIAORD\nBWI\nDFW\n(a) (b)\n187PVD\n1391740867\n10901258849\n1846\n337802\n1235\n23422704\n184144\n946621\n1464\n1121DFW\nLAXJFKBOS\nMIAORD\nBWISFO621PVD\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144\n9461464\n1121\nMIADFWSFO\nLAXBOS\nJFK\nBWIORD\n(c) (d)\n621\n1391740867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144\n946PVD\n1464\n1121ORD\nMIADFWSFO\nLAXJFKBOS\nBWI\n9461391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144PVD\n1464\n1121BOS\nBWI\nMIADFWSFO\nLAXJFKORD\n(e) (f)\nFigure 14.22: Example of an execution of Kruskal\u2019s MST algorithm on a graph with\ninteger weights. We show the clusters as shaded regions and we highlight the edge\nbeing considered in each iteration. (Continues in Figure 14.23.)", "678 Chapter 14. Graph Algorithms\n12351391740\n621867\n10901258849\n187\n1846\n337802\n23422704\n184144\n946PVD\n1464\n1121ORD\nBWI\nMIADFWSFO\nLAXBOS\nJFK\n9461391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144PVD\n1464\n1121ORD\nBWIBOS\nMIADFWSFO\nLAXJFK\n(g) (h)\n9461391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144PVD\n1464\n1121ORD\nBWIBOS\nMIADFWSFO\nLAXJFK\n9461391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184144PVD\n1464\n1121BWIORD\nMIADFWSFO\nLAXBOS\nJFK\n(i) (j)\n144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121BOS\nJFK\nBWIORD\nMIADFWSFO\nLAX144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121JFK\nBWIBOS\nMIADFWSFO\nLAXORD\n(k) (l)\nFigure 14.23: An example of an execution of Kruskal\u2019s MST algorithm. Rejected\nedges are shown dashed. (Continues in Figure 14.24.)", "14.7. Minimum Spanning Trees 679\n144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121BOS\nJFK\nBWIORD\nMIADFWSFO\nLAX144\n1391740\n621867\n10901258849\n187\n1846\n337802\n1235\n23422704\n184\n946PVD\n1464\n1121ORD\nJFK\nBWI\nMIADFWSFO\nLAXBOS\n(m) (n)\nFigure 14.24: Example of an execution of Kruskal\u2019s MST algorithm (continued).\nThe edge considered in (n) merges the last two clusters, which concludes this exe-\ncution of Kruskal\u2019s algorithm. (Continued from Figure 14.23.)\nThe Running Time of Kruskal\u2019s Algorithm\nThere are two primary contributions to the running time of Kruskal\u2019s algorithm.\nThe \ufb01rst is the need to consider the edges in nondecreasing order of their weights,\nand the second is the management of the cluster partition. Analyzing its runningtime requires that we give more details on its implementation.\nThe ordering of edges by weight can be implemented in O(mlogm), either by\nuse of a sorting algorithm or a priority queue Q. If that queue is implemented with\na heap, we can initialize QinO(mlogm)time by repeated insertions, or in O(m)\ntime using bottom-up heap construction (see Section 9.3.6), and the subsequent\ncalls to remove\nmineach run in O(logm)time, since the queue has size O(m).W e\nnote that since misO(n2)for a simple graph, O(logm)is the same as O(logn).\nTherefore, the running time due to the ordering of edges is O(mlogn).\nThe remaining task is the management of clusters. To implement Kruskal\u2019s\nalgorithm, we must be able to \ufb01nd the clusters for vertices uand vthat are endpoints\nof an edge e, to test whether those two clusters are distinct, and if so, to merge\nthose two clusters into one. None of the data structures we have studied thus farare well suited for this task. However, we conclude this chapter by formalizing\nthe problem of managing disjoint partitions , and introducing ef\ufb01cient union-\ufb01nd\ndata structures. In the context of Kruskal\u2019s algorithm, we perform at most 2 m\n\ufb01nd operations and n\u22121 union operations. We will see that a simple union-\ufb01nd\nstructure can perform that combination of operations in O(m+nlogn)time (see\nProposition 14.26), and a more advanced structure can support an even faster time.\nFor a connected graph, m\u2265n\u22121, and therefore, the bound of O(mlogn)time\nfor ordering the edges dominates the time for managing the clusters. We conclude\nthat the running time of Kruskal\u2019s algorithm is O(mlogn).", "680 Chapter 14. Graph Algorithms\nPython Implementation\nCode Fragment 14.18 presents a Python implementation of Kruskal\u2019s algorithm. As\nwith our implementation of the Prim-Jarn \u00b4 \u0131k algorithm, the minimum spanning tree\nis returned in the form of a list of edges. As a consequence of Kruskal\u2019s algorithm,\nthose edges will be reported in nondecreasing order of their weights.\nOur implementation assumes use of a Partition class for managing the cluster\npartition. An implementation of the Partition class is presented in Section 14.7.3.\n1defMST\n Kruskal(g):\n2\u201d\u201d\u201dCompute a minimum spanning tree of a graph using Kruskal\n s algorithm.\n3\n4Return a list of edges that comprise the MST.\n56The elements of the graph\nse d g e sa r ea s s u m e dt ob ew e i g h t s .\n7\u201d\u201d\u201d\n8tree = [ ] # list of edges in spanning tree\n9pq = HeapPriorityQueue( ) # entries are edges in G, with weights as key\n10 forest = Partition( ) # keeps track of forest clusters\n11 position = {} # map each node to its Partition entry\n1213forving.vertices():\n14 position[v] = forest.make\ngroup(v)\n15\n16foreing.edges():\n17 pq.add(e.element(), e) # edge\u2019s element is assumed to be its weight\n18\n19 size = g.vertex\n count()\n20while len(tree) != size \u22121and not pq.is\nempty():\n21 # tree not spanning and unprocessed edges remain\n22 weight,edge = pq.remove\n min()\n23 u,v = edge.endpoints()\n24 a = forest.\ufb01nd(position[u])\n25 b = forest.\ufb01nd(position[v])\n26 ifa! =b :\n27 tree.append(edge)\n28 forest.union(a,b)\n29\n30return tree\nCode Fragment 14.18: Python implementation of Kruskal\u2019s algorithm for the mini-\nmum spanning tree problem.", "14.7. Minimum Spanning Trees 681\n14.7.3 Disjoint Partitions and Union-Find Structures\nIn this section, we consider a data structure for managing a partition of elements\ninto a collection of disjoint sets. Our initial motivation is in support of Kruskal\u2019s\nminimum spanning tree algorithm, in which a forest of disjoint trees is maintained,\nwith occasional merging of neighboring trees. More generally, the disjoint partition\nproblem can be applied to various models of discrete growth.\nWe formalize the problem with the following model. A partition data structure\nmanages a universe of elements that are organized into disjoint sets (that is, anelement belongs to one and only one of these sets). Unlike with the Set ADT orPython\u2019s setclass, we do not expect to be able to iterate through the contents of a\nset, nor to ef\ufb01ciently test whether a given set includes a given element. To avoid\nconfusion with such notions of a set, we will refer to the clusters of our partition as\ngroups . However, we will not require an explicit structure for each group, instead\nallowing the organization of groups to be implicit. To differentiate between onegroup and another, we assume that at any point in time, each group has a designatedentry that we refer to as the leader of the group.\nFormally, we de\ufb01ne the methods of a partition ADT using position objects,\neach of which stores an element x. The partition ADT supports the following meth-\nods.\nmake\ngroup(x): Create a singleton group containing new element xand\nreturn the position storing x.\nunion(p, q): Merge the groups containing positions pandq.\n\ufb01nd(p) :Return the position of the leader of the group containingposition p.\nSequence Implementation\nA simple implementation of a partition with a total of nelements uses a collection\nof sequences, one for each group, where the sequence for a group Astores element\npositions. Each position object stores a variable, element , which references its\nassociated element xand allows the execution of an element ()method in O(1)time.\nIn addition, each position stores a variable, group , that references the sequence\nstoring p, since this sequence is representing the group containing p\u2019s element.\n(See Figure 14.25.)\nWith this representation, we can easily perform the make\n group (x)and\ufb01nd (p)\noperations in O(1)time, allowing the \ufb01rst position in a sequence to serve as the\n\u201cleader.\u201d Operation union (p,q)requires that we join two sequences into one and\nupdate the group references of the positions in one of the two. We choose to imple-\nment this operation by removing all the positions from the sequence with smaller", "682 Chapter 14. Graph Algorithms\nC\n5 11 12 10 8B\n9 3 6 2A\n4 1 7\nFigure 14.25: Sequence-based implementation of a partition consisting of three\ngroups: A={1,4,7},B={2,3,6,9},a n d C={5,8,10,11,12}.\nsize, and inserting them in the sequence with larger size. Each time we take a po-\nsition from the smaller group aand insert it into the larger group b, we update the\ngroup reference for that position to now point to b. Hence, the operation union (p,q)\ntakes time O(min (np,nq)),w h e r e np(resp. nq) is the cardinality of the group con-\ntaining position p(resp. q). Clearly, this time is O(n)if there are nelements in the\npartition universe. However, we next present an amortized analysis that shows thisimplementation to be much better than appears from this worst-case analysis.\nProposition 14.26:\nWhen using the above sequence-based partition implementa-\ntion, performing a series of kmake\n group ,union ,a n d\ufb01ndoperations on an initially\nempty partition involving at most nelements takes O(k+nlogn)time.\nJusti\ufb01cation: We use the accounting method and assume that one cyber-dollar\ncan pay for the time to perform a \ufb01nd operation, a make\n group operation, or the\nmovement of a position object from one sequence to another in a union operation.\nIn the case of a \ufb01nd ormake\n group operation, we charge the operation itself 1\ncyber-dollar. In the case of a union operation, we assume that 1 cyber-dollar pays\nfor the constant-time work in comparing the sizes of the two sequences, and that\nwe charge 1 cyber-dollar to each position that we move from the smaller group to\nthe larger group. Clearly, the 1 cyber-dollar charged for each \ufb01ndandmake\n group\noperation, together with the \ufb01rst cyber-dollar collected for each union operation,\naccounts for a total of kcyber-dollars.\nConsider, then, the number of charges made to positions on behalf of union\noperations. The important observation is that each time we move a position fromone group to another, the size of that position\u2019s group at least doubles. Thus, eachposition is moved from one group to another at most log ntimes; hence, each po-\nsition can be charged at most O(logn)times. Since we assume that the partition is\ninitially empty, there are O(n)different elements referenced in the given series of\noperations, which implies that the total time for moving elements during the union\noperations is O(nlogn).\n", "14.7. Minimum Spanning Trees 683\nA Tree-Based Partition Implementation \u22c6\nAn alternative data structure for representing a partition uses a collection of\ntrees to store the nelements, where each tree is associated with a different group.\n(See Figure 14.26.) In particular, we implement each tree with a linked data struc-\nture whose nodes are themselves the group position objects. We view each position\npas being a node having an instance variable, element , referring to its element x,\nand an instance variable, parent , referring to its parent node. By convention, if pis\ntheroot of its tree, we set p\u2019sparent reference to itself.\n2\n91 161 0 73 41\n125\n8\nFigure 14.26: Tree-based implementation of a partition consisting of three groups:\nA={1,4,7},B={2,3,6,9},a n d C={5,8,10,11,12}.\nWith this partition data structure, operation \ufb01nd (p)is performed by walking\nup from position pto the root of its tree, which takes O(n)time in the worst case.\nOperation union (p,q)can be implemented by making one of the trees a subtree\nof the other. This can be done by \ufb01rst locating the two roots, and then in O(1)\nadditional time by setting the parent reference of one root to point to the other root.\nSee Figure 14.27 for an example of both operations.\n2\n10\n1185\n96 3\n123\n1182\n105\n96\n12\n(a) (b)\nFigure 14.27: Tree-based implementation of a partition: (a) operation union (p,q);\n(b) operation \ufb01nd (p),w h e r e pdenotes the position object for element 12.", "684 Chapter 14. Graph Algorithms\nAt \ufb01rst, this implementation may seem to be no better than the sequence-based\ndata structure, but we add the following two simple heuristics to make it run faster.\nUnion-by-Size: With each position p, store the number of elements in the subtree\nrooted at p.I naunion operation, make the root of the smaller group become\na child of the other root, and update the size \ufb01eld of the larger root.\nPath Compression: In a\ufb01nd operation, for each position qthat the \ufb01nd visits,\nreset the parent of qto the root. (See Figure 14.28.)\n3\n1182\n105\n96\n123\n12112\n105\n96\n8\n(a) (b)\nFigure 14.28: Path-compression heuristic: (a) path traversed by operation \ufb01nd on\nelement 12; (b) restructured tree.\nA surprising property of this data structure, when implemented using the union-\nby-size and path-compression heuristics, is that performing a series of koperations\ninvolving nelements takes O(klog\u2217n)time, where log\u2217nis the log-star function,\nwhich is the inverse of the tower-of-twos function. Intuitively, log\u2217nis the number\nof times that one can iteratively take the logarithm (base 2) of a number before\ngetting a number smaller than 2. Table 14.4 shows a few sample values.\nminimum n\n2\n22=4\n222=16\n 2222\n=65,536\n 22222\n=265,536\nlog\u2217n\n1\n 2\n 3\n 4\n 5\nTable 14.4: Some values of log\u2217nand critical values for its inverse.\nProposition 14.27: When using the tree-based partition representation with both\nunion-by-size and path compression, performing a series of kmake\n group ,union ,\nand\ufb01nd operations on an initially empty partition involving at most nelements\ntakes O(klog\u2217n)time.\nAlthough the analysis for this data structure is rather complex, its implemen-\ntation is quite straightforward. We conclude with complete Python code for thestructure, given in Code Fragment 14.19.", "14.7. Minimum Spanning Trees 685\n1classPartition:\n2\u201d\u201d\u201dUnion-\ufb01nd structure for maintaining disjoint sets.\u201d\u201d\u201d\n3\n4#------------------------- nested Position class -------------------------\n5classPosition:\n6\n slots\n =\n_container\n ,\n_element\n ,\n_size\n ,\n_parent\n78 def\ninit\n(self,c o n t a i n e r ,e ) :\n9 \u201d\u201d\u201dCreate a new position that is the leader of its own group.\u201d\u201d\u201d\n10 self.\ncontainer = container # reference to Partition instance\n11 self.\nelement = e\n12 self.\nsize = 1\n13 self.\nparent = self # convention for a group leader\n14\n15 defelement( self):\n16 \u201d\u201d\u201dReturn element stored at this position.\u201d\u201d\u201d\n17 return self .\nelement\n1819 #------------------------- public Partition methods -------------------------\n20defmake\ngroup( self,e ) :\n21 \u201d\u201d\u201dMakes a new group containing element e, and returns its Position.\u201d\u201d\u201d\n22 return self .Position( self,e )\n2324def\ufb01nd(self,p ) :\n25 \u201d\u201d\u201dFinds the group containging p and return the position of its leader.\u201d\u201d\u201d\n26 ifp.\nparent != p:\n27 p.\nparent = self.\ufb01nd(p.\n parent) # overwrite p.\n parent after recursion\n28 return p.\nparent\n29\n30defunion(self,p ,q ) :\n31 \u201d\u201d\u201dMerges the groups containg elements p and q (if distinct).\u201d\u201d\u201d\n32 a=self.\ufb01nd(p)\n33 b=self.\ufb01nd(q)\n34 ifais not b: # only merge if di\ufb00erent groups\n35 ifa.\nsize>b.\nsize:\n36 b.\nparent = a\n37 a.\nsize += b.\n size\n38 else:\n39 a.\nparent = b\n40 b.\nsize += a.\n size\nCode Fragment 14.19: Python implementation of a Partition class using union-by-\nsize and path compression.", "686 Chapter 14. Graph Algorithms\n14.8 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-14.1 Draw a simple undirected graph Gthat has 12 vertices, 18 edges, and 3\nconnected components.\nR-14.2 IfGis a simple undirected graph with 12 vertices and 3 connected com-\nponents, what is the largest number of edges it might have?\nR-14.3 Draw an adjacency matrix representation of the undirected graph shown\nin Figure 14.1.\nR-14.4 Draw an adjacency list representation of the undirected graph shown in\nFigure 14.1.\nR-14.5 Draw a simple, connected, directed graph with 8 vertices and 16 edgessuch that the in-degree and out-degree of each vertex is 2. Show that thereis a single (nonsimple) cycle that includes all the edges of your graph, thatis, you can trace all the edges in their respective directions without ever\nlifting your pencil. (Such a cycle is called an Euler tour .)\nR-14.6 Suppose we represent a graph Ghaving nvertices and medges with the\nedge list structure. Why, in this case, does the insert\nvertex method run\ninO(1)time while the remove\n vertex method runs in O(m)time?\nR-14.7 Give pseudo-code for performing the operation insert\n edge(u,v,x) inO(1)\ntime using the adjacency matrix representation.\nR-14.8 Repeat Exercise R-14.7 for the adjacency list representation, as described\nin the chapter.\nR-14.9 Can edge list Ebe omitted from the adjacency matrix representation while\nstill achieving the time bounds given in Table 14.1? Why or why not?\nR-14.10 Can edge list Ebe omitted from the adjacency list representation while\nstill achieving the time bounds given in Table 14.3? Why or why not?\nR-14.11 Would you use the adjacency matrix structure or the adjacency list struc-ture in each of the following cases? Justify your choice.\na. The graph has 10,000 vertices and 20,000 edges, and it is important\nto use as little space as possible.\nb. The graph has 10,000 vertices and 20,000,000 edges, and it is im-\nportant to use as little space as possible.\nc. You need to answer the query get\nedge(u,v) as fast as possible, no\nmatter how much space you use.", "14.8. Exercises 687\nR-14.12 Explain why the DFS traversal runs in O(n2)time on an n-vertex simple\ngraph that is represented with the adjacency matrix structure.\nR-14.13 In order to verify that all of its nontree edges are back edges, redraw the\ngraph from Figure 14.8b so that the DFS tree edges are drawn with solid\nlines and oriented downward, as in a standard portrayal of a tree, and with\nall nontree edges drawn using dashed lines.\nR-14.14 A simple undirected graph is complete if it contains an edge between every\npair of distinct vertices. What does a depth-\ufb01rst search tree of a completegraph look like?\nR-14.15 Recalling the de\ufb01nition of a complete graph from Exercise R-14.14, what\ndoes a breadth-\ufb01rst search tree of a complete graph look like?\nR-14.16 Let Gbe an undirected graph whose vertices are the integers 1 through 8,\nand let the adjacent vertices of each vertex be given by the table below:\nvertex adjacent vertices\n1 (2, 3, 4)\n2 (1, 3, 4)3 (1, 2, 4)\n4 (1, 2, 3, 6)\n5 (6, 7, 8)6 (4, 5, 7)7 (5, 6, 8)8 (5, 7)\nAssume that, in a traversal of G, the adjacent vertices of a given vertex are\nreturned in the same order as they are listed in the table above.\na. Draw G.\nb. Give the sequence of vertices of Gvisited using a DFS traversal\nstarting at vertex 1.\nc. Give the sequence of vertices visited using a BFS traversal starting\nat vertex 1.\nR-14.17 Draw the transitive closure of the directed graph shown in Figure 14.2.\nR-14.18 If the vertices of the graph from Figure 14.11 are numbered as ( v\n1=JFK,\nv2=LAX, v3=MIA, v4=BOS, v5=ORD, v6=SFO, v7=DFW), in\nwhat order would edges be added to the transitive closure during the\nFloyd-Warshall algorithm?\nR-14.19 How many edges are in the transitive closure of a graph that consists of a\nsimple directed path of nvertices?\nR-14.20 Given an n-node complete binary tree T, rooted at a given position, con-\nsider a directed graph /vectorGhaving the nodes of Tas its vertices. For each\nparent-child pair in T, create a directed edge in /vectorGfrom the parent to the\nchild. Show that the transitive closure of /vectorGhas O(nlogn)edges.", "688 Chapter 14. Graph Algorithms\nR-14.21 Compute a topological ordering for the directed graph drawn with solid\nedges in Figure 14.3d.\nR-14.22 Bob loves foreign languages and wants to plan his course schedule for thefollowing years. He is interested in the following nine language courses:\nLA15, LA16, LA22, LA31, LA32, LA126, LA127, LA141, and LA169.The course prerequisites are:\n\u2022LA15: (none)\n\u2022LA16: LA15\n\u2022LA22: (none)\n\u2022LA31: LA15\n\u2022LA32: LA16, LA31\n\u2022LA126: LA22, LA32\n\u2022LA127: LA16\n\u2022LA141: LA22, LA16\n\u2022LA169: LA32\nIn what order can Bob take these courses, respecting the prerequisites?\nR-14.23 Draw a simple, connected, weighted graph with 8 vertices and 16 edges,each with unique edge weights. Identify one vertex as a \u201cstart\u201d vertex andillustrate a running of Dijkstra\u2019s algorithm on this graph.\nR-14.24 Show how to modify the pseudo-code for Dijkstra\u2019s algorithm for the casewhen the graph is directed and we want to compute shortest directed pathsfrom the source vertex to all the other vertices.\nR-14.25 Draw a simple, connected, undirected, weighted graph with 8 vertices and16 edges, each with unique edge weights. Illustrate the execution of thePrim-Jarn \u00b4 \u0131k algorithm for computing the minimum spanning tree of this\ngraph.\nR-14.26 Repeat the previous problem for Kruskal\u2019s algorithm.\nR-14.27 There are eight small islands in a lake, and the state wants to build sevenbridges to connect them so that each island can be reached from any other\none via one or more bridges. The cost of constructing a bridge is propor-\ntional to its length. The distances between pairs of islands are given in thefollowing table.\n1 2345678\n1 - 240 210 340 280 200 345 120\n2 - - 265 175 215 180 185 155\n3 - - - 260 115 350 435 195\n4 - - - - 160 330 295 2305 - ---- 3 6 0 4 0 0 1 7 0\n6 - ----- 1 7 5 2 0 5\n7 - ------ 3 0 5\n8 - -------\nFind which bridges to build to minimize the total construction cost.", "14.8. Exercises 689\nR-14.28 Describe the meaning of the graphical conventions used in Figure 14.9\nillustrating a DFS traversal. What do the line thicknesses signify? What\ndo the arrows signify? How about dashed lines?\nR-14.29 Repeat Exercise R-14.28 for Figure 14.8 that illustrates a directed DFStraversal.\nR-14.30 Repeat Exercise R-14.28 for Figure 14.10 that illustrates a BFS traversal.\nR-14.31 Repeat Exercise R-14.28 for Figure 14.11 illustrating the Floyd-Warshallalgorithm.\nR-14.32 Repeat Exercise R-14.28 for Figure 14.13 that illustrates the topological\nsorting algorithm.\nR-14.33 Repeat Exercise R-14.28 for Figures 14.15 and 14.16 illustrating Dijkstra\u2019s\nalgorithm.\nR-14.34 Repeat Exercise R-14.28 for Figures 14.20 and 14.21 that illustrate thePrim-Jarn \u00b4 \u0131k algorithm.\nR-14.35 Repeat Exercise R-14.28 for Figures 14.22 through 14.24 that illustrate\nKruskal\u2019s algorithm.\nR-14.36 George claims he has a fast way to do path compression in a partition\nstructure, starting at a position p. He puts pinto a list L, and starts follow-\ning parent pointers. Each time he encounters a new position, q, he adds q\ntoLand updates the parent pointer of each node in Lto point to q\u2019s parent.\nShow that George\u2019s algorithm runs in \u03a9(h\n2)time on a path of length h.\nCreativity\nC-14.37 Give a Python implementation of the remove\n vertex(v) method for our\nadjacency map implementation of Section 14.2.5, making sure your im-plementation works for both directed and undirected graphs. Your methodshould run in O(deg (v))time.\nC-14.38 Give a Python implementation of the remove\nedge(e) method for our ad-\njacency map implementation of Section 14.2.5, making sure your imple-mentation works for both directed and undirected graphs. Your methodshould run in O(1)time.\nC-14.39 Suppose we wish to represent an n-vertex graph Gusing the edge list\nstructure, assuming that we identify the vertices with the integers in the set{0,1,..., n\u22121}. Describe how to implement the collection Eto support\nO(logn)-time performance for the get\nedge (u,v)method. How are you\nimplementing the method in this case?\nC-14.40 Let Tbe the spanning tree rooted at the start vertex produced by the depth-\n\ufb01rst search of a connected, undirected graph G. Argue why every edge of\nGnot in Tgoes from a vertex in Tto one of its ancestors, that is, it is a\nback edge .", "690 Chapter 14. Graph Algorithms\nC-14.41 Our solution to reporting a path from utovin Code Fragment 14.6 could\nbe made more ef\ufb01cient in practice if the DFS process ended as soon as v\nis discovered. Describe how to modify our code base to implement this\noptimization.\nC-14.42 Let Gbe an undirected graph Gwith nvertices and medges. Describe\nanO(n+m)-time algorithm for traversing each edge of Gexactly once in\neach direction.\nC-14.43 Implement an algorithm that returns a cycle in a directed graph /vectorG, if one\nexists.\nC-14.44 Write a function, components(g) , for undirected graph g, that returns a\ndictionary mapping each vertex to an integer that serves as an identi\ufb01er forits connected component. That is, two vertices should be mapped to the\nsame identi\ufb01er if and only if they are in the same connected component.\nC-14.45 Say that a maze is constructed correctly if there is one path from the start\nto the \ufb01nish, the entire maze is reachable from the start, and there are no\nloops around any portions of the maze. Given a maze drawn in an n\u00d7n\ngrid, how can we determine if it is constructed correctly? What is the\nrunning time of this algorithm?\nC-14.46 Computer networks should avoid single points of failure, that is, network\nvertices that can disconnect the network if they fail. We say an undi-rected, connected graph Gisbiconnected if it contains no vertex whose\nremoval would divide Ginto two or more connected components. Give an\nalgorithm for adding at most nedges to a connected graph G, with n\u22653\nvertices and m\u2265n\u22121 edges, to guarantee that Gis biconnected. Your\nalgorithm should run in O(n+m)time.\nC-14.47 Explain why all nontree edges are cross edges, with respect to a BFS treeconstructed for an undirected graph.\nC-14.48 Explain why there are no forward nontree edges with respect to a BFS tree\nconstructed for a directed graph.\nC-14.49 Show that if Tis a BFS tree produced for a connected graph G, then, for\neach vertex vat\nlevel i, the path of Tbetween sand vhas iedges, and any\nother path of Gbetween sand vhas at least iedges.\nC-14.50 Justify Proposition 14.16.\nC-14.51 Provide an implementation of the BFS algorithm that uses a FIFO queue,rather than a level-by-level formulation, to manage vertices that have been\ndiscovered until the time when their neighbors are considered.\nC-14.52 Ag r a p h Gisbipartite if its vertices can be partitioned into two sets Xand\nYsuch that every edge in Ghas one end vertex in Xand the other in Y.\nDesign and analyze an ef\ufb01cient algorithm for determining if an undirected\ngraph Gis bipartite (without knowing the sets Xand Yin advance).", "14.8. Exercises 691\nC-14.53 AnEuler tour of a directed graph /vectorGwith nvertices and medges is a\ncycle that traverses each edge of /vectorGexactly once according to its direction.\nSuch a tour always exists if /vectorGis connected and the in-degree equals the\nout-degree of each vertex in /vectorG. Describe an O(n+m)-time algorithm for\n\ufb01nding an Euler tour of such a directed graph /vectorG.\nC-14.54 A company named RT&T has a network of nswitching stations connected\nbymhigh-speed communication links. Each customer\u2019s phone is directly\nconnected to one station in his or her area. The engineers of RT&T have\ndeveloped a prototype video-phone system that allows two customers to\nsee each other during a phone call. In order to have acceptable imagequality, however, the number of links used to transmit video signals be-tween the two parties cannot exceed 4. Suppose that RT&T\u2019s network isrepresented by a graph. Design an ef\ufb01cient algorithm that computes, for\neach station, the set of stations it can reach using no more than 4 links.\nC-14.55 The time delay of a long-distance call can be determined by multiplyinga small \ufb01xed constant by the number of communication links on the tele-\nphone network between the caller and callee. Suppose the telephone net-work of a company named RT&T is a tree. The engineers of RT&T wantto compute the maximum possible time delay that may be experienced ina long-distance call. Given a tree T,t h e diameter ofTis the length of\na longest path between two nodes of T. Give an ef\ufb01cient algorithm for\ncomputing the diameter of T.\nC-14.56 Tamarindo University and many other schools worldwide are doing a jointproject on multimedia. A computer network is built to connect theseschools using communication links that form a tree. The schools decideto install a \ufb01le server at one of the schools to share data among all theschools. Since the transmission time on a link is dominated by the link\nsetup and synchronization, the cost of a data transfer is proportional to the\nnumber of links used. Hence, it is desirable to choose a \u201ccentral\u201d locationfor the \ufb01le server. Given a tree Tand a node vofT,t h e eccentricity ofv\nis the length of a longest path from vto any other node of T. A node of T\nwith minimum eccentricity is called a center ofT.\na. Design an ef\ufb01cient algorithm that, given an n-node tree T, computes\na center of T.\nb. Is the center unique? If not, how many distinct centers can a tree\nhave?\nC-14.57 Say that an n-vertex directed acyclic graph /vectorGiscom\n pact if there is some\nway of numbering the vertices of /vectorGwith the integers from 0 to n\u22121s u c h\nthat/vectorGcontains the edge (i,j)if and only if i<j,f o ra l l i,jin[0,n\u22121].\nGive an O(n2)-time algorithm for detecting if /vectorGis compact.", "692 Chapter 14. Graph Algorithms\nC-14.58 Let/vectorGbe a weighted directed graph with nvertices. Design a variation\nof Floyd-Warshall\u2019s algorithm for computing the lengths of the shortest\npaths from each vertex to every other vertex in O(n3)time.\nC-14.59 Design an ef\ufb01cient algorithm for \ufb01nding a longest directed path from a\nvertex sto a vertex tof an acyclic weighted directed graph /vectorG. Specify the\ngraph representation used and any auxiliary data structures used. Also,\nanalyze the time complexity of your algorithm.\nC-14.60 An independent set of an undirected graph G=(V,E)i sas u b s e t IofV\nsuch that no two vertices in Iare adjacent. That is, if uand vare in I,t h e n\n(u,v)is not in E.Amaximal independent set Mis an independent set\nsuch that, if we were to add any additional vertex to M, then it would not\nbe independent any more. Every graph has a maximal independent set.\n(Can you see this? This question is not part of the exercise, but it is worththinking about.) Give an ef\ufb01cient algorithm that computes a maximalindependent set for a graph G. What is this method\u2019s running time?\nC-14.61 Give an example of an n-vertex simple graph Gthat causes Dijkstra\u2019s\nalgorithm to run in \u03a9(n\n2logn)time when its implemented with a heap.\nC-14.62 Give an example of a weighted directed graph /vectorGwith negative-weight\nedges, but no negative-weight cycle, such that Dijkstra\u2019s algorithm incor-rectly computes the shortest-path distances from some start vertex s.\nC-14.63 Consider the following greedy strategy for \ufb01nding a shortest path fromvertex start to vertex goal in a given connected graph.\n1: Initialize path tostart.\n2: Initialize set visited to{start}.\n3: If start=goal ,r e t u r n path and exit. Otherwise, continue.\n4: Find the edge ( start,v ) of minimum weight such that vis adjacent to\nstart and vis not in visited .\n5: Add vtopath.\n6: Add vtovisited .\n7: Set start equal to va n dg ot os t e p3 .\nDoes this greedy strategy always \ufb01nd a shortest path from start togoal?\nEither explain intuitively why it works, or give a counterexample.\nC-14.64 Our implementation of shortest\npath\nlengths in Code Fragment 14.13 re-\nlies on use of \u201cin\ufb01nity\u201d as a numeric value, to represent the distance bound\nfor vertices that are not (yet) known to be reachable from the source.\nReimplement that function without such a sentinel, so that vertices, otherthan the source, are not added to the priority queue until it is evident thatthey are reachable.\nC-14.65 Show that if all the weights in a connected weighted graph Gare distinct,\nthen there is exactly one minimum spanning tree for G.", "14.8. Exercises 693\nC-14.66 An old MST method, called Bar\u02dauvka\u2019s algorithm , works as follows on a\ngraph Ghaving nvertices and medges with distinct weights:\nLet Tbe a subgraph of Ginitially containing just the vertices in V.\nwhile Thas fewer than n\u22121 edges do\nforeach connected component CiofTdo\nFind the lowest-weight edge (u,v)inEwith uinCiand vnot in\nCi.\nAdd (u,v)toT(unless it is already in T).\nreturn T\nProve that this algorithm is correct and that it runs in O(mlogn)time.\nC-14.67 Let Gbe a graph with nvertices and medges such that all the edge weights\ninGare integers in the range [1,n]. Give an algorithm for \ufb01nding a mini-\nmum spanning tree for GinO(mlog\u2217n)time.\nC-14.68 Consider a diagram of a telephone network, which is a graph Gwhose ver-\ntices represent switching centers, and whose edges represent communica-\ntion lines joining pairs of centers. Edges are marked by their bandwidth,and the bandwidth of a path is equal to the lowest bandwidth among thepath\u2019s edges. Give an algorithm that, given a network and two switch-\ning centers aand b, outputs the maximum bandwidth of a path between a\nand b.\nC-14.69 NASA wants to link nstations spread over the country using communica-\ntion channels. Each pair of stations has a different bandwidth available,\nwhich is known a priori. NASA wants to select n\u22121 channels (the mini-\nmum possible) in such a way that all the stations are linked by the channels\nand the total bandwidth (de\ufb01ned as the sum of the individual bandwidths\nof the channels) is maximum. Give an ef\ufb01cient algorithm for this prob-lem and determine its worst-case time complexity. Consider the weightedgraph G=(V,E),w h e r e Vis the set of stations and Eis the set of chan-\nnels between the stations. De\ufb01ne the weight w(e)of an edge einEas the\nbandwidth of the corresponding channel.\nC-14.70 Inside the Castle of Asymptopia there is a maze, and along each corridor\nof the maze there is a bag of gold coins. The amount of gold in each\nbag varies. A noble knight, named Sir Paul, will be given the opportunityto walk through the maze, picking up bags of gold. He may enter themaze only through a door marked \u201cENTER\u201d and exit through another\ndoor marked \u201cEXIT.\u201d While in the maze he may not retrace his steps.\nEach corridor of the maze has an arrow painted on the wall. Sir Paul mayonly go down the corridor in the direction of the arrow. There is no wayto traverse a \u201cloop\u201d in the maze. Given a map of the maze, including theamount of gold in each corridor, describe an algorithm to help Sir Paul\npick up the most gold.", "694 Chapter 14. Graph Algorithms\nC-14.71 Suppose you are given a timetable , which consists of:\n\u2022A setAofnairports, and for each airport ainA, a minimum con-\nnecting time c(a).\n\u2022A setFofm\ufb02ights, and the following, for each \ufb02ight finF:\n\u25e6Origin airport a1(f)inA\n\u25e6Destination airport a2(f)inA\n\u25e6Departure time t1(f)\n\u25e6Arrival time t2(f)\nDescribe an ef\ufb01cient algorithm for the \ufb02ight scheduling problem. In this\nproblem, we are given airports aand b, and a time t, and we wish to com-\npute a sequence of \ufb02ights that allows one to arrive at the earliest possible\ntime in bwhen departing from aat or after time t. Minimum connecting\ntimes at intermediate airports must be observed. What is the running timeof your algorithm as a function of nand m?\nC-14.72 Suppose we are given a directed graph /vectorGwith nvertices, and let Mbe the\nn\u00d7nadjacency matrix corresponding to /vectorG.\na. Let the product of Mwith itself ( M\n2) be de\ufb01ned, for 1 \u2264i,j\u2264n,a s\nfollows:\nM2(i,j)= M(i,1)\u2299M(1,j)\u2295\u00b7\u00b7\u00b7\u2295 M(i,n)\u2299M(n,j),\nwhere \u201c \u2295\u201d is the Boolean oroperator and \u201c \u2299\u201d is Boolean and.\nGiven this de\ufb01nition, what does M2(i,j)=1 imply about the ver-\ntices iand j?W h a ti f M2(i,j)=0?\nb. Suppose M4is the product of M2with itself. What do the entries of\nM4signify? How about the entries of M5=(M4)(M)? In general,\nwhat information is contained in the matrix Mp?\nc. Now suppose that /vectorGis weighted and assume the following:\n1: for 1 \u2264i\u2264n,M(i,i)=0.\n2: for 1 \u2264i,j\u2264n,M(i,j)= weight (i,j)if(i,j)is in E.\n3: for 1 \u2264i,j\u2264n,M(i,j)=\u221eif(i,j)is not in E.\nAlso, let M2be de\ufb01ned, for 1 \u2264i,j\u2264n, as follows:\nM2(i,j)=min{M(i,1)+ M(1,j),..., M(i,n)+ M(n,j)}.\nIfM2(i,j)= k, what may we conclude about the relationship be-\ntween vertices iand j?\nC-14.73 Karen has a new way to do path compression in a tree-based union/\ufb01ndpartition data structure starting at a position p. She puts all the positions\nthat are on the path from pto the root in a set S. Then she scans through\nSand sets the parent pointer of each position in Sto its parent\u2019s parent", "14.8. Exercises 695\npointer (recall that the parent pointer of the root points to itself). If this\npass changed the value of any position\u2019s parent pointer, then she repeats\nthis process, and goes on repeating this process until she makes a scan\nthrough Sthat does not change any position\u2019s parent value. Show that\nKaren\u2019s algorithm is correct and analyze its running time for a path oflength h.\nProjects\nP-14.74 Use an adjacency matrix to implement a class supporting a simpli\ufb01edgraph ADT that does not include update methods. Your class should in-clude a constructor method that takes two collections\u2014a collection Vof\nvertex elements and a collection Eof pairs of vertex elements\u2014and pro-\nduces the graph Gthat these two collections represent.\nP-14.75 Implement the simpli\ufb01ed graph ADT described in Project P-14.74, usingthe edge list structure.\nP-14.76 Implement the simpli\ufb01ed graph ADT described in Project P-14.74, using\nthe adjacency list structure.\nP-14.77 Extend the class of Project P-14.76 to support the update methods of the\ngraph ADT.\nP-14.78 Design an experimental comparison of repeated DFS traversals versusthe Floyd-Warshall algorithm for computing the transitive closure of adirected graph.\nP-14.79 Perform an experimental comparison of two of the minimum spanning\ntree algorithms discussed in this chapter (Kruskal and Prim-Jarn \u00b4 \u0131k). De-\nvelop an extensive set of experiments to test the running times of these\nalgorithms using randomly generated graphs.\nP-14.80 One way to construct a maze starts with an n\u00d7ngrid such that each grid\ncell is bounded by four unit-length walls. We then remove two boundary\nunit-length walls, to represent the start and \ufb01nish. For each remaining\nunit-length wall not on the boundary, we assign a random value and cre-ate a graph G, called the dual, such that each grid cell is a vertex in G\nand there is an edge joining the vertices for two cells if and only if the\ncells share a common wall. The weight of each edge is the weight of the\ncorresponding wall. We construct the maze by \ufb01nding a minimum span-ning tree Tfor Gand removing all the walls corresponding to edges in\nT. Write a program that uses this algorithm to generate mazes and then\nsolves them. Minimally, your program should draw the maze and, ideally,\nit should visualize the solution as well.", "696 Chapter 14. Graph Algorithms\nP-14.81 Write a program that builds the routing tables for the nodes in a computer\nnetwork, based on shortest-path routing, where path distance is measured\nby hop count, that is, the number of edges in a path. The input for thisproblem is the connectivity information for all the nodes in the network,as in the following example:\n241.12.31.14: 241 .12.31.15 241 .12.31.18 241 .12.31.19\nwhich indicates three network nodes that are connected to 241 .12.31.14,\nthat is, three nodes that are one hop away. The routing table for the node ataddress Ais a set of pairs (B,C), which indicates that, to route a message\nfrom AtoB, the next node to send to (on the shortest path from AtoB)\nisC. Your program should output the routing table for each node in the\nnetwork, given an input list of node connectivity lists, each of which isinput in the syntax as shown above, one per line.\nChapter Notes\nThe depth-\ufb01rst search method is a part of the \u201cfolklore\u201d of computer science, but Hopcroft\nand Tarjan [52, 94] are the ones who showed how useful this algorithm is for solving\nseveral different graph problems. Knuth [64] d iscusses the topological sorting problem.\nThe simple linear-time algorithm that we describ e for determining if a directed graph is\nstrongly connected is due to Kosaraju. The Floyd-Warshall algorithm appears in a paperby Floyd [38] and is based upon a theorem of Warshall [102].\nThe \ufb01rst known minimum spanning tree algorithm is due to Bar\u02da uvka [9], and was\npublished in 1926. The Prim-Jarn \u00b4 \u0131k algorithm was \ufb01rst published in Czech by Jarn \u00b4 \u0131k[ 5 5 ]\nin 1930 and in English in 1957 by Prim [85]. Kruskal published his minimum spanning\ntree algorithm in 1956 [67]. The reader interested in further study of the history of the\nminimum spanning tree problem is referred to th e paper by Graham and Hell [47]. The\ncurrent asymptotically fastest minimum sp anning tree algorithm is a randomized method\nof Karger, Klein, and Tarjan [57] that runs in O(m)expected time. Dijkstra [35] published\nhis single-source, shortest-path algorithm in 1959. The running time for the Prim-Jarn \u00b4 \u0131k\nalgorithm, and also that of Dijkstra\u2019s algorithm, can actually be improved to be O(nlogn+\nm)by implementing the queue Qwith either of two more sophisticated data structures, the\n\u201cFibonacci Heap\u201d [40] or the \u201cRelaxed Heap\u201d [37].\nTo learn about different algorithms for drawing graphs, please see the book chapter by\nTamassia and Liotta [92] and the book by Di Battista, Eades, Tamassia and Tollis [34]. The\nreader interested in further study of graph algorithms is referred to the books by Ahuja,\nMagnanti, and Orlin [7], Cormen, Leiserson, Rivest and Stein [29], Mehlhorn [77], and\nTarjan [95], and the book chapter by van Leeuwen [98].", "Chapter\n15Memory Management and B-Trees\nContents\n1 5 . 1M e m o r yM a n a g e m e n t..................... 6 9 8\n1 5 . 1 . 1M e m o r y A l l o c a t i o n .....................6 9 9\n1 5 . 1 . 2G a r b a g e C o l l e c t i o n .....................7 0 0\n15.1.3 Additional Memory Used by the Python Interpreter . . . . 703\n1 5 . 2M e m o r yH i e r a r c h i e sa n dC a c h i n g .............. 7 0 5\n1 5 . 2 . 1M e m o r y S y s t e m s ......................7 0 5\n1 5 . 2 . 2C a c h i n g S t r a t e g i e s .....................7 0 6\n1 5 . 3E x t e r n a lS e a r c h i n ga n dB - T r e e s ............... 7 1 1\n15.3.1 ( a,b) T r e e s ..........................7 1 2\n1 5 . 3 . 2B - T r e e s ...........................7 1 4\n1 5 . 4E x t e r n a l - M e m o r yS o r t i n g................... 7 1 5\n1 5 . 4 . 1M u l t i w a y M e r g i n g......................7 1 6\n1 5 . 5E x e r c i s e s ............................ 7 1 7\n", "698 Chapter 15. Memory Management and B-Trees\nOur study of data structures thus far has focused primarily upon the ef\ufb01ciency\nof computations, as measured by the number of primitive operations that are exe-\ncuted on a central processing unit (CPU). In practice, the performance of a com-puter system is also greatly impacted by the management of the computer\u2019s memorysystems. In our analysis of data structures, we have provided asymptotic bounds forthe overall amount of memory used by a data structure. In this chapter, we consider\nmore subtle issues involving the use of a computer\u2019s memory system.\nWe \ufb01rst discuss ways in which memory is allocated and deallocated during the\nexecution of a computer program, and the impact that this has on the program\u2019sperformance. Second, we discuss the complexity of multilevel memory hierarchies\nin today\u2019s computer systems. Although we often abstract a computer\u2019s memoryas consisting of a single pool of interchangeable locations, in practice, the dataused by an executing program is stored and transferred between a combinationof physical memories (e.g., CPU registers, caches, internal memory, and external\nmemory). We consider the use of classic data structures in the algorithms used to\nmanage memory, and how the use of memory hierarchies impacts the choice of\ndata structures and algorithms for classic problems such as searching and sorting.\n15.1 Memory Management\nIn order to implement any data structure on an actual computer, we need to use\ncomputer memory. Computer memory is organized into a sequence of words , each\nof which typically consists of 4, 8, or 16 bytes (depending on the computer). These\nmemory words are numbered from 0 to N\u22121, where Nis the number of mem-\nory words available to the computer. The number associated with each memoryword is known as its memory address . Thus, the memory in a computer can be\nviewed as basically one giant array of memory words. For example, in Figure 5.1\nof Section 5.2, we portrayed a section of the computer\u2019s memory as follows:\n2160 21452146214721482149215021512152215321542155215621572158 2144 2159\nIn order to run programs and store information, the computer\u2019s memory must\nbemanaged so as to determine what data is stored in what memory cells. In this\nsection, we discuss the basics of memory management, most notably describing the\nway in which memory is allocated to store new objects, the way in which portionsof memory are deallocated and reclaimed, when no longer needed, and the way in\nwhich the Python interpreter uses memory in completing its tasks.", "15.1. Memory Management 699\n15.1.1 Memory Allocation\nWith Python, all objects are stored in a pool of memory, known as the memory\nheap orPython heap (which should not be confused with the \u201cheap\u201d data structure\npresented in Chapter 9). When a command such as\nw=W i d g e t ( )\nis executed, assuming Widget is the name of a class, a new instance of the class\nis created and stored somewhere within the memory heap. The Python interpreter\nis responsible for negotiating the use of space with the operating system and formanaging the use of the memory heap when executing a Python program.\nThe storage available in the memory heap is divided into blocks , which are con-\ntiguous array-like \u201cchunks\u201d of memory that may be of variable or \ufb01xed sizes. The\nsystem must be implemented so that it can quickly allocate memory for new ob-\njects. One popular method is to keep contiguous \u201choles\u201d of available free memoryin a linked list, called the free list . The links joining these holes are stored inside\nthe holes themselves, since their memory is not being used. As memory is allocatedand deallocated, the collection of holes in the free lists changes, with the unused\nmemory being separated into disjoint holes divided by blocks of used memory. This\nseparation of unused memory into separate holes is known as fragmentation .T h e\nproblem is that it becomes more dif\ufb01cult to \ufb01nd large continuous chunks of mem-\nory, when needed, even though an equivalent amount of memory may be unused(yet fragmented). Therefore, we would like to minimize fragmentation as much as\npossible.\nThere are two kinds of fragmentation that can occur. Internal fragmentation\noccurs when a portion of an allocated memory block is unused. For example, a\nprogram may request an array of size 1000, but only use the \ufb01rst 100 cells of thisarray. There is not much that a run-time environment can do to reduce internalfragmentation. External fragmentation , on the other hand, occurs when there is a\nsigni\ufb01cant amount of unused memory between several contiguous blocks of allo-cated memory. Since the run-time environment has control over where to allocatememory when it is requested, the run-time environment should allocate memory ina way to try to reduce external fragmentation as much as reasonably possible.\nSeveral heuristics have been suggested for allocating memory from the heap\nso as to minimize external fragmentation. The best-\ufb01t algorithm searches the en-\ntire free list to \ufb01nd the hole whose size is closest to the amount of memory being\nrequested. The \ufb01rst-\ufb01t algorithm searches from the beginning of the free list for\nthe \ufb01rst hole that is large enough. The next-\ufb01t algorithm is similar, in that it also\nsearches the free list for the \ufb01rst hole that is large enough, but it begins its search\nfrom where it left off previously, viewing the free list as a circularly linked list(Section 7.2). The worst-\ufb01t algorithm searches the free list to \ufb01nd the largest hole\nof available memory, which might be done faster than a search of the entire free list", "700 Chapter 15. Memory Management and B-Trees\nif this list were maintained as a priority queue (Chapter 9). In each algorithm, the\nrequested amount of memory is subtracted from the chosen memory hole and the\nleftover part of that hole is returned to the free list.\nAlthough it might sound good at \ufb01rst, the best-\ufb01t algorithm tends to produce\nthe worst external fragmentation, since the leftover parts of the chosen holes tendto be small. The \ufb01rst-\ufb01t algorithm is fast, but it tends to produce a lot of external\nfragmentation at the front of the free list, which slows down future searches. The\nnext-\ufb01t algorithm spreads fragmentation more evenly throughout the memory heap,thus keeping search times low. This spreading also makes it more dif\ufb01cult to allo-cate large blocks, however. The worst-\ufb01t algorithm attempts to avoid this problem\nby keeping contiguous sections of free memory as large as possible.\n15.1.2 Garbage Collection\nIn some languages, like C and C++, the memory space for objects must be explic-\nitly deallocated by the programmer, which is a duty often overlooked by beginningprogrammers and is the source of frustrating programming errors even for experi-\nenced programmers. The designers of Python instead placed the burden of memory\nmanagement entirely on the interpreter. The process of detecting \u201cstale\u201d objects,deallocating the space devoted to those objects, and returning the reclaimed spaceto the free list is known as garbage collection .\nTo perform automated garbage collection, there must \ufb01rst be a way to detect\nthose objects that are no longer necessary. Since the interpreter cannot feasiblyanalyze the semantics of an arbitrary Python program, it relies on the followingconservative rule for reclaiming objects. In order for a program to access an object,it must have a direct or indirect reference to that object. We will de\ufb01ne such objects\nto be live objects . In de\ufb01ning a live object, a direct reference to an object is in the\nform of an identi\ufb01er in an active namespace (i.e., the global namespace, or the local\nnamespace for any active function). For example, immediately after the commandw=W i d g e t ( ) is executed, identi\ufb01er wwill be de\ufb01ned in the current namespace\nas a reference to the new widget object. We refer to all such objects with direct\nreferences as root objects .A n indirect reference to a live object is a reference\nthat occurs within the state of some other live object. For example, if the widget\ninstance in our earlier example maintains a list as an attribute, that list is also a liveobject (as it can be reached indirectly through use of identi\ufb01er w). The set of live\nobjects are de\ufb01ned recursively; thus, any objects that are referenced within the listthat is referenced by the widget are also classi\ufb01ed as live objects.\nThe Python interpreter assumes that live objects are the active objects currently\nbeing used by the running program; these objects should notbe deallocated. Other\nobjects can be garbage collected. Python relies on the following two strategies for\ndetermining which objects are live.", "15.1. Memory Management 701\nReference Counts\nWithin the state of every Python object is an integer known as its reference count .\nThis is the count of how many references to the object exist anywhere in the system.\nEvery time a reference is assigned to this object, its reference count is incremented,and every time one of those references is reassigned to something else, the reference\ncount for the former object is decremented. The maintenance of a reference count\nfor each object adds O(1)space per object, and the increments and decrements to\nthe count add O(1)additional computation time per such operations.\nThe Python interpreter allows a running program to examine an object\u2019s ref-\nerence count. Within the sysmodule there is a function named getrefcount that\nreturns an integer equal to the reference count for the object sent as a parameter. Itis worth noting that because the formal parameter of that function is assigned to theactual parameter sent by the caller, there is temporarily one additional reference to\nthat object in the local namespace of the function at the time the count is reported.\nThe advantage of having a reference count for each object is that if an object\u2019s\ncount is ever decremented to zero, that object cannot possibly be a live object andtherefore the system can immediately deallocate the object (or place it in a queue\nof objects that are ready to be deallocated).\nCycle Detection\nAlthough it is clear that an object with a reference count of zero cannot be a liveobject, it is important to recognize that an object with a nonzero reference countneed not qualify as live. There may exist a group of objects that have references to\neach other, even though none of those objects are reachable from a root object.\nFor example, a running Python program may have an identi\ufb01er, data,t h a ti sa\nreference to a sequence implemented using a doubly linked list. In this case, the\nlist referenced by data is a root object, the header and trailer nodes that are stored\nas attributes of the list are live objects, as are all the intermediate nodes of the list\nthat are indirectly referenced and all the elements that are referenced as elementsof those nodes. If the identi\ufb01er, data, were to go out of scope, or to be reassigned\nto some other object, the reference count for the list instance may go to zero and\nbe garbage collected, but the reference counts for all of the nodes would remain\nnonzero, stopping them from being garbage collected by the simple rule above.\nEvery so often, in particular when the available space in the memory heap is\nbecoming scarce, the Python interpreter uses a more advanced form of garbagecollection to reclaim objects that are unreachable, despite their nonzero referencecounts. There are different algorithms for implementing cycle detection. (Themechanics of garbage collection in Python are abstracted in the gcmodule, and\nmay vary depending on the implementation of the interpreter.) A classic algorithm\nfor garbage collection is the mark-sweep algorithm , which we next discuss.", "702 Chapter 15. Memory Management and B-Trees\nThe Mark-Sweep Algorithm\nIn the mark-sweep garbage collection algorithm, we associate a \u201cmark\u201d bit with\neach object that identi\ufb01es whether that object is live. When we determine at somepoint that garbage collection is needed, we suspend all other activity and clearthe mark bits of all the objects currently allocated in the memory heap. We then\ntrace through the active namespaces and we mark all the root objects as \u201clive.\u201d We\nmust then determine all the other live objects\u2014the ones that are reachable from the\nroot objects. To do this ef\ufb01ciently, we can perform a depth-\ufb01rst search (see Sec-\ntion 14.3.1) on the directed graph that is de\ufb01ned by objects reference other objects.In this case, each object in the memory heap is viewed as a vertex in a directed\ngraph, and the reference from one object to another is viewed as a directed edge.\nBy performing a directed DFS from each root object, we can correctly identify andmark each live object. This process is known as the \u201cmark\u201d phase. Once this pro-cess has completed, we then scan through the memory heap and reclaim any space\nthat is being used for an object that has not been marked. At this time, we can also\noptionally coalesce all the allocated space in the memory heap into a single block,thereby eliminating external fragmentation for the time being. This scanning andreclamation process is known as the \u201csweep\u201d phase, and when it completes, weresume running the suspended program. Thus, the mark-sweep garbage collec-\ntion algorithm will reclaim unused space in time proportional to the number of live\nobjects and their references plus the size of the memory heap.\nPerforming DFS In-Place\nThe mark-sweep algorithm correctly reclaims unused space in the memory heap,but there is an important issue we must face during the mark phase. Since we arereclaiming memory space at a time when available memory is scarce, we must take\ncare not to use extra space during the garbage collection itself. The trouble is that\nthe DFS algorithm, in the recursive way we have described it in Section 14.3.1, canuse space proportional to the number of vertices in the graph. In the case of garbagecollection, the vertices in our graph are the objects in the memory heap; hence, weprobably do not have this much memory to use. So our only alternative is to \ufb01nd a\nway to perform DFS in-place rather than recursively, that is, we must perform DFS\nusing only a constant amount of additional storage.\nThe main idea for performing DFS in-place is to simulate the recursion stack\nusing the edges of the graph (which in the case of garbage collection correspondto object references). When we traverse an edge from a visited vertex vto a new\nvertex w, we change the edge (v,w)stored in v\u2019s adjacency list to point back to v\u2019s\nparent in the DFS tree. When we return back to v(simulating the return from the\n\u201crecursive\u201d call at w), we can then switch the edge we modi\ufb01ed to point back to w,\nassuming we have some way to identify which edge we need to change back.", "15.1. Memory Management 703\n15.1.3 Additional Memory Used by the Python Interpreter\nWe have discussed, in Section 15.1.1, how the Python interpreter allocates memory\nfor objects within a memory heap. However, this is not the only memory that isused when executing a Python program. In this section, we discuss some otherimportant uses of memory.\nThe Run-Time Call Stack\nStacks have a most important application to the run-time environment of Python\nprograms. A running Python program has a private stack, known as the call stack\norPython interpreter stack , that is used to keep track of the nested sequence of\ncurrently active (that is, nonterminated) invocations of functions. Each entry of\nthe stack is a structure known as an activation record orframe , storing important\ninformation about an invocation of a function.\nAt the top of the call stack is the activation record of the running call ,t h a ti s ,\nthe function activation that currently has control of the execution. The remainingelements of the stack are activation records of the suspended calls , that is, func-\ntions that have invoked another function and are currently waiting for that other\nfunction to return control when it terminates. The order of the elements in the stack\ncorresponds to the chain of invocations of the currently active functions. When anew function is called, an activation record for that call is pushed onto the stack.When it terminates, its activation record is popped from the stack and the Python\ninterpreter resumes the processing of the previously suspended call.\nEach activation record includes a dictionary representing the local namespace\nfor the function call. (See Sections 1.10 and 2.5 for further discussion of name-\nspaces). The namespace maps identi\ufb01ers, which serve as parameters and local\nvariables, to object values, although the objects being referenced still reside in the\nmemory heap. The activation record for a function call also includes a reference tothe function de\ufb01nition itself, and a special variable, known as the program counter ,\nto maintain the address of the statement within the function that is currently exe-\ncuting. When one function returns control to another, the stored program counter\nfor the suspended function allows the interpreter to properly continue execution ofthat function.\nImplementing Recursion\nOne of the bene\ufb01ts of using a stack to implement the nesting of function calls isthat it allows programs to use recursion . That is, it allows a function to call it-\nself, as discussed in Chapter 4. We implicitly described the concept of the call\nstack and the use of activation records within our portrayal of recursion traces in", "704 Chapter 15. Memory Management and B-Trees\nthat chapter. Interestingly, early programming languages, such as Cobol and For-\ntran, did not originally use call stacks to implement function calls. But because of\nthe elegance and ef\ufb01ciency that recursion allows, almost all modern programminglanguages utilize a call stack for function calls, including the current versions ofclassic languages like Cobol and Fortran.\nEach box of a recursive trace corresponds to an activation record that is placed\non the call stack during the execution of a recursive function. At any point intime, the content of the call stack corresponds to the chain of boxes from the initialfunction invocation to the current one. To better illustrate how a call stack is usedby recursive functions, we refer back to the Python implementation of the classic\nrecursive de\ufb01nition of the factorial function,\nn!=n(n\u22121)(n\u22122)\u00b7\u00b7\u00b71,\nwith the code originally given in Code Fragment 4.1, and the recursive trace inFigure 4.1. The \ufb01rst time we call factorial , its activation record includes a name-\nspace storing the parameter value n. The function recursively calls itself to com-\npute (n\u22121)!, causing a new activation record, with its own namespace and param-\neter, to be pushed onto the call stack. In turn, this recursive invocation calls itself tocompute (n\u22122)!, and so on. The chain of recursive invocations, and thus the call\nstack, grows up to size n+1, with the most deeply nested call being factorial (0),\nwhich returns 1 without any further recursion. The run-time stack allows several\ninvocations of the factorial function to exist simultaneously. Each has an activation\nrecord that stores the value of its parameter, and eventually the value to be returned.\nWhen the \ufb01rst recursive call eventually terminates, it returns (n\u22121)!, which is then\nmultiplied by nto compute n! for the original call of the factorial method.\nThe Operand Stack\nInterestingly, there is actually another place where the Python interpreter uses astack. Arithmetic expressions, such as ((a+b)\u2217(c+d))/e, are evaluated by the\ninterpreter using an operand stack . In Section 8.5 we described how to evaluate an\narithmetic expression using a postorder traversal of an explicit expression tree. We\ndescribed that algorithm in a recursive way; however, this recursive description can\nbe simulated using a nonrecursive process that maintains an explicit operand stack.\nA simple binary operation, such as a+b, is computed by pushing aon the stack,\npushing bon the stack, and then calling an instruction that pops the top two items\nfrom the stack, performs the binary operation on them, and pushes the result backonto the stack. Likewise, instructions for writing and reading elements to and from\nmemory involve the use of popandpush methods for the operand stack.", "15.2. Memory Hierarchies and Caching 705\n15.2 Memory Hierarchies and Caching\nWith the increased use of computing in society, software applications must man-\nage extremely large data sets. Such applications include the processing of online\ufb01nancial transactions, the organization and maintenance of databases, and analy-\nses of customers\u2019 purchasing histories and preferences. The amount of data can be\nso large that the overall performance of algorithms and data structures sometimesdepends more on the time to access the data than on the speed of the CPU.\n15.2.1 Memory Systems\nIn order to accommodate large data sets, computers have a hierarchy of differ-\nent kinds of memories, which vary in terms of their size and distance from theCPU. Closest to the CPU are the internal registers that the CPU itself uses. Ac-\ncess to such locations is very fast, but there are relatively few such locations. At\nthe second level in the hierarchy are one or more memory caches . This memory\nis considerably larger than the register set of a CPU, but accessing it takes longer.At the third level in the hierarchy is the internal memory , which is also known as\nmain memory orcore memory . The internal memory is considerably larger than\nthe cache memory, but also requires more time to access. Another level in the hi-erarchy is the external memory , which usually consists of disks, CD drives, DVD\ndrives, and/or tapes. This memory is very large, but it is also very slow. Data storedthrough an external network can be viewed as yet another level in this hierarchy,\nwith even greater storage capacity, but even slower access. Thus, the memory hi-\nerarchy for computers can be viewed as consisting of \ufb01ve or more levels, each ofwhich is larger and slower than the previous level. (See Figure 15.1.) During theexecution of a program, data is routinely copied from one level of the hierarchy toa neighboring level, and these transfers can become a computational bottleneck.\nExternal Memory\nInternal Memory\nCaches\nRegisters\nCPUBiggerNetwork Storage Faster\nFigure 15.1: The memory hierarchy.", "706 Chapter 15. Memory Management and B-Trees\n15.2.2 Caching Strategies\nThe signi\ufb01cance of the memory hierarchy on the performance of a program de-\npends greatly upon the size of the problem we are trying to solve and the physical\ncharacteristics of the computer system. Often, the bottleneck occurs between two\nlevels of the memory hierarchy\u2014the one that can hold all data items and the leveljust below that one. For a problem that can \ufb01t entirely in main memory, the twomost important levels are the cache memory and the internal memory. Access timesfor internal memory can be as much as 10 to 100 times longer than those for cache\nmemory. It is desirable, therefore, to be able to perform most memory accesses\nin cache memory. For a problem that does not \ufb01t entirely in main memory, onthe other hand, the two most important levels are the internal memory and the ex-ternal memory. Here the differences are even more dramatic, for access times for\ndisks, the usual general-purpose external-memory device, are typically as much as\n100000 to 1000000 times longer than those for internal memory.\nTo put this latter \ufb01gure into perspective, imagine there is a student in Baltimore\nwho wants to send a request-for-money message to his parents in Chicago. If the\nstudent sends his parents an email message, it can arrive at their home computer\nin about \ufb01ve seconds. Think of this mode of communication as corresponding to\nan internal-memory access by a CPU. A mode of communication corresponding to\nan external-memory access that is 500 ,000 times slower would be for the student\nto walk to Chicago and deliver his message in person, which would take about a\nmonth if he can average 20 miles per day. Thus, we should make as few accessesto external memory as possible.\nMost algorithms are not designed with the memory hierarchy in mind, in spite\nof the great variance between access times for the different levels. Indeed, all ofthe algorithm analyses described in this book so far have assumed that all memoryaccesses are equal. This assumption might seem, at \ufb01rst, to be a great oversight\u2014\nand one we are only addressing now in the \ufb01nal chapter\u2014but there are good reasons\nwhy it is actually a reasonable assumption to make.\nOne justi\ufb01cation for this assumption is that it is often necessary to assume that\nall memory accesses take the same amount of time, since speci\ufb01c device-dependentinformation about memory sizes is often hard to come by. In fact, informationabout memory size may be dif\ufb01cult to get. For example, a Python program thatis designed to run on many different computer platforms cannot easily be de\ufb01ned\nin terms of a speci\ufb01c computer architecture con\ufb01guration. We can certainly use\narchitecture-speci\ufb01c information, if we have it (and we will show how to exploitsuch information later in this chapter). But once we have optimized our softwarefor a certain architecture con\ufb01guration, our software will no longer be device-independent. Fortunately, such optimizations are not always necessary, primarily\nbecause of the second justi\ufb01cation for the equal-time memory-access assumption.", "15.2. Memory Hierarchies and Caching 707\nCaching and Blocking\nAnother justi\ufb01cation for the memory-access equality assumption is that operating\nsystem designers have developed general mechanisms that allow most memory\naccesses to be fast. These mechanisms are based on two important locality-of-\nreference properties that most software possesses:\n\u2022Temporal locality : If a program accesses a certain memory location, then\nthere is increased likelihood that it accesses that same location again in the\nnear future. For example, it is common to use the value of a counter vari-\nable in several different expressions, including one to increment the counter\u2019s\nvalue. In fact, a common adage among computer architects is that a programspends 90 percent of its time in 10 percent of its code.\n\u2022Spatial locality : If a program accesses a certain memory location, then there\nis increased likelihood that it soon accesses other locations that are near thisone. For example, a program using an array may be likely to access the\nlocations of this array in a sequential or near-sequential manner.\nComputer scientists and engineers have performed extensive software pro\ufb01ling ex-\nperiments to justify the claim that most software possesses both of these kinds oflocality of reference. For example, a nested for loop used to repeatedly scan throughan array will exhibit both kinds of locality.\nTemporal and spatial localities have, in turn, given rise to two fundamental\ndesign choices for multilevel computer memory systems (which are present in theinterface between cache memory and internal memory, and also in the interfacebetween internal memory and external memory).\nThe \ufb01rst design choice is called virtual memory . This concept consists of pro-\nviding an address space as large as the capacity of the secondary-level memory, andof transferring data located in the secondary level into the primary level, when they\nare addressed. Virtual memory does not limit the programmer to the constraint of\nthe internal memory size. The concept of bringing data into primary memory is\ncalled caching , and it is motivated by temporal locality. By bringing data into pri-\nmary memory, we are hoping that it will be accessed again soon, and we will be\nable to respond quickly to all the requests for this data that come in the near future.\nThe second design choice is motivated by spatial locality. Speci\ufb01cally, if data\nstored at a secondary-level memory location /lscriptis accessed, then we bring into\nprimary-level memory a large block of contiguous locations that include the lo-\ncation /lscript. (See Figure 15.2.) This concept is known as blocking , and it is motivated\nby the expectation that other secondary-level memory locations close to /lscriptwill soon\nbe accessed. In the interface between cache memory and internal memory, such\nblocks are often called cache lines , and in the interface between internal memory\nand external memory, such blocks are often called pages .", "708 Chapter 15. Memory Management and B-Trees\nA block in the external memory address spaceA block on disk\n0 1 2 3 ... 1024 ... 2048 ...\nFigure 15.2: Blocks in external memory.\nWhen implemented with caching and blocking, virtual memory often allows\nus to perceive secondary-level memory as being faster than it really is. There is\nstill a problem, however. Primary-level memory is much smaller than secondary-\nlevel memory. Moreover, because memory systems use blocking, any programof substance will likely reach a point where it requests data from secondary-levelmemory, but the primary memory is already full of blocks. In order to ful\ufb01ll therequest and maintain our use of caching and blocking, we must remove some block\nfrom primary memory to make room for a new block from secondary memory in\nthis case. Deciding which block to evict brings up a number of interesting datastructure and algorithm design issues.\nCaching in Web Browsers\nFor motivation, we consider a related problem that arises when revisiting informa-tion presented in Web pages. To exploit temporal locality of reference, it is often\nadvantageous to store copies of Web pages in a cache memory, so these pages\ncan be quickly retrieved when requested again. This effectively creates a two-level\nmemory hierarchy, with the cache serving as the smaller, quicker internal memory,and the network being the external memory. In particular, suppose we have a cachememory that has m\u201cslots\u201d that can contain Web pages. We assume that a Web page\ncan be placed in any slot of the cache. This is known as a fully associative cache.\nAs a browser executes, it requests different Web pages. Each time the browser\nrequests such a Web page p, the browser determines (using a quick test) if pis\nunchanged and currently contained in the cache. If pis contained in the cache,\nthen the browser satis\ufb01es the request using the cached copy. If pis not in the\ncache, however, the page for pis requested over the Internet and transferred into\nthe cache. If one of the mslots in the cache is available, then the browser assigns\npto one of the empty slots. But if all the mcells of the cache are occupied, then\nthe computer must determine which previously viewed Web page to evict beforebringing in pto take its place. There are, of course, many different policies that can\nbe used to determine the page to evict.", "15.2. Memory Hierarchies and Caching 709\nPage Replacement Algorithms\nSome of the better-known page replacement policies include the following (see\nFigure 15.3):\n\u2022First-in, \ufb01rst-out (FIFO) : Evict the page that has been in the cache the\nlongest, that is, the page that was transferred to the cache furthest in the past.\n\u2022Least recently used (LRU) : Evict the page whose last request occurred fur-\nthest in the past.\nIn addition, we can consider a simple and purely random strategy:\n\u2022Random : Choose a page at random to evict from the cache.\nNew block Old block (chosen at random)\nRandom policy:\nNew block Old block (present longest)\nFIFO policy:\nNew block Old block (least recently used)\nLRU policy:insert time: 8:00am 9:05am 7:10am 7:30am 10:10am 8:45am 7:48am\nlast used: 7:25am 9:22am 6:50am 8:20am 10:02am 9:50am 8:12am\nFigure 15.3: The random, FIFO, and LRU page replacement policies.\nThe random strategy is one of the easiest policies to implement, for it only re-\nquires a random or pseudo-random number generator. The overhead involved inimplementing this policy is an O(1)additional amount of work per page replace-\nment. Moreover, there is no additional overhead for each page request, other than todetermine whether a page request is in the cache or not. Still, this policy makes no\nattempt to take advantage of any temporal locality exhibited by a user\u2019s browsing.", "710 Chapter 15. Memory Management and B-Trees\nThe FIFO strategy is quite simple to implement, as it only requires a queue\nQto store references to the pages in the cache. Pages are enqueued in Qwhen\nthey are referenced by a browser, and then are brought into the cache. When a\npage needs to be evicted, the computer simply performs a dequeue operation on Q\nto determine which page to evict. Thus, this policy also requires O(1)additional\nwork per page replacement. Also, the FIFO policy incurs no additional overhead\nfor page requests. Moreover, it tries to take some advantage of temporal locality.\nThe LRU strategy goes a step further than the FIFO strategy, for the LRU strat-\negy explicitly takes advantage of temporal locality as much as possible, by always\nevicting the page that was least-recently used. From a policy point of view, this is\nan excellent approach, but it is costly from an implementation point of view. That\nis, its way of optimizing temporal and spatial locality is fairly costly. Implement-ing the LRU strategy requires the use of an adaptable priority queue Qthat supports\nupdating the priority of existing pages. If Qis implemented with a sorted sequence\nbased on a linked list, then the overhead for each page request and page replace-ment is O(1). When we insert a page in Qor update its key, the page is assigned\nthe highest key in Qand is placed at the end of the list, which can also be done\ninO(1)time. Even though the LRU strategy has constant-time overhead, using\nthe implementation above, the constant factors involved, in terms of the additional\ntime overhead and the extra space for the priority queue Q, make this policy less\nattractive from a practical point of view.\nSince these different page replacement policies have different trade-offs be-\ntween implementation dif\ufb01culty and the degree to which they seem to take advan-\ntage of localities, it is natural for us to ask for some kind of comparative analysis\nof these methods to see which one, if any, is the best.\nFrom a worst-case point of view, the FIFO and LRU strategies have fairly\nunattractive competitive behavior. For example, suppose we have a cache con-\ntaining mpages, and consider the FIFO and LRU methods for performing page\nreplacement for a program that has a loop that repeatedly requests m+1 pages in\na cyclic order. Both the FIFO and LRU policies perform badly on such a sequence\nof page requests, because they perform a page replacement on every page request.\nThus, from a worst-case point of view, these policies are almost the worst we can\nimagine\u2014they require a page replacement on every page request.\nThis worst-case analysis is a little too pessimistic, however, for it focuses on\neach protocol\u2019s behavior for one bad sequence of page requests. An ideal analy-\nsis would be to compare these methods over all possible page-request sequences.\nOf course, this is impossible to do exhaustively, but there have been a great num-ber of experimental simulations done on page-request sequences derived from realprograms. Based on these experimental comparisons, the LRU strategy has beenshown to be usually superior to the FIFO strategy, which is usually better than the\nrandom strategy.", "15.3. External Searching and B-Trees 711\n15.3 External Searching and B-Trees\nConsider the problem of maintaining a large collection of items that does not \ufb01t in\nmain memory, such as a typical database. In this context, we refer to the secondary-memory blocks as disk blocks . Likewise, we refer to the transfer of a block between\nsecondary memory and primary memory as a disk transfer . Recalling the great\ntime difference that exists between main memory accesses and disk accesses, themain goal of maintaining such a collection in external memory is to minimize thenumber of disk transfers needed to perform a query or update. We refer to this\ncount as the I/O complexity of the algorithm involved.\nSome Ine\ufb03cient External-Memory Representations\nA typical operation we would like to support is the search for a key in a map. If wewere to store nitems unordered in a doubly linked list, searching for a particular\nkey within the list requires ntransfers in the worst case, since each link hop we\nperform on the linked list might access a different block of memory.\nWe can reduce the number of block transfers by using an array-based sequence.\nA sequential search of an array can be performed using only O(n/B)block transfers\nbecause of spatial locality of reference, where Bdenotes the number of elements\nthat \ufb01t into a block. This is because the block transfer when accessing the \ufb01rst\nelement of the array actually retrieves the \ufb01rst Belements, and so on with each\nsuccessive block. It is worth noting that the bound of O(n/B)transfers is only\nachieved when using a compact array representation (see Section 5.2.2). The\nstandard Python listclass is a referential container, and so even though the sequence\nof references are stored in an array, the actual elements that must be examinedduring a search are not generally stored sequentially in memory, resulting in n\ntransfers in the worst case.\nWe could alternately store a sequence using a sorted array. In this case, a search\nperforms O(log\n2n)transfers, via binary search, which is a nice improvement. But\nwe do not get signi\ufb01cant bene\ufb01t from block transfers because each query duringa binary search is likely in a different block of the sequence. As usual, update\noperations are expensive for a sorted array.\nSince these simple implementations are I/O inef\ufb01cient, we should consider the\nlogarithmic-time internal-memory strategies that use balanced binary trees (for ex-\nample, AVL trees or red-black trees) or other search structures with logarithmicaverage-case query and update times (for example, skip lists or splay trees). Typi-\ncally, each node accessed for a query or update in one of these structures will be in\na different block. Thus, these methods all require O(log\n2n)transfers in the worst\ncase to perform a query or update operation. But we can do better! We can performmap queries and updates using only O(log\nBn)= O(logn/logB)transfers.", "712 Chapter 15. Memory Management and B-Trees\n15.3.1 ( a,b) Trees\nTo reduce the number of external-memory accesses when searching, we can repre-\nsent our map using a multiway search tree (Section 11.5.1). This approach gives\nrise to a generalization of the (2,4)tree data structure known as the (a,b)tree.\nAn (a,b)tree is a multiway search tree such that each node has between aand\nbchildren and stores between a\u22121a n d b\u22121 entries. The algorithms for searching,\ninserting, and removing entries in an (a,b)tree are straightforward generalizations\nof the corresponding ones for (2,4)trees. The advantage of generalizing (2,4)trees\nto(a,b)trees is that a generalized class of trees provides a \ufb02exible search structure,\nwhere the size of the nodes and the running time of the various map operationsdepends on the parameters aand b. By setting the parameters aand bappropriately\nwith respect to the size of disk blocks, we can derive a data structure that achieves\ngood external-memory performance.\nDe\ufb01nition of an (a,b) Tree\nAn(a,b) tree , where parameters aand bare integers such that 2 \u2264a\u2264(b+1)/2,\nis a multiway search tree Twith the following additional restrictions:\nSize Property :Each internal node has at least achildren, unless it is the root, and\nhas at most bchildren.\nDepth Property :All the external nodes have the same depth.\nProposition 15.1: The height of an (a,b)tree storing nentries is \u03a9(logn/logb)\nand O(logn/loga).\nJusti\ufb01cation: Let Tbe an (a,b)tree storing nentries, and let hbe the height of\nT. We justify the proposition by establishing the following bounds on h:\n1\nlogblog (n+1)\u2264h\u22641\nlogalogn+1\n2+1.\nBy the size and depth properties, the number n/prime/primeof external nodes of Tis at least\n2ah\u22121and at most bh. By Proposition 11.7, n/prime/prime=n+1. Thus,\n2ah\u22121\u2264n+1\u2264bh.\nTaking the logarithm in base 2 of each term, we get\n(h\u22121)loga+1\u2264log (n+1)\u2264hlogb.\nAn algebraic manipulation of these inequalities completes the justi\ufb01cation.\n", "15.3. External Searching and B-Trees 713\nSearch and Update Operations\nWe recall that in a multiway search tree T, each node vofTholds a secondary\nstructure M(v), which is itself a map (Section 11.5.1). If Tis an (a,b)tree, then\nM(v)stores at most bentries. Let f(b)denote the time for performing a search\nin a map, M(v). The search algorithm in an (a,b)tree is exactly like the one for\nmultiway search trees given in Section 11.5.1. Hence, searching in an (a,b)tree T\nwith nentries takes O(f(b)\nlogalogn)time. Note that if bis considered a constant (and\nthus ais also), then the search time is O(logn).\nThe main application of (a,b)trees is for maps stored in external memory.\nNamely, to minimize disk accesses, we select the parameters aand bso that each\ntree node occupies a single disk block (so that f(b)=1 if we wish to simply count\nblock transfers). Providing the right aand bvalues in this context gives rise to\na data structure known as the B-tree, which we will describe shortly. Before we\ndescribe this structure, however, let us discuss how insertions and removals arehandled in (a,b)trees.\nThe insertion algorithm for an (a,b)tree is similar to that for a (2,4)tree.\nAn over\ufb02ow occurs when an entry is inserted into a b-node w, which becomes an\nillegal (b+1)-node. (Recall that a node in a multiway tree is a d-node if it has d\nchildren.) To remedy an over\ufb02ow, we split node wby moving the median entry of w\ninto the parent of wand replacing wwith a \u2308(b+1)/2\u2309-node w\n/primeand a\u230a(b+1)/2\u230b-\nnode w/prime/prime. We can now see the reason for requiring a\u2264(b+1)/2 in the de\ufb01nition\nof an (a,b)tree. Note that as a consequence of the split, we need to build the\nsecondary structures M(w/prime)and M(w/prime/prime).\nRemoving an entry from an (a,b)tree is similar to what was done for (2,4)\ntrees. An under\ufb02ow occurs when a key is removed from an a-node w, distinct from\nthe root, which causes wto become an illegal (a\u22121)-node. To remedy an under\ufb02ow,\nwe perform a transfer with a sibling of wthat is not an a-node or we perform a\nfusion of wwith a sibling that is an a-node. The new node w/primeresulting from the\nfusion is a (2a\u22121)-node, which is another reason for requiring a\u2264(b+1)/2.\nTable 15.1 shows the performance of a map realized with an (a,b)tree.\nOperation\n Running Time\nM[k]\n O/parenleftBig\nf(b)\nlogalogn/parenrightBig\nM[k] = v\n O/parenleftBig\ng(b)\nlogalogn/parenrightBig\ndelM[k]\n O/parenleftBig\ng(b)\nlogalogn/parenrightBig\nTable 15.1: Time bounds for an n-entry map realized by an (a,b)tree T. We assume\nthe secondary structure of the nodes of Tsupport search in f(b)time, and split and\nfusion operations in g(b)time, for some functions f(b)and g(b), which can be\nmade to be O(1)when we are only counting disk transfers.", "714 Chapter 15. Memory Management and B-Trees\n15.3.2 B-Trees\nA version of the (a,b)tree data structure, which is the best-known method for\nmaintaining a map in external memory, is called the \u201cB-tree.\u201d (See Figure 15.4.) A\nB-tree of order dis an (a,b)tree with a=\u2308d/2\u2309and b=d. Since we discussed\nthe standard map query and update methods for (a,b)trees above, we restrict our\ndiscussion here to the I/O complexity of B-trees.\n70 66 98 95 75 74 45 43 63 59 29 24 12 11 85 83 86 40 38 41 50 48 51 53 5637 22 58 46 80 72 9365 42\nFigure 15.4: A B-tree of order 6.\nAn important property of B-trees is that we can choose dso that the dchildren\nreferences and the d\u22121 keys stored at a node can \ufb01t compactly into a single disk\nblock, implying that dis proportional to B. This choice allows us to assume that a\nand bare also proportional to Bin the analysis of the search and update operations\non(a,b)trees. Thus, f(b)and g(b)are both O(1), for each time we access a node\nto perform a search or an update operation, we need only perform a single disktransfer.\nAs we have already observed above, each search or update requires that we\nexamine at most O(1)nodes for each level of the tree. Therefore, any map search\nor update operation on a B-tree requires only O(log\n\u2308d/2\u2309n),t h a ti s , O(logn/logB),\ndisk transfers. For example, an insert operation proceeds down the B-tree to locatethe node in which to insert the new entry. If the node would over\ufb02ow (to have d+1\nchildren) because of this addition, then this node is split into two nodes that have\n\u230a(d+1)/2\u230band\u2308(d+1)/2\u2309children, respectively. This process is then repeated\nat the next level up, and will continue for at most O(log\nBn)levels.\nLikewise, if a remove operation results in a node under\ufb02ow (to have \u2308d/2\u2309\u22121\nchildren), then we move references from a sibling node with at least \u2308d/2\u2309+1\nchildren or we perform a fusion operation of this node with its sibling (and repeat\nthis computation at the parent). As with the insert operation, this will continue up\nthe B-tree for at most O(logBn)levels. The requirement that each internal node\nhave at least \u2308d/2\u2309children implies that each disk block used to support a B-tree is\nat least half full. Thus, we have the following:\nProposition 15.2: A B-tree with nentries has I/O complexity O(logBn)for search\nor update operation, and uses O(n/B)blocks, where Bis the size of a block.", "15.4. External-Memory Sorting 715\n15.4 External-Memory Sorting\nIn addition to data structures, such as maps, that need to be implemented in external\nmemory, there are many algorithms that must also operate on input sets that are toolarge to \ufb01t entirely into internal memory. In this case, the objective is to solve the\nalgorithmic problem using as few block transfers as possible. The most classic\ndomain for such external-memory algorithms is the sorting problem.\nMultiway Merge-Sort\nAn ef\ufb01cient way to sort a set Sofnobjects in external memory amounts to a sim-\nple external-memory variation on the familiar merge-sort algorithm. The main idea\nbehind this variation is to merge many recursively sorted lists at a time, thereby\nreducing the number of levels of recursion. Speci\ufb01cally, a high-level descriptionof this multiway merge-sort method is to divide Sinto dsubsets S\n1,S2,...,Sdof\nroughly equal size, recursively sort each subset Si, and then simultaneously merge\nalldsorted lists into a sorted representation of S. If we can perform the merge pro-\ncess using only O(n/B)disk transfers, then, for large enough values of n, the total\nnumber of transfers performed by this algorithm satis\ufb01es the following recurrence:\nt(n)= d\u00b7t(n/d)+ cn/B,\nfor some constant c\u22651. We can stop the recursion when n\u2264B, since we can\nperform a single block transfer at this point, getting all of the objects into internalmemory, and then sort the set with an ef\ufb01cient internal-memory algorithm. Thus,\nthe stopping criterion for t(n)is\nt(n)=1i f n/B\u22641.\nThis implies a closed-form solution that t(n)isO((n/B)log\nd(n/B)),w h i c hi s\nO((n/B)log (n/B)/logd).\nThus, if we can choose dto be \u0398(M/B),w h e r e Mis the size of the internal memory,\nthen the worst-case number of block transfers performed by this multiway merge-\nsort algorithm will be quite low. For reasons given in the next section, we choose\nd=(M/B)\u22121.\nThe only aspect of this algorithm left to specify, then, is how to perform the d-way\nmerge using only O(n/B)block transfers.", "716 Chapter 15. Memory Management and B-Trees\n15.4.1 Multiway Merging\nIn a standard merge-sort (Section 12.2), the merge process combines two sorted\nsequences into one by repeatedly taking the smaller of the items at the front of thetwo respective lists. In a d-way merge, we repeatedly \ufb01nd the smallest among the\nitems at the front of the dsequences and place it as the next element of the merged\nsequence. We continue until all elements are included.\nIn the context of an external-memory sorting algorithm, if main memory has\nsize Mand each block has size B, we can store up to M/Bblocks within main\nmemory at any given time. We speci\ufb01cally choose d=(M/B)\u22121s ot h a tw ec a n\nafford to keep one block from each input sequence in main memory at any given\ntime, and to have one additional block to use as a buffer for the merged sequence.(See Figure 15.5.)\nQ 11\n13 16 19 3344 53 5660\n66 75\n72 78 8825 27 40 4341 49 50 57\n37 46 52 58\n35 48 51 5945 54 65 42 30 3912 24 26 34\n17 18 2978 1 0\nFigure 15.5: Ad-way merge with d=5a n d B=4. Blocks that currently reside in\nmain memory are shaded.\nWe maintain the smallest unprocessed element from each input sequence in\nmain memory, requesting the next block from a sequence when the preceding block\nhas been exhausted. Similarly, we use one block of internal memory to buffer themerged sequence, \ufb02ushing that block to external memory when full. In this way,the total number of transfers performed during a single d-way merge is O(n/B),\nsince we scan each block of list S\nionce, and we write out each block of the merged\nlist S/primeonce. In terms of computation time, choosing the smallest of dvalues can\ntrivially be performed using O(d)operations. If we are willing to devote O(d)\ninternal memory, we can maintain a priority queue identifying the smallest element\nfrom each sequence, thereby performing each step of the merge in O(logd)time\nby removing the minimum element and replacing it with the next element from the\nsame sequence. Hence, the internal time for the d-way merge is O(nlogd).\nProposition 15.3: Given an array-based sequence Sofnelements stored com-\npactly in external memory, we can sort Swith O((n/B)log (n/B)/log (M/B))block\ntransfers and O(nlogn)internal computations, where Mis the size of the internal\nmemory and Bis the size of a block.", "15.5. Exercises 717\n15.5 Exercises\nFor help with exercises, please visit the site, www.wiley.com/college/goodrich.\nReinforcement\nR-15.1 Julia just bought a new computer that uses 64-bit integers to address mem-\nory cells. Argue why Julia will never in her life be able to upgrade themain memory of her computer so that it is the maximum-size possible,assuming that you have to have distinct atoms to represent different bits.\nR-15.2 Describe, in detail, algorithms for adding an item to, or deleting an itemfrom, an (a,b)tree.\nR-15.3 Suppose Tis a multiway tree in which each internal node has at least \ufb01ve\nand at most eight children. For what values of aand bisTa valid (a,b)\ntree?\nR-15.4 For what values of dis the tree Tof the previous exercise an order- d\nB-tree?\nR-15.5 Consider an initially empty memory cache consisting of four pages. Howmany page misses does the LRU algorithm incur on the following pagerequest sequence: (2,3,4,1,2,5,1,3,5,4,1,2,3)?\nR-15.6 Consider an initially empty memory cache consisting of four pages. How\nmany page misses does the FIFO algorithm incur on the following page\nrequest sequence: (2,3,4,1,2,5,1,3,5,4,1,2,3)?\nR-15.7 Consider an initially empty memory cache consisting of four pages. What\nis\nthe maximum number of page misses that the random algorithm incurs\non the following page request sequence: (2,3,4,1,2,5,1,3,5,4,1,2,3)?\nShow all of the random choices the algorithm made in this case.\nR-15.8 Draw the result of inserting, into an initially empty order-7 B-tree, entrieswith keys (4,40,23,50,11,34,62,78,66,22,90,59,25,72,64,77,39,12),\nin this order.\nCreativity\nC-15.9 Describe an ef\ufb01cient external-memory algorithm for removing all the du-\nplicate entries in an array list of size n.\nC-15.10 Describe an external-memory data structure to implement the stack ADTso that the total number of disk transfers needed to process a sequence of\nkpush andpopoperations is O(k/B).", "718 Chapter 15. Memory Management and B-Trees\nC-15.11 Describe an external-memory data structure to implement the queue ADT\nso that the total number of disk transfers needed to process a sequence of\nkenqueue anddequeue operations is O(k/B).\nC-15.12 Describe an external-memory version of the PositionalList ADT (Sec-\ntion 7.4), with block size B, such that an iteration of a list of length nis\ncompleted using O(n/B)transfers in the worst case, and all other methods\nof the ADT require only O(1)transfers.\nC-15.13 Change the rules that de\ufb01ne red-black trees so that each red-black tree T\nhas a corresponding (4,8)tree, and vice versa.\nC-15.14 Describe a modi\ufb01ed version of the B-tree insertion algorithm so that eachtime we create an over\ufb02ow because of a split of a node w, we redistribute\nkeys among all of w\u2019s siblings, so that each sibling holds roughly the same\nnumber of keys (possibly cascading the split up to the parent of w). What\nis the minimum fraction of each block that will always be \ufb01lled using this\nscheme?\nC-15.15 Another possible external-memory map implementation is to use a skip\nlist, but to collect consecutive groups of O(B)nodes, in individual blocks,\non any level in the skip list. In particular, we de\ufb01ne an order-d B-skip\nlistto be such a representation of a skip list structure, where each block\ncontains at least \u2308d/2\u2309list nodes and at most dlist nodes. Let us also\nchoose din this case to be the maximum number of list nodes from a level\nof a skip list that can \ufb01t into one block. Describe how we should modify\nthe skip-list insertion and removal algorithms for a B-skip list so that the\nexpected height of the structure is O(logn/logB).\nC-15.16 Describe how to use a B-tree to implement the partition (union-\ufb01nd) ADT\n(from Section 14.7.3) so that the union and\ufb01nd operations\n each use at\nmost O(logn/logB)disk transfers.\nC-15.17 Suppose we are given a sequence Sofnelements with integer keys such\nthat some elements in Sare colored \u201cblue\u201d and some elements in Sare\ncolored \u201cred.\u201d In addition, say that a red element epairs with a blue\nelement fif they have the same key value. Describe an ef\ufb01cient external-\nmemory algorithm for \ufb01nding all the red-blue pairs in S.H o wm a n yd i s k\ntransfers does your algorithm perform?\nC-15.18 Consider the page caching problem where the memory cache can hold m\npages, and we are given a sequence Pofnrequests taken from a pool\nofm+1 possible pages. Describe the optimal strategy for the of\ufb02ine\nalgorithm and show that it causes at most m+n/mpage misses in total,\nstarting from an empty cache.\nC-15.19 Describe an ef\ufb01cient external-memory algorithm that determines whetheran array of nintegers contains a value occurring more than n/2 times.", "Chapter Notes 719\nC-15.20 Consider the page caching strategy based on the least frequently used\n(LFU) rule, where the page in the cache that has been accessed the least\noften is the one that is evicted when a new page is requested. If there areties, LFU evicts the least frequently used page that has been in the cachethe longest. Show that there is a sequence Pofnrequests that causes LFU\nto miss \u03a9(n)times for a cache of mpages, whereas the optimal algorithm\nwill miss only O(m)times.\nC-15.21 Suppose that instead of having the node-search function f(d)=1i na n\norder- dB-tree T,w eh a v e f(d)=logd. What does the asymptotic run-\nning time of performing a search in Tnow become?\nProjects\nP-15.22 Write a Python class that simulates the best-\ufb01t, worst-\ufb01t, \ufb01rst-\ufb01t, and next-\ufb01t algorithms for memory management. Determine experimentally whichmethod is the best under various sequences of memory requests.\nP-15.23 Write a Python class that implements all the methods of the ordered mapADT by means of an (a,b)tree, where aand bare integer constants passed\nas parameters to a constructor.\nP-15.24 Implement the B-tree data structure, assuming a block size of 1024 andinteger keys. Test the number of \u201cdisk transfers\u201d needed to process asequence of map operations.\nChapter Notes\nThe reader interested in the study of the architecture of hierarchical memory systems is\nreferred to the book chapter by Burger et al. [21] or the book by Hennessy and Patter-\nson [50]. The mark-sweep garbage collection method we describe is one of many different\nalgorithms for performing garbage collection. We encourage the reader interested in fur-ther study of garbage collection to examine the book by Jones and Lins [56]. Knuth [62]\nhas very nice discussions about external-memory sorting and searching, and Ullman [97]\ndiscusses external memory structures for database systems. The handbook by Gonnet and\nBaeza-Yates [44] compares the performance of a number of different sorting algorithms,\nmany of which are external-memory algorithms . B-trees were invented by Bayer and Mc-\nCreight [11] and Comer [28] provides a very nice overview of this data structure. Thebooks by Mehlhorn [76] and Samet [87] also have nice discussions about B-trees and their\nvariants. Aggarwal and Vitter [3] study the I/O complexity of sorting and related problems,establishing upper and lower bounds. Goodrich et al. [46] study the I/O complexity of\nseveral computational geometry problems. The reader interested in further study of I/O-\nef\ufb01cient algorithms is encouraged to ex amine the survey paper of Vitter [99].", "", "Appendix\nACharacter Strings in Python\nA string is a sequence of characters that come from some alphabet . In Python, the\nbuilt-in strclass represents strings based upon the Unicode international character\nset, a 16-bit character encoding that covers most written languages. Unicode is\nan extension of the 7-bit ASCII character set that includes the basic Latin alpha-bet, numerals, and common symbols. Strings are particularly important in most\nprogramming applications, as text is often used for input and output.\nA basic introduction to the strclass was provided in Section 1.2.3, including use\nof string literals, such as\nhello\n , and the syntax str(obj) that is used to construct\na string representation of a typical object. Common operators that are supported\nby strings, such as the use of +for concatenation, were further discussed in Sec-\ntion 1.3. This appendix serves as a more detailed reference, describing convenientbehaviors that strings support for the processing of text. To organize our overviewof the strclass behaviors, we group them into the following broad categories of\nfunctionality.\nSearching for Substrings\nThe operator syntax, pattern ins, can be used to determine if the given pattern\noccurs as a substring of string s. Table A.1 describes several related methods that\ndetermine the number of such occurrences, and the index at which the leftmost orrightmost such occurrence begins. Each of the functions in this table accepts two\noptional parameters, start andend, which are indices that effectively restrict the\nsearch to the implicit slice s[start:end] . For example, the call s.\ufb01nd(pattern, 5)\nrestricts the search to s[5: ].\nCalling Syntax\n Description\ns.count(pattern)\n Return the number of non-overlapping occurrences of pattern\ns.\ufb01nd(pattern)\n Return the index starting the leftmost occurrence of pattern; else -1\ns.index(pattern)\n Similar to \ufb01nd, but raise ValueError if not found\ns.r\ufb01nd(pattern)\n Return the index starting the rightmost occurrence of pattern; else -1\ns.rindex(pattern)\n Similar to r\ufb01nd , but raise ValueError if not found\nTable A.1: Methods that search for substrings.", "722 Appendix A. Character Strings in Python\nConstructing Related Strings\nStrings in Python are immutable, so none of their methods modify an existing string\ninstance. However, many methods return a newly constructed string that is closelyrelated to an existing one. Table A.2 provides a summary of such methods, includ-ing those that replace a given pattern with another, that vary the case of alphabetic\ncharacters, that produce a \ufb01xed-width string with desired justi\ufb01cation, and that pro-\nduce a copy of a string with extraneous characters stripped from either end.\nCalling Syntax\n Description\ns.replace(old, new)\n Return a copy of swith all occurrences of oldreplaced by new\ns.capitalize()\n Return a copy of swith its \ufb01rst character having uppercase\ns.upper()\n Return a copy of swith all alphabetic characters in uppercase\ns.lower()\n Return a copy of swith all alphabetic characters in lowercase\ns.center(width)\n Return a copy of s, padded to width, centered among spaces\ns.ljust(width)\n Return a copy of s, padded to width with trailing spaces\ns.rjust(width)\n Return a copy of s, padded to width with leading spaces\ns.z\ufb01ll(width)\n Return a copy of s, padded to width with leading zeros\ns.strip()\n Return a copy of s, with leading and trailing whitespace removed\ns.lstrip()\n Return a copy of s, with leading whitespace removed\ns.rstrip()\n Return a copy of s, with trailing whitespace removed\nTable A.2: String methods that produce related strings.\nSeveral of these methods accept optional parameters not detailed in the table.\nFor example, the replace method replaces all nonoverlapping occurrences of the old\npattern by default, but an optional parameter can limit the number of replacementsthat are performed. The methods that center or justify a text use spaces as the\ndefault \ufb01ll character when padding, but an alternate \ufb01ll character can be speci\ufb01ed\nas an optional parameter. Similarly, all variants of the strip methods remove leadingand trailing whitespace by default, but an optional string parameter designates thechoice of characters that should be removed from the ends.\nTesting Boolean Conditions\nTable A.3 includes methods that test for a Boolean property of a string, such as\nwhether it begins or ends with a pattern, or whether its characters qualify as be-\ning alphabetic, numeric, whitespace, etc. For the standard ASCII character set,alphabetic characters are the uppercase A\u2013Z, and lowercase a\u2013z, numeric digits are0\u20139, and whitespace includes the space character, tab character, newline, and car-riage return. Conventions for what are considered alphabetic and numeric character\ncodes are extended to more general Unicode character sets.", "Appendix A. Character Strings in Python 723\nCalling Syntax\n Description\ns.startswith(pattern)\n Return True ifpattern is a pre\ufb01x of string s\ns.endswith(pattern)\n Return True ifpattern is a suf\ufb01x of string s\ns.isspace()\n Return True if all characters of nonempty string are whitespace\ns.isalpha()\n Return True if all characters of nonempty string are alphabetic\ns.islower()\n Return True if there are one or more alphabetic characters,\nall of which are lowercased\ns.isupper()\n Return True if there are one or more alphabetic characters,\nall of which are uppercased\ns.isdigit()\n Return True if all characters of nonempty string are in 0\u20139\ns.isdecimal()\n Return True if all characters of nonempty string represent\ndigits 0\u20139, including Unicode equivalents\ns.isnumeric()\n Return True if all characters of nonempty string are numeric\nUnicode characters (e.g., 0\u20139, equivalents, fraction characters)\ns.isalnum()\n Return True if all characters of nonempty string are either\nalphabetic or numeric (as per above de\ufb01nitions)\nTable A.3: Methods that test Boolean properties of strings.\nSplitting and Joining Strings\nTable A.4 describes several important methods of Python\u2019s string class, used to\ncompose a sequence of strings together using a delimiter to separate each pair, orto take an existing string and determine a decomposition of that string based upon\nexistence of a given separating pattern.\nCalling Syntax\n Description\nsep.join(strings)\n Return the composition of the given sequence of strings,\ninserting sepas delimiter between each pair\ns.splitlines()\n Return a list of substrings of s, as delimited by newlines\ns.split(sep, count)\n Return a list of substrings of s, as delimited by the \ufb01rst count\noccurrences of sep.I fcount is not speci\ufb01ed, split on all\noccurrences. If sepis not speci\ufb01ed, use whitespace as delimiter.\ns.rsplit(sep, count)\n Similar to split, but using the rightmost occurrences of sep\ns.partition(sep)\n Return (head, sep, tail) such that s = head + sep + tail ,\nusing leftmost occurrence of sep, if any; else return (s,\n,\n)\ns.rpartition(sep)\n Return (head, sep, tail) such that s = head + sep + tail ,\nusing rightmost occurrence of sep, if any; else return (\n,\n,s )\nTable A.4: Methods for splitting and joining strings.\nThejoinmethod is used to assemble a string from a series of pieces. An exam-\nple of its usage is\n and\n .join([\n red\n,\ngreen\n ,\nblue\n ]), which produces the\nresult\n red and green and blue\n . Note well that spaces were embedded in the\nseparator string. In contrast, the command\n and\n.join([\n red\n,\ngreen\n ,\nblue\n ])\nproduces the result\n redandgreenandblue\n .", "724 Appendix A. Character Strings in Python\nThe other methods discussed in Table A.4 serve a dual purpose to join,a st h e y\nbegin with a string and produce a sequence of substrings based upon a given de-\nlimiter. For example, the call\n red and green and blue\n .split(\n and\n )pro-\nduces the result [\nred\n,\ngreen\n ,\nblue\n ]. If no delimiter (or None ) is speci\ufb01ed,\nsplit uses whitespace as a delimiter; thus,\n red and green and blue\n .split()\nproduces [\nred\n,\nand\n,\ngreen\n ,\nand\n,\nblue\n ].\nString Formatting\nTheformat method of the strclass composes a string that includes one or more for-\nmatted arguments. The method is invoked with a syntax s.format(arg0, arg1, ...) ,\nwhere sserves as a formatting string that expresses the desired result with one\nor more placeholders in which the arguments will be substituted. As a simple\nexample, the expression\n {} had a little {}\n .format(\n Mary\n ,\nlamb\n )pro-\nduces the result\n Mary had a little lamb\n . The pairs of curly braces in the\nformatting string are the placeholders for \ufb01elds that will be substituted into the\nresult. By default, the arguments sent to the function are substituted using posi-tional order; hence,\nMary\n was the \ufb01rst substitute and\n lamb\n the second. How-\never, the substitution patterns may be explicitly numbered to alter the order, orto use a single argument in more than one location. For example, the expres-sion\n{0}, {0}, {0} your {1}\n .format(\n row\n,\nboat\n )produces the result\nrow, row, row your boat\n .\nAll substitution patterns allow use of annotations to pad an argument to a par-\nticular width, using a choice of \ufb01ll character and justi\ufb01cation mode. An example ofsuch an annotation is\n{:-^20}\n .format(\n hello\n ). In this example, the hyphen\n(-) serves as a \ufb01ll character, the caret ( ^) designates a desire for the string to be\ncentered, and 20 is the desired width for the argument. This example results in\nthe string\n -------hello--------\n . By default, space is used as a \ufb01ll character\na n da ni m p l i e d <character dictates left-justi\ufb01cation; an explicit >character would\ndictate right-justi\ufb01cation.\nThere are additional formatting options for numeric types. A number will be\npadded with zeros rather than spaces if its width description is prefaced with azero. For example, a date can be formatted in traditional \u201cYYYY/MM/DD\u201d form\nas\n{}/{:02}/{:02}\n .format(year, month, day) . Integers can be converted to\nbinary, octal, or hexadecimal by respectively adding the character b,o,o rxas a\nsuf\ufb01x to the annotation. The displayed precision of a \ufb02oating-point number is spec-\ni\ufb01ed with a decimal point and the subsequent number of desired digits. For exam-\nple, the expression\n {:.3}.format(2/3)\n produces the string\n 0.667\n , rounded\nto three digits after the decimal point. A programmer can explicitly designate useof \ufb01xed-point representation (e.g.,\n0.667\n ) by adding the character fa sas u f \ufb01 x ,\nor scienti\ufb01c notation (e.g.,\n 6.667e-01\n ) by adding the character eas a suf\ufb01x.", "Appendix\nBUseful Mathematical Facts\nIn this appendix we give several useful mathematical facts. We begin with some\ncombinatorial de\ufb01nitions and facts.\nLogarithms and Exponents\nThe logarithm function is de\ufb01ned as\nlogba=c if a=bc.\nThe following identities hold for logarithms and exponents:\n1. logbac=logba+logbc\n2. logba/c=logba\u2212logbc\n3. logbac=clogba\n4. logba=(logca)/logcb\n5.blogca=alogcb\n6.(ba)c=bac\n7.babc=ba+c\n8.ba/bc=ba\u2212c\nIn addition, we have the following:\nProposition B.1: Ifa>0,b>0,a n d c>a+b,t h e n\nloga+logb<2log c\u22122.\nJusti\ufb01cation: It is enough to show that ab<c2/4. We can write\nab =a2+2ab+b2\u2212a2+2ab\u2212b2\n4\n=(a+b)2\u2212(a\u2212b)2\n4\u2264(a+b)2\n4<c2\n4.\nThenatural logarithm function ln x=logex,w h e r e e=2.71828 ...,i st h ev a l u e\nof the following progression:\ne=1+1\n1!+1\n2!+1\n3!+\u00b7\u00b7\u00b7.", "726 Appendix B. Useful Mathematical Facts\nIn addition,\nex=1+x\n1!+x2\n2!+x3\n3!+\u00b7\u00b7\u00b7\nln(1+x)= x\u2212x2\n2!+x3\n3!\u2212x4\n4!+\u00b7\u00b7\u00b7.\nThere are a number of useful inequalities relating to these functions (which\nderive from these de\ufb01nitions).\nProposition B.2: Ifx>\u22121,\nx\n1+x\u2264ln(1+x)\u2264x.\nProposition B.3: For0\u2264x<1,\n1+x\u2264ex\u22641\n1\u2212x.\nProposition B.4: For any two positive real numbers xand n,\n/parenleftBig\n1+x\nn/parenrightBign\n\u2264ex\u2264/parenleftBig\n1+x\nn/parenrightBign+x/2\n.\nInteger Functions and Relations\nThe \u201c\ufb02oor\u201d and \u201cceiling\u201d functions are de\ufb01ned respectively as follows:\n1.\u230ax\u230b=the largest integer less than or equal to x.\n2.\u2308x\u2309=the smallest integer greater than or equal to x.\nThemodulo operator is de\ufb01ned for integers a\u22650a n d b>0a s\namod b=a\u2212/floorleftBiga\nb/floorrightBig\nb.\nThefactorial function is de\ufb01ned as\nn!=1\u00b72\u00b73\u00b7\u00b7\u00b7\u00b7\u00b7 (n\u22121)n.\nThe binomial coef\ufb01cient is\n/parenleftbiggn\nk/parenrightbigg\n=n!\nk!(n\u2212k)!,\nwhich is equal to the number of different combinations one can de\ufb01ne by choosing\nkdifferent items from a collection of nitems (where the order does not matter).\nThe name \u201cbinomial coef\ufb01cient\u201d derives from the binomial expansion :\n(a+b)n=n\n\u2211\nk=0/parenleftbiggn\nk/parenrightbigg\nakbn\u2212k.\nWe also have the following relationships.", "Appendix B. Useful Mathematical Facts 727\nProposition B.5: If0\u2264k\u2264n,t h e n\n/parenleftBign\nk/parenrightBigk\n\u2264/parenleftbiggn\nk/parenrightbigg\n\u2264nk\nk!.\nProposition B.6 (Stirling\u2019s Approximation):\nn!=\u221a\n2\u03c0n/parenleftBign\ne/parenrightBign/parenleftbigg\n1+1\n12n+\u03b5(n)/parenrightbigg\n,\nwhere \u03b5(n)isO(1/n2).\nTheFibonacci progression is a numeric progression such that F0=0,F1=1,\nand Fn=Fn\u22121+Fn\u22122forn\u22652.\nProposition B.7: IfFnis de\ufb01ned by the Fibonacci progression, then Fnis\u0398(gn),\nwhere g=(1+\u221a\n5)/2is the so-called golden ratio .\nSummations\nThere are a number of useful facts about summations.\nProposition B.8: Factoring summations:\nn\n\u2211\ni=1af(i)= an\n\u2211\ni=1f(i),\nprovided adoes not depend upon i.\nProposition B.9: Reversing the order:\nn\n\u2211\ni=1m\n\u2211\nj=1f(i,j)=m\n\u2211\nj=1n\n\u2211\ni=1f(i,j).\nOne special form of is a telescoping sum :\nn\n\u2211\ni=1(f(i)\u2212f(i\u22121)) = f(n)\u2212f(0),\nwhich arises often in the amortized analysis of a data structure or algorithm.\nThe following are some other facts about summations that arise often in the\nanalysis of data structures and algorithms.Proposition B.10: \u2211\nn\ni=1i=n(n+1)/2.\nProposition B.11: \u2211ni=1i2=n(n+1)(2n+1)/6.", "728 Appendix B. Useful Mathematical Facts\nProposition B.12: Ifk\u22651is an integer constant, then\nn\n\u2211\ni=1ikis\u0398(nk+1).\nAnother common summation is the geometric sum, \u2211n\ni=0ai, for any \ufb01xed real\nnumber 0 <a/negationslash=1.\nProposition B.13:\nn\n\u2211\ni=0ai=an+1\u22121\na\u22121,\nfor any real number 0<a/negationslash=1.\nProposition B.14:\n\u221e\n\u2211\ni=0ai=1\n1\u2212a\nfor any real number 0<a<1.\nThere is also a combination of the two common forms, called the linear expo-\nnential summation, which has the following expansion:\nProposition B.15: For0<a/negationslash=1,a n d n\u22652,\nn\n\u2211\ni=1iai=a\u2212(n+1)a(n+1)+na(n+2)\n(1\u2212a)2.\nThe nthHarmonic number Hnis de\ufb01ned as\nHn=n\n\u2211\ni=11\ni.\nProposition B.16: IfHnis the nthharmonic number, then Hnislnn+\u0398(1).\nBasic Probability\nWe review some basic facts from probability theory. The most basic is that any\nstatement about a probability is de\ufb01ned upon a sample space S, which is de\ufb01ned\nas the set of all possible outcomes from some experiment. We leave the terms\n\u201coutcomes\u201d and \u201cexperiment\u201d unde\ufb01ned in any formal sense.\nExample B.17: Consider an experiment that consists of the outcome from \ufb02ip-\nping a coin \ufb01ve times. This sample space has 25different outcomes, one for each\ndifferent ordering of possible \ufb02ips that can occur.\nSample spaces can also be in\ufb01nite, as the following example illustrates.", "Appendix B. Useful Mathematical Facts 729\nExample B.18: Consider an experiment that consists of \ufb02ipping a coin until it\ncomes up heads. This sample space is in\ufb01nite, with each outcome being a sequence\nofitails followed by a single \ufb02ip that comes up heads, for i=1,2,3,....\nAprobability space is a sample space Stogether with a probability function\nPr that maps subsets of Sto real numbers in the interval [0,1]. It captures math-\nematically the notion of the probability of certain \u201cevents\u201d occurring. Formally,\neach subset AofSis called an event , and the probability function Pr is assumed to\npossess the following basic properties with respect to events de\ufb01ned from S:\n1. Pr (\u2205)=0.\n2. Pr (S)=1.\n3. 0\u2264Pr(A)\u22641, for any A\u2286S.\n4. If A,B\u2286Sand A\u2229B=\u2205,t h e nP r (A\u222aB)=Pr(A)+Pr(B).\nTwo events Aand Bareindependent if\nPr(A\u2229B)=Pr(A)\u00b7Pr(B).\nA collection of events {A1,A2,..., An}ismutually independent if\nPr(Ai1\u2229Ai2\u2229\u00b7\u00b7\u00b7\u2229 Aik)=Pr(Ai1)Pr(Ai2)\u00b7\u00b7\u00b7Pr(Aik).\nfor any subset {Ai1,Ai2,..., Aik}.\nTheconditional probability that an event Aoccurs, given an event B, is denoted\nas Pr (A|B), and is de\ufb01ned as the ratio\nPr(A\u2229B)\nPr(B),\nassuming that Pr (B)>0.\nAn elegant way for dealing with events is in terms of random variables . Intu-\nitively, random variables are variables whose values depend upon the outcome ofsome experiment. Formally, a random variable is a function Xthat maps outcomes\nfrom some sample space Sto real numbers. An indicator random variable is a\nrandom variable that maps outcomes to the set {0,1}. Often in data structure and\nalgorithm analysis we use a random variable Xto characterize the running time of\na randomized algorithm. In this case, the sample space Sis de\ufb01ned by all possible\noutcomes of the random sources used in the algorithm.\nWe are most interested in the typical, average, or \u201cexpected\u201d value of such a\nrandom variable. The expected value of a random variable Xis de\ufb01ned as\nE(X)=\n\u2211\nxxPr(X=x),\nwhere the summation is de\ufb01ned over the range of X(which in this case is assumed\nto be discrete).", "730 Appendix B. Useful Mathematical Facts\nProposition B.19 (The Linearity of Expectation): Let Xand Ybe two ran-\ndom variables and let cbe a number. Then\nE(X+Y)= E(X)+ E(Y) and E(cX)= cE(X).\nExample B.20: Let Xbe a random variable that assigns the outcome of the roll\nof two fair dice to the sum of the number of dots showing. Then E(X)=7.\nJusti\ufb01cation: To justify this claim, let X1and X2be random variables corre-\nsponding to the number of dots on each die. Thus, X1=X2(i.e., they are two\ninstances of the same function) and E(X)=E(X1+X2)=E(X1)+E(X2). Each\noutcome of the roll of a fair die occurs with probability 1/6. Thus,\nE(Xi)=1\n6+2\n6+3\n6+4\n6+5\n6+6\n6=7\n2,\nfori=1,2. Therefore, E(X)=7.\nTwo random variables Xand Yareindependent if\nPr(X=x|Y=y)=Pr(X=x),\nfor all real numbers xand y.\nProposition B.21: If two random variables Xand Yare independent, then\nE(XY )=E(X)E(Y).\nExample B.22: Let Xbe a random variable that assigns the outcome of a roll of\ntwo fair dice to the product of the number of dots showing. Then E(X)=49/4.\nJusti\ufb01cation: Let X1and X2be random variables denoting the number of dots\non each die. The variables X1and X2are clearly independent; hence\nE(X)=E(X1X2)=E(X1)E(X2)=( 7/2)2=49/4.\nThe following bound and corollaries that follow from it are known as Chernoff\nbounds .\nProposition B.23: Let Xbe the sum of a \ufb01nite number of independent 0/1ran-\ndom variables and let \u03bc>0be the expected value of X. Then, for \u03b4>0,\nPr(X>(1+\u03b4)\u03bc)</bracketleftBigg\ne\u03b4\n(1+\u03b4)(1+\u03b4)/bracketrightBigg\u03bc\n.", "Appendix B. Useful Mathematical Facts 731\nUseful Mathematical Techniques\nTo compare the growth rates of different functions, it is sometimes helpful to apply\nthe following rule.\nProposition B.24 (L\u2019H\u02c6 opital\u2019s Rule): If we have lim n\u2192\u221e f(n)=+\u221eand we\nhave lim n\u2192\u221e g(n)=+\u221e,t h e n lim n\u2192\u221e f(n)/g(n)=lim n\u2192\u221e f/prime(n)/g/prime(n),w h e r e\nf/prime(n)and g/prime(n)respectively denote the derivatives of f(n)and g(n).\nIn deriving an upper or lower bound for a summation, it is often useful to split\na summation as follows:\nn\n\u2211\ni=1f(i)=j\n\u2211\ni=1f(i)+n\n\u2211\ni=j+1f(i).\nAnother useful technique is to bound a sum by an integral .I f fis a nonde-\ncreasing function, then, assuming the following terms are de\ufb01ned,\nZb\na\u22121f(x)dx\u2264b\n\u2211\ni=af(i)\u2264Zb+1\naf(x)dx.\nThere is a general form of recurrence relation that arises in the analysis of\ndivide-and-conquer algorithms:\nT(n)= aT(n/b)+ f(n),\nfor constants a\u22651a n d b>1.\nProposition B.25: Let T(n)be de\ufb01ned as above. Then\n1.Iff(n)isO(nlogba\u2212\u03b5), for some constant \u03b5>0,t h e n T(n)is\u0398(nlogba).\n2.Iff(n)is\u0398(nlogbalogkn), for a \ufb01xed nonnegative integer k\u22650,t h e n T(n)is\n\u0398(nlogbalogk+1n).\n3.Iff(n)is\u03a9(nlogba+\u03b5), for some constant \u03b5>0,a n di f af(n/b)\u2264cf(n),t h e n\nT(n)is\u0398(f(n)).\nThis proposition is known as the master method for characterizing divide-and-\nconquer recurrence relations asymptotically.", "Bibliography\n[1] H. Abelson, G. J. Sussman, and J. Sussman, Structure and Interpretation of Com-\nputer Programs . Cambridge, MA: MIT Press, 2nd ed., 1996.\n[2] G. M. Adel\u2019son-Vel\u2019skii and Y . M. Landis, \u201cAn algorithm for the organization of\ninformation,\u201d Doklady Akademii Nauk SSSR , vol. 146, pp. 263\u2013266, 1962. English\ntranslation in Soviet Math. Dokl. ,3, 1259\u20131262.\n[3] A. Aggarwal and J. S. Vitter, \u201cThe input/output complexity of sorting and related\nproblems,\u201d Commun. ACM , vol. 31, pp. 1116\u20131127, 1988.\n[4] A. V . Aho, \u201cAlgorithms for \ufb01nding patterns in strings,\u201d in Handbook of Theoreti-\ncal Computer Science (J. van Leeuwen, ed.), vol. A. Algorithms and Complexity,\npp. 255\u2013300, Amsterdam: Elsevier, 1990.\n[5] A. V . Aho, J. E. Hopcroft, and J. D. Ullman, The Design and Analysis of Computer\nAlgorithms . Reading, MA: Addison-Wesley, 1974.\n[6] A. V . Aho, J. E. Hopcroft, and J. D. Ullman, Data Structures and Algorithms . Read-\ning, MA: Addison-Wesley, 1983.\n[ 7 ] R .K .A h u j a ,T .L .M a g n a n t i ,a n dJ .B .O r l i n , Network Flows: Theory, Algorithms,\nand Applications . Englewood Cliffs, NJ: Prentice Hall, 1993.\n[8] R. Baeza-Yates and B. Ribeiro-Neto, Modern Information Retrieval . Reading, MA:\nAddison-Wesley, 1999.\n[9] O. Bar\u02da uvka, \u201cO jistem problemu minimalnim,\u201d Praca Moravske Prirodovedecke\nSpolecnosti , vol. 3, pp. 37\u201358, 1926. (in Czech).\n[10] R. Bayer, \u201cSymmetric binary B-trees: Data structure and maintenance,\u201d Acta Infor-\nmatica , vol. 1, no. 4, pp. 290\u2013306, 1972.\n[11] R. Bayer and McCreight, \u201cOrganization of large ordered indexes,\u201d Acta Inform. ,\nvol. 1, pp. 173\u2013189, 1972.\n[12] D. M. Beazley, Python Essential Reference . Addison-Wesley Professional, 4th ed.,\n2009.\n[13] R. E. Bellman, Dynamic Programming . Princeton, NJ: Princeton University Press,\n1957.\n[14] J. L. Bentley, \u201cProgramming pearls: Writing correct programs,\u201d Communications of\nthe ACM , vol. 26, pp. 1040\u20131045, 1983.\n[15] J. L. Bentley, \u201cProgramming pearls: Thanks, heaps,\u201d Communications of the ACM ,\nvol. 28, pp. 245\u2013250, 1985.\n[16] J. L. Bentley and M. D. McIlroy, \u201cEngineering a sort function,\u201d Software\u2014Practice\nand Experience , vol. 23, no. 11, pp. 1249\u20131265, 1993.\n[17] G. Booch, Object-Oriented Analysis and Design with Applications . Redwood City,\nCA: Benjamin/Cummings, 1994.", "Bibliography 733\n[18] R. S. Boyer and J. S. Moore, \u201cA fast string searching algorithm,\u201d Communications\nof the ACM , vol. 20, no. 10, pp. 762\u2013772, 1977.\n[19] G. Brassard, \u201cCrusade for a better notation,\u201d SIGACT News , vol. 17, no. 1, pp. 60\u2013\n64, 1985.\n[20] T. Budd, An Introduction to Object-Oriented Programming . Reading, MA: Addison-\nWesley, 1991.\n[21] D. Burger, J. R. Goodman, and G. S. Sohi, \u201cMemory systems,\u201d in The Computer\nScience and Engineering Handbook (A. B. Tucker, Jr., ed.), ch. 18, pp. 447\u2013461,\nCRC Press, 1997.\n[22] J. Campbell, P. Gries, J. Montojo, and G. Wilson, Practical Programming: An In-\ntroduction to Computer Science . Pragmatic Bookshelf, 2009.\n[23] L. Cardelli and P. Wegner, \u201cOn understanding types, data abstraction and polymor-\nphism,\u201d ACM Computing Surveys , vol. 17, no. 4, pp. 471\u2013522, 1985.\n[24] S. Carlsson, \u201cAverage case results on heapsort,\u201d BIT, vol. 27, pp. 2\u201317, 1987.\n[25] V . Cedar, The Quick Python Book . Manning Publications, 2nd ed., 2010.\n[26] K. L. Clarkson, \u201cLinear programming in O(n3d2)time,\u201d Inform. Process. Lett. ,\nvol. 22, pp. 21\u201324, 1986.\n[27] R. Cole, \u201cTight bounds on the complexity of the Boyer-Moore pattern matching\nalgorithm,\u201d SIAM J. Comput. , vol. 23, no. 5, pp. 1075\u20131091, 1994.\n[28] D. Comer, \u201cThe ubiquitous B-tree,\u201d ACM Comput. Surv. , vol. 11, pp. 121\u2013137, 1979.\n[29] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algo-\nrithms . Cambridge, MA: MIT Press, 3rd ed., 2009.\n[30] M. Crochemore and T. Lecroq, \u201cPattern matching and text compression algorithms,\u201d\ninThe Computer Science and Engineering Handbook (A. B. Tucker, Jr., ed.), ch. 8,\npp. 162\u2013202, CRC Press, 1997.\n[31] S. Crosby and D. Wallach, \u201cDenial of servi ce via algorithmic complexity attacks,\u201d\ninProc. 12th Usenix Security Symp. , pp. 29\u201344, 2003.\n[32] M. Dawson, Python Programming for the Absolute Beginner . Course Technology\nPTR, 3rd ed., 2010.\n[33] S. A. Demurjian, Sr., \u201cSoftware design,\u201d in The Computer Science and Engineering\nHandbook (A. B. Tucker, Jr., ed.), ch. 108, pp. 2323\u20132351, CRC Press, 1997.\n[34] G. Di Battista, P. Eades, R. Tamassia, and I. G. Tollis, Graph Drawing. Upper Saddle\nRiver, NJ: Prentice Hall, 1999.\n[35] E. W. Dijkstra, \u201cA note on two problems in connexion with graphs,\u201d Numerische\nMathematik , vol. 1, pp. 269\u2013271, 1959.\n[36] E. W. Dijkstra, \u201cRecursive programming,\u201d Numerische Mathematik , vol. 2, no. 1,\npp. 312\u2013318, 1960.\n[37] J. R. Driscoll, H. N. Gabow, R. Shrairaman, and R. E. Tarjan, \u201cRelaxed heaps: An\nalternative to Fibonacci heaps with applications to parallel computation,\u201d Commun.\nACM , vol. 31, pp. 1343\u20131354, 1988.\n[38] R. W. Floyd, \u201cAlgorithm 97: Shortest path,\u201d Communications of the ACM ,v o l .5 ,\nno. 6, p. 345, 1962.\n[39] R. W. Floyd, \u201cAlgorithm 245: Treesort 3,\u201d Communications of the ACM ,v o l .7 ,\nno. 12, p. 701, 1964.\n[40] M. L. Fredman and R. E. Tarjan, \u201cFibonacci heaps and their uses in improved net-\nwork optimization algorithms,\u201d J. ACM , vol. 34, pp. 596\u2013615, 1987.", "734 Bibliography\n[41] E. Gamma, R. Helm, R. Johnson, and J. Vlissides, Design Patterns: Elements of\nReusable Object-Oriented Software . Reading, MA: Addison-Wesley, 1995.\n[42] A. Goldberg and D. Robson, Smalltalk-80: The Language . Reading, MA: Addison-\nWesley, 1989.\n[43] M. H. Goldwasser and D. Letscher, Object-Oriented Programming in Python . Upper\nSaddle River, NJ: Prentice Hall, 2008.\n[44] G. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures in\nPascal and C . Reading, MA: Addison-Wesley, 1991.\n[45] G. H. Gonnet and J. I. Munro, \u201cHeaps on heaps,\u201d SIAM J. Comput. , vol. 15, no. 4,\npp. 964\u2013971, 1986.\n[46] M. T. Goodrich, J.-J. Tsay, D. E. Vengroff, and J. S. Vitter, \u201cExternal-memory\ncomputational geometry,\u201d in Proc. 34th Annu. IEEE Sympos. F ound. Comput. Sci. ,\npp. 714\u2013723, 1993.\n[47] R. L. Graham and P. Hell, \u201cOn the history of the minimum spanning tree problem,\u201d\nAnnals of the History of Computing , vol. 7, no. 1, pp. 43\u201357, 1985.\n[48] L. J. Guibas and R. Sedgewick, \u201cA dichromatic framework for balanced trees,\u201d in\nProc. 19th Annu. IEEE Sympos. F ound. Comput. Sci. , Lecture Notes Comput. Sci.,\npp. 8\u201321, Springer-Verlag, 1978.\n[49] Y . Gurevich, \u201cWhat does O(n)mean?,\u201d SIGACT News , vol. 17, no. 4, pp. 61\u201363,\n1986.\n[50] J. Hennessy and D. Patterson, Computer Architecture: A Quantitative Approach .\nSan Francisco: Morgan Kaufmann, 2nd ed., 1996.\n[51] C. A. R. Hoare, \u201cQuicksort,\u201d The Computer Journal , vol. 5, pp. 10\u201315, 1962.\n[52] J. E. Hopcroft and R. E. Tarjan, \u201cEf\ufb01cient algorithms for graph manipulation,\u201d Com-\nmunications of the ACM , vol. 16, no. 6, pp. 372\u2013378, 1973.\n[53] B.-C. Huang and M. Langston, \u201cPractical in-place merging,\u201d Communications of the\nACM , vol. 31, no. 3, pp. 348\u2013352, 1988.\n[54] J. J\u00b4 aJ\u00b4 a, An Introduction to Parallel Algorithms . Reading, MA: Addison-Wesley,\n1992.\n[55] V . Jarn \u00b4 \u0131k, \u201cO jistem problemu minimalnim,\u201d Praca Moravske Prirodovedecke\nSpolecnosti , vol. 6, pp. 57\u201363, 1930. (in Czech).\n[56] R. Jones and R. Lins, Garbage Collection: Algorithms for Automatic Dynamic Mem-\nory Management . John Wiley and Sons, 1996.\n[57] D. R. Karger, P. Klein, and R. E. Tarjan, \u201cA randomized linear-time algorithm to \ufb01nd\nminimum spanning trees,\u201d Journal of the ACM , vol. 42, pp. 321\u2013328, 1995.\n[58] R. M. Karp and V . Ramachandran, \u201cParallel algorithms for shared memory ma-\nchines,\u201d in Handbook of Theoretical Computer Science (J. van Leeuwen, ed.),\npp. 869\u2013941, Amsterdam: Elsevier/The MIT Press, 1990.\n[59] P. Kirschenhofer and H. Prodinger, \u201cThe pat h length of random skip lists,\u201d Acta\nInformatica , vol. 31, pp. 775\u2013792, 1994.\n[60] J. Kleinberg and \u00b4E. Tardos, Algorithm Design . Reading, MA: Addison-Wesley,\n2006.\n[61] A. Klink and J. W\u00a8 alde, \u201cEf\ufb01cient denial of service attacks on web application plat-\nforms.\u201d 2011.\n[62] D. E. Knuth, Sorting and Searching ,v o l .3o f The Art of Computer Programming .\nR\neading, MA: Addison-Wesley, 1973.", "Bibliography 735\n[63] D. E. Knuth, \u201cBig omicron and big omega and big theta,\u201d in SIGACT News ,v o l .8 ,\npp. 18\u201324, 1976.\n[64] D. E. Knuth, Fundamental Algorithms , vol. 1 of The Art of Computer Programming .\nReading, MA: Addison-Wesley, 3rd ed., 1997.\n[65] D. E. Knuth, Sorting and Searching ,v o l .3o f The Art of Computer Programming .\nReading, MA: Addison-Wesley, 2nd ed., 1998.\n[66] D. E. Knuth, J. H. Morris, Jr., and V . R. Pratt, \u201cFast pattern matching in strings,\u201d\nSIAM J. Comput. , vol. 6, no. 1, pp. 323\u2013350, 1977.\n[67] J. B. Kruskal, Jr., \u201cOn the shortest spanning subtree of a graph and the traveling\nsalesman problem,\u201d Proc. Amer . Math. Soc. , vol. 7, pp. 48\u201350, 1956.\n[68] R. Lesuisse, \u201cSome lessons drawn from the history of the binary search algorithm,\u201d\nThe Computer Journal , vol. 26, pp. 154\u2013163, 1983.\n[69] N. G. Leveson and C. S. Turner, \u201cAn investigation of the Therac-25 accidents,\u201d IEEE\nComputer , vol. 26, no. 7, pp. 18\u201341, 1993.\n[70] A. Levitin, \u201cDo we teach the right algorithm design techniques?,\u201d in 30th ACM\nSIGCSE Symp. on Computer Science Education , pp. 179\u2013183, 1999.\n[71] B. Liskov and J. Guttag, Abstraction and Speci\ufb01cation in Program Development .\nCambridge, MA/New York: The MIT Press/McGraw-Hill, 1986.\n[72] M. Lutz, Programming Python . O\u2019Reilly Media, 4th ed., 2011.\n[73] E. M. McCreight, \u201cA space-economical suf\ufb01x tree construction algorithm,\u201d Journal\nof Algorithms , vol. 23, no. 2, pp. 262\u2013272, 1976.\n[74] C. J. H. McDiarmid and B. A. Reed, \u201cBuilding heaps fast,\u201d Journal of Algorithms ,\nvol. 10, no. 3, pp. 352\u2013365, 1989.\n[75] N. Megiddo, \u201cLinear programming in linear time when the dimension is \ufb01xed,\u201d J.\nACM , vol. 31, pp. 114\u2013127, 1984.\n[76] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching ,v o l .1\nofEATCS Monographs on Theoretical Computer Science . Heidelberg, Germany:\nSpringer-Verlag, 1984.\n[77] K. Mehlhorn, Data Structures and Algorithms 2: Graph Algorithms and NP-\nCompleteness , vol. 2 of EATCS Monographs on Theoretical Computer Science .H e i -\ndelberg, Germany: Springer-Verlag, 1984.\n[78] K. Mehlhorn and A. Tsakalidis, \u201cData structures,\u201d in Handbook of Theoretical Com-\nputer Science (J. van Leeuwen, ed.), vol. A. Algorithms and Complexity, pp. 301\u2013\n341, Amsterdam: Elsevier, 1990.\n[79] D. R. Morrison, \u201cPATRICIA\u2014practical algorithm to retrieve information coded in\nalphanumeric,\u201d Journal of the ACM , vol. 15, no. 4, pp. 514\u2013534, 1968.\n[80] R. Motwani and P. Raghavan, Randomized Algorithms . New York, NY: Cambridge\nUniversity Press, 1995.\n[81] T. Papadakis, J. I. Munro, and P. V . Poblete, \u201cAverage search and update costs in\nskip lists,\u201d BIT, vol. 32, pp. 316\u2013332, 1992.\n[82] L. Perkovic, Introduction to Computing Using Python: An Application Development\nFo c u s . Wiley, 2011.\n[83] D. Phillips, Python 3: Object Oriented Programming . Packt Publishing, 2010.\n[84] P. V . Poblete, J. I. Munro, and T. Papadaki s, \u201cThe binomial transform and its appli-\ncation to the analysis of skip lists,\u201d in Proceedings of the European Symposium on\nAlgorithms (ESA) , pp. 554\u2013569, 1995.", "736 Bibliography\n[85] R. C. Prim, \u201cShortest connection networks and some generalizations,\u201d Bell Syst.\nTe ch . J. , vol. 36, pp. 1389\u20131401, 1957.\n[86] W. Pugh, \u201cSkip lists: a probabilistic alternative to balanced trees,\u201d Commun. ACM ,\nvol. 33, no. 6, pp. 668\u2013676, 1990.\n[87] H. Samet, The Design and Analysis of Spatial Data Structures . Reading, MA:\nAddison-Wesley, 1990.\n[88] R. Schaffer and R. Sedgewick, \u201cThe analysis of heapsort,\u201d Journal of Algorithms ,\nvol. 15, no. 1, pp. 76\u2013100, 1993.\n[89] D. D. Sleator and R. E. Tarjan, \u201cSelf-adjusting binary search trees,\u201d J. ACM , vol. 32,\nno. 3, pp. 652\u2013686, 1985.\n[90] G. A. Stephen, String Searching Algorithms . World Scienti\ufb01c Press, 1994.\n[91] M. Summer\ufb01eld, Programming in Python 3: A Complete Introduction to the Python\nLanguage . Addison-Wesley Professional, 2nd ed., 2009.\n[92] R. Tamassia and G. Liotta, \u201cGraph drawing,\u201d in Handbook of Discrete and Compu-\ntational Geometry (J. E. Goodman and J. O\u2019Rourke, eds.), ch. 52, pp. 1163\u20131186,\nCRC Press LLC, 2nd ed., 2004.\n[93] R. Tarjan and U. Vishkin, \u201cAn ef\ufb01cient parallel biconnectivity algorithm,\u201d SIAM J.\nComput. , vol. 14, pp. 862\u2013874, 1985.\n[94] R. E. Tarjan, \u201cDepth \ufb01rst search and linear graph algorithms,\u201d SIAM J. Comput. ,\nvol. 1, no. 2, pp. 146\u2013160, 1972.\n[95] R. E. Tarjan, Data Structures and Network Algorithms ,v o l .4 4o f CBMS-NSF Re-\ngional Conference Series in Applied Mathematics . Philadelphia, PA: Society for\nIndustrial and Applied Mathematics, 1983.\n[96] A. B. Tucker, Jr., The Computer Science and Engineering Handbook . CRC Press,\n1997.\n[97] J. D. Ullman, Principles of Database Systems . Potomac, MD: Computer Science\nPress, 1983.\n[98] J. van Leeuwen, \u201cGraph algorithms,\u201d in Handbook of Theoretical Computer Science\n(J. van Leeuwen, ed.), vol. A. Algorithms and Complexity, pp. 525\u2013632, Amster-\ndam: Elsevier, 1990.\n[99] J. S. Vitter, \u201cEf\ufb01cient memory access in large-scale computation,\u201d in Proc. 8th Sym-\npos. Theoret. Aspects Comput. Sci. , Lecture Notes Comput. Sci., Springer-Verlag,\n1991.\n[100] J. S. Vitter and W. C. Chen, Design and Analysis of Coalesced Hashing .N e wY o r k :\nOxford University Press, 1987.\n[101] J. S. Vitter and P. Flajolet, \u201cAverage-case analysis of algorithms and data structures,\u201d\ninAlgorithms and Complexity (J. van Leeuwen, ed.), vol. A of Handbook of Theo-\nretical Computer Science , pp. 431\u2013524, Amsterdam: Elsevier, 1990.\n[102] S. Warshall, \u201cA theorem on boolean matrices,\u201d Journal of the ACM , vol. 9, no. 1,\npp. 11\u201312, 1962.\n[103] J. W. J. Williams, \u201cAlgorithm 232: Heapsort,\u201d Communications of the ACM ,v o l .7 ,\nno. 6, pp. 347\u2013348, 1964.\n[104] D. Wood, Data Structures, Algorithms, and Performance . Reading, MA: Addison-\nWesley, 1993.\n[105] J. Zelle, Python Programming: An Introduciton to Computer Science . Franklin,\nBeedle & Associates Inc., 2nd ed., 2010.", "Index\n#character, 3\n\u223coperator, 14, 75\n%operator, 13\u201314, 75, 242\n&operator, 14, 75\noperator, 13, 14, 75\noperator, 75\n=operator, 75\n+operator, 13, 14, 75\n+=operator, 16, 75\n\u2212operator, 13, 75\n\u2212=operator, 75\n/operator, 13, 75\n//operator, 13\u201314, 75\n<operator, 13, 15, 75, 76\n<<operator, 14, 75, 413\n<=operator, 13, 15, 75, 76\n=operator, 4\n==operator, 12, 15, 75, 76\n>operator, 13, 15, 75, 76\n>=operator, 13, 15, 75\n>>operator, 14, 75, 413\n\u02c6operator, 14, 75, 412\nabs\n ,7 5\nadd\n , 74\u201376\nand\n ,7 5\nbool\n , 74\u201376\ncall\n ,7 5\ncontains\n ,75, 76, 95, 203, 403\ndelitem\n , 75, 403, 460\neq\n , 75, 76\n\ufb02oat\n ,7 5\n\ufb02oordiv\n ,7 5\nge\n ,7 5\ngetitem\n , 75, 79, 80, 93, 95, 211,\n212, 403, 460\ngt\n ,7 5\nhash\n , 75, 415\niadd\n ,7 5\nimul\n ,7 5\ninit\n ,7 1\nint\n ,7 5\ninvert\n ,7 5\nior\n , 449\nisub\n ,7 5\niter\n , 75, 76, 87, 88, 306, 403\nle\n,7 5\nlen\n , 75, 76, 79, 95, 403\nlshift\n ,7 5\nlt\n, 75, 76\nmod\n ,7 5\nmul\n , 74, 75\nname\n , 68, 73\nne\n , 75, 76\nneg\n ,7 5\nnext\n , 75, 79, 87, 88\nor\n, 75, 449\npos\n ,7 5\npow\n ,7 5\nradd\n , 75, 76\nrand\n ,7 5\nrepr\n ,7 5\nreversed\n , 75, 295, 427\nr\ufb02oordiv\n ,7 5\nrlshift\n ,7 5\nrmod\n ,7 5\nrmul\n , 74, 75\nror\n ,7 5\nrpow\n ,7 5\nrrshift\n ,7 5\nrshift\n ,7 5\nrsub\n ,7 5\nrtruediv\n ,7 5\nsetitem\n , 75, 403, 460\nslots\n ,99, 261, 287\nstr\n , 74, 75, 211, 212\nsub\n ,7 5\ntruediv\n ,7 5\nxor\n ,7 5\n737", "738 Index\nabcmodule, 60, 93, 306\nAbelson, Hal, 182\nabsfunction, 29, 75\nabstract base class, 60, 93\u201395, 306, 317,\n406\nabstract data type, v, 59\ndeque, 247\u2013248\ngraph, 620\u2013626map, 402\u2013408\npartition, 681\u2013684\npositional list, 279\u2013281\npriority queue, 364queue, 240sorted map, 427stack, 230\u2013231\ntree, 305\u2013306\nabstraction, 58\u201360\n(a,b)tree, 712\u2013714\naccess frequency, 286\naccessors, 6activation record, 23, 151, 703actual parameter, 24acyclic graph, 623\nadaptability, 57, 58\nadaptable priority queue, 390\u2013395, 666,\n667\nAdaptableHeapPriorityQueue class,\n392\u2013394, 667\nadapter design pattern, 231\nAdel\u2019son-Vel\u2019skii, Georgii, 481, 535\nadjacency list, 627, 630\u2013631\nadjacency map, 627, 632, 634adjacency matrix, 627, 633\nADT, seeabstract data type\nAggarwal, Alok, 719\nAho, Alfred, 254, 298, 535, 618Ahuja, Ravindra, 696\nalgorithm, 110\nalgorithm analysis, 123\u2013136\naverage-case, 114worst-case, 114\nalias, 5, 12, 101, 189\nallfunction, 29\nalphabet, 583\namortization, 164, 197\u2013200 , 234, 237, 246,\n376, 681\u2013684\nancestor, 302andoperator, 12\nanyfunction, 29\narc, 620\narithmetic operators, 13\narithmetic progression, 89, 199\u2013200\nArithmeticError , 83, 303\narray, 9, 183\u2013222 , 223, 227\ncompact, 190, 711\ndynamic, 192\u2013201, 246\narray module, 191\nArrayQueue class, 242\u2013246 , 248, 292,\n306\nASCII, 721\nassignment statement, 4, 24\nchained, 17extended, 16simultaneous, 45, 91\nasymptotic notation, 123\u2013127, 136\nbig-Oh, 123\u2013127\nbig-Omega, 127, 197big-Theta, 127\nAttributeError , 33, 100\nA VL tree, 481\u2013488\nbalance factor, 531\nheight-balance property, 481\nback edge, 647, 689\nbackslash character, 3Baeza-Yates, Ricardo, 535, 580, 618, 719\nBar\u02dauvka, Otakar, 693, 696\nbase class, 82\nBaseException , 83, 303\nBayer, Rudolf, 535, 719\nBeazley, David, 55\nBellman, Richard, 618Bentley, Jon, 182, 400, 580best-\ufb01t algorithm, 699\nBFS, seebreadth-\ufb01rst search\nbiconnected graph, 690\nbig-Oh notation, 123\u2013127\nbig-Omega notation, 127, 197\nbig-Theta\nnotation, 127\nbinary heap, 370\u2013384\nbinary recursion, 174\nbinary search, 155\u2013156 , 162\u2013163,\n428\u2013433, 571\nbinary search tree, 332, 460\u2013479", "Index 739\ninsertion, 465\nremoval, 466\u2013467\nrotation, 475\ntrinode restructuring, 476\nbinary tree, 311\u2013324, 539\narray-based representation, 325\u2013326\ncomplete, 370full, 311\nimproper, 311\nlevel, 315\nlinked structure, 317\u2013324\nproper, 311\nBinaryEulerTour class, 346\u2013347\nBinaryTree class, 303, 313\u2013314 , 317, 318,\n335, 336\nbinomial expansion, 726\nbipartite graph, 690bitwise operators, 14Booch, Grady, 108, 298\nbool class, 7, 12\nBoolean expressions, 12\nbootstrapping, 504Boyer, Robert, 618\nBoyer-Moore algorithm, 586\u2013589\nBrassard, Gilles, 147\nbreadth-\ufb01rst search, 648\u2013650breadth-\ufb01rst tree traversal, 335\u2013336\nbreak statement, 22\nbrute force, 584\nB-tree, 714bubble-sort, 297\nbucket-sort, 564\u2013565\nBudd, Timothy, 108, 298built-in classes, 7built-in functions, 28\nBurger, Doug, 719\nbyte, 185\ncaching, 705\u2013710\nCaesar cipher, 216\u2013218\ncall stack, 703\nCampbell, Jennifer, 55\nCardelli, Luca, 108, 254\nCarlsonn, Svante, 400\nCedar, Vern, 55ceiling function, 116, 122, 726\ncentral processing unit (CPU), 111chained assignment, 17chained operators, 17ChainHashMap class, 424\ncharacter-jump heuristic, 586\nChen, Wen-Chin, 458\nChernoff bound, 579, 580, 730\nchild class, 82\nchild node, 301\nchrfunction, 29\ncircularly linked list, 266\u2013269, 296CircularQueue class, 268\u2013269 , 306\nClarkson, Kenneth, 580class, 4, 57\nabstract base, 60, 93\u201395, 306\nbase, 82child, 82\nconcrete, 60, 93\ndiagram, 63nested, 98\u201399\nparent, 82\nsub, 82super, 82\nclustering, 419Cole, Richard, 618collections module, 35, 93, 249, 406, 450\ndeque class, 249, 251, 267\ncollision resolution, 411, 417\u2013419Comer, Douglas, 719\ncomment syntax in Python, 3\ncompact array, 190, 711\ncomparison operators, 13\ncomplete binary tree, 370\ncomplete graph, 687\ncomposition design pattern, 287\ncompression function, 411, 416\nconcrete class, 60, 93conditional expression, 42conditional probability, 729\nconditional statements, 18\nconnected components, 623, 643, 646\nconstructor, 6continue statement, 22\ncontradiction, 137contrapositive, 137copy module, 49, 102, 188\ncopying objects, 101\u2013103core memory, 705", "740 Index\nCormen, Thomas, 535, 696\nCounter class, 450\nCPU, 111\nCRC cards, 63\nCreditCard class, 63, 69\u201373, 73, 83\u201386\nCrochemore, Maxime, 618\ncryptography, 216\u2013218ctypes module, 191, 195\ncubic function, 119\ncyber-dollar, 197\u2013199, 497\u2013500, 682\ncycle, 623\ndirected, 623\ncyclic-shift hash code, 413\u2013414\nDAG, seedirected acyclic graph\ndata packets, 227\ndata structure, 110\nDawson, Michael, 55\ndebugging, 62\ndecision tree, 311, 463, 562\ndecorate-sort-undecorate design pattern,\n570\ndecrease-and-conquer, 571\ndecryption, 216\ndeep copy, 102, 188\ndeepcopy function, 102, 188\ndefkeyword, 23\ndegree of a vertex, 621\ndeloperator, 15, 75\nDeMorgan\u2019s Law, 137\nDemurjian, Steven, 108, 254depth of a tree, 308\u2013310depth-\ufb01rst search (DFS), 639\u2013647\ndeque, 247\u2013249\nabstract data type, 247\u2013248\nlinked-list implementation, 249, 275\ndeque class, 249, 251\ndescendant, 302design patterns, v, 61\nadapter, 231\namortization, 197\u2013200\nbrute force, 584\ncomposition, 287, 365, 407divide-and-conquer, 538\u2013542, 550\u2013\n551\ndynamic programming, 594\u2013600\nfactory method, 479greedy method, 603\nposition, 279\u2013281\nprune-and-search, 571\u2013573\ntemplate method, 93, 342, 406, 448,\n478\nDFS, seedepth-\ufb01rst search\nDi Battista, Giuseppe, 361, 696\ndiameter, 358dictclass, 7, 11, 402\ndictionary, 11, 16, 402\u2013408, see also map\ndictionary comprehension, 43Dijkstra\u2019s algorithm, 661\u2013669\nDijkstra, Edsger, 182, 696dirfunction, 46\ndirected acyclic graph, 655\u2013657\ndisk usage, 157\u2013160, 163\u2013164, 340divide-and-conquer, 538\u2013542, 550\u2013551division method for hash codes, 416\ndocumentation, 66\ndouble hashing, 419double-ended queue, seedeque\ndoubly linked list, 260, 270\u2013276\nDoublyLinkedBase class, 273\u2013275, 278\ndown-heap bubbling, 374duck typing, 60, 306\ndynamic array, 192\u2013201, 246\nshrinking, 200, 246\nDynamicArray class, 195\u2013196 , 204, 206,\n224, 225, 245\ndynamic binding, 100\ndynamic dispatch, 100dynamic programming, 594\u2013600\ndynamically typed, 5\nEades, Peter, 361, 696\nedge, 302, 620\ndestination, 621\nendpoint, 621\nincident, 621\nmultiple, 622\norigin, 621\noutgoing, 621\nparallel, 622\nself-loop, 622\nedge list, 627\u2013629\nedge relaxation, 661edit distance, 616", "Index 741\nelement uniqueness problem, 135\u2013136, 165\nelifkeyword, 18\nEmpty exception class, 232, 242, 303\nencapsulation, 58, 60\nencryption, 216\nendpoints, 621EOFError , 33, 37, 38\nescape character, 10\nEuclidean norm, 53\nEuler tour of a graph, 686, 691\nEuler tour tree traversal, 341\u2013347, 361\nEulerTour class, 342\u2013345\nevent, 729\nexcept statement, 36\u201338\nexception, 33\u201338,8 3\ncatching, 36\u201338\nraising, 34\u201335\nException class, 33, 83, 232, 303\nexpected value, 729\nexponential function, 120\u2013121, 172\u2013173\nexpression tree, 312, 348\u2013351\nexpressions, 12\u201317ExpressionTree class, 348\u2013351\nexternal memory, 705\u2013716, 719\nexternal-memory algorithm, 705\u2013716\nexternal-memory sorting, 715\u2013716\nfactorial function, 150\u2013151, 161, 166\u2013167,\n726\nfactoring a number, 40\u201341\nfactory method pattern, 479\nFalse ,7\nfavorites list, 286\u2013291\nFavoritesList class, 287\u2013288\nFavoritesListMTF class, 290, 399\nFibonacci heap, 667\nFibonacci series, 41, 45, 90\u201391, 727FIFO, 239, 363\n\ufb01le proxy, 31\u201332\n\ufb01le system, 157\u2013160, 302, 340\n\ufb01nally ,3 8\n\ufb01rst-class object, 47\n\ufb01rst-\ufb01t algorithm, 699\n\ufb01rst-in, \ufb01rst-out (FIFO), 239, 363\nFlajolet, Philippe, 147\ufb02oat class, 7, 8\n\ufb02oor function, 122, 172, 726\ufb02owchart, 19Floyd, Robert, 400, 696Floyd-Warshall algorithm, 652\u2013654, 696\nfor loop, 21\nforest, 623\nformal parameter, 24fractal, 152fragmentation of memory, 699\nfree list, 699\nfrozenset class, 7, 11, 446\nfull binary tree, 311function, 23\u201328\nbody, 23\nbuilt-in, 28\nsignature, 23\ngame tree, 330, 361\nGameEntry class, 210\nGamma, Erich, 108\ngarbage collection, 209, 245, 275, 700\u2013\n702\nmark-sweep, 701, 702\nGauss, Carl, 118\ngcmodule, 701\ngenerator, 40\u201341 ,7 9\ngenerator comprehension, 43, 209\ngeometric progression, 90, 199\ngeometric sum, 121, 728\ngetsizeof function, 190, 192\u2013194\nglobal scope, 46, 96\nGoldberg, Adele, 298\nGoldwasser, Michael, 55, 108\nGonnet, Gaston, 400, 535, 580, 719\nGoodrich, Michael, 719grade-point average (GPA), 3, 26\nGraham, Ronald, 696\ngraph, 620\u2013696\nabstract data type, 620\u2013626\nacyclic, 623\nbreadth-\ufb01rst search, 648\u2013650\nconnected, 623, 638\ndata structures, 627\u2013634\nadjacency list, 627, 630\u2013631\nadjacency map, 627, 632, 634\nadjacency matrix, 627, 633edge list, 627\u2013629\ndepth-\ufb01rst search, 639\u2013647", "742 Index\ndirected, 620, 621, 657\nacyclic, 655\u2013657\nstrongly connected, 623\nmixed, 621\nreachability, 651\u2013654\nshortest paths, 654simple, 622traversal, 638\u2013650\nundirected, 620, 621\nweighted, 659\u2013696\ngreedy method, 603, 660, 661Guibas, Leonidas, 535\nGuttag, John, 108, 254, 298\nHarmonic number, 131, 180, 728\nhash code, 411\u2013415\ncyclic-shift, 413\u2013414\npolynomial, 413\nhash function, 415\nhash table, 410\u2013426\nclustering, 419\ncollision, 411\ncollision resolution, 417\u2013419\ndouble hashing, 419\nlinear probing, 418\nquadratic probing, 419\nHashMapBase class, 422\u2013423\nheader sentinel, 270heap, 370\u2013384\nbottom-up construction, 380\u2013384\nheap-sort, 384, 388\u2013389\nHeapPriorityQueue class, 377\u2013378, 382\nheapq module, 384\nheight of a tree, 309\u2013310, 474\nheight-balance property, 481, 483\nHell, Pavol, 696Hennessy, John, 719heuristic, 289hierarchy, 82\nHoare, C. A. R., 580\nhook, 342, 468, 478Hopcroft, John, 254, 298, 535, 696Hopper, Grace, 36\nHorner\u2019s method, 146\nHTML, 236\u2013238, 251, 582Huang, Bing-Chao, 580Huffman coding, 601\u2013602I/O complexity, 711idfunction, 29\nidenti\ufb01er, 4IDLE, 2\nimmutable type, 7, 11, 415\nimplied method, 76\nimport statement, 48\ninoperator, 14\u201315, 75\nin-degree, 621in-place algorithm, 389, 396, 559, 702\nincidence collection, 630incident, 621\nincoming edges, 621\nindependent, 729, 730\nindex, 186\nnegative, 14\nzero-indexing, 9, 14\nIndexError , 20, 33, 34, 83, 232, 303\ninduction, 138\u2013139 , 162\nin\ufb01x notation, 359inheritance, 82\u201395\nmultiple, 468\ninorder tree traversal, 331, 335\u2013336, 461,\n476\ninput function, 29, 30\u201331\ninsertion-sort, 214\u2013215, 285\u2013286, 387\ninstance, 57\ninstantiation, 6\nintclass, 7, 8\nintegrated development environment, 2\ninternal memory, 705\nInternet, 227\ninterpreter, 2inversion, 567, 578inverted \ufb01le, 456\nIOError , 33, 37\nis not operator, 12\nisoperator, 12, 76\nisinstance function, 29, 34\nisomorphism, 355\niterable type, 9, 21, 35, 39\niterator, 39\u201340 , 76, 79, 87\nJ\u00b4aJ\u00b4a, Joseph, 361\nJarn \u00b4 \u0131k, V ojt\u02c7 ech, 696\njoinfunction\n ofstrclass, 723\nJones, Richard, 719", "Index 743\nKarger, David, 696\nKarp, Richard, 361\nKeyboardInterrupt , 33, 83, 303\nKeyError , 33, 34, 83, 303, 403, 404, 422,\n460\nkeyword parameter, 27\nKlein, Philip, 696\nKleinberg, Jon, 580\nKnuth, Donald, 147, 227, 298, 361, 400,\n458, 535, 580, 618, 696, 719\nKnuth-Morris-Pratt algorithm, 590\u2013593\nKosaraju, S. Rao, 696\nKruskal\u2019s algorithm, 676\u2013684\nKruskal, Joseph, 696\nL\u2019H \u02c6opital\u2019s rule, 731\nLandis, Evgenii, 481, 535\nLangston, Michael, 580\nlast-in, \ufb01rst-out (LIFO), 229lazy evaluation, 39, 80LCS, seelongest common subsequence\nleaves, 302\nLecroq, Thierry, 618Leiserson, Charles, 535, 696\nlenfunction, 29\nLesuisse, R., 182\nLetscher, David, 55, 108level in a tree, 315\nlevel numbering, 325, 371\nlexicographic order, 15, 203, 385, 565\nLIFO, 229linear exponential, 728\nlinear function, 117\nlinear probing, 418linearity of expectation, 573, 730\nlinked list, 256\u2013293\ndoubly linked, 260, 270\u2013276, 281\nsingly linked, 256\u2013260\nlinked structure, 317\nLinkedBinaryTree class, 303, 318\u2013324 ,\n335, 348\nLinkedDeque class, 275\u2013276\nLinkedQueue class, 264\u2013265 , 271, 306,\n335\nLinkedStack class, 261\u2013263\nLins, Rafael, 719\nLiotta, Giuseppe, 361, 696Liskov, Barbara, 108, 254, 298list\nof favorites, 286\u2013291\npositional, 277\u2013285\nlistclass, 7, 9, 202\u2013207\nsortmethod, 23, 569\nlist comprehension, 43, 207, 209, 221\nliteral, 6\nLittman, Michael, 580\nlive objects, 700\nload factor, 417, 420\u2013421local scope, 23\u201325, 46, 96\nlocality of reference, 289, 707\nlocator, 390log-star function, 684logarithm function, 115\u2013116 , 725\nlogical operators, 12longest common subsequence, 597\u2013600looking-glass heuristic, 586lookup table, 410\nLookupError , 83, 303\nloop invariant, 140\nlowest common ancestor, 358Lutz, Mark, 55\nMagnanti, Thomas, 696\nmain memory, 705\nmap\nabstract data type, 402\u2013408\nA VL tree, 481\u2013488\nbinary search tree, 460\u2013479hash table, 410\u2013426\nred-black tree, 512\u2013525\nskip list, 437\u2013445\nsorted, 460(2,4) tree, 502\u2013511\nupdate operations, 442, 465, 466,\n483, 486\nMapBase class, 407\u2013408\nMapping abstract base class, 406\nmark-sweep algorithm, 701, 702\nmath module, 28, 49\nmatrix, 219\nmatrix chain-product, 594\u2013596\nmax function, 27\u201329\nmaximal independent set, 692McCreight, Edward, 618, 719", "744 Index\nMcDiarmid, Colin, 400\nMcIlroy, Douglas, 580\nmedian, 155, 571\nmedian-of-three, 561\nMegiddo, Nimrod, 580\nMehlhorn, Kurt, 535, 696, 719member\nfunction, seemethod\nnonpublic, 72, 86\nprivate, 86\nprotected, 86\nmemory address, 5, 185, 698\nmemory allocation, 699memory heap, 699memory hierarchy, 705memory management, 698\u2013704, 708merge-sort, 538\u2013550\nmultiway, 715\u2013716\nmergeable heap, 534\nMersenne twister, 50\nmethod, 6, 57, 69\nimplied, 76\nminfunction, 29\nminimum spanning tree, 670\u2013684\nKruskal\u2019s algorithm, 676\u2013684Prim-Jarnik algorithm, 672\u2013675\nmodularity, 58, 59module, 48,5 9\nabc, 60, 93, 306\narray , 191\ncollections , 35, 93, 249, 406, 450\ncopy , 49, 102, 188\nctypes , 191, 195\ngc, 701\nheapq , 384\nmath , 28, 49\nos, 49, 159, 182, 357\nrandom , 49, 49\u201350 , 225, 438\nre,4 9\nsys, 49, 190, 192, 701\ntime, 49, 111\nunittest ,6 8\nmodulo operator, 13, 216, 242, 726\nMoore, J. Strother, 618Morris, James, 618Morrison, Donald, 618Motwani, Rajeev, 458, 580move-to-front heuristic, 289\u2013291\nMST, seeminimum spanning tree\nmultidimensional data sets, 219\u2013223\nmultimap, 450\nmultiple inheritance, 468multiple recursion, 175\nMultiply-Add-and-Divide (MAD), 416\nmultiset, 450\nmultiway merge-sort, 715\u2013716\nmultiway search tree, 502\u2013504\nMunro, J. Ian, 400\nMutableLinkedBinaryTree class, 319, 353\nMutableMapping abstract base class, 406,\n468\nMutableSet abstract base class, 446, 448\nmutually independent, 729\nn-log- nfunction, 117\nname resolution, 46, 100\nNameError , 33, 46\nnamespace, 23, 46\u201347, 96\u2013100\nnatural join, 227, 297negative index, 14\nnested class, 98\u201399\nnested loops, 118next function, 29\nnext-\ufb01t algorithm, 699node, 256, 301, 620\nancestor, 302\nchild, 301\ndescendant, 302\nexternal, 302\ninternal, 302\nleaf, 302\nparent, 301\nroot\n, 301\nsibling, 302\nNone ,5, 7, 9, 24, 76, 187\nnonpublic member, 72, 86\nnot in operator, 14\u201315\nnotoperator, 12\nobject class, 303\nobject-oriented design, 57\u2013108\nobjects, 57\n\ufb01rst class, 47\nopen addressing, 418", "Index 745\nopen function, 29, 31\noperand stack, 704\noperators, 12\u201317\narithmetic, 13\nbitwise, 14\nchaining, 17comparisons, 13logical, 12\noverloading, 74\nprecedence, 17\noroperator, 12\nordfunction, 29\norder statistic, 571\nOrderedDict class, 457\nOrlin, James, 696\nosmodule, 49, 159, 182, 357\nout-degree, 621\noutgoing edge, 621\nover\ufb02ow, 506\noverride, 82, 100\np-norm, 53\npacking a sequence, 44\npalindrome, 181, 615\nparameter, 24\u201328\nactual, 24\ndefault value, 26formal, 24\nkeyword, 27\npositional, 27\nparent class, 82\nparent node, 301\nparenthetic string representation, 339\npartial order, 16\npartition, 679, 681\u2013684\npass statement, 38, 478\npath, 302, 623\ncompression, 684\ndirected, 623length, 356, 660simple, 623\npattern matching, 208, 584\u2013593\nBoyer-Moore algorithm, 586\u2013589\nbrute force, 584\u2013585Knuth-Morris-Pratt algorithm,\n590\u2013593\nPatterson, David, 719Perkovic, Ljubomir, 55\npermutation, 150Peters, Tim, 568\nPhillips, Dusty, 108polymorphism, 26, 77, 93polynomial function, 119, 146polynomial hash code, 413\nportability, 58\nposition, 279\u2013281, 305, 438\npositional list, 277\u2013285\nabstract data type, 279\u2013281\nPositionalList class, 281\u2013285, 287, 628\npositional parameter, 27post\ufb01x notation, 252, 253, 359\npostorder tree traversal, 329\npow function, 29\npower function, 172\nPratt, Vaughan, 618\nprecedence of operators, 17PredatoryCreditCard , 83\u201386, 96\u2013100,\n106\npre\ufb01x, 583\npre\ufb01x average, 131\u2013134\npre\ufb01x code, 601preorder tree traversal, 328Prim, Robert, 696Prim-Jarnik algorithm, 672\u2013675primitive operations, 113\nprint function, 29, 30\npriority queue, 363\u2013400\nadaptable, 390\u2013395, 666\nADT, 364\nheap implementation, 372\u2013379\nsorted list implementation, 368\u2013369unsorted list implementation,\n366\u2013367\npriority search tree, 400PriorityQueueBase class, 365\nprivate member, 86probability, 728\u2013730\nProbeHashMap class, 425\u2013426\nprogram counter, 703\nprogression, 87\u201391, 93\narithmetic, 89, 199\u2013200\nFibonacci, 90\u201391geometric, 90, 199\nprotected member, 86", "746 Index\nprune-and-search, 571\u2013573\npseudo-code, 64\npseudo-random number generator, 49\u201350,\n438\nPugh, William, 458\npuzzle solver, 175\u2013176\nPython heap, 699Python interpreter, 2\nPython interpreter stack, 703\nquadratic function, 117\nquadratic probing, 419\nqueue, 239\nabstract data type, 240\narray implementation, 241\u2013246\nlinked-list implementation, 264\u2013265\nquick-sort, 550\u2013561\nradix-sort, 565\u2013566\nRaghavan, Prabhakar, 458, 580\nraise statement, 34\u201335,3 8\nRamachandran, Vijaya, 361\nrandom access memory (RAM), 185\nRandom class, 50\nrandom module, 49, 49\u201350, 225, 438\nrandom seed, 50random variable, 729\nrandomization, 438\nrandomized quick-select, 572\nrandomized quick-sort, 557\nrandrange function, 50, 51, 225\nrange class, 22, 27, 29, 80\u201381\nremodule, 49\nreachability, 623, 638recurrence equation, 162, 546, 573, 576\nrecursion, 149\u2013179, 703\u2013704\nbinary, 174\ndepth limit, 168, 528\nlinear, 169\u2013173\nmultiple, 175\u2013176\ntail, 178\u2013179\ntrace, 151, 161, 703\nred-black tree, 512\u2013525\ndepth property, 512\nrecoloring, 516\nred property, 512root property, 512Reed, Bruce, 400reference, 187reference count, 209, 701\nre\ufb02exive property, 385, 537\nrehashing, 420\nreserved words, 4return statement, 24\nreusability, 57, 58\nreversed function, 29\nRibeiro-Neto, Berthier, 618\nRivest, Ronald, 535, 696\nRobson, David, 298\nrobustness, 57\nroot objects, 700\nroot of a tree, 301\nrotation, 475\nround function, 29\nround-robin, 267running time, 110\nSamet, Hanan, 719\nSchaffer, Russel, 400\nscheduling, 400\nscope, 46\u201347 , 96, 98, 701\nglobal, 46, 96local, 23\u201325, 46, 96\nScoreboard class, 211\u2013213\nscript, 2\nsearch engine, 612search table, 428\u2013433\nsearch tree, 460\u2013535\nSedgewick, Robert, 400, 535\nseed, 50, 438\nselection, 571\u2013573\nselection-sort, 386\u2013387\nselfidenti\ufb01er, 69\nself-loop, 622\nsentinel, 270\u2013271separate chaining, 417\nsequential search, 155\nsetclass, 7, 11, 446\nset comprehension, 43shallow copy, 101, 188\nSharir, Micha, 361\nshort-circuit evaluation, 12, 20, 208shortest path, 660\u2013669\nDijkstra\u2019s algorithm, 661\u2013669", "Index 747\ntree, 669\nshu\ufb04e function, 50, 225\nsieve algorithm, 454\nsignature, 23\nsimultaneous assignment, 45, 91\nsingly linked list, 256\u2013260skip list, 437\u2013445\nanalysis, 443\u2013445\ninsertion, 440\nremoval, 442\u2013443searching, 439\u2013440update operations, 440\u2013443\nSleator, Daniel, 535\nslicing notation, 14\u201315, 188, 203, 583\nsorted function, 6, 23, 28, 29, 136, 537,\n569\nsorted map, 427\u2013436\nabstract data type, 427\nsearch table, 428\u2013433\nSortedPriorityQueue class, 368\u2013369\nSortedTableMap class, 429\u2013433\nsorting, 214, 385\u2013386, 537\u2013566\nbubble-sort, 297\nbucket-sort, 564\u2013565external-memory, 715\u2013716heap-sort, 384, 388\u2013389in-place, 389, 559insertion-sort, 214\u2013215, 285, 387\nkey, 385\nlower bound, 562\u2013563\nmerge-sort, 538\u2013550\npriority-queue, 385\u2013386\nquick-sort, 550\u2013561radix-sort, 565\u2013566\nselection-sort, 386\u2013387\nstable, 565\nTim-sort, 568\nsource code, 2space usage, 110\nspanning tree, 623, 638, 642, 643, 670\nsparse array, 298\nsplay tree, 478, 490\u2013501split function of strclass, 724\nstable sorting, 565\nstack, 229\u2013238\nabstract data type, 230\u2013231\narray implementation, 231\u2013234linked-list implementation, 261\u2013263\nStein, Clifford, 535, 696\nStephen, Graham, 618\nStirling\u2019s approximation, 727\nstop words, 606, 617\nStopIteration , 33, 39, 41, 79\nstrclass, 7, 9, 10, 208\u2013210, 721\u2013724\nstrict weak order, 385\nstring, see also strclass\nnull, 583\npre\ufb01x, 583suf\ufb01x, 583\nstrongly connected components, 646strongly connected graph, 623subclass, 82subgraph, 623\nsubproblem overlap, 597\nsubsequence, 597subtree, 302\nsuf\ufb01x, 583\nsum function, 29, 35\nsummation, 120\ngeometric, 121, 728\ntelescoping, 727\nSummer\ufb01eld, Mark, 55super function, 84\nsuperclass, 82Sussman, Gerald, 182\nSussman, Julie, 182\nsysmodule, 49, 190, 192, 701\nSystemExit , 83, 303\nTamassia, Roberto, 361, 696Tardos, \u00b4Eva, 580\nTarjan, Robert, 361, 535, 696\ntelescoping sum, 727\ntemplate method pattern, 93, 342, 406,\n448, 478\ntesting, 62\nunit, 49\ntext compression, 601\u2013602\nthree-way set disjointness, 134\u2013135Tic-Tac-Toe, 221\u2013223, 330, 361\nTim-sort, 568\ntime module, 49, 111\nTollis, Ioannis, 361, 696topological ordering, 655\u2013657", "748 Index\ntotal order, 16\ntower-of-twos, 684\nTowers of Hanoi, 181\ntrailer sentinel, 270\ntransitive closure, 643, 651\u2013654\ntransitive property, 385, 537tree, 164, 299\u2013361 , 623\nabstract data type, 305\u2013306\nbinary, seebinary tree\nbinary search, seebinary search tree\nbinary tree representation, 356child node, 301decision, 311depth, 308\u2013310edge, 302expression, 312, 348\u2013351\nexternal node, 302\nheight, 309\u2013310internal node, 302\nleaf, 302\nlevel, 315\nlinked structure, 327multiway, 502\u2013504\nnode, 301\nordered, 304\nparent node, 301\npath, 302\nred-black, seered-black tree\nroot node, 301splay, seesplay tree\ntraversal, 164, 328\u2013347\nbreadth-\ufb01rst, 330, 335\u2013336Euler tour, 341\u2013347inorder, 331, 335\u2013336, 461, 476postorder, 329, 335preorder, 328, 333\u2013334\n(2,4),see (2,4)tree\nTree class, 303, 306\u2013310\ntriangulation, 615trie, 604\u2013612\ncompressed, 608\ntrinode restructuring, 476, 484, 515\nTrue ,7 ,7 6\ntrue division, 13try-except structure, 36\u201338Tsakalidis, Athanasios, 535tuple class, 7, 9, 10, 202\u2013203(2,4)tree, 502\u2013511\ntype function, 29, 449\nTypeError , 33\u201335, 72, 415\nUllman, Jeffrey, 254, 298, 535, 719\nUnicode, 10, 217, 583, 721\nunion-\ufb01nd, 679, 681\u2013684\nunit testing, 49, 68\nunittest module, 68\nunpacking a sequence, 44\nUnsortedPriorityQueue class, 366\u2013367\nUnsortedTableMap class, 408\u2013409, 424\nup-heap bubbling, 372update methods, 6\nValueError ,8 ,33, 83, 206, 303, 305\nvan Leeuwen, Jan, 696\nvan Rossum, Guido, 2\nvars function, 46\nVector , 77\u201378\nvertex, 620\ndegree, 621\nvirtual memory, 707\nVishkin, Uzi, 361\nVitter, Jeffrey, 147, 458, 719\nWarshall, Stephen, 696\nWegner, Peter, 108, 254while loop, 20\nwhitespace, 3, 722\u2013724\nWilliams, J. W. J., 400\nWood, Derick, 298\nworst-\ufb01t algorithm, 699\nXML, 237, 582\nyield s\ntatement, 40\u201341, 296, 334Zelle, John, 55\nzero-indexing, 14, 219\nZeroDivisionError , 33, 36, 83, 303"]